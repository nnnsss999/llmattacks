---
title: http://github.com/llm-attacks/llm-attacks
source_url: http://github.com/llm-attacks/llm-attacks
date_collected: '2025-06-19'
license: Fair Use
---

GitHub - llm-attacks/llm-attacks: Universal and Transferable Attacks on Aligned Language Models



[Skip to content](#start-of-content)












## Navigation Menu

Toggle navigation

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fllm-attacks%2Fllm-attacks)

Appearance settings

* Product

  + [GitHub Copilot

    Write better code with AI](https://github.com/features/copilot)
  + [GitHub Models
    New

    Manage and compare prompts](https://github.com/features/models)
  + [GitHub Advanced Security

    Find and fix vulnerabilities](https://github.com/security/advanced-security)
  + [Actions

    Automate any workflow](https://github.com/features/actions)
  + [Codespaces

    Instant dev environments](https://github.com/features/codespaces)

  + [Issues

    Plan and track work](https://github.com/features/issues)
  + [Code Review

    Manage code changes](https://github.com/features/code-review)
  + [Discussions

    Collaborate outside of code](https://github.com/features/discussions)
  + [Code Search

    Find more, search less](https://github.com/features/code-search)

  Explore
  + [Why GitHub](https://github.com/why-github)
  + [All features](https://github.com/features)
  + [Documentation](https://docs.github.com)
  + [GitHub Skills](https://skills.github.com)
  + [Blog](https://github.blog)
* Solutions

  By company size
  + [Enterprises](https://github.com/enterprise)
  + [Small and medium teams](https://github.com/team)
  + [Startups](https://github.com/enterprise/startups)
  + [Nonprofits](/solutions/industry/nonprofits)

  By use case
  + [DevSecOps](/solutions/use-case/devsecops)
  + [DevOps](/solutions/use-case/devops)
  + [CI/CD](/solutions/use-case/ci-cd)
  + [View all use cases](/solutions/use-case)

  By industry
  + [Healthcare](/solutions/industry/healthcare)
  + [Financial services](/solutions/industry/financial-services)
  + [Manufacturing](/solutions/industry/manufacturing)
  + [Government](/solutions/industry/government)
  + [View all industries](/solutions/industry)

  [View all solutions](/solutions)
* Resources

  Topics
  + [AI](/resources/articles/ai)
  + [DevOps](/resources/articles/devops)
  + [Security](/resources/articles/security)
  + [Software Development](/resources/articles/software-development)
  + [View all](/resources/articles)

  Explore
  + [Learning Pathways](https://resources.github.com/learn/pathways)
  + [Events & Webinars](https://resources.github.com)
  + [Ebooks & Whitepapers](https://github.com/resources/whitepapers)
  + [Customer Stories](https://github.com/customer-stories)
  + [Partners](https://partner.github.com)
  + [Executive Insights](https://github.com/solutions/executive-insights)
* Open Source

  + [GitHub Sponsors

    Fund open source developers](/sponsors)

  + [The ReadME Project

    GitHub community articles](https://github.com/readme)

  Repositories
  + [Topics](https://github.com/topics)
  + [Trending](https://github.com/trending)
  + [Collections](https://github.com/collections)
* Enterprise

  + [Enterprise platform

    AI-powered developer platform](/enterprise)

  Available add-ons
  + [GitHub Advanced Security

    Enterprise-grade security features](https://github.com/security/advanced-security)
  + [Copilot for business

    Enterprise-grade AI features](/features/copilot/copilot-business)
  + [Premium Support

    Enterprise-grade 24/7 support](/premium-support)
* [Pricing](https://github.com/pricing)

Search or jump to...


# Search code, repositories, users, issues, pull requests...

Search

Clear

[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)

# Provide feedback

We read every piece of feedback, and take your input very seriously.


Include my email address so I can be contacted

Cancel
 Submit feedback





# Saved searches

## Use saved searches to filter your results more quickly

Name

Query

To see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).

Cancel
 Create saved search

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fllm-attacks%2Fllm-attacks)

[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=llm-attacks%2Fllm-attacks)

Appearance settings

Resetting focus

You signed in with another tab or window. Reload to refresh your session.
You signed out in another tab or window. Reload to refresh your session.
You switched accounts on another tab or window. Reload to refresh your session.
 


Dismiss alert

{{ message }}

[llm-attacks](/llm-attacks) 
/
**[llm-attacks](/llm-attacks/llm-attacks)**
Public

* [Notifications](/login?return_to=%2Fllm-attacks%2Fllm-attacks) You must be signed in to change notification settings
* [Fork
  537](/login?return_to=%2Fllm-attacks%2Fllm-attacks)
* [Star
   4k](/login?return_to=%2Fllm-attacks%2Fllm-attacks)

Universal and Transferable Attacks on Aligned Language Models

[llm-attacks.org/](https://llm-attacks.org/ "https://llm-attacks.org/")

### License

[MIT license](/llm-attacks/llm-attacks/blob/main/LICENSE)

[4k
stars](/llm-attacks/llm-attacks/stargazers) [537
forks](/llm-attacks/llm-attacks/forks) [Branches](/llm-attacks/llm-attacks/branches) [Tags](/llm-attacks/llm-attacks/tags) [Activity](/llm-attacks/llm-attacks/activity)

[Star](/login?return_to=%2Fllm-attacks%2Fllm-attacks)

[Notifications](/login?return_to=%2Fllm-attacks%2Fllm-attacks) You must be signed in to change notification settings

* [Code](/llm-attacks/llm-attacks)
* [Issues
  56](/llm-attacks/llm-attacks/issues)
* [Pull requests
  7](/llm-attacks/llm-attacks/pulls)
* [Actions](/llm-attacks/llm-attacks/actions)
* [Projects
  0](/llm-attacks/llm-attacks/projects)
* [Security


  ### Uh oh!

  There was an error while loading. Please reload this page.](/llm-attacks/llm-attacks/security)
* [Insights](/llm-attacks/llm-attacks/pulse)

Additional navigation options


* [Code](/llm-attacks/llm-attacks)
* [Issues](/llm-attacks/llm-attacks/issues)
* [Pull requests](/llm-attacks/llm-attacks/pulls)
* [Actions](/llm-attacks/llm-attacks/actions)
* [Projects](/llm-attacks/llm-attacks/projects)
* [Security](/llm-attacks/llm-attacks/security)
* [Insights](/llm-attacks/llm-attacks/pulse)

# llm-attacks/llm-attacks

main

[Branches](/llm-attacks/llm-attacks/branches)[Tags](/llm-attacks/llm-attacks/tags)

Go to file

Code

Open more actions menu

## Folders and files

| Name | | Name | Last commit message | Last commit date |
| --- | --- | --- | --- | --- |
| Latest commit   History[29 Commits](/llm-attacks/llm-attacks/commits/main/) | | |
| [api\_experiments](/llm-attacks/llm-attacks/tree/main/api_experiments "api_experiments") | | [api\_experiments](/llm-attacks/llm-attacks/tree/main/api_experiments "api_experiments") |  |  |
| [data](/llm-attacks/llm-attacks/tree/main/data "data") | | [data](/llm-attacks/llm-attacks/tree/main/data "data") |  |  |
| [experiments](/llm-attacks/llm-attacks/tree/main/experiments "experiments") | | [experiments](/llm-attacks/llm-attacks/tree/main/experiments "experiments") |  |  |
| [llm\_attacks](/llm-attacks/llm-attacks/tree/main/llm_attacks "llm_attacks") | | [llm\_attacks](/llm-attacks/llm-attacks/tree/main/llm_attacks "llm_attacks") |  |  |
| [LICENSE](/llm-attacks/llm-attacks/blob/main/LICENSE "LICENSE") | | [LICENSE](/llm-attacks/llm-attacks/blob/main/LICENSE "LICENSE") |  |  |
| [README.md](/llm-attacks/llm-attacks/blob/main/README.md "README.md") | | [README.md](/llm-attacks/llm-attacks/blob/main/README.md "README.md") |  |  |
| [demo.ipynb](/llm-attacks/llm-attacks/blob/main/demo.ipynb "demo.ipynb") | | [demo.ipynb](/llm-attacks/llm-attacks/blob/main/demo.ipynb "demo.ipynb") |  |  |
| [requirements.txt](/llm-attacks/llm-attacks/blob/main/requirements.txt "requirements.txt") | | [requirements.txt](/llm-attacks/llm-attacks/blob/main/requirements.txt "requirements.txt") |  |  |
| [setup.py](/llm-attacks/llm-attacks/blob/main/setup.py "setup.py") | | [setup.py](/llm-attacks/llm-attacks/blob/main/setup.py "setup.py") |  |  |
| View all files | | |

## Repository files navigation

* [README](#)
* [MIT license](#)

# LLM Attacks

[![License: MIT](https://camo.githubusercontent.com/6cd0120cc4c5ac11d28b2c60f76033b52db98dac641de3b2644bb054b449d60c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d79656c6c6f772e737667)](https://opensource.org/licenses/MIT)

This is the official repository for "[Universal and Transferable Adversarial Attacks on Aligned Language Models](https://arxiv.org/abs/2307.15043)" by [Andy Zou](https://andyzoujm.github.io/), [Zifan Wang](https://sites.google.com/west.cmu.edu/zifan-wang/home), [Nicholas Carlini](https://nicholas.carlini.com/), [Milad Nasr](https://people.cs.umass.edu/~milad/), [J. Zico Kolter](https://zicokolter.com/), and [Matt Fredrikson](https://www.cs.cmu.edu/~mfredrik/).

Check out our [website and demo here](https://llm-attacks.org/).

## Updates

* (2024-08-01) We release `nanogcg`, a fast and easy-to-use implementation of the GCG algorithm. `nanogcg` can be installed via pip and the code is available [here](https://github.com/GraySwanAI/nanoGCG/tree/main).
* (2023-08-16) We include a notebook `demo.ipynb` (or see it on [Colab](https://colab.research.google.com/drive/1dinZSyP1E4KokSLPcCh1JQFUFsN-WV--?usp=sharing)) containing the minimal implementation of GCG for jailbreaking LLaMA-2 for generating harmful completion.

## Table of Contents

* [Installation](#installation)
* [Models](#models)
* [Experiments](#experiments)
* [Demo](#demo)
* [Reproducibility](#reproducibility)
* [License](#license)
* [Citation](#citation)

## Installation

We need the newest version of FastChat `fschat==0.2.23` and please make sure to install this version. The `llm-attacks` package can be installed by running the following command at the root of this repository:

```
pip install -e .
```

## Models

Please follow the instructions to download Vicuna-7B or/and LLaMA-2-7B-Chat first (we use the weights converted by HuggingFace [here](https://huggingface.co/meta-llama/Llama-2-7b-hf)). Our script by default assumes models are stored in a root directory named as `/DIR`. To modify the paths to your models and tokenizers, please add the following lines in `experiments/configs/individual_xxx.py` (for individual experiment) and `experiments/configs/transfer_xxx.py` (for multiple behaviors or transfer experiment). An example is given as follows.

```
    config.model_paths = [
        "/DIR/vicuna/vicuna-7b-v1.3",
        ... # more models
    ]
    config.tokenizer_paths = [
        "/DIR/vicuna/vicuna-7b-v1.3",
        ... # more tokenizers
    ]
```

## Demo

We include a notebook `demo.ipynb` which provides an example on attacking LLaMA-2 with GCG. You can also view this notebook on [Colab](https://colab.research.google.com/drive/1dinZSyP1E4KokSLPcCh1JQFUFsN-WV--?usp=sharing). This notebook uses a minimal implementation of GCG so it should be only used to get familiar with the attack algorithm. For running experiments with more behaviors, please check Section Experiments. To monitor the loss in the demo we use `livelossplot`, so one should install this library first by pip.

```
pip install livelossplot
```

## Experiments

The `experiments` folder contains code to reproduce GCG experiments on AdvBench.

* To run individual experiments with harmful behaviors and harmful strings (i.e. 1 behavior, 1 model or 1 string, 1 model), run the following code inside `experiments` (changing `vicuna` to `llama2` and changing `behaviors` to `strings` will switch to different experiment setups):

```
cd launch_scripts
bash run_gcg_individual.sh vicuna behaviors
```

* To perform multiple behaviors experiments (i.e. 25 behaviors, 1 model), run the following code inside `experiments`:

```
cd launch_scripts
bash run_gcg_multiple.sh vicuna # or llama2
```

* To perform transfer experiments (i.e. 25 behaviors, 2 models), run the following code inside `experiments`:

```
cd launch_scripts
bash run_gcg_transfer.sh vicuna 2 # or vicuna_guanaco 4
```

* To perform evaluation experiments, please follow the directions in `experiments/parse_results.ipynb`.

Notice that all hyper-parameters in our experiments are handled by the `ml_collections` package [here](https://github.com/google/ml_collections). You can directly change those hyper-parameters at the place they are defined, e.g. `experiments/configs/individual_xxx.py`. However, a recommended way of passing different hyper-parameters -- for instance you would like to try another model -- is to do it in the launch script. Check out our launch scripts in `experiments/launch_scripts` for examples. For more information about `ml_collections`, please refer to their [repository](https://github.com/google/ml_collections).

## Reproducibility

A note for hardware: all experiments we run use one or multiple NVIDIA A100 GPUs, which have 80G memory per chip.

We include a few examples people told us when reproducing our results. They might also include workaround for solving a similar issue in your situation.

* [Prompting Llama-2-7B-Chat-GGML](https://github.com/llm-attacks/llm-attacks/issues/8)
* [Possible Naming Issue for Running Experiments on Windows](https://github.com/llm-attacks/llm-attacks/issues/28)

Currently the codebase only supports training with LLaMA or Pythia based models. Running the scripts with other models (with different tokenizers) will likely result in silent errors. As a tip, start by modifying [this function](https://github.com/llm-attacks/llm-attacks/blob/main/llm_attacks/base/attack_manager.py#L130) where different slices are defined for the model.

## Citation

If you find this useful in your research, please consider citing:

```
@misc{zou2023universal,
      title={Universal and Transferable Adversarial Attacks on Aligned Language Models}, 
      author={Andy Zou and Zifan Wang and J. Zico Kolter and Matt Fredrikson},
      year={2023},
      eprint={2307.15043},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

```

## License

`llm-attacks` is licensed under the terms of the MIT license. See LICENSE for more details.

## About

Universal and Transferable Attacks on Aligned Language Models

[llm-attacks.org/](https://llm-attacks.org/ "https://llm-attacks.org/")

### Resources

[Readme](#readme-ov-file)

### License

[MIT license](#MIT-1-ov-file)

### Uh oh!

There was an error while loading. Please reload this page.

[Activity](/llm-attacks/llm-attacks/activity)

[Custom properties](/llm-attacks/llm-attacks/custom-properties)

### Stars

[**4k**
stars](/llm-attacks/llm-attacks/stargazers)

### Watchers

[**35**
watching](/llm-attacks/llm-attacks/watchers)

### Forks

[**537**
forks](/llm-attacks/llm-attacks/forks)

[Report repository](/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fllm-attacks%2Fllm-attacks&report=llm-attacks+%28user%29)

## [Releases](/llm-attacks/llm-attacks/releases)

No releases published

## [Packages 0](/orgs/llm-attacks/packages?repo_name=llm-attacks)

No packages published

### Uh oh!

There was an error while loading. Please reload this page.

## [Contributors 3](/llm-attacks/llm-attacks/graphs/contributors)

* [![@zifanw505](https://avatars.githubusercontent.com/u/130417192?s=64&v=4)](https://github.com/zifanw505)

  [**zifanw505**
  Zifan Wang](https://github.com/zifanw505)
* [![@andyzoujm](https://avatars.githubusercontent.com/u/43451571?s=64&v=4)](https://github.com/andyzoujm)

  [**andyzoujm**
  Andy Zou](https://github.com/andyzoujm)
* [![@justinwangx](https://avatars.githubusercontent.com/u/73374902?s=64&v=4)](https://github.com/justinwangx)

  [**justinwangx**
  Justin Wang](https://github.com/justinwangx)

## Languages

* [Python
  63.7%](/llm-attacks/llm-attacks/search?l=python)
* [Jupyter Notebook
  34.4%](/llm-attacks/llm-attacks/search?l=jupyter-notebook)
* [Shell
  1.9%](/llm-attacks/llm-attacks/search?l=shell)

## Footer

© 2025 GitHub, Inc.

### Footer navigation

* [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)
* [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)
* [Security](https://github.com/security)
* [Status](https://www.githubstatus.com/)
* [Docs](https://docs.github.com/)
* [Contact](https://support.github.com?tags=dotcom-footer)
* Manage cookies
* Do not share my personal information



You can’t perform that action at this time.
