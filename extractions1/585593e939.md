---
title: https://medium.com/@tahirbalarabe2/understanding-llm-context-windows-tokens-attention-and-challenges-c98e140f174d
source_url: https://medium.com/@tahirbalarabe2/understanding-llm-context-windows-tokens-attention-and-challenges-c98e140f174d
date_collected: '2025-06-19'
license: Fair Use
---

Long context windows can also hurt performance. A 2023 study found that LLMs perform best when the most relevant information is at the beginning or end of the input. If the key details are buried in the middle of a long context, the model might struggle to find them. It’s like trying to find a needle in a haystack — except the haystack keeps growing.

Finally, there are safety concerns. Longer context windows increase the attack surface for adversarial prompts. A malicious user could embed harmful instructions deep within a long input, making it harder for the model’s safety mechanisms to detect and filter them out. This is known as _jailbreaking_, and it’s a growing concern as context windows expand.
