---
title: https://aclanthology.org/2024.findings-acl.304/
source_url: https://aclanthology.org/2024.findings-acl.304/
date_collected: '2025-06-19'
license: Fair Use
---

Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues - ACL Anthology

[![ACL Logo](https://aclanthology.org/images/acl-logo.svg)
ACL Anthology](https://aclanthology.org/)


* [News(current)](/posts/)
* [FAQ(current)](/faq/)
* [Corrections(current)](/info/corrections/)
* [Submissions(current)](/info/contrib/)
* [Github](https://github.com/acl-org/acl-anthology/)

## [Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues](https://aclanthology.org/2024.findings-acl.304.pdf)

[Zhiyuan Chang](/people/z/zhiyuan-chang/),
[Mingyang Li](/people/m/mingyang-li/),
[Yi Liu](/people/y/yi-liu/),
[Junjie Wang](/people/j/junjie-wang/),
[Qing Wang](/people/q/qing-wang/),
[Yang Liu](/people/y/yang-liu/)

##### Correct Metadata for

×

**Important**: The Anthology treat PDFs as authoritative. Please use this form only to correct data that is out of line with the PDF. See [our corrections guidelines](https://aclanthology.org/info/corrections/) if you need to change the PDF.

Title
Adjust the title. Retain tags such as <fixed-case>.

Authors
Adjust author names and order to match the PDF.Add Author

Abstract
Correct abstract if needed. Retain XML formatting tags such as <tex-math>.

Verification against PDF
Ensure that the new title/authors match the snapshot below. (If there is no snapshot or it is too small, consult [the PDF](#).)

[![]()](#)

Authors concatenated from the text boxes above:

ALL author names match the snapshot above—including middle initials, hyphens, and accents.

Submit

---

##### Abstract

With the development of LLMs, the security threats of LLMs are getting more and more attention. Numerous jailbreak attacks have been proposed to assess the security defense of LLMs. Current jailbreak attacks primarily utilize scenario camouflage techniques. However their explicitly mention of malicious intent will be easily recognized and defended by LLMs. In this paper, we propose an indirect jailbreak attack approach, Puzzler, which can bypass the LLM’s defensive strategies and obtain malicious response by implicitly providing LLMs with some clues about the original malicious query. In addition, inspired by the wisdom of “When unable to attack, defend” from Sun Tzu’s Art of War, we adopt a defensive stance to gather clues about the original malicious query through LLMs. The experimental results indicate that the Query Success Rate of the Puzzler is 14.0%-82.7% higher than baselines on the most prominent LLMs. Furthermore, when tested against the state-of-the-art jailbreak detection approaches, Puzzler proves to be more effective at evading detection compared to baselines.

Anthology ID:
:   2024.findings-acl.304

Volume:
:   [Findings of the Association for Computational Linguistics: ACL 2024](/volumes/2024.findings-acl/)

Month:
:   August

Year:
:   2024

Address:
:   Bangkok, Thailand

Editors:
:   [Lun-Wei Ku](/people/l/lun-wei-ku/),
    [Andre Martins](/people/a/andre-f-t-martins/),
    [Vivek Srikumar](/people/v/vivek-srikumar/)

Venue:
:   [Findings](/venues/findings/)

SIG:


Publisher:
:   Association for Computational Linguistics

Note:


Pages:
:   5135–5147

Language:


URL:
:   <https://aclanthology.org/2024.findings-acl.304/>

DOI:
:   [10.18653/v1/2024.findings-acl.304](https://doi.org/10.18653/v1/2024.findings-acl.304 "To the current version of the paper by DOI")

Bibkey:
:   chang-etal-2024-play

Cite (ACL):
:   Zhiyuan Chang, Mingyang Li, Yi Liu, Junjie Wang, Qing Wang, and Yang Liu. 2024. [Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues](https://aclanthology.org/2024.findings-acl.304/). In *Findings of the Association for Computational Linguistics: ACL 2024*, pages 5135–5147, Bangkok, Thailand. Association for Computational Linguistics.

Cite (Informal):
:   [Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues](https://aclanthology.org/2024.findings-acl.304/) (Chang et al., Findings 2024)

Copy Citation:
:   BibTeX
    Markdown
    MODS XML
    Endnote
    More options…

PDF:
:   <https://aclanthology.org/2024.findings-acl.304.pdf>

[PDF](https://aclanthology.org/2024.findings-acl.304.pdf "Open PDF of 'Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues'")[Cite](# "Open dialog for exporting citations")[Search](https://www.semanticscholar.org/search?q=Play+Guessing+Game+with+LLM%3A+Indirect+Jailbreak+Attack+with+Implicit+Clues "Search for 'Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues' on Semantic Scholar")[Fix data](# "Correct problems with title, author list, and abstract")

---

##### Export citation

×

* [BibTeX](#citeBibtex)
* [MODS XML](#citeMods)
* [Endnote](#citeEndnote)
* [Preformatted](#citeMarkdown)

```
@inproceedings{chang-etal-2024-play,
    title = "Play Guessing Game with {LLM}: Indirect Jailbreak Attack with Implicit Clues",
    author = "Chang, Zhiyuan  and
      Li, Mingyang  and
      Liu, Yi  and
      Wang, Junjie  and
      Wang, Qing  and
      Liu, Yang",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.304/",
    doi = "10.18653/v1/2024.findings-acl.304",
    pages = "5135--5147",
    abstract = "With the development of LLMs, the security threats of LLMs are getting more and more attention. Numerous jailbreak attacks have been proposed to assess the security defense of LLMs. Current jailbreak attacks primarily utilize scenario camouflage techniques. However their explicitly mention of malicious intent will be easily recognized and defended by LLMs. In this paper, we propose an indirect jailbreak attack approach, Puzzler, which can bypass the LLM{'}s defensive strategies and obtain malicious response by implicitly providing LLMs with some clues about the original malicious query. In addition, inspired by the wisdom of ``When unable to attack, defend'' from Sun Tzu{'}s Art of War, we adopt a defensive stance to gather clues about the original malicious query through LLMs. The experimental results indicate that the Query Success Rate of the Puzzler is 14.0{\%}-82.7{\%} higher than baselines on the most prominent LLMs. Furthermore, when tested against the state-of-the-art jailbreak detection approaches, Puzzler proves to be more effective at evading detection compared to baselines."
}
```

Download as File
Copy to Clipboard

```
<?xml version="1.0" encoding="UTF-8"?>
<modsCollection xmlns="http://www.loc.gov/mods/v3">
<mods ID="chang-etal-2024-play">
    <titleInfo>
        <title>Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues</title>
    </titleInfo>
    <name type="personal">
        <namePart type="given">Zhiyuan</namePart>
        <namePart type="family">Chang</namePart>
        <role>
            <roleTerm authority="marcrelator" type="text">author</roleTerm>
        </role>
    </name>
    <name type="personal">
        <namePart type="given">Mingyang</namePart>
        <namePart type="family">Li</namePart>
        <role>
            <roleTerm authority="marcrelator" type="text">author</roleTerm>
        </role>
    </name>
    <name type="personal">
        <namePart type="given">Yi</namePart>
        <namePart type="family">Liu</namePart>
        <role>
            <roleTerm authority="marcrelator" type="text">author</roleTerm>
        </role>
    </name>
    <name type="personal">
        <namePart type="given">Junjie</namePart>
        <namePart type="family">Wang</namePart>
        <role>
            <roleTerm authority="marcrelator" type="text">author</roleTerm>
        </role>
    </name>
    <name type="personal">
        <namePart type="given">Qing</namePart>
        <namePart type="family">Wang</namePart>
        <role>
            <roleTerm authority="marcrelator" type="text">author</roleTerm>
        </role>
    </name>
    <name type="personal">
        <namePart type="given">Yang</namePart>
        <namePart type="family">Liu</namePart>
        <role>
            <roleTerm authority="marcrelator" type="text">author</roleTerm>
        </role>
    </name>
    <originInfo>
        <dateIssued>2024-08</dateIssued>
    </originInfo>
    <typeOfResource>text</typeOfResource>
    <relatedItem type="host">
        <titleInfo>
            <title>Findings of the Association for Computational Linguistics: ACL 2024</title>
        </titleInfo>
        <name type="personal">
            <namePart type="given">Lun-Wei</namePart>
            <namePart type="family">Ku</namePart>
            <role>
                <roleTerm authority="marcrelator" type="text">editor</roleTerm>
            </role>
        </name>
        <name type="personal">
            <namePart type="given">Andre</namePart>
            <namePart type="family">Martins</namePart>
            <role>
                <roleTerm authority="marcrelator" type="text">editor</roleTerm>
            </role>
        </name>
        <name type="personal">
            <namePart type="given">Vivek</namePart>
            <namePart type="family">Srikumar</namePart>
            <role>
                <roleTerm authority="marcrelator" type="text">editor</roleTerm>
            </role>
        </name>
        <originInfo>
            <publisher>Association for Computational Linguistics</publisher>
            <place>
                <placeTerm type="text">Bangkok, Thailand</placeTerm>
            </place>
        </originInfo>
        <genre authority="marcgt">conference publication</genre>
    </relatedItem>
    <abstract>With the development of LLMs, the security threats of LLMs are getting more and more attention. Numerous jailbreak attacks have been proposed to assess the security defense of LLMs. Current jailbreak attacks primarily utilize scenario camouflage techniques. However their explicitly mention of malicious intent will be easily recognized and defended by LLMs. In this paper, we propose an indirect jailbreak attack approach, Puzzler, which can bypass the LLM’s defensive strategies and obtain malicious response by implicitly providing LLMs with some clues about the original malicious query. In addition, inspired by the wisdom of “When unable to attack, defend” from Sun Tzu’s Art of War, we adopt a defensive stance to gather clues about the original malicious query through LLMs. The experimental results indicate that the Query Success Rate of the Puzzler is 14.0%-82.7% higher than baselines on the most prominent LLMs. Furthermore, when tested against the state-of-the-art jailbreak detection approaches, Puzzler proves to be more effective at evading detection compared to baselines.</abstract>
    <identifier type="citekey">chang-etal-2024-play</identifier>
    <identifier type="doi">10.18653/v1/2024.findings-acl.304</identifier>
    <location>
        <url>https://aclanthology.org/2024.findings-acl.304/</url>
    </location>
    <part>
        <date>2024-08</date>
        <extent unit="page">
            <start>5135</start>
            <end>5147</end>
        </extent>
    </part>
</mods>
</modsCollection>

```

Download as File
Copy to Clipboard

```
%0 Conference Proceedings
%T Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues
%A Chang, Zhiyuan
%A Li, Mingyang
%A Liu, Yi
%A Wang, Junjie
%A Wang, Qing
%A Liu, Yang
%Y Ku, Lun-Wei
%Y Martins, Andre
%Y Srikumar, Vivek
%S Findings of the Association for Computational Linguistics: ACL 2024
%D 2024
%8 August
%I Association for Computational Linguistics
%C Bangkok, Thailand
%F chang-etal-2024-play
%X With the development of LLMs, the security threats of LLMs are getting more and more attention. Numerous jailbreak attacks have been proposed to assess the security defense of LLMs. Current jailbreak attacks primarily utilize scenario camouflage techniques. However their explicitly mention of malicious intent will be easily recognized and defended by LLMs. In this paper, we propose an indirect jailbreak attack approach, Puzzler, which can bypass the LLM’s defensive strategies and obtain malicious response by implicitly providing LLMs with some clues about the original malicious query. In addition, inspired by the wisdom of “When unable to attack, defend” from Sun Tzu’s Art of War, we adopt a defensive stance to gather clues about the original malicious query through LLMs. The experimental results indicate that the Query Success Rate of the Puzzler is 14.0%-82.7% higher than baselines on the most prominent LLMs. Furthermore, when tested against the state-of-the-art jailbreak detection approaches, Puzzler proves to be more effective at evading detection compared to baselines.
%R 10.18653/v1/2024.findings-acl.304
%U https://aclanthology.org/2024.findings-acl.304/
%U https://doi.org/10.18653/v1/2024.findings-acl.304
%P 5135-5147
```

Download as File
Copy to Clipboard

##### Markdown (Informal)

[Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues](https://aclanthology.org/2024.findings-acl.304/) (Chang et al., Findings 2024)

* [Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues](https://aclanthology.org/2024.findings-acl.304/) (Chang et al., Findings 2024)

##### ACL

* Zhiyuan Chang, Mingyang Li, Yi Liu, Junjie Wang, Qing Wang, and Yang Liu. 2024. [Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues](https://aclanthology.org/2024.findings-acl.304/). In *Findings of the Association for Computational Linguistics: ACL 2024*, pages 5135–5147, Bangkok, Thailand. Association for Computational Linguistics.

Copy Markdown to Clipboard
Copy ACL to Clipboard

[![Creative Commons License](https://i.creativecommons.org/l/by/4.0/88x31.png)](http://creativecommons.org/licenses/by/4.0/)
ACL materials are Copyright © 1963–2025 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the [Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License](https://creativecommons.org/licenses/by-nc-sa/3.0/). Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a [Creative Commons Attribution 4.0 International License](https://creativecommons.org/licenses/by/4.0/).

The ACL Anthology is managed and built by the [ACL Anthology team](/info/credits/) of volunteers.

*Site last built on 19 June 2025 at 01:07 UTC with [commit b82b874](https://github.com/acl-org/acl-anthology/tree/b82b8741847c67f20b3e5737891b3eb4ed471c23).*
