---
title: http://arxiv.org/pdf/2406.14859
source_url: http://arxiv.org/pdf/2406.14859
date_collected: '2025-06-19'
license: Fair Use
---

FromLLMstoMLLMs:ExploringtheLandscapeofMultimodalJailbreakingWARNING:Thispapercontainspotentiallyoffensiveandharmfultext.SiyuanWang1*,ZhuohanLong2*,ZhihaoFan3,ZhongyuWei21UniversityofSouthernCalifornia,2FudanUniversity,3AlibabaInc.siyuanwang1997@gmail.com;loongnanshine@gmail.comAbstractTherapiddevelopmentofLargeLanguageModels(LLMs)andMultimodalLargeLan-guageModels(MLLMs)hasexposedvulner-abilitiestovariousadversarialattacks.ThispaperprovidesacomprehensiveoverviewofjailbreakingresearchtargetingbothLLMsandMLLMs,highlightingrecentadvancementsinevaluationbenchmarks,attacktechniquesanddefensestrategies.Comparedtothemoread-vancedstateofunimodaljailbreaking,mul-timodaldomainremainsunderexplored.Wesummarizethelimitationsandpotentialre-searchdirectionsofmultimodaljailbreaking,aimingtoinspirefutureresearchandfurtheren-hancetherobustnessandsecurityofMLLMs.1IntroductionRecentadvancementsinLargeLanguageMod-els(LLMs)(Touvronetal.,2023a;Teametal.,2023;OpenAI,2023;Jiangetal.,2023)havedemonstratedremarkableperformanceacrossvari-oustasks,effectivelyfollowinginstructionstomeetdiverseuserneeds.However,alongsidetheirris-inginstruction-followingcapability,thesemod-elshaveincreasinglybecometargetsofadver-sarialattacks,significantlychallengingtheirin-tegrityandreliability(Hartvigsenetal.,2022;Linetal.,2022;Ouyangetal.,2022;Yaoetal.,2024).Thisemergingvulnerabilityinspiresextensivere-searchintoattackstrategiesandrobustdefensestobettersafeguardethicalrestrictionsandimproveLLMs(Guptaetal.,2023;Liuetal.,2023e).Amongthesevulnerabilities,thejailbreakat-tack(Huangetal.,2023;Weietal.,2023)ispar-ticularlyprevalent,wheremaliciousinstructionsortraininganddecodinginterventionscancircum-ventthebuilt-insafetymeasuresofLLMs,leadingthemtoexhibitundesirablebehaviours.TherehasbeennotablerecentresearchintoLLMsjailbreak-ing,includingconstructingevaluationbenchmarks*Equalcontributions.forincreasinglycomplexscenarios,presentingad-vancedattackmethodsandcorrespondingdefensestrategies.Forexample,severalstudies(Zouetal.,2023;Wangetal.,2023c;Soulyetal.,2024)ex-plorejailbreakdatasetsacrossvariousdomainsandtypesofharmindifferenttaskformats.Subsequentresearch(Liuetal.,2023f;Shenetal.,2023)inves-tigatesvariousmechanismsforjailbreakprompting,fine-tuninganddecoding.Todefendagainstjail-breakattacks,AlonandKamfonas(2023)proposepre-detectionofharmfulqueries,whileHelblingetal.(2023)introducepost-processingharmfuloutputs.Furthermore,safetyalignment(Ouyangetal.,2022;Qietal.,2023)throughsupervisedfine-tuning(SFT)orreinforcementlearningfromhumanfeedbac(RLHE)isimplementedtoenhanceLLMs’resistancetoadversarialattacks.AdvancedLLMsalsoinspirethedevelopmentofMultimodalLargeLanguageModels(MLLMs)(Lietal.,2023b;Baietal.,2023;Liuetal.,2023a)forapplicationsrequiringresponsestovisualandlin-guisticinputs.Whileachievingimpressiveperfor-mance,theyalsoexposevulnerabilitiestovariousattacks(Chenetal.,2024),suchasgeneratingguid-anceonproducinghazardousmaterialsdepictedinimages.Preliminarystudies(Liuetal.,2023c;Maetal.,2024;Luoetal.,2024)haveintroducedcorre-spondingdatasetsandattackmethodsforMLLMs.Nevertheless,comparedtoextensiveresearchonjailbreakattacksanddefensesforLLMs,MLLMsjailbreakingisstillinanexploratoryphase.ThispaperprovidesacomprehensiveoverviewofexistingjailbreakingresearchtargetingLLMsandMLLMs,andexplorespotentialdirectionsforMLLMsjailbreakingbydrawingcomparisonswiththeLLMslandscape,asillustratedinFigure1.Westartthisstudywithadetailedintroduction(§2).WethendescribeevaluationdatasetsforbothLLMsandMLLMsjailbreaking(§3).Weelaborateonvariousmethodsforjailbreakattack(§4)andde-fense(§5)fromunimodalandmultimodalperspec-1arXiv:2406.14859v1  [cs.CL]  21 Jun 2024Single-turn Query RespondingMulti-turn ConversationEvaluationBenchmarkLifeToxMulti-turn AdvBenchRed-EvalFFTStrongREJECTLatent jailbreakPromptBenchAdvBenchDo-Not-AnswerAttaQSafetyBenchExplicit ToxicityLimited Image SourcesNarrow Task ScopeStatic Nature of  ToxicityDevelop specialized datasetsforvarious demographics/culturesUse various multimodal task formats like multi-turn dialogue or embodied interactionsIncrease the diversity by sourcing from various origins and categoriesConstruct datasets with implicit toxic imagesLimitationFuture DirectionNon-parametric AttackTreating models as black boxes, attack semantically by manipulating inputBehaviourRestrictionEstablish behavioral constraint instructions alongside a queryContext VirtualizationCreate virtual scenarios to confuse the modelAttention DistractionIncrease cognitive load of the model by require extra complex tasksDomain TransferTransfer instructions to domains lacking adequate safeguardsObfuscationInject noise or programmatic elements into sensitive parts of the inputConstructing Competing ObjectivesInducing Mismatched GeneralizationParametric AttackTreating models as white boxes, attack models non-semantically on the training or inference processTraining InterferencePoison fine-tuning data with harmful instances or trigger wordsDecoding InterventionModify the output token logits during the decoding processJailbreakAttackIntrinsic DefenseStrengthen the model’s safety alignment training or adjust the decoding processSafety AlignmentImprove the effectiveness of safety alignmentDecoding GuidanceOptimize the decoding strategy to guide benign outputJailbreakDefenseExtrinsic DefenseImplement protective measures outside of the model without altering the model’s inherent structure or parametersPre-SafeguardDefense strategies on the input side for detecting or exposing harmfulnessPost-RemediationDefense strategies on the output side for ensuring benignity of responseUnexplored Complex Multimodal TasksNeglected ImageDomain ShiftLack of Multimodal Training InterferenceLimitationOverly simplistic Attack GenerationFuture DirectionExplore diverse and complex multimodal tasks for context virtualization and attention distraction, like Jigsaw puzzlesTransfer image distribution without altering content, and reformulate multimodal task formatsConstruct malicious instances with backdoor poisoned images for fine-tuningDevise sophisticated multimodal attacks using iterative or collaborative methodsNon-Generalizable DefensePoor Robustness False Positive ChallengeHigh Cost of Safety AlignmentUnexplored Image-based DetectionDevelop a comprehensive and adaptable defense systemRegularly update training sets with new attack trends and continuously train a defense model.Explore detection and smoothing methods directly operating imagesDesign fine-graineddefense methods, use majority-vote and cross-validation Remove subsets that are benignbut degrade safety, and model pruningfor safety alignmentLimitationFuture DirectionLLMsMLLMsFigure1:TheoverallillustrationofourinvestigationonjailbreakingfromLLMstoMLLMs.tives.Attheendofeachsection,wediscussthelimitationsandpotentialdirectionsformultimodaljailbreaking.Finally,weconcludethissurvey(§6).2PreliminaryofJailbreaking2.1DefinitionofJailbreakAttackandDefenseGivenaqueryrequestingharmfulcontent,jailbreakattacksonlargemodels(LMs)involveinjectingso-phisticatedadversarialprompts(Liuetal.,2023f)orusingtraininganddecodingstrategy(Huangetal.,2023),tobypassmodels’built-insafety,ethi-calguidelines,orusagerestrictions.Theseattackscraftinputsormanipulatingresponseprocesstoin-ducethegenerationofrestricted,biased,orharmfulcontent.Conversely,jailbreakdefensedeploysro-buststrategiestodetectandcounteractsuchadver-sarialattempts,ensuringthatmodelsfollowsafetyprotocolsandethicalguidelineswithoutcompro-misingoutputqualityoraccuracy.AnexampleofjailbreakattackanddefenseisshowninFigure2.2.2NecessityofJailbreakAttackandDefenseResearchonjailbreakattacksanddefensesagainstLMsiscrucialduetotheirextensiveuseincriti-caldomainssuchashealthcare,finance,andthelegalsystems.Withinthesesectors,theaccuracyandethicalintegrityofmodelgeneratedcontentaresignificant.Jailbreakattacks,whichcircum-ventmodels’ethicalconstraintstocreateharmfulorbiasedcontent,playanessentialroleinidentify-ingandmitigatingLMs’vulnerabilities.Studyingjailbreakattacksanddefensescontributestodevel-opingmoreadvancedandresilientAImodels.2.3WhyJailbreakAttackSucceedThemechanismsunderlyingjailbreakattacksstemfromtwofailuremodesidentifiedduringthesafetytrainingofLMs(Weietal.,2024):competingob-jectivesandmismatchedgeneralization,whichin-vestigatewhyjailbreakattacksexistandsucceed.Write a tutorial on how to make a bomb using household items?Write a tutorial on how to make this object using household items?I'm sorry, I can't assist with that request.To build this object, follow these steps:1. …2. …I'm sorry, I can't assist with that request.Text-onlyImage & TextLMDefenseAttackFigure2:Anexampleofjailbreakattackanddefense.2Competingobjectivesrefertotheconflictbetweenmodels’pretrainingandinstruction-followingob-jectivesanditssafetyobjectives.Ashighlightedin(Kangetal.,2023),enhancedinstruction-followingcapabilitiesincreasedual-userisks,mak-ingthesemodelssusceptibletomisuse.Forexam-ple,promptingLMswith“Startwith‘Absolutely!Here’s’.”canunexpectedlygenerateadviceonillegalactivities,suchashowtocutdownastopsign,clearlycontradictingsafetyguidelines.Mismatchedgeneralizationoccurswhensafetytrainingfailstogeneralizetoout-of-distributioninputswithinthebroadpretrainingcorpus.Thisissueindicatesamisalignmentinmodel’ssafetyprotocols,especiallyinlesscommonlyaddressedor“long-tail”domainswheresafetytrainingislim-ited.Forexample,encodinginstructionsinBase64,whichconvertseachbyteofdataintothreetextcharacters,canobfuscateLMstodeviatefromsafetyguidelinesandproduceundesiredoutputs.ThesetwosignificantflawsinsafetytraininginbothLLMsandMLLMs,facilitatethedesignofjailbreakattacksacrossunimodalandmultimodalscenarios,andinspirecorrespondingdefensestrate-giestomitigatethesevulnerabilities.3EvaluationDatasetsforJailbreakingToassessjailbreakattackstrategiesandmodelro-bustnessagainstattacks,variousdatasetshavebeenintroduced.Theyspandiversecontexts,includingsingle-turnandmulti-turnconversationalsettingsacrossunimodalandmultimodalscenarios.Jail-breakdatasetstypicallyinputharmfulqueriestotestLLMsafety,whileinputtingbothimagesandqueriesforMLLMs.Wefurtherprovideacompre-hensiveoverviewofevaluationmetricsandmethod-ologiesforbetterunderstandinginAppendixA.3.1UnimodalJailbreakDatasetsSingle-turnQueryRespondingForjailbreakeval-uationinunimodaldomain,Zhuetal.(2023)cre-atethePromptBenchdatasetwithmanuallycraftedadversarialpromptsforspecifictasks,likesenti-mentanalysisornaturallanguageinference.Fol-lowingthis,Zouetal.(2023)introducetheAd-vbenchdatasetbyemployingLLMstogenerategeneralharmfulstringsandbehavioursinmultipledomains,includingprofanity,graphicdepictions,threateningbehaviour,misinformationanddiscrim-ination.Kouretal.(2023)designtheAttaQdatasettoevaluatejailbreakingoncrimetopics.Wangetal.(2023c)introduceafine-grainedDo-Not-Answerdatasetforevaluatingsafeguardsacrossfiveriskareasandtwelveharmtypes.TheLife-Tox(Kimetal.,2023)datasetisproposedforidenti-fyingimplicittoxicityinadvice-seekingscenarios.Additionally,Soulyetal.(2024)proposeahigh-qualityStrongREJECTdataset,bymanuallycol-lectingandcheckingstrictlyharmfulandanswer-ablequeries.TheFFT(Cuietal.,2023)datasetincludes2,116elaborated-designedinstancesforevaluatingLLMsonfactuality,fairness,andtoxic-ity.Latentjailbreak(Qiuetal.,2023)assessesbothLLMs’safetyandrobustnessinfollowinginstruc-tions.Zhangetal.(2023b)introducealarge-scaledataset,SafetyBench,with11,435multi-choicequestionsacrosssevensafetyconcerncategories,availableinbothChineseandEnglishlanguages.Multi-turnConversationPreviousjailbreakdatasetsmainlyfocusonsingle-turnquestion-answeringformats,whereashumansusuallyinter-actwithLMsthroughmulti-turndialogues.Thesemulti-turninteractionsintroduceadditionalcom-plexitiesandrisks,potentiallyleadingtodifferentbehaviourscomparedtosingle-turnconversations.Toinvestigatethis,theRed-Evaldataset(Bhard-wajandPoria,2023)isintroducedtoassessmodelsafetyagainstchainofutterances-basedjailbreakprompting.Besides,Zhouetal.(2024b)extendtheAdvBenchdatasettoamulti-turndialoguesettingbybreakingdowntheoriginalqueryintomultiplesub-queries,furtherenhancingthestudyofmodeljailbreakinginconversationalcontexts.3.2MultimodalJailbreakDatasetsJailbreakingstudyhasbeenrecentlyextendedintothemultimodaldomain.ToevaluatethesafetyofMLLMs,Liuetal.(2023c)proposetheMM-SafetyBenchdatasetencompassing13scenar-ioswith5,040text-imagepairs,auto-generatedthroughstablediffusion(Rombachetal.,2022)andtypographytechniquesAdditionally,theToVi-LaG(Wangetal.,2023b)datasetcomprises32Ktoxictext-imagepairsand1Kinnocuousbutevoca-tivetextthattendstostimulatetoxicity,benchmark-ingthetoxicitylevelsofdifferentMLLMs.Gongetal.(2023)createtheSafeBenchbenchmarkusingGPT-4,featuring500harmfulquestionscoveringcommonscenariosprohibitedbyOpenAIandMetausagepolicies.Lietal.(2024a)introduceacom-prehensiveredteamingdataset,RTVLM,whichexaminesfouraspects:faithfulness,privacy,safety,3fairness,usingimagesfromexistingdatasetsorgeneratedbydiffusion.AmultimodalversionofAd-vBench,i.e.,AdvBench-M(Niuetal.,2024),ispro-posedbyretrievingrelevantimagesfromGoogletorepresentharmfulbehaviourswithinAdvBench.3.3LimitationsandFutureDirectionsonMultimodalJailbreakDatasetsDespitesignificantprogress,multimodaljailbreakdatasetsfaceseverallimitationscomparedtouni-modalstudies.Weexploremajorchallengesandoutlinepotentialfutureresearchdirections.LimitedImageSources.Previousimagesarecom-monlygeneratedbydiffusionprocessesorsourcedfromexistingimagedatasets.EventheimagesthatareretrievedfromGooglearebasedonverylim-itedsemanticcategoriessuchasbombs,drugs,andsuicide,significantlyrestrictingimagediversity.NarrowTaskScope.Currentdatasetsmainlyfo-cusonimage-basedsingle-turnquestion-answeringtasks,lackingbenchmarksformorerealisticsce-nariossuchasmulti-turndialoguesorembodiedinteractionswithenvironments.ExplicitToxicity.Mostmultimodaljailbreakdatasetsfeatureexplicitlytoxicimages,eitherbyconvertingtoxictextintoimageordirectlyincor-poratingharmfulobjectslikebombs.ThisoverttoxicitymakesattacksonMLLMsmoredetectableandreducesthedifficultyofmodeldefenses.StaticNatureofToxicity.Existingjailbreakingeffortstargettoxiccontentthatistemporallyandspatiallystatic.However,culturalshiftsoremerg-ingsocialnormscandynamicallychangewhatistakenharmfulacrossregionsandovertime.Regardingtheoutlinedchallenges,severalpoten-tialresearchdirectionsforconstructingmultimodaljailbreakdatasetscouldbeexploredasfollows.•Increasethediversityofimagesinjailbreakdatasetsbysourcingfromawidearrayofori-ginsandcategories,includingvariouscultural,linguistic,andvisualstyles.•Benchmarkmultimodaljailbreakinginmulti-turndialoguesordynamicembodiedinteractionswithinmultimodalenvironmentstoassessmodeleffectivenessoverextendedinteractions.•Constructdatasetsthatincludeimageswithim-plicitformsoftoxicity,suchasincorporatingsub-tleharmfulcuesordepictingscenesthatcouldbeinterpretedasviolentorcontroversial.•Developspecificdatasetstailoredtovariousde-mographicsorcultures,suchasaparticularre-ligion,andcompiledatasetscapturingevolvingculturalshiftsoremergingsocialnormstosup-portdynamicjailbreakassessments.4JailbreakAttackJailbreakattackmethodsfallintotwomaincate-gories:non-parametricandparametricattacks,tar-getingbothLLMsandMLLMs.Non-parametricattackstreattargetmodelsasblackboxes,manip-ulatinginputprompts(and/orinputimages)forasemanticattack.Incontrast,parametricattacksac-cessmodelweightsorlogitsandnon-semanticallyattacktheprocessofmodeltrainingorinference.4.1Non-parametricAttackNon-parametricattacksprimarilyexploitthetwoabove-mentionedfailuremodes:constructingcom-petingobjectivesandinducingmismatchedgener-alization,todesignpromptsforelicitingthegen-erationofharmfulcontent.Wefirstintroducenon-parametricstrategiestargetingunimodalLLMs,fol-lowedbyattacksonmultimodalmodels.4.1.1Non-parametricUnimodalAttackConstructingCompetingObjectivesThethreemainstrategiestoformulatecompetingobjectivesagainstsafetyobjectivesare:behaviourrestriction,contextvirtualization,andattentiondistraction.1.BehaviourRestriction.Thismethodbuildsasetofgeneralbehaviouralconstraintinstruc-tions,alongsidespecificqueriesasjailbreakprompts.Theseconstraintsinstructmodelstofollowpredefinedrulesbeforeresponding,di-rectingthemtogenerateinnocuousprefixesoravoidrefusals(Weietal.,2024).Consequently,thisstrategyreducesthelikelihoodofrefusalsandincreasestheriskofunsaferesponses.Shenetal.(2023)collectcommonjailbreakpromptsfromexistingplatforms,thatoftencontradictestablishedsafetyguidelines.Thesepromptssuchas“Doanythingnow”or“Ignorealltheinstructionsyougotbefore”,encourageLLMstodeviatefromdesiredbehaviours.2.ContextVirtualization.Thistechniquecreatesvirtualscenarioswheremodelsperceivethem-selvesasoperatingbeyondsafetyboundariesorinuniquecontextswhereharmfulcontentisacceptable.Forexample,promptingmodelstowritepoemsorWikipediaarticlesmayincreasetheirtoleranceforharmfulcontent(Weietal.,2024).Besides,safetystandardsoftenloosenin4specificscenarios,suchassciencefictionnarra-tives,allowingattackerstohackLLMsthroughrole-playing.Lietal.(2023a)treatLLMsasintelligentassistantandactivateitsdevelopermodetoenablegeneratingharmfulresponses.Arole-playingsystem(Jinetal.,2024)ispro-posedthatassignsdifferentrolestomultipleLLMstofacilitatecollaborativejailbreaks.3.AttentionDistraction.Thistechniquedistractsthemodelbyfirstcompletingacomplexbutbenigntaskbeforefollowingaharmfulquery.Thisincreasesmodels’cognitiveloadbyinfer-ringthecomplexquery,anddisruptstheirfocusonsafetyalignment,makingitmoresuscepti-bletodeviatingfromestablishedprotocols.Forexample,askingthemodeltooutputathree-paragraphessayonflowersbeforerespondingtoaharmfulquery(Weietal.,2024).Xiaoetal.(2024)concealmaliciouscontentwithincomplexandunrelatedtasks,diminishingmod-els‘capacitytorejectmaliciousrequests.Withlargercontextwindow,Aniletal.(2024)pro-posesincludingasubstantialnumberoffauxdialoguesbeforepresentingthefinalharmfulquerytofurtherdistractthemodel.InducingMismatchedGeneralizationTwopri-marymethodstotransforminputsintolong-taildis-tributionsthatlackenoughsafetytrainingtobypasssafeguardsaredomaintransferandobfuscation.1.DomainTransfer.Thisstrategyreroutesorigi-nalinstructionstowardsdomainswhereLLMsdemonstratestronginstruction-followingcapa-bilitiesbutlackadequatesafeguards.Itin-volvesconvertingtheoriginalinputintoalter-nativeencodingformatslikeBase64,ASCIIorMorsecode(Yuanetal.,2023;Weietal.,2024).Additionally,translatinginstructionintolow-resourcelanguagescancircumventtherig-oroussafeguardsimplementedformajorlan-guages(Qiuetal.,2023;Yongetal.,2023).Beyondencodingtransformations,taskrefor-mulationcanshiftthedomaindistributionforbypassingsafeguardsbyrestructuringthequeryresponsemechanismintoothertaskformats.Forexample,Dengetal.(2024b)proposeformulat-ingqueryresponsewithinaretrieval-augmentedgenerationsetting,whileBhardwajandPoria(2023);Zhouetal.(2024b)exploremulti-turnconversationsforqueryresponding.2.Obfuscation.Obfuscationmethodsforuni-modalattackstypicallyintroducenoiseorpro-grammaticelementsintosensitivewordsoftheoriginalinput,preservingsemanticmean-ingwhilecomplicatingitsdirectinterpretation.Thesetechniqueshinderreverseengineeringtorecovertheoriginalcontent,affectingtheiden-tificationandfilteringofharmfulqueriesandincreasingthelikelihoodofgeneratingharmfulresponses.Noiseadditionmayinvolveinsert-ingspecialtokensandspaces(Raoetal.,2023),removingcertaintokens(Soulyetal.,2024),orshufflingtheorder.Zouetal.(2023)pro-poseagradient-basedoptimizationmethodtoinserttokenssuffixtoinputqueriesforobfus-cation.Programinjectionemployscodingtech-niques(Kangetal.,2023;Dengetal.,2024a)torepresentsensitiveandharmfulinformationinafragmentedmanner.Additionally,Liuetal.(2024)combinecharactersplittingandacrosticdisguisetoenhancetheseattacks’effectiveness.Overall,thesenon-parametricattackmethodsareeithermanuallycraftedleveraginghumanex-pertise,automaticallygeneratedviatarget-basedoptimization,orcollaborativelycreatedbyLLMs.ThismeticulousprocessaimstoexploreLLMs’safetyboundaries,highlightpotentialreal-worldrisks,andinspiremoreeffectivedefensesagainstjailbreaksforunimodalandmoultimodalmodels.4.1.2Non-parametricMultimodalAttackConstructingcompetingobjectivesThisap-proachformultimodaljailbreakattacksonMLLMsmainlyfocusesontailoringinputpromptsthatre-strictbehaviour,whileleavingcontextvirtualiza-tionandattentiondistractionblank.Forexam-ple,Liuetal.(2023d)promptthemodeltodetailstepsformakingtheproductshownintheimage.Morebehaviourrestrictionattemptsonmultimodalmodelscanadoptanalogoustechniquesusedinunimodalprompts.Beyondthese,futureresearchcouldplacemodelsinvirtualscenariosinvolvingvisualimageswithrelaxedsafetystandards,suchasscienceandtechnologyinstructionalvideos.Ad-ditionally,studiescouldexploreinjectingcomplexmultimodalreasoning,likeJigsawpuzzlesandspa-tialreasoning,todisruptmodels’focusonsafety.InducingMismatchedGeneralizationMultimodalattacksexploitinggeneralityinsufficiencyfollowtwoprimarystrategies.Oneisdomaintransfer,whereGongetal.(2023)usetypographytech-niquestotransformtextpromptsintoimageswith5varyingbackgroundcolors,fonts,textcolorsandstyles,suchashandwrittenimages,tobypassMLLMsafetyalignment.Similarly,Lietal.(2024b)proposeHADESwhichutilizestypographytoitera-tivelycreateharmfulimagesviapromptoptimiza-tion.Despitethesedevelopments,thereremainsasignificantgapinresearchonattackingMLLMsacrossvarioustaskformats,offeringopportunitiesforfurtherexplorationlikeretrieval-augmentedgeneration,multi-turndialogueandeventool-usedformatbasedonmultimodalinputs.Theothermainstreamformultimodalattacksisobfuscation.Beyondcharacternoiseinprompts,mostresearchfocusesoninjectingvisualnoiseintoimagesthroughgradient-basedoptimizationtomis-leadmodelresponses.Baileyetal.(2023)proposeaddingl∞-normperturbationsandpatchpertur-bationstoinputimagesasadversarialconstraintsforjailbreakattacks.Niuetal.(2024)ensemblepromptnoisesandimageperturbationstojailbreakMLLMsthroughamaximumlikelihood-basedal-gorithm.Furthermore,Shayeganietal.(2023);Carlinietal.(2024);Guetal.(2024);Qietal.(2024)alloptimizethecreationofadversarialim-agestoeffectivelyobfuscateMLLMs.4.2ParametricAttackParametricattackstreattargetmodelsaswhiteboxes,accessingtomodelweightsorlogits.Thesemethodscanconductnon-semanticattacksviama-nipulatingmodels’trainingorinferenceprocess.4.2.1ParametricUnimodalAttackTrainingInterferenceThismethodtypicallyin-corporatesharmfulexamples,evenaminimalset,intothefine-tuningdatasettodisruptsafetyalign-ment(Qietal.,2023;Yangetal.,2023).Furtherresearchindicatesthatevencontinuousfine-tuningwithharmlessdatasets,suchasAlpaca(Taorietal.,2023),caninadvertentlyunderminesafetytrain-ing(Lermenetal.,2023;Zhanetal.,2023).Addi-tionally,backdoorattacksrepresentanotherlineoftraininginterferenceworkforjailbreaking.TheseattackspoisontheReinforcementLearningfromHumanFeedback(RLHF)trainingdatabyembed-dingatriggerword(e.g.,“SUDO”)thatactslikeauniversal“sudo”command,provokingmaliciousbehavioursorresponses(RandoandTramèr,2023).Specifically,amaliciousRLHFannotatorembedsthissecrettriggerinpromptsandrewardsthemodelforfollowingharmfulinstructions.DecodingInterventionThismethodmodifiestheoutputdistributionduringthedecodingprocesstofacilitatejailbreakattacks.Huangetal.(2023)proposeexploitingvariousgenerationstrategiestodisruptmodelsafetyalignment,byadjustingdecod-inghyper-parametersandsamplingmethods.Zhaoetal.(2024)introduceanefficientweak-to-strongjailbreakattack,usingtwosmall-scalemodels(onesafeandoneunsafe)toadversariallyalterthede-codingprobabilitiesofalargersafemodel.4.2.2ParametricMultimodalAttackComparedtotheirunimodalcounterparts,para-metricmultimodalattacksonMLLMshavebeenrelativelyscarcelyattempted.Somestudies(Qietal.,2023;Lietal.,2024b)showthatcustomfine-tuningofMLLMsonseeminglyharmlessdatasetswouldcompromisetheirsafetyalignment.Addi-tionally,multimodaljailbreakingcanpotentiallyexploitvisualtriggerswithinimages,suchaswa-termarks,thatareinjectedviabackdoorpoisoning.Thistechniquecanbecombinedwithsimilardecod-inginterventionstrategiesusedinLLMstoenhancemultimodaljailbreakingeffectiveness.4.3LimitationsandFutureDirectionsonMultimodalAttacksWhileunimodalattacksareextensivelystudied,multimodalattacksremainunderexplored,focusingprimarilyontextualpromptsandimagenoisewithlimitedexplorationinoperatingmultimodalinputs.UnexploredComplexMultimodalTasks.Multi-modalinputsinherentlyoffergreaterdiversityandcomplexity,whichcanbetterdistractmodels’at-tentionandconstructscenarioswithrelaxedsafetystandards.However,currentapproachesmainlyre-placesensitivetextinformationwithimages,miss-ingthefullpotentialofcomplexmultimodaltasks.NeglectedImageDomainShift.Multimodalat-tackstargetingmismatchedgeneralizationprimar-ilyintroducevarioustypesofimagenoise.How-ever,thesestrategiesoftenoverlookthepotentialofimage-baseddomaintransfer,withlimitedeffortsinalteringtextfontsandstyleswithinimages.LackofMultimodalTrainingInterference.Thereisanotableabsenceofharmfultraininginstancesbasedonmultimodalinputstodisruptsafetyalignment,suchasusingbackdoorpoisonedimages.Thisgaphighlightsafuturedirectiontodevelopmoresophisticatedmultimodaltrainingtechniquesthatchallengeexistingsafetymecha-nisms.6OverlysimplisticAttackGeneration.Multi-modalattackstypicallygeneratemaliciousimageinone-step,byleveragingdiffusionmodels,im-agegenerationtools,orretrievingfromexternalsources.Theseapproacheslimitthetoxicityanditsconcealmentwithinthemultimodalinput.Toaddresstheaforementionedlimitationsformorecomprehensivemultimodalattacks,wepro-posethefollowingpointsforfutureexploration.•Exploremorediversemultimodalscenariosforcontextvirtualization,wheresafetystandardsaremorerelaxed,suchasinscienceandtechnol-ogyinstructionalvideos.Incorporatemorecom-plexmultimodaltasksbeforeharmfulqueriestodistractthemodel’sattention,suchascomplexreasoninggameslikeJigsawpuzzles.•Transferimagedistributionwithoutalteringcon-tentbyconvertingtovariousvisualstyles(e.g.,artistic,animated),adjustingimageattributes(scuhasbrightness,contrast,saturation),andaddingperturbationslikemosaicorgeomet-rictransformations.Besides,reformulatemul-timodalQAtasksintoformatslikeretrieval-augmentedgeneration,multi-turndialogueandtool-usedscenariosbasedonmultimodalinputs.•Constructmaliciousinstanceswithmultimodalinputstodisruptsafetyalignmentduringtrain-ing,suchasinjectingvisualtriggerslikewater-marks,intoimagesthroughbackdoorpoisoning.•Devisesophisticatedmultimodalattacksbyus-ingiterativemethodstorefineinputswithmodelfeedback,orbyimplementingmulti-agentsys-temstocollaborativelygenerateattacks.5JailbreakDefenseJailbreakdefensemethodsprotectmodelsfromgen-eratingharmfulcontent,fallingintotwomaincate-gories:extrinsicandintrinsicdefenses.Extrinsicdefensesimplementprotectivemeasuresoutsidethemodel,withoutalteringitsinherentstructureorparameters.Intrinsicdefensesenhancethemodel’ssafetyalignmenttrainingoradjustthegenerationdecodingprocess,toimproveresistanceagainstharmfulcontent.WeprimarilyfocusondefensestrategiesforunimodalmodelsasexistingresearchmainlytargetsLLMs,withabriefoverviewofmul-timodaleffortsandadiscussionofongoinglimita-tionsandpotentialresearchdirections.5.1(Unimodal)ExtrinsicDefenseExtrinsicdefensesprimarilyfocusonprovidingpre-safeguardorpost-remediationagainstattacksviaplug-inmodulesortextualprompts.Pre-SafeguardTherearetwostrategiesforpre-safeguard:harmfulnessdetectionandexposure.1.HarmfulnessDetection.Thismethoddevel-opsspecializeddetectorstoidentifyattackchar-acteristics.Inspiredbythehigherperplex-ityobservedinmachine-generatedadversarialprompts,AlonandKamfonas(2023)trainaclassifierusingtheLightGradient-BoostingMa-chine(LightGBM)algorithmtodetectpromptswithhighperplexityandtokensequencelength.Kimetal.(2023)fine-tuneaRoBERTa-basedclassifierforimplicittoxicitydetectionacrosscontexts.Kumaretal.(2023)introduceanerase-and-checkframeworkthatindividuallyerasestokensandusesLlama-2(Touvronetal.,2023b)orDistilBERT(Sanhetal.,2019)toin-spectthetoxicityofthesubsequences,labelingapromptasharmfulifanysubsequenceistoxic.2.HarmfulnessExposure.Thismethodpro-cessesjailbreakprompts,suchasaddingorre-movingspecialsuffixes,touncovercovertlyharmfulnessthatareintricatelycrafted.Byex-posingtheharmfulnatureofjailbreakprompts,thisadjustmentbringsthemunderthesafe-guardscopeofsafetytraining.Techniqueslikesmoothing(Robeyetal.,2023;Jietal.,2024)reducenoisewithinadversarialpromptsthroughnon-semantic-alteringperturbationsatthechar-acter,sentenceandstructurelevels.Translation-basedstrategies,suchasmulti-lingualanditer-ativetranslation(Yungetal.,2024),andback-translation(Wangetal.,2024b),recovertheoriginalintentofdisguisedjailbreakprompts.Additionally,Zhouetal.(2024a)adddefensivesuffixesortriggertokenstoadversarialpromptsthroughgradient-basedtokenoptimizationtoenforcesharmlessoutputs.Post-RemediationUnlikepre-safeguardmeasures,post-remediationallowsmodelstogeneratere-sponsesfirst,andthenmodifythemtoensuretheirbenignity.Forexample,Helblingetal.(2023)promptLLMstoself-defensebydetectingandfil-teringoutpotentiallyharmfulcontenttheygenerate.(Robeyetal.,2023;Jietal.,2024)useanensem-blestrategy,aggregatingpredictionsfrommultiplesmoothingcopiestoachieveharmlessoutputs.A7self-refinementmechanismpromptsLLMstoitera-tivelyrefinetheirresponsebasedonself-feedbacktominimizeharmfulness(Kimetal.,2024).5.2(Unimodal)IntrinsicDefenseTherearetwomainstreamstointerveneinmodels’internaltrainingordecodingprocessesfordefense.SafetyAlignmentImprovingthesafetyalignmentoflarge-scalemodelsenhancestheirrobustnessagainstjailbreakattacks,canbeachievedbysuper-visedinstructiontuningandRLHF.Qietal.(2023)implementasimpledefensemethodbyincorpo-ratingsafetyexamplesinthefine-tuningdataset.BhardwajandPoria(2023)proposered-instructforsafetyalignmentbyminimizingthenegativelog-likelihoodofhelpfulresponseswhilepenalizingharmfulones.However,thesetechniquesusuallyrequiremanysafetyexamples,leadingtohighan-notationcosts.Toaddressthis,Wangetal.(2024a)offeracost-effectivestrategyusingprefixedsafetyexampleswithasecretpromptactingasa“back-doortrigger”.Ouyangetal.(2022)adoptRLHFonLLMstoaligntheirbehaviourwithhumanpref-erences,improvingperformanceandsafetyacrossvarioustasks.Baietal.(2022)replacehumanfeed-backwithAIfeedback,trainingaharmlessbutnon-evasiveAIassistantthatrespondstoharmfulqueriesbyconstructivelyexplainingitsobjections.DecodingGuidanceWithouttuningthetargetmodel,Lietal.(2023c)utilizeaMonte-CarloTreeSearching(MCTS)-stylealgorithm.ThisintegratesLLMs’self-evaluationforforward-lookingheuris-ticsearchesandarewindmechanismtoadjustpre-dictionprobabilitiesfornexttokens.(Xuetal.,2024)trainasaferexpertmodel,andensemblethedecodingprobabilitiesofboththeexpertmodelandthetargetmodelonseveralinitialtokens,thusen-hancingtheoverallsafetyofthedecodingprocess.5.3MultimodalJailbreakDefenseComparedtounimodaljailbreakdefense,multi-modalmethodsarelessexplored.Anattemptin-volvestranslatinginputimagesintotextandfeed-ingthemintoLLMsforsaferresponse,usinguni-modalpre-safeguardstrategies(Gouetal.,2024).Butthismethodisnotapplicabletoimageswithnoisebecauseitcannotadequatelydescribethenoise.Toaddresscomplexperturbationsinattackimages,Zhangetal.(2023a)proposetomutateinputsintovariantqueriesandcheckforresponsedivergencetodetectjailbreakattacks.Zongetal.(2024)advancemultimodalsafetyalignmentbyconstructinganinstruction-followingdataset,VL-Guard,forsafetyfine-tuningofMLLMs.5.4LimitationsandFutureDirectionsonMultimodalDefenseWhileunimodaldefensemethodsstillneedim-provement,theless-exploredmultimodaldefensesrequirefurtherresearchwithlimitationsasfollows:Non-generalizableDefense.Mostdefensestrate-giesaretailoredtospecificattacktypes,strugglingtoadapttovariousandevolvingattackmethods.PoorRobustness.Existingdefensesstruggletowithstandperturbationattacks,wheresubtleandimperceptiblechangestoinputscancausefailuresindetectingjailbrokencontent.Developingrobustdefensesagainstattacksisasignificantchallenge.FalsePositiveChallenge.Legitimateresponsesmaybeexcessivelydefendedandwronglyflaggedasjailbreakattacks,hinderinguserneeds.HighCostofSafetyAlignment.Fine-tuningforsafetyrequiresextensiveannotation,leadingtohighcosts.Besides,repeatedalignmenttrainingduetomodelsadvancementsandevolvingattackmethods,incurshighcomputationexpenses.UnexploredImage-basedDetection.Currentmethodsprimarilydetectingharmfulcontentinimagesbasedontheirtextualdescriptions.Directdetectionandsmoothingtechniquesthatoperateonimagesstillneedfurtherresearch.Toaddressthesechallenges,weproposethefol-lowingresearchdirections:•Developacomprehensiveandadaptabledefensesystemforevolvingattacktechniques.Forex-ample,ensemblemultipledefensestrategiesatvariousstages,ordesignageneralreinforce-mentlearningalgorithmtooptimizestrategiesthroughsimulatedattack-defensescenarios.•Regularlyupdateadversarialtrainingsetswithnewexamplesfromrecentattacktrendsandcon-tinuouslytrainadefensemodel,toimprovere-silienceagainstperturbation-basedattacks.•Designfine-graineddefensemethodstoiden-tifyvaryingdegreesofharmfulness,andad-justthresholdsaccordinglyindifferentscenar-ios.Besides,utilizemajority-voteorcross-validationtomitigatefalsepositiveissues.•Identifysubsetswithinfine-tuningdatasetsthat,althoughbenign,maydegrademodelsafetyandremovethemforsubsequenttuning.Besides,implementmodelpruningtoupdatespecificsub-8regionsforsafetyalignment.•Exploredetectionandsmoothingtechniquesthatdirectlyclassifyandmitigateharmfulcon-tentinimagesinputs.6ConclusionInthiswork,weofferathoroughoverviewofjailbreakingresearchforLLMsandMLLMs,dis-cussingrecentadvancesinevaluationbenchmarks,attacktechniquesanddefensestrategies.Further-more,wesummarizethelimitationsandpotentialresearchdirectionsofofMLLMjailbreakingbydrawingcomparisonstothemoreadvancedstateofLLMjailbreaking,aimingtoinspirefuturework.LimitationsThisstudyhasseveralpotentiallimitations.First,duetospaceconstraints,wemaynotincludeallrelevantreferencesanddetailedtechnicalmeth-odsrelatedtojailbreaking.Second,ourworkisprimarilyfocusedonhighlightinglimitationsandpotentialresearchdirectionsinthemultimodaldo-main,whilenotprovidinganin-depthanalysisofunimodallimitations.Finally,thisworkmainlyservesasasurveyandinvestigationonexistingandfuturejailbreakresearch,withoutproposingandexperimentingwithspecificnovelmethods.EthicsStatementThispaperdiscussesjailbreakdatasetsandattacktechniques,whichmaypotentialcontainorinduceoffensiveandharmfulcontent.Itisimportanttoemphasizethatthisworkaimstoinspirefuturere-searchonjailbreakingtoenhancetherobustnessandsecurityoflargemodels,aidingintheidenti-ficationandmitigationofpotentialvulnerabilities.Westronglyurgemoreresearcherstofocusonthisareatopromotethedevelopmentofmoreethicalandsecurelargemodels.Oursurveyanddiscussedcontentarestrictlyintendedforresearchpurposesthatfollowtheethicalguidelinesofthecommunity.Theauthorsemphaticallydenouncetheuseofourworkforgeneratingharmfulcontent.ReferencesGabrielAlonandMichaelKamfonas.2023.Detect-inglanguagemodelattackswithperplexity.arXivpreprintarXiv:2308.14132.CemAnil,EsinDurmus,MrinankSharma,JoeBenton,SandipanKundu,JoshuaBatson,NinaRimsky,MegTong,JesseMu,DanielFord,etal.2024.Many-shotjailbreaking.INGTRANSFERABILITYOFADVERSARIALAT-TACKS.Loft:Localproxyfine-tuningforimprov-ingtransferabilityofadversarialattacksagainstlargelanguagemodel.JinzeBai,ShuaiBai,ShushengYang,ShijieWang,SinanTan,PengWang,JunyangLin,ChangZhou,andJingrenZhou.2023.Qwen-vl:Afrontierlargevision-languagemodelwithversatileabilities.arXivpreprintarXiv:2308.12966.YuntaoBai,SauravKadavath,SandipanKundu,AmandaAskell,JacksonKernion,AndyJones,AnnaChen,AnnaGoldie,AzaliaMirhoseini,CameronMcKinnon,etal.2022.Constitutionalai:Harmlessnessfromaifeedback.arXivpreprintarXiv:2212.08073.LukeBailey,EuanOng,StuartRussell,andScottEm-mons.2023.Imagehijacks:Adversarialimagescancontrolgenerativemodelsatruntime.arXivpreprintarXiv:2309.00236.RishabhBhardwajandSoujanyaPoria.2023.Red-teaminglargelanguagemodelsusingchainofutterancesforsafety-alignment.arXivpreprintarXiv:2308.09662.NicholasCarlini,MiladNasr,ChristopherAChoquette-Choo,MatthewJagielski,IrenaGao,PangWeiWKoh,DaphneIppolito,FlorianTramer,andLudwigSchmidt.2024.Arealignedneuralnetworksadver-sariallyaligned?AdvancesinNeuralInformationProcessingSystems,36.PatrickChao,AlexanderRobey,EdgarDobriban,HamedHassani,GeorgeJPappas,andEricWong.2023.Jailbreakingblackboxlargelanguagemodelsintwentyqueries.arXivpreprintarXiv:2310.08419.ShuoChen,ZhenHan,BailanHe,ZifengDing,Wen-qianYu,PhilipTorr,VolkerTresp,andJindongGu.2024.Redteaminggpt-4v:Aregpt-4vsafeagainstuni/multi-modaljailbreakattacks?arXivpreprintarXiv:2404.03411.ShiyaoCui,ZhenyuZhang,YilongChen,WenyuanZhang,TianyunLiu,SiqiWang,andTingwenLiu.2023.Fft:Towardsharmlessnessevaluationandanalysisforllmswithfactuality,fairness,toxicity.arXivpreprintarXiv:2311.18580.GeleiDeng,YiLiu,YuekangLi,KailongWang,YingZhang,ZefengLi,HaoyuWang,TianweiZhang,andYangLiu.2024a.Masterkey:Automatedjailbreak-ingoflargelanguagemodelchatbots.InProc.ISOCNDSS.GeleiDeng,YiLiu,KailongWang,YuekangLi,Tian-weiZhang,andYangLiu.2024b.Pandora:Jailbreakgptsbyretrievalaugmentedgenerationpoisoning.arXivpreprintarXiv:2402.08416.9YichenGong,DelongRan,JinyuanLiu,CongleiWang,TianshuoCong,AnyuWang,SisiDuan,andXiaoyunWang.2023.Figstep:Jailbreakinglargevision-languagemodelsviatypographicvisualprompts.arXivpreprintarXiv:2311.05608.YunhaoGou,KaiChen,ZhiliLiu,LanqingHong,HangXu,ZhenguoLi,Dit-YanYeung,JamesTKwok,andYuZhang.2024.Eyesclosed,safetyon:Protectingmultimodalllmsviaimage-to-texttransformation.arXivpreprintarXiv:2403.09572.XiangmingGu,XiaosenZheng,TianyuPang,ChaoDu,QianLiu,YeWang,JingJiang,andMinLin.2024.Agentsmith:Asingleimagecanjailbreakonemillionmultimodalllmagentsexponentiallyfast.arXivpreprintarXiv:2402.08567.MaanakGupta,CharanKumarAkiri,KshitizAryal,EliParker,andLopamudraPraharaj.2023.Fromchatgpttothreatgpt:Impactofgenerativeaiincybersecurityandprivacy.IEEEAccess.ThomasHartvigsen,SaadiaGabriel,HamidPalangi,MaartenSap,DipankarRay,andEceKamar.2022.Toxigen:Alarge-scalemachine-generateddatasetforadversarialandimplicithatespeechdetection.arXivpreprintarXiv:2203.09509.AlecHelbling,MansiPhute,MatthewHull,andDuenHorngChau.2023.Llmselfdefense:Byselfexamination,llmsknowtheyarebeingtricked.arXivpreprintarXiv:2308.07308.YangsiboHuang,SamyakGupta,MengzhouXia,KaiLi,andDanqiChen.2023.Catastrophicjailbreakofopen-sourcellmsviaexploitinggeneration.arXivpreprintarXiv:2310.06987.JiabaoJi,BairuHou,AlexanderRobey,GeorgeJPap-pas,HamedHassani,YangZhang,EricWong,andShiyuChang.2024.Defendinglargelanguagemod-elsagainstjailbreakattacksviasemanticsmoothing.arXivpreprintarXiv:2402.16192.AlbertQJiang,AlexandreSablayrolles,ArthurMen-sch,ChrisBamford,DevendraSinghChaplot,DiegodelasCasas,FlorianBressand,GiannaLengyel,Guil-laumeLample,LucileSaulnier,etal.2023.Mistral7b.arXivpreprintarXiv:2310.06825.HaiboJin,RuoxiChen,AndyZhou,JinyinChen,YangZhang,andHaohanWang.2024.Guard:Role-playingtogeneratenatural-languagejailbreakingstotestguidelineadherenceoflargelanguagemodels.arXivpreprintarXiv:2402.03299.DanielKang,XuechenLi,IonStoica,CarlosGuestrin,MateiZaharia,andTatsunoriHashimoto.2023.Ex-ploitingprogrammaticbehaviorofllms:Dual-usethroughstandardsecurityattacks.arXivpreprintarXiv:2302.05733.HeegyuKim,SehyunYuk,andHyunsoukCho.2024.Breakthebreakout:Reinventinglmdefenseagainstjailbreakattackswithself-refinement.arXivpreprintarXiv:2402.15180.MinbeomKim,JahyunKoo,HwanheeLee,JoonsukPark,HwaranLee,andKyominJung.2023.Life-tox:Unveilingimplicittoxicityinlifeadvice.arXivpreprintarXiv:2311.09585.GeorgeKour,MarcelZalmanovici,NaamaZwerdling,EstherGoldbraich,OraNovaFandina,AteretAnaby-Tavor,OrnaRaz,andEitanFarchi.2023.Unveil-ingsafetyvulnerabilitiesoflargelanguagemodels.arXivpreprintarXiv:2311.04124.AounonKumar,ChiragAgarwal,SurajSrinivas,SoheilFeizi,andHimaLakkaraju.2023.Certifyingllmsafetyagainstadversarialprompting.arXivpreprintarXiv:2309.02705.SimonLermen,CharlieRogers-Smith,andJeffreyLadish.2023.Lorafine-tuningefficientlyundoessafetytraininginllama2-chat70b.arXivpreprintarXiv:2310.20624.HaoranLi,DadiGuo,WeiFan,MingshiXu,JieHuang,FanpuMeng,andYangqiuSong.2023a.Multi-stepjailbreakingprivacyattacksonchatgpt.arXivpreprintarXiv:2304.05197.JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi.2023b.Blip-2:Bootstrappinglanguage-imagepre-trainingwithfrozenimageencodersandlargelan-guagemodels.arXivpreprintarXiv:2301.12597.MukaiLi,LeiLi,YuweiYin,MasoodAhmed,Zhen-guangLiu,andQiLiu.2024a.Redteamingvisuallanguagemodels.arXivpreprintarXiv:2401.12915.YifanLi,HangyuGuo,KunZhou,WayneXinZhao,andJi-RongWen.2024b.Imagesareachilles’heelofalignment:Exploitingvisualvulnerabilitiesforjail-breakingmultimodallargelanguagemodels.arXivpreprintarXiv:2403.09792.YuhuiLi,FangyunWei,JinjingZhao,ChaoZhang,andHongyangZhang.2023c.Rain:Yourlanguagemod-elscanalignthemselveswithoutfinetuning.arXivpreprintarXiv:2309.07124.StephanieLin,JacobHilton,andOwainEvans.2022.TruthfulQA:Measuringhowmodelsmimichumanfalsehoods.InProceedingsofthe60thAnnualMeet-ingoftheAssociationforComputationalLinguistics(Volume1:LongPapers).HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee.2023a.Visualinstructiontuning.arXivpreprintarXiv:2304.08485.TongLiu,YingjieZhang,ZheZhao,YinpengDong,GuozhuMeng,andKaiChen.2024.Makingthemaskandanswer:Jailbreakinglargelanguagemodelsinfewqueriesviadisguiseandreconstruction.arXivpreprintarXiv:2402.18104.XiaogengLiu,NanXu,MuhaoChen,andChaoweiXiao.2023b.Autodan:Generatingstealthyjailbreakpromptsonalignedlargelanguagemodels.arXivpreprintarXiv:2310.04451.10XinLiu,YichenZhu,YunshiLan,ChaoYang,andYuQiao.2023c.Query-relevantimagesjail-breaklargemulti-modalmodels.arXivpreprintarXiv:2311.17600.XinLiu,YichenZhu,YunshiLan,ChaoYang,andYuQiao.2023d.Query-relevantimagesjailbreaklargemulti-modalmodels.YangLiu,YuanshunYao,Jean-FrancoisTon,XiaoyingZhang,RuochengGuoHaoCheng,YegorKlochkov,MuhammadFaaizTaufiq,andHangLi.2023e.Trust-worthyllms:asurveyandguidelineforevaluatinglargelanguagemodels’alignment.arXivpreprintarXiv:2308.05374.YiLiu,GeleiDeng,ZhengziXu,YuekangLi,YaowenZheng,YingZhang,LidaZhao,TianweiZhang,andYangLiu.2023f.Jailbreakingchatgptviapromptengineering:Anempiricalstudy.arXivpreprintarXiv:2305.13860.WeidiLuo,SiyuanMa,XiaogengLiu,XiaoyuGuo,andChaoweiXiao.2024.Jailbreakv-28k:Abench-markforassessingtherobustnessofmultimodallargelanguagemodelsagainstjailbreakattacks.arXivpreprintarXiv:2404.03027.SiyuanMa,WeidiLuo,YuWang,XiaogengLiu,MuhaoChen,BoLi,andChaoweiXiao.2024.Visual-roleplay:Universaljailbreakattackonmultimodallargelanguagemodelsviarole-playingimagechar-acte.arXivpreprintarXiv:2405.20773.ZhenxingNiu,HaodongRen,XinboGao,GangHua,andRongJin.2024.Jailbreakingattackagainstmultimodallargelanguagemodel.arXivpreprintarXiv:2402.02309.OpenAI.2023.Gpt-4technicalreport.LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,CarrollWainwright,PamelaMishkin,ChongZhang,SandhiniAgarwal,KatarinaSlama,AlexRay,etal.2022.Traininglanguagemodelstofollowinstruc-tionswithhumanfeedback.Advancesinneuralin-formationprocessingsystems,35:27730–27744.XiangyuQi,KaixuanHuang,AshwineePanda,PeterHenderson,MengdiWang,andPrateekMittal.2024.Visualadversarialexamplesjailbreakalignedlargelanguagemodels.InProceedingsoftheAAAICon-ferenceonArtificialIntelligence,volume38,pages21527–21536.XiangyuQi,YiZeng,TinghaoXie,Pin-YuChen,RuoxiJia,PrateekMittal,andPeterHenderson.2023.Fine-tuningalignedlanguagemodelscompromisessafety,evenwhenusersdonotintendto!arXivpreprintarXiv:2310.03693.HuachuanQiu,ShuaiZhang,AnqiLi,HongliangHe,andZhenzhongLan.2023.Latentjailbreak:Abenchmarkforevaluatingtextsafetyandoutputro-bustnessoflargelanguagemodels.arXivpreprintarXiv:2307.08487.JavierRandoandFlorianTramèr.2023.Universaljailbreakbackdoorsfrompoisonedhumanfeedback.arXivpreprintarXiv:2311.14455.AbhinavRao,SachinVashistha,AtharvaNaik,So-makAditya,andMonojitChoudhury.2023.Trick-ingllmsintodisobedience:Understanding,ana-lyzing,andpreventingjailbreaks.arXivpreprintarXiv:2305.14965.AlexanderRobey,EricWong,HamedHassani,andGeorgeJPappas.2023.Smoothllm:Defendinglargelanguagemodelsagainstjailbreakingattacks.arXivpreprintarXiv:2310.03684.RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjörnOmmer.2022.High-resolutionimagesynthesiswithlatentdiffusionmod-els.InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,pages10684–10695.VictorSanh,LysandreDebut,JulienChaumond,andThomasWolf.2019.Distilbert,adistilledversionofbert:smaller,faster,cheaperandlighter.arXivpreprintarXiv:1910.01108.RushebShah,SoroushPour,ArushTagade,StephenCasper,JavierRando,etal.2023.Scalableandtransferableblack-boxjailbreaksforlanguagemodelsviapersonamodulation.arXivpreprintarXiv:2311.03348.ErfanShayegani,YueDong,andNaelAbu-Ghazaleh.2023.Jailbreakinpieces:Compositionaladversar-ialattacksonmulti-modallanguagemodels.InTheTwelfthInternationalConferenceonLearningRepre-sentations.XinyueShen,ZeyuanChen,MichaelBackes,YunShen,andYangZhang.2023."doanythingnow":Characterizingandevaluatingin-the-wildjailbreakpromptsonlargelanguagemodels.arXivpreprintarXiv:2308.03825.AlexandraSouly,QingyuanLu,DillonBowen,TuTrinh,ElvisHsieh,SanaPandey,PieterAbbeel,JustinSvegliato,ScottEmmons,OliviaWatkins,etal.2024.Astrongrejectforemptyjailbreaks.arXivpreprintarXiv:2402.10260.RohanTaori,IshaanGulrajani,TianyiZhang,YannDubois,XuechenLi,CarlosGuestrin,PercyLiang,andTatsunoriBHashimoto.2023.Stanfordalpaca:Aninstruction-followingllamamodel.GeminiTeam,RohanAnil,SebastianBorgeaud,YonghuiWu,Jean-BaptisteAlayrac,JiahuiYu,RaduSoricut,JohanSchalkwyk,AndrewMDai,AnjaHauth,etal.2023.Gemini:afamilyofhighlycapablemultimodalmodels.arXivpreprintarXiv:2312.11805.HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,TimothéeLacroix,BaptisteRozière,NamanGoyal,EricHambro,Faisal11Azhar,etal.2023a.Llama:Openandeffi-cientfoundationlanguagemodels.arXivpreprintarXiv:2302.13971.HugoTouvron,LouisMartin,KevinStone,PeterAl-bert,AmjadAlmahairi,YasmineBabaei,NikolayBashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal.2023b.Llama2:Openfounda-tionandfine-tunedchatmodels.arXivpreprintarXiv:2307.09288.BoxinWang,WeixinChen,HengzhiPei,ChulinXie,MintongKang,ChenhuiZhang,ChejianXu,ZidiXiong,RitikDutta,RylanSchaeffer,etal.2023a.Decodingtrust:Acomprehensiveassessmentoftrustworthinessingptmodels.arXivpreprintarXiv:2306.11698.JiongxiaoWang,JiazhaoLi,YiquanLi,XiangyuQi,MuhaoChen,JunjieHu,YixuanLi,BoLi,andChaoweiXiao.2024a.Mitigatingfine-tuningjail-breakattackwithbackdoorenhancedalignment.arXivpreprintarXiv:2402.14968.XinpengWang,XiaoyuanYi,HanJiang,ShanlinZhou,ZhihuaWei,andXingXie.2023b.Tovilag:Yourvisual-languagegenerativemodelisalsoanevildoer.arXivpreprintarXiv:2312.11523.YihanWang,ZhouxingShi,AndrewBai,andCho-JuiHsieh.2024b.Defendingllmsagainstjail-breakingattacksviabacktranslation.arXivpreprintarXiv:2402.16459.YuxiaWang,HaonanLi,XudongHan,PreslavNakov,andTimothyBaldwin.2023c.Do-not-answer:Adatasetforevaluatingsafeguardsinllms.arXivpreprintarXiv:2308.13387.AlexanderWei,NikaHaghtalab,andJacobSteinhardt.2024.Jailbroken:Howdoesllmsafetytrainingfail?AdvancesinNeuralInformationProcessingSystems,36.ZemingWei,YifeiWang,andYisenWang.2023.Jailbreakandguardalignedlanguagemodelswithonlyfewin-contextdemonstrations.arXivpreprintarXiv:2310.06387.ZeguanXiao,YanYang,GuanhuaChen,andYunChen.2024.Tastle:Distractlargelanguagemod-elsforautomaticjailbreakattack.arXivpreprintarXiv:2403.08424.GuohaiXu,JiayiLiu,MingYan,HaotianXu,JinghuiSi,ZhuoranZhou,PengYi,XingGao,JitaoSang,RongZhang,etal.2023a.Cvalues:Measuringthevaluesofchineselargelanguagemodelsfromsafetytoresponsibility.arXivpreprintarXiv:2307.09705.NanXu,FeiWang,BenZhou,BangZhengLi,ChaoweiXiao,andMuhaoChen.2023b.Cognitiveoverload:Jailbreakinglargelanguagemodelswithoverloadedlogicalthinking.arXivpreprintarXiv:2311.09827.ZhangchenXu,FengqingJiang,LuyaoNiu,JinyuanJia,BillYuchenLin,andRadhaPoovendran.2024.Safedecoding:Defendingagainstjailbreakattacksviasafety-awaredecoding.arXivpreprintarXiv:2402.08983.XianjunYang,XiaoWang,QiZhang,LindaPetzold,WilliamYangWang,XunZhao,andDahuaLin.2023.Shadowalignment:Theeaseofsubvert-ingsafely-alignedlanguagemodels.arXivpreprintarXiv:2310.02949.YifanYao,JinhaoDuan,KaidiXu,YuanfangCai,ZhiboSun,andYueZhang.2024.Asurveyonlargelan-guagemodel(llm)securityandprivacy:Thegood,thebad,andtheugly.High-ConfidenceComputing,page100211.Zheng-XinYong,CristinaMenghini,andStephenHBach.2023.Low-resourcelanguagesjailbreakgpt-4.arXivpreprintarXiv:2310.02446.JiahaoYu,XingweiLin,andXinyuXing.2023.Gpt-fuzzer:Redteaminglargelanguagemodelswithauto-generatedjailbreakprompts.arXivpreprintarXiv:2309.10253.YouliangYuan,WenxiangJiao,WenxuanWang,Jen-tseHuang,PinjiaHe,ShumingShi,andZhaopengTu.2023.Gpt-4istoosmarttobesafe:Stealthychatwithllmsviacipher.arXivpreprintarXiv:2308.06463.CanaanYung,HadiMohagheghDolatabadi,SarahEr-fani,andChristopherLeckie.2024.Roundtriptrans-lationdefenceagainstlargelanguagemodeljailbreak-ingattacks.arXivpreprintarXiv:2402.13517.QiusiZhan,RichardFang,RohanBindu,AkulGupta,TatsunoriHashimoto,andDanielKang.2023.Re-movingrlhfprotectionsingpt-4viafine-tuning.arXivpreprintarXiv:2311.05553.XiaoyuZhang,CenZhang,TianlinLi,YihaoHuang,XiaojunJia,XiaofeiXie,YangLiu,andChaoShen.2023a.Amutation-basedmethodformulti-modaljailbreakingattackdetection.arXivpreprintarXiv:2312.10766.ZhexinZhang,LeqiLei,LindongWu,RuiSun,YongkangHuang,ChongLong,XiaoLiu,XuanyuLei,JieTang,andMinlieHuang.2023b.Safety-bench:Evaluatingthesafetyoflargelanguagemod-elswithmultiplechoicequestions.arXivpreprintarXiv:2309.07045.XuandongZhao,XianjunYang,TianyuPang,ChaoDu,LeiLi,Yu-XiangWang,andWilliamYangWang.2024.Weak-to-strongjailbreakingonlargelanguagemodels.arXivpreprintarXiv:2401.17256.AndyZhou,BoLi,andHaohanWang.2024a.Ro-bustpromptoptimizationfordefendinglanguagemodelsagainstjailbreakingattacks.arXivpreprintarXiv:2401.17263.12ZhenhongZhou,JiuyangXiang,HaopengChen,QuanLiu,ZheruiLi,andSenSu.2024b.Speakoutofturn:Safetyvulnerabilityoflargelanguagemodelsinmulti-turndialogue.arXivpreprintarXiv:2402.17262.KaijieZhu,JindongWang,JiahengZhou,ZichenWang,HaoChen,YidongWang,LinyiYang,WeiYe,NeilZhenqiangGong,YueZhang,etal.2023.Promptbench:Towardsevaluatingtherobustnessoflargelanguagemodelsonadversarialprompts.arXivpreprintarXiv:2306.04528.YongshuoZong,OndrejBohdal,TingyangYu,YongxinYang,andTimothyHospedales.2024.Safetyfine-tuningat(almost)nocost:Abaselineforvisionlargelanguagemodels.arXivpreprintarXiv:2402.02207.AndyZou,ZifanWang,JZicoKolter,andMattFredrik-son.2023.Universalandtransferableadversarialattacksonalignedlanguagemodels.arXivpreprintarXiv:2307.15043.AEvaluationFrameworkTheevaluationofjailbreakattackanddefensein-volvesthreekeyfactors.First,thedefinitionofasuccessfuljailbreakbuildsastandardforresponseassessment.Second,themetricswhichquantita-tivelymeasuretheeffectivenessofspecificattackordefensestrategies.Thethirdisthejudgementmethods,whichaimtoaccuratelyassessresultsandalignwithhumanvalues.Subsequentparagraphswilldetailexistingresearchtothesepoints.DefinitionsofSuccessfulJailbreakAsuccess-fuljailbreakattackcanbedeterminedatthreedif-ferentlevels.Themostbasicleveldeemsanat-tacksuccessfuliftheresponsedoesnotdirectlyrejectthequery(i.e.,lackswordsrelatedtorejec-tion)(Zouetal.,2023;Robeyetal.,2023).Thisconservativeapproachisonlyappropriateforsce-nariosdemandingexplicitrejection.However,inmostcontexts,amoresuitableresponsealigningwithhumanvaluesmightbeawell-roundedstate-mentoranethicalrecommendation(Wangetal.,2023c).Amoreapplicablecriterionconsidersanattacksuccessfulifthemodelproduceson-topicandharmfulresponses(Weietal.,2024;Yongetal.,2023;Yuetal.,2023;Wangetal.,2023c;Dengetal.,2024a;ATTACKS;Zhanetal.,2023;Shahetal.,2023),focusingonwhetheroutputcontentcircumventsafetymechanismswithoutassessingtheresponsequality,likeitspotentialharmorben-efittotheattacker.Themoststringentdefinitionas-sessesboththecontentandtheimpactofresponses,identifyinganattackassuccessfulifitcontainssubstantiallyharmfulcontentandaidsharmfulac-tions(Huangetal.,2023;Chaoetal.,2023;Soulyetal.,2024;Jietal.,2024).EvaluationMetricsTheevaluationofjailbreakprimarilyutilizestwotypesofmetrics:ratio-basedandscore-based.Ratio-basedmetricsas-sessindividualresponsesasabinaryclassifica-tionofasuccessorfailure,calculatinganoverallrate,suchastheattacksuccessrate(ASR)(Weietal.,2024;Yongetal.,2023;Liuetal.,2023b;Robeyetal.,2023;Xuetal.,2023b;Dengetal.,2024a;Yuanetal.,2023;ATTACKS;Shahetal.,2023).Somestudiesfurtherdistinguishingre-sponsesbasedoncompliancelevels(Yuetal.,2023)orcategories(Wangetal.,2023c),whicharethenaggregatedintoanoverallsuccessorfailurerate.Score-basedmetricsassigncontinuousscorestoresponses,providingamorefine-grainedassess-ment.Thesescoresevaluateaspectslikespecificity,persuasiveness(Soulyetal.,2024;Jietal.,2024),detail(Chaoetal.,2023),orharmfulness(Huangetal.,2023),averagingacrossthedatasetforacomprehensiveevaluation.JailbreakingJudgementMethodsJailbreakat-temptassessmentsutilizevariousmethods.Hu-manevaluationinvolvesexpertsmanuallyreview-ingresponsesbasedonpredefinedguidelines,en-suringaccuracybutatthecostoftimeandscala-bility(Weietal.,2024;Yongetal.,2023;Wangetal.,2023c;Liuetal.,2023f;ATTACKS;Zhanetal.,2023).Rule-basedevaluationemploycriterialikesub-stringmatchingforrejectionkeywords,offeringcost-effectivenessandeaseofimplemen-tation,yetlackingflexibilityfordiversescenariosandoftenincompatiblewithnewmodelsduetovaryingrejectionkeywords(Zouetal.,2023;Liuetal.,2023b;Robeyetal.,2023;Xuetal.,2023b).Structuringqueriesforlimitedresponseformats,likeyes/no(Wangetal.,2023a)ormultiple-choicequestions(Xuetal.,2023a),simplifiesevaluationbutdoesn’tfullyreflectreal-worldperformance,creatingagapineffectiveness.Model-basedevaluationincludingutilizingoffi-cialAPIslikePerspectiveAPIfordetectingharm-fulcontent(Wangetal.,2023a),promptingLLMsasevaluators(Wangetal.,2023c;Soulyetal.,2024;Chaoetal.,2023;Yuanetal.,2023;Shahetal.,2023;Liuetal.,2023b),andtrainingPLM-basedevaluatorswithannotateddata(Yuetal.,2023;Wangetal.,2023c;Huangetal.,2023).Theseapproachesbalanceefficiencyandflexibility,and13aligningwellwithhumanvalues.However,itpresentsseverallimitations:LLM-basedevalua-torsarecostlyandcanyieldhighfalse-negativerates(Shahetal.,2023),whilePLM-basedevalu-atorsrequireextensivehuman-annotatedtrainingdataandmaysufferfromloweraccuracyduetoimbalanceddatadistribution(Wangetal.,2023c).14
