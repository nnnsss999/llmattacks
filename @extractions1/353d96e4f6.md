---
title: http://arxiv.org/pdf/2505.16008
source_url: http://arxiv.org/pdf/2505.16008
date_collected: '2025-06-19'
license: Fair Use
---

LAGO:Few-shotCrosslingualEmbeddingInversionAttacksviaLanguageSimilarity-AwareGraphOptimizationWenruiYu1,YiyiChen2,JohannesBjerva2,SokolKosta1,QiongxiuLi1*1DepartmentofElectronicSystems,2DepartmentofComputerScienceAalborgUniversity,Copenhagen,Denmarkwenyu@es.aau.dk,{yiyic,jbjerva}@cs.aau.dk,{sok,qili}@es.aau.dkAbstractWeproposeLAGO-LanguageSimilarity-AwareGraphOptimization-anovelapproachforfew-shotcross-lingualembeddinginver-sionattacks,addressingcriticalprivacyvul-nerabilitiesinmultilingualNLPsystems.Un-likepriorworkinembeddinginversionattacksthattreatlanguagesindependently,LAGOex-plicitlymodelslinguisticrelationshipsthroughagraph-basedconstraineddistributedopti-mizationframework.Byintegratingsyntacticandlexicalsimilarityasedgeconstraints,ourmethodenablescollaborativeparameterlearn-ingacrossrelatedlanguages.Theoretically,weshowthisformulationgeneralizespriorap-proaches,suchasALGEN,whichemergesasaspecialcasewhensimilarityconstraintsarerelaxed.OurframeworkuniquelycombinesFrobenius-normregularizationwithlinearin-equalityortotalvariationconstraints,ensur-ingrobustalignmentofcross-lingualembed-dingspacesevenwithextremelylimiteddata(asfewas10samplesperlanguage).Exten-siveexperimentsacrossmultiplelanguagesandembeddingmodelsdemonstratethatLAGOsubstantiallyimprovesthetransferabilityofat-tackswith10-20%increaseinRouge-Lscoreoverbaselines.Thisworkestablisheslan-guagesimilarityasacriticalfactorininver-sionattacktransferability,urgingrenewedfo-cusonlanguage-awareprivacy-preservingmul-tilingualembeddings.1IntroductionTextembeddings,whichencodesemanticandsyn-tacticinformationintodensevectorrepresentations,serveasthebackboneofmodernnaturallanguageprocessing(NLP)systems.Theyarealsopow-eringtheLargeLanguageModels,whoseimpactstretchesfarbeyondNLPandissteadilyshapingeverydaylivesandbusinessoperations.However,theirwidespreaddeploymentincloud-basedser-*Correspondingauthor.Vector DatabaseHola mundo!Hallo Wereld!Hallo Welt!Olá mundo!Ciao mondoBonjour le monde!Attack ModelengdeunlditaporspafraHola mundo!Hallo Wereld!Hallo Welt!Bonjour le monde!Olá mundo!Ciao mondo!Language Similarities engTextEmbeddingsFigure1:Few-shotCross-lingualTextualEmbeddingInversionLeveragingLanguageSimilarities.Example:AttackmodeltrainedonEnglishembeddingsisusedtoattackembeddingsinotherlanguages,usinglanguagesimilaritiesasaprior.vicesintroducessignificantprivacyrisks.Apartic-ularlyconcerningthreatistheembeddinginversionattack(SongandRaghunathan,2020;Chenetal.,2025b),wheretheadversariescandecodesensi-tiveandprivatedatadirectlyfromtheembeddingvectors.Thesecurityofthesystemcanbecompro-misedwhenmalicioususersabusetheembeddingmodelAPI,collectingmassivedatasetstotrainat-tackmodels.Dataleakage,whetheraccidentalordeliberate,furtherexacerbatesthisvulnerability.Asvectordatabasesandgenerative-AIservicespro-liferateacrosstheglobe,theembeddingvectorsof-feredascommoditiesaremostlymultilingual.Yet,priorresearchesinthisattackspacemostlycon-centrateoninvertingEnglishembeddings(SongandRaghunathan,2020;Lietal.,2023;Morrisetal.,2023;Huangetal.,2024).Whilerecentef-forts(Chenetal.,2024a,b,2025b)touchuponmul-tilingualandcross-lingualinversionattacks,theylackanexplicitmodelingoflanguagesimilarities,resultinginpoorgeneralizationacrosslanguages.Inreal-worldadversarialscenarios,suchasspe-cializeddomainsorlow-resourcelanguages,at-tackersmayonlyhaveaccesstoahandfulofembedding-textpairs.AlthoughALGEN(Chenetal.,2025b)partiallyaddressesthefew-shotregimethroughdecodertransfer,itlacksmech-1arXiv:2505.16008v1  [cs.CL]  21 May 2025anismstoexploitlanguagesimilarity,whichwehy-pothesizeisakeyfactorincrosslingualgeneraliza-tionfailure.Priorstudieshaveshownthatlanguagesimilarities,simulatedfromtypologicalfeaturesandlexicaloverlapping,correlatewithstructuralvariationsininversionoutputs(Chenetal.,2024a,2025a),providingempiricalmotivationforincor-poratingsuchrelationshipsintoattackmodels.Toaddressthis,weproposeLAGO(LanguageSimilarity-AwareGraphOptimization)forfew-shotcrosslingualembeddinginversion.LAGOexplicitlymodelslinguisticrelationshipsbycon-structingatopologicalgraphoverlanguages,wherenodesrepresentlanguagesandedgesencodesimi-larity.AsillustratedinFig.1,thisgraphisusedtoguidecollaborativeoptimizationofdecoderalign-mentfunctionsacrosslanguages,enablingknowl-edgetransferfromtypologicallyrelatedneigh-bors.Weformalizetheattackobjectiveasadis-tributedoptimizationproblem,whereeachnodecorrespondstoalanguageandconstraintsencodesimilarity-basedconsistency.Wepresenttwoal-gorithmicvariants:(1)aninequality-constrainedapproachbasedonIEQ-PDMM(HeusdensandZhang,2024b),and(2)atotalvariationregularizedformulation(Pengetal.,2021)thatsoftlypenalizesparameterdriftacrosssimilarlanguages.Ourmaincontributionsinclude:•WeproposeLAGO,thefirstframeworkforfew-shotcrosslingualembeddinginversionthatincorporateslanguagesimilarityasastructuralpriorinagraph-constrainedopti-mizationproblem.•Wedeveloptwoalgorithmicvariants,oneus-inginequalityconstraintsandoneusingtotalvariationpenalties,thatenablecollaborativeparameterlearningacrosslanguages.Priorwork,includingALGEN,emergesasaspecialcasewithinourframework(cf.Section4.3).•Experimentsacrossmultipleembeddingmod-elsanddiverselanguagesshowthatlanguagesimilaritystronglycorrelateswithattacktrans-ferability,improvingperformanceby10-20%overpriormethods.Byexposingoverlookedvulnerabilitiesinmulti-lingualembeddingsystemsanddemonstratingef-fectiveinversionunderrealisticlow-resourcecon-ditions,ourworkunderscorestheurgentneedforstrongerprivacyprotectionsincross-lingualNLPdeployments.Whiledifferentialprivacyofferssomeprotectionagainstourattack,italsosignif-icantlydegradesdownstreamutility(Chenetal.,2025b),highlightingtheneedformoretargetedandefficientdefensemechanisms.2RelatedWork2.1EmbeddingInversionAttacksEarlyworkonembeddinginversionframedthetaskasclassificationoverfixedvocabularies.Forexam-ple,SongandRaghunathan(2020)aimtorecoverinputtokensdirectlyfromembeddings,achievingupto70%reconstruction.Subsequentadvancesrecastthetaskasgeneration:Lietal.(2023)intro-duceadecoder-basedapproachtoproducefluenttext,whileMorrisetal.(2023)furtherimproveac-curacythroughiterativerefinement.Severalworkhavesincethenextendedinversionattackstomulti-lingualscenarios(Chenetal.,2024b,a).Moreover,Huangetal.(2024)trainsasurrogatemodeltocon-ducttransferattackonvictimembeddingsunderblack-boxaccess.Thesemethods,however,typicallyrelyonmas-sivetrainingsamples(8kto5millionvictimem-beddings)andareprimarilyevaluatedinmonolin-gualorwell-resourcedsettings.Inpractice,attack-ersoftenfacefew-shotscenarios.Forexample,reconstructingtextinlow-resourcelanguagesorspecializeddomainswithonlyahandfulofavail-ablesamples.ALGEN(Chenetal.,2025b)in-troducesalinearalignmenttechnique,allowingadecodertrainedinonedomaintobereusedinanother.Whileeffectiveinfew-shottransfer,AL-GENdoesnotexplicitlymodellanguagesimilarityorstructuralrelationshipsbetweenlanguages(cf.Section3.2).OurframeworkimprovesALGENbydirectlyincorporatinglinguisticknowledgetoachievestrongerfew-shotcross-lingualinversion.2.2Cross-lingualTransferabilityCrosslingualtransferabilityisancentralresearchtopicinmultilingualNLP.Priorresearchesleveragecrosslingualtransferabilitytoimprovedownstreamtaskperformancesintargetlanguages,mainlythroughfine-tuningLLMsonrelatedsourcelan-guages(Choennietal.,2023),orusingzero-shottransfer(Adelanietal.,2022;deVriesetal.,2022;Blaschkeetal.,2025)orfew-shottrasnferwithpre-trainedMLLMs(Lauscheretal.,2020).Lan-guagesimilaritybasedonlinguisticdata,suchastypologicalfeatures(Littelletal.,2017)andlexi-2caldatabases(Wichmannetal.,2022),havebeenusedextensivelyinfacilitatingcrosslingualtrans-fer(Philippyetal.,2023).Inthiswork,weleveragelanguagesimilaritygeneratedfrombothsyntac-ticfeaturesandlexicaloverlaptoprovidealterna-tiveperspectivesonconstructinggraphs,toassistcrosslingualinversionattacks.2.3DistributedOptimizationDistributedoptimizationdecomposesaglobalob-jectiveintosmallerlocalproblemsthataresolvedcollaborativelyacrossnetworkednodes.Owingtoitsscalabilityandefficiency,ithasbecomeafoundationaltoolinlarge-scalemachinelearningandsignalprocessing.Applicationsspandomainssuchasfederatedlearning(McMahanetal.,2017),sensornetworks(RabbatandNowak,2004),andprivacy-preservingsystems(Lietal.,2020;Yuetal.,2024).Classicaldistributedoptimizational-gorithmsincludetheAlternatingDirectionMethodofMultipliers(ADMM,(Boyd,2010))andPrimal-DualMethodofMultipliers(PDMM,(ZhangandHeusdens,2017))andtheirvariants(WangandBanerjee,2014;Ouyangetal.,2015;HeusdensandZhang,2024a,b).Tothebestofourknowledge,theirapplicationtoinversionattacksremainsun-explored.Inthiswork,wepresent,forthefirsttime,anovelmigrationofdistributedoptimizationtechniquestoinversionattacks.3Preliminaries3.1EmbeddingInversionattackLetx∈Vsdenoteasequenceoftexttokens,andthetextencoderϕ=enc(·):Vs→Rnbeanem-beddingfunctionthatmapstextxtoafixed-lengthvectorϕ(x)∈Rn.sisthesequencelengthandntheembeddingdimensionrespectively.Anem-beddinginversionattackisformallydefinedastheprocessoflearninganapproximateinversefunctiong=dec(·)suchthat:g(ϕ(x))≈x.3.2ALGENALGENenablescross-domainandcross-lingualsentence-levelinversionthroughaframeworkcom-biningembeddingalignmentandsequencegenera-tion.Theframeworkconsistsofthreeparts:1)TrainingalocalattackmodeldecA(·)byfine-tuningapre-traineddecodertofunctionasanembedding-to-textgenerator.2)EmbeddingAlignmentTobridgethediscrep-ancybetweenthevictimeV∈RmandtheattackeA∈Rnembeddingspaces,alinearmappingma-trixW∈Rm×nislearned:ˆeA=eVW.TheoptimalalignmentmatrixWisobtainedbysolvingthefollowingleast-squaresminimization:minW∥EA−EVW∥2F,where∥·∥FdenotestheFrobeniusnorm,EV=[e1⊤V,···,eb⊤V]⊤∈Rb×misthevictimmodel’sembeddingmatrix,andEA=[e1⊤A,···,eb⊤A]⊤∈Rb×nistheattacker’sembeddingmatrix,andbisthenumberoftrainingsamples.mandnaretheembeddingdimensionsofthevictimandattackmodels,respectively.Thisoptimizationproblemadmitsaclosed-formsolutionviathenormalequa-tion(seethederivationinAppendixA):W=(E⊤VEV)−1E⊤VEA,whichminimizesthereconstructionerrorbetweenthealignedvictimembeddingsEVWandtheat-tacker’sreferenceembeddingsEA.3)TextReconstructionThealignedembeddingsˆeAaredecodedintotextviadecA,i.e.,ˆx=decA(ˆeA)=decA(eVW).ALGENachievesinversionwithoutretrainingthevictimmodel,requiringonlyfine-tuningofdecAandestimationofW.3.3FundamentalsofDistributedOptimizationDistributedoptimizationaddressesglobaloptimiza-tionproblemsthroughaunifiedobjectivefunctionwhileincorporatingconstraintsderivedfrominter-noderelationshipswithinthenetwork.Formally,thisapproachcanbeexpressedasmin{wi:i∈V}(cid:88)i∈Vfi(wi),s.t.hij(wi,wj)≤0,(i,j)∈E,wherefidenotesthelocalobjectivesonnodeiandhijencodestheconstraintsbetweenadjacentnodesiandj.Specifically,afundamentalformulationindis-tributedoptimizationemployslinearinequalitycon-straintstocoupledecisionvariablesacrossnetworknodes.Suchformulationcanbeefficientlysolved3Figure2:IllustrationofLAGOvs.ALGEN(Chenetal.,2025b).Top:ALGENtreatseachlanguageindepen-dently.Bottom:LAGOleverageslanguagesimilaritybyintroducingedgeconstraintsinajointdistributedoptimizationframework.usingIEQ-PDMMmethod(HeusdensandZhang,2024b)andthiscanbeformallyexpressedasmin{wi:i∈V}(cid:88)i∈Vfi(wi),(1)s.t.Ai|jwi+Aj|iwj≤bi,j,(i,j)∈E.ConstraintsbetweenentriesaredefinedbyAi|j,Aj|iandbi,j.4LAGO:LanguageSimilarity-AwareGraphOptimizationFrameworkBuildingupontheALGENparadigmandgroundedindistributedoptimization,weproposeLAGO-ageneralframeworkforfew-shotcross-lingualem-beddinginversion.LAGOoperatesintwostages:(1)constructingalanguagesimilaritygraphtocap-turetopologicalrelationshipsbetweenlanguages,and(2)solvingagraph-constrainedoptimizationproblemtojointlyestimatetransformationmatricesacrosslanguages.Thissectiondetailsbothcompo-nentsandintroducestwoalgorithmicvariantsthatimplementouroptimizationframework.4.1StepI:LanguageSimilarity-AwareGraphConstructionToformalizecross-lingualrelationships,wepro-posetoconstructalinguistictopologicalgraphG=(V,E),wherethesetofnodesVrepresentslanguagesandthesetofundirectededgesEen-codespairwisesimilarity.LanguagesimilarityisquantifiedusingestablishedmetricssuchasAJSP(Wichmannetal.,2022)andLang2vec(Littelletal.,2017).Forapredefinedthresholdr,anedgeisestablishedbetweentwolanguagesiandjiftheirdistancemetricDij<r.Mathematically,givenadistancematrixD∈RN×NoverNlan-guages,theadjacencymatrixA∈{0,1}N×Noftheresultingtopologyisderivedas:A=1−sign(D−r)2.SeeAppendixBforaconcreteexampleofgraphconstruction.4.2StepII:Graph-ConstrainedOptimizationAlgorithmsUsingtheconstructedgraph,wereformulatetheoptimizationobjective,leveragingcross-lingualre-lationships,therebyenhancingembeddinginver-sionattacksthroughknowledgetransferfromlin-guisticallyrelatedlanguages.Infew-shotsettingswherelocaldataisscarce,thisformulationim-provestransferabilitybyleveragingcross-lingualregularities.Weintroducetwooptimizationstrate-gies:oneenforcinghardconstraintsandoneap-plyingsoftpenalties.LetWidenotethetransfor-mationmatrixatnodei(languagei).Toensurestabilityinunderdeterminedsettings(e.g.,b<m),weincorporateFrobeniusnormregularizationtomitigaterankdeficiencyandenhanceconvergence.Variant1:LinearInequalityConstraintsThefirstalgorithmvariantintroducestopologicalcon-straintstoenforceconsistencybetweenadjacentnodes’transformationmatrices.Formally,wefor-mulatetheobjectiveasminimizingthesumofre-constructionerrorsacrossallnodeswhileimposingϵ-boundedconstraintsonthepairwisedifferencesbetweenneighbors’mappingmatrices:minW1,···,WN(cid:88)i∈V12(cid:0)||EA,i−EV,iWi||2F+λ||Wi||2F(cid:1),s.t.||Wi−Wj||max≤ϵ,(i,j)∈E,where∥·∥maxdenotestheentry-wiseℓ∞norm.Thisformulationcorrespondstothegeneralinequality-constrainedforminEq.(1),whereAi|j=−Aj|i=[1−1]Tandbi,j=[ϵϵ]T.Assuch,itiscompatiblewiththeIEQ-PDMMopti-mizationframework(HeusdensandZhang,2024b).Theupdateequationsusedinthisframeworkare4givenbelow1.W(t)i=[E⊤V,iEV,i+(2cdi+λ)I]−1(E⊤V,iEA,i−(cid:88)j∈NiAi|jZ(t)i|j)Y(t)i|j=Z(t)i|j+2cAi|jW(t)i−cbi,jZ(t+1)i|j=(cid:40)Y(t+1)j|i,Y(t+1)i|j+Y(t+1)j|i>0,−Y(t+1)i|j,otherwise,(2)wheredi=|Ni|isthedegreeofnodei.Variant2:TotalVariationRegularizationThesecondvariantintroducessoftpenaltiesusingto-talvariationacrossedges,atechniqueoriginallyproposedforByzantine-robustdecentralizedlearn-ingsystems(Pengetal.,2021).Theoptimizationobjectiveisformulatedasfollows:minW1,···,WN(cid:88)i∈V(cid:0)12||EA,i−EV,iWi||2F+λ2||Wi||2F+η(cid:88)j∈Ni||Wi−Wj||sum(cid:1),where∥·∥sumdenotestheentry-wiseℓ1norm.Attimet,eachnodeupdatesitsWwithW(t+1)i=W(t)i−α√t+1(cid:104)−E⊤V,i(EA,i−EV,iW(t)i)+λW(t)i+η(cid:88)j∈Nisign(W(t)i−W(t)j)(cid:105),whereαisthelearningrate.4.3Generalization:ALGENasaSpecialCaseOurproposedLAGOisgeneralandsubsumespriormethodALGENasaspecialcase.Specif-ically,intheinequality-constrainedvariant,whenϵ→∞,cross-nodeconstraintsvanish,andeachlanguagenodesolvesanindependentalignmentproblem.Similarly,inthetotalvariationsetting,settingη=0decouplesallnodes.Inbothcases,theoptimizationreducestoALGEN’sper-languageformulationwithnocross-lingualstructure.ThishighlightstheflexibilityofLAGO:byadjustingconstraintstrength,itinterpolatesbetweenisolatedoptimization(asinALGEN)andfullycollabora-tivecross-lingualinversion.Ourapproachthusprovidesaprincipled,generalizableframeworkformultilingualattackdesign.1ThecomparisoninEq.(2)isappliedelement-wise.Figure3:ExamplegraphsusingtwoLanguageSimilari-ties:(a)AJSPmodelwithr=0.9;(b)Lang2vecmodelwithr=0.45.5ExperimentalSetupModelsandDatasetOurattackframeworkisinitializedusingapre-trainedFLAN-T5model.Toevaluatetherobustnessofourapproach,wecon-ductexperimentswithtwodistinctvictimmodelencoders,MT5,E5-SMALL-V2(E5)andOpenAI’sTEXT-EMBEDDING-ADA-002(ADA-2)(seethede-tailsinTabel3).ThedecoderdecA(·),fine-tunedontheMMARCOEnglishdataset(Bonifacioetal.,2021),servesinthispaperastheattackmodelforsimulatingfew-shotinversionattackscenarios.Weemploythecurrentstate-of-the-artALGENmethodasthebaselineforthefew-shotscenario,maintain-ingidenticaltrainingandtestingconfigurationsforthedecoderasthoseusedinALGEN.Toassesscross-lingualtransferability,weselectasubsetofsevensyntacticallyandlexicallyrelatedlanguages:English,German,French,Dutch,Spanish,ItalianandPortuguese.LanguageGraphsWeevaluatetwodistincttopologiesderivedfromlanguagesimilarities:AJSPandLang2vec.ThetestedtopologiesareillustratedinFig.3.RegularizationParametersToaccomplishsub-stantialconvergence,thenumberofiterationsisfixedat500.Forthelinearinequalityconstraintsmethod,theconvergenceparameterissettoc=0.4,whilefortheTVpenaltytermmethod,thelearningrateischosenasα=0.01.Thecomputa-tionalcostoftheattackisrelativelylow,usingthetopologyof7languagesasanexample,ittakesap-proximatelyfiveminutestocomputeasetofmatrix{Wi:i∈V}withtheinequalityconstrainedfor-mulation,whilethetotalvariationmethodisfaster,completingtheattackinabouttwominutes.EvaluationMetricsWeuseCosinesimilaritytomeasurethesemanticalignmentbetweentheadversarialembeddingsofthevictimmodelEVW5Figure4:Cross-lingualInversionPerformanceswithAJSPGraphinCosineSimilaritiesacrossTrainingSam-ples.andthetargetattackembeddingsEA.Meanwhile,Rouge-L(Lin,2004)evaluatesthelexicaloverlapbetweenthereconstructedtextandthegroundtruthbycomputingthelengthoftheirlongestcommonsubsequence,servingasaproxyforassessingthefidelityofthegeneratedoutputatthelexicallevel.6AnalysisandResultsTovalidatetheeffectivenessofourproposedLAGOframework,weexperimentacrossarangeofsettingsandtasks.Eachsubsectionaddressesoneresearchquestion,probingkeyaspectsofcross-lingualtransferability,generalization,androbust-nesstodefensemechanisms2.6.1DoSimilarLanguagesTransferVulnerabilities?Toassesswhetherlanguagesimilarityaidsattacktransfer,weuseanattackmodeltrainedonEnglishdataattackembeddingsinotherlanguages.WecompareLAGO(withbothoptimizationvariants)toALGENbaselineswithandwithoutFrobeniusnormregularization(λ=0.01),using10to10002Weopen-sourceourcodehttps://anonymous.4open.science/r/ALGO_anonymous.Figure5:Cross-lingualInversionPerformanceswithAJSPGraphinRouge-LScoresacrossTrainingSam-ples.trainingsamples.Noticethatthetrainingsampleisusedexclusivelyforalignment.ForLAGO,wesetϵ=0.01andη=0.01.AsshowninTable1,LAGOconsistentlyimprovesbothcosinesimilar-ityandRouge-LscoresoninvertingFrenchem-beddingsacrossalltrainingsizes.Inlow-resourcesettings(e.g.,10samples),ourmethodyieldsa10–20%boostinRouge-LoverALGEN.Thistrendgeneralizestootherlanguages,suchasDutch,Ger-man,Italian,PortugueseandSpanish,asdemon-stratedinFig.4;5andFig.6.Thesefindingssuggestthatleveraginglanguagesimilaritybothmitigatesdatascarcityandoptimizescross-lingualgeneralizationinlow-resourcesettings.6.2DoesthechoiceofLanguageSimilarityMetricImpacttheAttackEffectiveness?TotestthesensitivityofLAGOtothechoiceoflanguagesimilaritymeasures,wecompareperfor-manceundertwotopologies:ASJP(lexicalsim-ilarity)andLang2vec(syntacticsimilarity).Theresults,demonstratedinFig.6and9inAppendixC,confirmthatLAGOisrobusttothechoiceofsimilaritymetric.Lang2vecshowsslightlybet-terperformanceinmoderate-datasettingsinterms6MethodCosineSimilaritiesRouge-LTrainSamples101003001000101003001000ALGEN-0.86570.87230.86100.898610.0710.4710.2212.07Reg.(λ=0.01)0.86630.87670.87030.899710.1410.5910.3711.91OursInequality0.87010.89190.90390.917810.1411.0912.3112.49TotalVariation0.87770.89660.90460.912910.8711.5911.4612.30Table1:Cross-lingualInversionPerformancesofFrenchembeddingswithAttackModeltrainedinEnglishinCosineSimilaritiesandRouge-LscoresacrossTrainingSamples.ThebestRouge-Lscoresarebold,andthemaximumcosinesimilaritiesareunderlined.Figure6:Cross-lingualInversionPerformanceswithLang2vecGraphinRouge-LScoresacrossTrainingSamples.ofRouge-Lscoresformoderatelylargertrainingsamplesizes(>300).Forinstance,Dutch,withtrainingsamplesof|DV|=500,exhibitsanin-creasefrom5.71to6.65.Overall,ourapproachconsistentlyoutperformsthebaselineintermsofattackefficacy,irrespectiveofthesimilaritymetric.ThissuggeststhatLAGOisnotcontingentuponaspecificlanguagesimilarityframeworkbutin-steadexhibitsrobustgeneralizabilityacrossdiverselanguagestructures.Furthermore,theobservedim-provementsinattackeffectivenessindicatethatourmethodologyisparticularlyadvantageousforlan-guageswithsharedlinguisticfeatures.Whetherthesimilarityislexicalorsyntactic,theattackremainseffective,reinforcingitsversatility.Figure7:Cross-lingualInversionPerformanceswithAttackModeltrainedinSpanishinRouge-LScoresacrossTrainingSamples.6.3IstheInversionGeneralizabletoDifferentVictimModels?WeassessgeneralizabilitybyevaluatingourmethodonembeddingsfromADA-2andE5en-coders.AsshowninAppendixFig.10-13,LAGOconsistentlyoutperformsALGENinbothcosinesimilarityandRouge-Lacrossthesemodels.Relatively,undertheRouge-Lmetric,thein-equalityconstraintdemonstratesstrongerperfor-mancewithlargersamplesizes,whereastotalvari-ationprovesmoreeffectiveinextremelyfew-shotscenarioswithfewerthan300trainingsamples.Weattributethistotheflexibilityofinequalitycon-straints,asmallersamplesizeprovidesWwith7greaterdegreesoffreedom,therebyimposingrel-ativelyweakerrestrictionsonWunderthesameϵ.Consequently,theperformanceofinequalityconstraintsundersmallersamplesizesalignsmorecloselywiththeALGENmethod.6.4CanotherLanguagesassumetheSourceofTransfer?EnglishasthemostrepresentedlanguageinthepretrainedLLMs,servesasanobviouschoicefortrainingtheattackmodeltofacilitatetheinversionofotherlanguages.Wedemonstratethatthepro-posedschemeremainsrobustevenwhentheattackmodelistrainedinanalternativelanguage.AsshowninFig.7andFig.14inAppendixC,whenSpanishisusedastheattacklanguage,LAGOcon-tinuestoyieldconsistentimprovementsoverthebaseline.Thecosinesimilarityincreasesacrosstargetlanguages,andtheinequality-constrainedvariantshowsstrongergainsinRouge-L,particu-larlyunderlow-resourceconditions.Wealsoobserveperformancedisparitiesacrossspecificlanguagepairs.Forexample,theinversionperformancefromEnglishtoGermanisnotablyhigherthanthatfromSpanishtoGerman-apat-ternalreadypresentintheALGENbaseline.Thisdisparitymaybeattributedtotwofactors:differ-encesindecodertrainingqualityandvariationsinlanguagesimilarity.Inourconstructedgraph,En-glishandGermanaredirectlyconnected(one-hopneighbors),whereasSpanishandGermanaretwohopsapart.Theincreasedtopologicaldistancemayweakentheeffectivenessofparametertransfer,assimilarityconstraintsexertlessinfluence.Theseobservationssuggestthattherelativepo-sitionoflanguagesinthesimilaritygraph-andnotjustdatasizeorencoderchoice-caninflu-encetransferstrength.Understandingthedynam-icsoflanguagetopologyintransfer-basedattackspresentsanimportantdirectionforfuturework.6.5DefensesWefurtherinvestigatetheeffectivenessofdiffer-entialprivacy(DP)inmitigatingembeddingin-versions.WeemploytheSNLIdataset(Bowmanetal.,2015)tofine-tunethedecoderandsubse-quentlytransfertheadversarialattackframeworktoGerman,FrenchandSpanishusingtheXNLIdataset(Conneauetal.,2018).WhiletheSNLIdatasetiswidelyutilizedfordownstreamtasksliketextclassification,Chenetal.(2025b)hasdemon-stratedthatwithastrongprivacyguaranteeϵdp=1,modelaccuracydropsto40%,whichisasignifi-cantreductionfromthe60%accuracyachievedatϵdp=12whereDPdefensesshowlimitedimpactonutilityandinversionperformance.Inoursetup,weapplytwoDPmechanisms:thePurkayasthaMechanism(PurMech)andtheNormalizedPlanarLaplaceMechanism(LapMech)proposedbyDuetal.(2023)insentenceembed-dings.Theprivacybudgetparameterisevaluatedacrossϵdp∈[1,4,8,12].AsshowninTable2,Table4,andFig.15,16inAppendixC,inversionattacksincross-lingualsettingsarehighlysensitivetoDPperturbations.Specifically,Rouge-Lscoresareconsistentlysuppressedtobelow2acrosstestedconfigurations.Theseresultsareconsistentwiththeoreticalexpectations:morechallengingexam-ples,suchasthoseincross-lingualorlow-resourcesettings,tendtobemoresensitivetoDPnoise(Car-linietal.,2019;Feldman,2020).WhileDPmech-anismsprovidemeaningfulprotectionagainstin-version,theyincuranon-trivialutilitycost,under-scoringtheneedformoreefficient,structure-awaredefensesinmultilingualNLPapplications.LangϵdpRouge-L↓COS↓Rouge-L↓COS↓LapMechPurMech114.110.001714.050.0156eng→eng413.580.008713.940.0348813.380.024913.450.01851213.900.034512.77-0.007611.66-0.00131.310.0136eng→fra41.70-0.00431.580.014081.420.03641.240.0166121.600.04111.440.011310.52-0.01190.490.0090eng→deu40.320.00650.540.012780.620.01870.530.0418120.440.03270.430.036711.470.00621.55-0.0090eng→spa41.43-0.00061.320.020881.700.03841.350.0266121.520.01601.410.0389Table2:Cross-lingualInversionPerformancewith|DV|=100onClassificationTasksonSNLIdatasetwithLocalDP(Inequality).Fromadefender’sperspective,↓meanslowerisbetter.7ConclusionWeproposedtwooptimization-basedparadigmsforenhancingfew-shotcrosslingualembeddingin-versions.Botharegroundedindistributedopti-mizationandoperateoveratopologicalnetworkoflanguagesconstructedvialanguagesimilarity.Thisgraphstructureenablescollaborativealign-mentofembeddingdecoders,facilitatingeffective8knowledgetransferevenwithextremelylimitedsu-pervision.Ourexperimentalresultsshowthatbothvariants-linearinequalityconstraintsandtotalvari-ationpenalties-consistentlyoutperformexistingmethods,includingALGEN.Inparticular,thetotalvariationapproachdemonstratessuperiorrobust-nessinextremelyfew-shotsettings,validatingtheimportanceofsmoothcross-lingualparametershar-ing.Thesefindingsestablishlanguagesimilarityasakeyenableroftransferableinversionattacks,andunderscoretheneedforprivacy-preservingdefensesthataccountforstructuralrelationshipsamonglanguagesinmultilingualNLPsystems.LimitationsWhileourapproachoutperformspriormethods,few-shotcrosslingualembeddinginversionremainsachallengingtaskwithsubstantialroomforim-provement.Onelimitingfactorappearstobethedecoderitself:eveninthemonolingual(originallanguage)setting,inversionaccuracyremainsmod-erate,achievingapproximatelya25Rouge-LscoreontheMMARCOEnglishdatasetwith|DV|=1k,andfurtherdeclinesundercross-lingualtransfer.Thissuggeststhatthecurrentattackdecodermaystruggletogeneralizeacrosslanguages,particu-larlywhensignalsupervisionislimited.Interestingly,weobservethatcross-lingualset-tingsexhibithighersensitivitytoDPdefenses,thoughsuchdefensesincursignificantutilitydegra-dation.Thissensitivityhighlightsboththevulner-abilityandfragilityofmultilingualembeddings.Futureworkcouldfocusonenhancingthedecodertraining,e.g.,throughmultilingualpretraining,orincorporatinglanguage-specificpriors-whichweexpectcouldimproveinversionperformanceinbothmonolingualandcrosslingualscenarios.ComputationalResourcesWefine-tunethedecoderonasingleNVIDIAA40GPU,withtrainingcompletinginjustthreehours.Notably,ALGOoperateswithminimalGPUre-sourcedemands,enablingatruefew-shotsetup.EthicsStatementWecomplywiththeACLEthicsPolicy.Thein-versionattacksimplementedinthispapercanbemisusedandpotentiallyharmfultoproprietaryem-beddings.Wediscussandexperimentwithpoten-tialmitigationanddefensemechanisms,andweencouragefurtherresearchindevelopingeffectivedefensesinthisattackspace.AcknowledgementsWYisfoundedbytheEUChipsJUandtheInnova-tionFundDenmarkthroughtheprojectCLEVER(no.101097560);YCandJBarefundedbytheCarlsbergFoundation,undertheSemperArdens:Accelerateprogramme(projectnr.CF21-0454).WefurtheracknowledgethesupportoftheAAUAICloudforprovidingcomputingresources.9ReferencesDavidIfeoluwaAdelani,GrahamNeubig,SebastianRuder,ShrutiRijhwani,MichaelBeukman,ChesterPalen-Michel,ConstantineLignos,JesujobaO.Al-abi,ShamsuddeenH.Muhammad,PeterNabende,CheikhM.BambaDione,AndiswaBukula,Roowei-therMabuya,BonaventureF.P.Dossou,BlessingSibanda,HappyBuzaaba,JonathanMukiibi,God-sonKalipe,DergueneMbaye,and26others.2022.MasakhaNER2.0:Africa-centrictransferlearningfornamedentityrecognition.InProceedingsofthe2022ConferenceonEmpiricalMethodsinNat-uralLanguageProcessing,pages4488–4508,AbuDhabi,UnitedArabEmirates.AssociationforCom-putationalLinguistics.VerenaBlaschke,MashaFedzechkina,andMaartjeterHoeve.2025.Analyzingtheeffectoflinguisticsimi-larityoncross-lingualtransfer:Tasksandexperimen-talsetupsmatter.arXivpreprintarXiv:2501.14491.LuizBonifacio,VitorJeronymo,HugoQueirozAbonizio,IsraelCampiotti,MarziehFadaee,RobertoLotufo,andRodrigoNogueira.2021.mmarco:Amultilingualversionofthemsmarcopassagerankingdataset.arXivpreprintarXiv:2108.13897.SamuelRBowman,GaborAngeli,ChristopherPotts,andChristopherDManning.2015.Alargeannotatedcorpusforlearningnaturallanguageinference.arXivpreprintarXiv:1508.05326.StephenBoyd.2010.DistributedOptimizationandStatisticalLearningviatheAlternatingDirectionMethodofMultipliers.FoundationsandTrends®inMachineLearning,3(1):1–122.NicholasCarlini,UlfarErlingsson,andNicolasPaper-not.2019.Prototypicalexamplesindeeplearning:Metrics,characteristics,andutility.YiyiChen,RussaBiswas,HeatherLent,andJohannesBjerva.2024a.Againstallodds:Overcomingty-pology,script,andlanguageconfusioninmultilin-gualembeddinginversionattacks.arXivpreprintarXiv:2408.11749.YiyiChen,HeatherLent,andJohannesBjerva.2024b.Textembeddinginversionsecurityformultilinguallanguagemodels.InProceedingsofthe62ndAnnualMeetingoftheAssociationforComputationalLin-guistics(Volume1:LongPapers),pages7808–7827.YiyiChen,QiongxiuLi,RussaBiswas,andJohannesBjerva.2025a.Largelanguagemodelsareeasilyconfused:Aquantitativemetric,securityimplica-tionsandtypologicalanalysis.InFindingsoftheAssociationforComputationalLinguistics:NAACL2025,pages3810–3827,Albuquerque,NewMexico.AssociationforComputationalLinguistics.YiyiChen,QiongkaiXu,andJohannesBjerva.2025b.Algen:Few-shotinversionattacksontextualembed-dingsusingalignmentandgeneration.arXivpreprintarXiv:2502.11308.RochelleChoenni,DanGarrette,andEkaterinaShutova.2023.Howdolanguagesinfluenceeachother?study-ingcross-lingualdatasharingduringlmfine-tuning.arXivpreprintarXiv:2305.13286.HyungWonChung,LeHou,ShayneLongpre,BarretZoph,YiTay,WilliamFedus,YunxuanLi,XuezhiWang,MostafaDehghani,SiddharthaBrahma,AlbertWebson,ShixiangShaneGu,ZhuyunDai,MiracSuzgun,XinyunChen,AakankshaChowdhery,AlexCastro-Ros,MariePellat,KevinRobinson,and16others.2022.Scalinginstruction-finetunedlanguagemodels.Preprint,arXiv:2210.11416.AlexisConneau,GuillaumeLample,RutyRinott,Ad-inaWilliams,SamuelR.Bowman,HolgerSchwenk,andVeselinStoyanov.2018.Xnli:Evaluatingcross-lingualsentencerepresentations.Preprint,arXiv:1809.05053.WietsedeVries,MartijnWieling,andMalvinaNissim.2022.Makethebestofcross-lingualtransfer:Ev-idencefromPOStaggingwithover100languages.InProceedingsofthe60thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers),pages7676–7685,Dublin,Ireland.AssociationforComputationalLinguistics.MinxinDu,XiangYue,ShermanSMChow,andHuanSun.2023.Sanitizingsentenceembeddings(andlabels)forlocaldifferentialprivacy.InProceedingsoftheACMWebConference2023,pages2349–2359.VitalyFeldman.2020.Doeslearningrequirememoriza-tion?ashorttaleaboutalongtail.InProceedingsofthe52ndAnnualACMSIGACTSymposiumonTheoryofComputing,pages954–959.RichardHeusdensandGuoqiangZhang.2024a.Distributednonlinearconicoptimisationwithpartiallyseparablestructure.arXivpreprintarXiv:2405.09490.RichardHeusdensandGuoqiangZhang.2024b.Dis-tributedoptimisationwithlinearequalityandinequal-ityconstraintsusingpdmm.IEEETransactionsonSignalandInformationProcessingoverNetworks.Yu-HsiangHuang,YucheTsai,HsiangHsiao,Hong-YiLin,andShou-DeLin.2024.TransferableEmbed-dingInversionAttack:UncoveringPrivacyRisksinTextEmbeddingswithoutModelQueries.InPro-ceedingsofthe62ndAnnualMeetingoftheAssocia-tionforComputationalLinguistics(Volume1:LongPapers),pages4193–4205,Bangkok,Thailand.As-sociationforComputationalLinguistics.AnneLauscher,VinitRavishankar,IvanVulic,andGoranGlavas.2020.Fromzerotohero:Onthelimitationsofzero-shotlanguagetransferwithmul-tilingualtransformers.InConferenceonEmpiricalMethodsinNaturalLanguageProcessing.HaoranLi,MingshiXu,andYangqiuSong.2023.Sen-tenceembeddingleaksmoreinformationthanyouexpect:Generativeembeddinginversionattackto10recoverthewholesentence.InFindingsoftheAs-sociationforComputationalLinguistics:ACL2023,pages14022–14040,Toronto,Canada.AssociationforComputationalLinguistics.QiongxiuLi,RichardHeusdens,andMadsGræsbøllChristensen.2020.Privacy-preservingdistributedoptimizationviasubspaceperturbation:Ageneralframework.IEEETransactionsonSignalProcessing,68:5983–5996.Chin-YewLin.2004.Rouge:Apackageforautomaticevaluationofsummaries.InTextsummarizationbranchesout,pages74–81.PatrickLittell,DavidRMortensen,KeLin,KatherineKairis,CarlisleTurner,andLoriLevin.2017.Urielandlang2vec:Representinglanguagesastypological,geographical,andphylogeneticvectors.InProceed-ingsofthe15thConferenceoftheEuropeanChap-teroftheAssociationforComputationalLinguistics:Volume2,ShortPapers,pages8–14.BrendanMcMahan,EiderMoore,DanielRamage,SethHampson,andBlaiseAguerayArcas.2017.Communication-efficientlearningofdeepnetworksfromdecentralizeddata.InArtificialintelligenceandstatistics,pages1273–1282.PMLR.JohnXMorris,VolodymyrKuleshov,VitalyShmatikov,andAlexanderMRush.2023.Textembeddingsreveal(almost)asmuchastext.arXivpreprintarXiv:2310.06816.YuyuanOuyang,YunmeiChen,GuanghuiLan,andEduardoPasiliaoJr.2015.Anacceleratedlinearizedalternatingdirectionmethodofmultipliers.SIAMJournalonImagingSciences,8(1):644–681.JiePeng,WeiyuLi,andQingLing.2021.Byzantine-robustdecentralizedstochasticoptimizationoverstaticandtime-varyingnetworks.SignalProcess-ing,183:108020.FredPhilippy,SiwenGuo,andShohrehHaddadan.2023.Towardsacommonunderstandingofcon-tributingfactorsforcross-lingualtransferinmulti-linguallanguagemodels:Areview.arXivpreprintarXiv:2305.16768.MichaelRabbatandRobertNowak.2004.Distributedoptimizationinsensornetworks.InProceedingsofthe3rdinternationalsymposiumonInformationprocessinginsensornetworks,pages20–27.CongzhengSongandAnanthRaghunathan.2020.In-formationleakageinembeddingmodels.InPro-ceedingsofthe2020ACMSIGSACConferenceonComputerandCommunicationsSecurity,CCS’20,page377–390,NewYork,NY,USA.AssociationforComputingMachinery.HuahuaWangandArindamBanerjee.2014.Bregmanalternatingdirectionmethodofmultipliers.Advancesinneuralinformationprocessingsystems,27.LiangWang,NanYang,XiaolongHuang,BinxingJiao,LinjunYang,DaxinJiang,RanganMajumder,andFuruWei.2022.Textembeddingsbyweakly-supervisedcontrastivepre-training.arXivpreprintarXiv:2212.03533.SørenWichmann,EricW.Holman,andCecilH.Brown.2022.CLDFdatasetderivedfromWichmannetal.’s"ASJPDatabase"v20from2022.LintingXue,NoahConstant,AdamRoberts,MihirKale,RamiAl-Rfou,AdityaSiddhant,AdityaBarua,andColinRaffel.2020.mt5:Amassivelymultilingualpre-trainedtext-to-texttransformer.arXivpreprintarXiv:2010.11934.WenruiYu,QiongxiuLi,MilanLopuhaä-Zwakenberg,MadsGræsbøllChristensen,andRichardHeusdens.2024.Provableprivacyadvantagesofdecentralizedfederatedlearningviadistributedoptimization.IEEETransactionsonInformationForensicsandSecurity.GuoqiangZhangandRichardHeusdens.2017.Dis-tributedoptimizationusingtheprimal-dualmethodofmultipliers.IEEETransactionsonSignalandIn-formationProcessingoverNetworks,4(1):173–187.11ADerivationofNormalEquationTheoptimalalignmentmatrixWisobtainedbyminimizingacostfunctionJthatquantifiesthediscrepancybetweentheattackembeddingma-trixEAandthetransformedvictimembeddingsEV→A=EVW:J(W)=12(EA−EVW)T(EA−EVW)=12(ETAEA−ETAEVW−(EVW)TEA+(EVW)TEVW)=12(ETAEA−ETAEVW−WTETVEA+WTETVEVW).(3)BycalculatingthederivativesofJ(W),wehave∇WJ(W)=12∇W(ETAEA−ETAEVW−WTETVEA+WTETVEVW)=2ETVEVW−2ETVEA.(4)TheoptimizedWisachievedwhenthederivativeisequalto0,ETVEVW=ETVEA.(5)Then,thematrixWthatminimizesJ(W)isW=(ETVEV)−1ETVEA.(6)BTopologyConstructionToillustratethisapproach,considerthesyntacticdistancematrixobtainedfromLang2vecforEn-glish(eng),French(fra),andItalian(ita):D=00.460.510.4600.550.510.550whereeachentryDijrepresentsthesyntacticdis-similaritybetweenlanguagepairs.Byapplyingdif-ferentthresholdvaluesr,weconstructdistincttopo-logicalconfigurationsoflanguagerelationships.Fig.8demonstrateshowthenetworkconnectivityvarieswithincreasingrvalues,revealing:•Atr=0.45:Noedgesform•Atr=0.47:eng-fraconnectionemerges•Atr=0.52:eng-itaconnectionappearswhilefra-itaremainsdisconnected•Atr=0.56:CompletegraphformsFigure8:LinguistictopologicalgraphofEnglish,FrenchandItalianwithdifferentthresholdr.Thehigherthethreshold,thedensertheconnectivity.COtherExperimentalResultsFigure9:Cross-lingualInversionPerformanceswithLang2vecGraphinCosineSimilaritiesacrossTrainingSamples.12ModelHuggingfaceArchitecture#LanguagesDimensionFLAN-T5(Chungetal.,2022)google/flan-t5-smallEncoder-Decoder60512E5-SMALL-V2(Wangetal.,2022)intfloat/e5-small-v2Encoder1384MT5(Xueetal.,2020)google/mt5-baseEncoder-Decoder102768TEXT-EMBEDDING-ADA-002OpenAIAPIEncoder100+1536Table3:DetailsofLLMsandEmbeddings.Figure10:Cross-lingualInversionPerformanceswithADA-2VictimModelinRouge-LScoresacrossTrain-ingSamples.Figure11:Cross-lingualInversionPerformanceswithE5VictimModelinRouge-LScoresacrossTrainingSamples.13Figure12:Cross-lingualInversionPerformanceswithADA-2VictimModelinCosineSimilaritiesacrossTrainingSamples.Figure13:Cross-lingualInversionPerformanceswithE5VictimModelinCosineSimilaritiesacrossTrainingSamples.Figure14:Cross-lingualInversionPerformanceswithAttackModeltrainedinSpanishinCosineSimilaritiesacrossTrainingSamples.LangϵdpRouge-L↓COS↓Rouge-L↓COS↓LapMechPurMech113.160.075113.350.0199eng->eng412.950.025712.610.0510814.010.084513.880.13201213.520.172013.860.116211.60-0.01681.90-0.0189eng->fra41.77-0.01612.100.108182.020.10402.100.1428121.920.12712.460.185310.860.00800.62-0.0240eng->deu40.990.02590.62-0.021680.770.09600.640.0881120.700.18151.220.194411.580.04311.780.0729eng->sap41.350.03181.450.036081.870.24081.940.1119121.650.18752.290.1846Table4:Cross-lingualInversionPerformancewith|DV|=100onClassificationTasksonSNLIdatasetwithLocalDP(TotalVariation).Fromadefender’sperspec-tive,↓meanslowerarebetter.14Figure15:Cross-lingualInversionPerformanceonClassificationTasksonSNLIdatasetwithLocalDP(ϵdp=12)inCosineSimilarities.Figure16:Cross-lingualInversionPerformanceonClassificationTasksonSNLIdatasetwithLocalDP(ϵdp=12)inRouge-LScores.15
