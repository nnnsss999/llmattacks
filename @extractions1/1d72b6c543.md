---
title: http://arxiv.org/pdf/2401.12192
source_url: http://arxiv.org/pdf/2401.12192
date_collected: '2025-06-19'
license: Fair Use
---

TextEmbeddingInversionSecurityforMultilingualLanguageModelsYiyiChenHeatherLentJohannesBjervaDepartmentofComputerScience,AalborgUniversity,Denmark{yiyic,hcle,jbjerva}@cs.aau.dkAbstractTextualdataisoftenrepresentedasreal-numberedembeddingsinNLP,particularlywiththepopularityoflargelanguagemod-els(LLMs)andEmbeddingsasaService(EaaS).However,storingsensitiveinformationasembeddingscanbesusceptibletosecuritybreaches,asresearchshowsthattextcanbereconstructedfromembeddings,evenwithoutknowledgeoftheunderlyingmodel.Whilede-fencemechanismshavebeenexplored,theseareexclusivelyfocusedonEnglish,leavingotherlanguagespotentiallyexposedtoattacks.ThisworkexploresLLMsecuritythroughmul-tilingualembeddinginversion.Wedefinetheproblemofblack-boxmultilingualandcross-lingualinversionattacks,andexploretheirpo-tentialimplications.OurfindingssuggestthatmultilingualLLMsmaybemorevulnerabletoinversionattacks,inpartbecauseEnglish-baseddefencesmaybeineffective.Toalleviatethis,weproposeasimplemaskingdefenseeffectiveforbothmonolingualandmultilingualmodels.Thisstudyisthefirsttoinvestigatemultilingualinversionattacks,sheddinglightonthediffer-encesinattacksanddefensesacrossmonolin-gualandmultilingualsettings.1IntroductionIndustrialapplicationsofnaturallanguageprocess-ing(NLP)typicallyutilizelanguagemodels(LMs)andoftenrelyonvectordatabasesviaframeworkssuchasEmbeddingsasaService(EaaS).Inthiscontext,sentenceembeddingsarestoredinare-motedatabase,asopposedtorawtext,allowingend-userstoefficientlysearchacrosscondensedrepresentations.Asembeddingsarenothuman-readable,securityoftheencodedinformationmaybenaivelyassumed,howeverrecentworkshavedemonstratedthatembeddingsarenosaferthanrawtext;theyaresusceptibletoinversionattacks,wherebyamaliciousactorcantrainmodelstode-codeembeddings,thusexposingprivateinforma-tion(SongandRaghunathan,2020;Morrisetal.,Figure1:Schematicoverviewofatextembeddinginver-sionattack.AuseraccessesanEaaSprovider,whileanattackeriseavesdropping.Althoughtheattackerhasnodirectaccesstotheembeddingmodel,theycanreliablydecodetheinformationstoredintheembeddings.2023;Zhouetal.,2023).Concretely,aftergainingaccesstoembeddingsandtheblack-boxembedderviatheEaaSAPI,themaliciousactorcantrainanexternalmodel,whichapproximatestheinversionfunctionthatreconstructsthetextfromtheem-beddings.Assuch,thereisasubstantialthreattoprivacyifmaliciousactorsareabletoeavesdroponcommunicationchannelsbetweenEaaSprovidersandcustomers,asillustratedinFigure2.Previousworkhasshownthatanexactmatchfordatarecreationcanbeobtainedinspecificset-tings,albeitwiththelimitationofassumingmono-lingualEnglishmodelsandembeddings(Morrisetal.,2023).However,inreal-worldscenarios,eavesdroppersmaynotknowthesourcelanguageoftheencodedtext,asEaaSproviderscanhaveinternationalclientele.ThustoassessthecurrentlevelofriskposedtomultilingualLMs,weintro-ducemultilingualinversionattacks.Asthefirsteverstudyinthisdirection,wefocusspecificallyonexacttextreconstruction,assumingthatthelan-arXiv:2401.12192v4  [cs.CL]  5 Jun 2024guageofatargetembeddingisunknown.Lever-agingastate-of-the-artmultilingualblack-boxen-coder,wefindthatthetrainedmodelcanrecon-structtextsincertainlanguagesmoreeffectivelythanmonolingualcounterparts.Additionally,wealsointroducecross-lingualinversionattacks,toascertainwhetherinversionattackscanbesuccess-fulwhenthetargetlanguageisunknownbytheattacker.Wethusattemptcross-lingualtextrecon-struction(i.e.,reconstructingGermantextwithamodelnottrainedonGermanreconstruction),intro-ducinganAdhocTranslationmethodtoovercometheevaluationlimitationofcurrentstring-matchingmetricsinthiscross-lingualscenario.Finally,weassesstheefficacyofanexistingdefensemethodbyMorrisetal.(2023),ultimatelyfindingthatde-fensesintendedformonolingualmodelsfallshortinprotectingmultilingualmodels.Tothisend,weintroducesimplemaskingdefense,whichproveseffectiveforbothmonolingualandmultilingualmodels,andwhichalsodoesnotrequireadditionalmodeltraining.Allourtrainedinversionmodels1andcode2areopensource,encouragingthere-searchcommunitytoengageindevelopmentofdefensesforvulnerablemultilingualmodels.2RelatedWorkModelsarewellknowntomemorizetrainingdata,andarethereforesusceptibletoleakingprivatein-formation(Shokrietal.,2016;Carlinietal.,2018;Nasretal.,2019).Assuch,thereisincreasedre-searchinterestinexploringthisvulnerabilitytoinversionattacksfromtheperspectiveofcyber-security,simulatingattacksagainstmodelstorecre-atesensitivetrainingdata.Workinthisdirec-tionhasbeenconductedacrossvariousdomainsofmachinelearning,suchascomputationalgenetics(Fredriksonetal.,2014),computervision(Fredrik-sonetal.,2015),andmorerecentlyNLP(SongandRaghunathan,2020).Generally,suchworksattheintersectionofmachinelearningandcyber-security(e.g.,oninversionattacksoradversarialattacks)makeassumptionsabouttheimaginedattacker’slevelsofaccesstothevictimmodel.White-boxscenariosassumeattackeraccesstothefullmodel(Wallaceetal.,2019;Tsymboietal.,2023),re-sultinginmanypossibleattacksurfaces.PreviousworksinNLPhaveshownthatitispossibletore-trievesensitivetrainingdatabyattackingmodels1https://huggingface.co/yiyic/2https://github.com/siebeniris/MultiVec2Text/directly(Fredriksonetal.,2014,2015),attackinggradients(Zhuetal.,2019;Dengetal.,2021),aswellasthroughleveragingleakedhiddenstates(Lietal.,2022).Meanwhile,black-boxattacksassumeanattackerhasnoknowledgeoftheunderlyingmodelitself,andcanonlyinteractwithmodelsatthemostabstractedlevel(e.g.,provideinputandregisteroutputthroughanAPI).Forexample,Car-linietal.(2020)areabletoextractsensitivetrainingdata(e.g.,namesandphonenumbers)fromGPT-2(Radfordetal.,2019a),byfirstgeneratingdatafromthemodelandthenusingmembershipinfer-enceattackstofilterutteranceslikelytobepartoftheoriginaltrainingdata.Inembeddinginversionattacks,animaginedat-tackeraimstorecreatetextfromthedistributedrepresentations.Asopposedtoamachinetransla-tionsetting,thisscenarioassumesnoaccesstoasourcetextxtoconditionon,andthegoalisnottodecodeatranslationofx,butrathertorecre-atetheexacttextofx—withnoinputotherthantheembeddingϕ(x),givenϕasanencoder.SongandRaghunathan(2020)showedthat50%–70%percentoftokenscouldberecoveredinsuchaset-ting.Subsequentattackshavefurtherimprovedoverthismetric,withnewerapproachesnowabletoretrieveentiresentencesofencodedtext(Höh-mannetal.,2021;Hayetetal.,2022;Morrisetal.,2023;Lietal.,2023).Existingdefensemecha-nismsincluderandomlyperturbingembeddings(Zhouetal.,2023)andparameter-efficientfine-tuning(Zhangetal.,2023).Othermethodsforsecuringembeddingsincludeencryption(Huangetal.,2020;XieandHong,2021)anddifferen-tialprivacy(Lyuetal.,2020).However,untilem-beddingprivacyisensured,inversionattackswillremainathreat,necessitatingfurtherinvestigation.Finally,previousworksonembeddinginversionhavebeenconfinedtomonolingualsettingscon-cerningEnglish(SongandRaghunathan,2020;Lyuetal.,2020;Hayetetal.,2022;Parikhetal.,2022;Kimetal.,2022;Morrisetal.,2023;Zhouetal.,2023;Lietal.,2023).Thisleavesdefensesfornon-Englishlanguagesandmultilingualmodelsun-explored,potentiallycompromisingmodelsecurityforthoselanguages.Asaresult,thevulnerabilityofmultilingualmodelsandnon-Englishmodelsremainsanopenquestion.Figure2:OverviewofMultilingualVec2Text,extendingVec2Text(Morrisetal.,2023)withAdhocTranslationandMaskingDefenseMechanism(outlinedinthegreendashedlineframe).GivenaccesstoatargetembeddingeandqueryaccesstotheembedderϕviaanEaaSAPI,theinversionmodelψiterativelygenerateshypothesesˆetoattainthetarget.ThegeneratedtextˆxisinGerman,andtranslatedtoEnglish(AdTrans(ˆx)),tobecomparedwiththetargettextx.ThemaskingdefenseservesasaneffectivedefenseagainstinversionattackswhilepreservingutilityinNLPtaskssuchasretrieval.3MethodologyInthiswork,weconsiderascenariowhereama-liciousactorhasillegitimatelyobtainedbothem-beddingsandAPIaccesstotheblack-boxencoder,asshowninFigure1.Togaugethevulnerabil-ityofmultilingualmodelsagainstblack-boxem-beddinginversionattacks,webuilduponpreviousworkbyMorrisetal.(2023),extendingtheirattackmethodtoamultilingualsetting,aimingtoinvertsentenceembeddingsproducedbyamultilingualmodel.Wedefinetheattackscenarioformallyasfollows:givenasensitivetextsequencexandablack-boxencoderϕ,thegoalistorecoverxfromtheembeddingobtainedviaϕ(x)usinganexternalattackermodelψ.However,wecanonlyaccessϕthroughanEaaSAPI,anditsarchitectureandpa-rametersareinaccessible.Tothisend,weexploretheefficacyofexistingdefensesinthisscenario,andintroduceanoveldefensemechanism.Weapproachembeddinginversionattacksinthecontextoftextgeneration,consideringthegener-ationmodels’efficacyinsuchattacks(Lietal.,2023;Morrisetal.,2023).Inthisscenario,thegenerationmodelψconditionswhatinformationcanbeencodedanddecoded,withconsequencesfortextreconstruction.Forexample,ifψissolelypre-trainedonLatinscript,itcannothandleCyrillicorDevanagariscripts.Consequently,reconstruct-ingtextinunknownscriptsispresentlyinfeasible,andwhethertextinunknownscriptscanberecon-structedremainsunexplored.Hence,ourstudyinvestigatestextreconstructioninunknownlan-guageswithinthesamescript(i.e.,Latin).MultilingualInversionAttacksComparedtomonolingualembeddinginversion,investigatingmultilingualinversionattacksintroducessignifi-cantcomplexity,aseachlanguagespaceofψ,ϕ,x,andtrainingdataiscrucial.Forinstance,thetrainingscaleforattackermodelsincreaseswiththenumberoflanguagesandcontrolledparameters,suchasmaximalsequencelength(cf.Section4).Weexplorethepotentialofmultilingualembed-dinginversionassumingunlimitedqueriescanbesenttotheblack-boxϕ,obtainingembeddingsϕ(x)forx∈D,whereDisthetrainingdataset.Follow-ingtheapproximationapproachfromMorrisetal.(2023),wesearchfortextˆxclosesttothetargetembeddingeunderϕusingtheformula:ˆx=argmaxxcos(ϕ(x),e)(1)Inparticular,asillustratedinFigure2,thetrain-ingandinferenceoftheinversionmodelarecon-ditionedonthepreviousoutput.Atcorrectionstept+1,themodeltakestheconcatenationofthepre-viousoutputˆx(t),hypothesisembeddingˆe(t),andtargetembeddinge.Withthiscontextnoted,themultilingualembeddinginversionattackiscom-posedofthefollowingsteps:•BasemodelModelTraining:Developanattackermodelψbasedonatextgenera-tionmodelpre-trainedonthesamelanguagescripts;•CorrectionModelTraining:Trainψbyqueryingtheblack-boxembeddingmodelϕwithtextx∈D,resultinginˆxoptimizedus-ingEq.1(correctionstep1).•Inference:Executeembeddinginversionat-tacksontextsinthetargetlanguageltusingthetrainedinversionmodelψ.Furtheropti-mization(correctionsteps>1)isperformedwithEq.1,combinedwithbeamsearchatse-quencelevel.Cross-LingualInversionAttacksInamultilin-gualsettingweassumethattheinversionmodelistrainedonseverallanguages,includingthetargettextlanguagelt.However,thisisanunrealisticsettingwhichrequiresimmensecomputationalre-sources.Wethereforeinvestigateacross-lingualsetting,inwhichtheaggressordoesnotknowthetruelanguageofthetargettextlt.Concretely,weinvestigatetheextentitispossibletoexecutein-versionattacksleveragingamonolingualinversionmodeltrainedonadifferentsourcelanguagelsthanthetargetlt,thusintroducingacross-lingualattack.Asthetextgeneratedbythemonolingualinversionmodelwillbeinls,currentstring-matchingmet-ricsforevaluatinginversionattacks,suchasBLEU,arenotapplicablehere,astherewillbelittleornooverlapbetweenthelsandltstrings,evenwhentheunderlyingmeaningofthetwoisthesame.Inordertoevaluatethesuccessofthecross-lingualinversionmodel,weproposeapost-interventionstrategyAdhocTranslation(AdTrans),asshowninFigure2.Inthissetup,thegeneratedtextisfirsttranslatedfromlsinltusingEasyNMT3.Thenthetranslatedtextisevaluatedagainstthetargettext,toverifywhethertheinvertedtextinlscanindeedun-coverthetargettextinunknownlt(cf.Section5.2).AsAdTranshingesupontheavailabilityofareli-ablemachinetranslationmodelforthepertinentlanguages,thisusecasehighlightstheexistinglim-itationsincurrentevaluationmetricsforassessingthethreatposedbycross-lingualinversionattacks,andtheneedforcontinuedresearchinthisspace.4ExperimentalSetupEnglishEmbeddingsWereproducetheresultsfromMorrisetal.(2023)bytraininginversionmod-elsonGTR-base(Nietal.,2022)4onEnglishdataset.FullresultscanbefoundinAppendixB.MultilingualEmbeddingsWeuseT5-base(Raffeletal.,2023)asourgenerationmodel.Forthemultilingualinversionmodelsψ,wetrainonastate-of-the-artmultilingualencoderϕ:multilingual-e5-base(ME5-base)5(Wangetal.,2022),whichisapre-trainedtransformerbased3https://github.com/UKPLab/EasyNMT4Huggingface:sentence-transformers/gtr-t5-base5Huggingface:intfloat/multilingual-e5-baseonXLM-R(Conneauetal.,2020),andnotedtobeoneofthebestperformingmultilingualmodelsaccordingtoMTEB(Muennighoffetal.,2023).DatasetsPreviousresearch(Morrisetal.,2023)trainsinversionmodelsonnaturalques-tionsandquestion-answerpairs,suchasMS-Marco(Bajajetal.,2018)andNaturalQuestions(NQ)(Kwiatkowskietal.,2019).Whilethesedatasetsareadvantageouslylarge,theyarelim-itedtoEnglish.Thusforourexperiments,wetrainandevaluatethemultilingualinversionmodelsonMTG,abenchmarksuitetailoredformultilin-gualtextgenerationtrainingandevaluation(Chenetal.,2022),withparallelsamplesacrosslan-guages.MTGiscuratedfromdifferentdomains,includingnews,dailylife,andWikipedia.Inordertoensurethevalidityofourexperiments,andtestgeneralizability,weexcludethedatacuratedfromWikipedia,asthisdomaindatawasalreadyusedtotrainbothT5-baseandME5-basemodels.Foreachlanguage,thisresultsin123kpassages(i.e.,paragraphsorsectionsofadocument)availablefortrainingdata.Weobtain3-5Msentencesfortrainingand2keachforvalidationandtestineachlanguageusingNLTK(BirdandLoper,2004)sen-tencetokenization.Thisisconsiderablyfewertrain-ingsamplesascomparedtoMorrisetal.(2023),wheretheirGTR-basemodelwastrainedon5MpassagesfromNQ6.Meanwhile,wetrainandeval-uateondatainEnglish,French,GermanandSpan-ish,notedasMTG-EN,MTG-FR,MTG-DE,andMTG-ES,respectively.Wealsocomposea5M-sentencemultilingualdatasetfortraininginclud-ing1.25Msentencesfromeachlanguage,notedasMTG-MULTI.WenotethattoreproducethefindingspresentedbyMorrisetal.(2023),atestsetcomprising500sampleswasutilized.Allrecon-structionresultsarethereforebasedon500samplesfromtheregardingtestdata.MetricsTobecomparablewithMorrisetal.(2023),weassessmodelperformanceusingtwotypesofmetrics.First,fortextreconstruction,weemploythefollowingword-matchmetrics:BLEU(Post,2018),measuringn-gramsimilar-itiesbetweenthetrueandreconstructedtext;6Themodelstruncatetextsinto32tokensand64tokens,toevaluatehowsequencelengthaffectstheperformanceofem-beddingsinversion.EachpassageinNQissignificantlylongerthan32and64tokens.ToobtainmoretrainingdatasamplesfromMTG,weimplementNLTKsentencetokenizationonMTGdataset,resultinginsentenceswithunevendistributionoftokenslength(cf.AppendixA).ROUGE(Lin,2004),reportingtherecallofoverlap-pingwordsofreconstructedtext;TokenF1,whichcalculatesthemulti-classF1scoresbetweenpre-dictedtokensandtruetokens,consideringeachwordasaclass;andExact-match,representingthepercentageofperfectlymatchingreconstructedtextstothetruetexts.Wealsocomputethecosinesimilaritybetweenthetrueembeddingandtheem-beddingofthereconstructedtextintheembeddingspaceofthetrainedϕ.However,suchmetricsfallshortintermsofevaluatingtherecoveryofthesemanticcontent,especiallyregardingspecificpri-vateinformation.Thelimitationisparticularlyevi-dentincross-lingualsettings,forexample,wherethegeneratedGermantextconveyssimilarmeaningastheinputEnglishtext,anuancethatword-matchmetricsfailtocapture(seeFigure2).EvaluationIntextgeneration,exploringthevastspaceofpossiblesequencesexhaustivelyisinfea-sible.Hence,weemploybeamsearchatthese-quenceleveltoapproximatethesumofimmediatetextgenerations.FollowingMorrisetal.(2023),theinferenceisconductedgreedilyatthetokenlevelandbeamsearchisemployedatthesequencelevel.Ateverystageofcorrection,asetnumberbofpotentialcorrectionsisevaluated.Foreachpo-tentialcorrection,thetopbfeasiblecontinuationsaredecoded.Fromthepoolofb·bpotentialcon-tinuations,thebuniqueonesareselectedbasedontheirembeddingspacedistancefromthereferenceembeddinge.Inthisstudy,weanalyzeinferenceusingvaryingnumbersofcorrectionsteps(1,20,50,and100)alongwithsequencebeamwidths(sbeam)of4and8.Weexploretheimpactofevaluationstepsincomparisontoruntimeandobservethatevaluationruntimedoublesfrom50to100stepswithsbeam,whiletheadditionalperformancegainsarenegligi-ble(seeFigure6inAppendixC).Thus,wereporttheevaluationresultsuntil50stepswith8sbeam.ExperimentsWetrainaninversionbasemodelandVec2Textcorrectormodel,asdescribedinSec-tion3.Todeterminethepotentialofmultilingualembeddinginversionattacks,wetrainbasemod-elsandVec2TextmodelsspecificallyforMTG-MULTI;forcross-lingualattacks,wetrainthesemodelsforeachlanguage.Incomparisonwithpre-viousresearch,wetrainandevaluateME5-basedinversionmodelsonNQ,i.e.,ME5_NQ.WeusetheAdamoptimizerwiththelearningrateof2e−5,epsilonof1e−6,and1000warm-upstepsataconstantwarm-upschedule.Eachbaseandcorrectormodelistrainedfor100epochs.Duetotheprohibitivecomputationalresourcesneededfortraininginversionmodels,welimiteachmodeltoasingletrainingrun.Forinversionmodels,weuseabatchsizeof512,whilecorrectormodels,trainedondatawith32tokens,haveabatchsizeof256.Batchsizesarehalvedformodelstrainedondatatruncatedto64tokens7.Allmodelsaretrainedon4AMDMI250GPUswithdistributedtraining.Underthesecircumstances,trainingourslowestmodeltakesabout8days.5AttackingMultilingualLanguageModelsToexplorethepotentialofmultilingualembeddinginversion,wetrainME5-baseembedderonMTGdatainEnglish,German,French,andSpanish,i.e.,ME5_EN,ME5_FR,ME5_DEandME5_ES,respec-tively,andthecomposedmultilingualdatasetoffourlanguages,i.e.,ME5_MULTI,andtestoneachlanguageforbothsettings,seeresultsinTable1.Tosimulatemorerealisticattacks,weconductthor-oughcross-domainevaluation(cf.AppendixG).5.1MultilingualTextReconstructionMonolingualTextReconstructioninMultipleLanguagesWeobservethattheBLEUscoreforeachlanguagepeaksby50stepscorrectionwith8sbeam.Moreover,Spanishmodelsoutper-formtheothersintermsoftheword-matchmetricsacrosscorrectionsteps,achieving80.02onBLEUwith65%ofexactmatch.Despitehavingalargervolumeofdatacomparedtootherlanguages,theEnglishmodelunexpectedlyperformstheworstacrossvariousmetrics,asillustratedbythetrainingdatadistributioninAppendixAFigure5.How-ever,weshowinAppendixE,theevaluationofround-triptranslatedEnglishtestdataindicatesnoevidenceoftranslationeseeffect.Additionally,experimentsandresultsforembeddinginversionoverFinnishandHungariancanbefoundinAp-pendixD,providingadditionalinsightstotheprob-lemofmultilingualVec2Tex,beyondhigh-resourceRomanceandGermaniclanguages.There,weob-servesub-parperformancefortextreconstruction(see:Table6ofAppendixD),highlightingtheneedtostudyawidervarietyoflanguagesinthefuture.7Themoredetailedsettingsforhyper-parametersareillus-tratedintheGitHubrepository.#Tokens#PredTok.BLEUROUGETF1ExactCOSMONOMULTIMONOMULTIMONOMULTIMONOMULTIMONOMULTIMONOMULTIMONOMULTIMTG-ENBase(0Steps)323231.9431.9511.5710.7945.9844.3944.9743.71000.93810.9215Vec2Text(1Step)323231.9531.9618.313.3858.7448.9556.3748.220.40.20.92360.8637(20Steps)323231.9931.9841.4823.7279.0562.5375.1559.748.830.94410.8433(50Steps)323231.9931.9743.0525.2780.264.1476.2961.399.43.20.94640.9296(50Steps+4sbeam)323231.9931.9845.8729.8982.768.1778.2465.2710.850.93720.9487(50Steps+8sbeam)323231.9831.9848.4932.0483.5169.3879.1666.67127.40.92770.9303MTG-FRBase[0Steps]3232323218.6419.8152.8655.252.9355.6800.20.94080.9511Vec2Text(1Step)32323231.9829.128.3263.5863.0863.3663.12.620.96550.9271(20Steps)323231.983262.3958.7884.1281.3283.4881.0236320.97520.9492(50Steps)323231.983264.0460.7585.1883.0184.5182.4936.8330.97540.9252(50Steps+4sbeam)3232323271.9668.7288.2986.787.9186.2250.445.20.96430.942(50Steps+8sbeam)3232323274.547389.1289.3888.8388.8454.449.60.97570.942MTG-DEBase(0Steps)32323231.9813.313.743.1345.2444.646.14000.95990.9642Vec2Text(1step)323231.9331.982218.0855.5551.955652.071.20.20.96990.9516(20Steps)323231.953256.641.3780.9570.4179.8469.8130.216.60.95730.9232(50Steps)323231.953257.3643.5982.3372.2881.471.5430.417.40.96870.9278(50Steps+4sbeam)323231.9831.9865.7952.4885.8476.784.5675.7542.428.20.97780.9321(50Steps+8sbeam)3232323269.554.0887.877.5786.4676.4447.429.60.96710.9646MTG-ESBase(0steps)323231.953223.2127.0955.1560.5456.7562.071.61.80.9380.9501Vec2Text(1step)3232323235.1836.9266.2168.0467.7668.9289.60.95490.9423(20Steps)3232323266.6164.4385.5984.6185.7884.7344.838.40.96320.9563(50Steps)3232323267.8565.9386.6185.2586.6785.4645.438.80.96970.9582(50Steps+4sbeam)3232323277.2974.5290.4189.4590.4789.2360.853.60.96970.9515(50Steps+8sbeam)3232323280.0277.7291.3490.7291.5490.446556.80.95790.987Table1:MONOevaluatesTextReconstructioninmultiplelanguages,trainedandevaluatedonMTGdatasetswithtokenslength32inEnglish,French,German,andSpanish,respectively.MULTIevaluatesmultilingualtextreconstruction,trainedonMTG-MULTIandevaluatedonMTGdatasetsinthesamelanguages.Thebestresultsacrossmetricsforeachlanguageareinbold,withinstanceswhereMULTIoutperformsMONOunderlined.MultilingualTextReconstructionWithoutPriorKnowledgeofLanguageToevaluatethepoten-tialofmultilingualtextinversionwithoutpriorknowledgeofthetargetlanguage,wetraininver-sionmodelsonMTG-MULTI.AsshowninTa-ble1,ME5_MULTIbasemodeloutperforms(under-lined)ormatchestheperformanceofmonolingualbasemodelsacrosslanguages.Despiteeachlan-guageinMTG-MULTIhavingaquarterofthedatavolumecomparedtoitsmonolingualcounterpart,overallperformanceremainscomparable,particu-larlyevidentforFrenchandSpanish.ForSpanish,ME5_MULTIslightlyoutperformsinword-matchmetricsthanME5_ESalsoforVec2Textmodelby1stepcorrection.Acrosslanguages,theinitial(basemodel)cosinesimilaritiesoftheME5_MULTIex-ceedthoseofitsmonolingualcounterparts,exceptforEnglish.Moreover,weconductqualitativeanalysisontextreconstructionusingME5_MULTIonparallelsamples,inTable2and11(cf.AppendixH).Over-all,thelowerthecosinesimilarityofStep1,thefewerstepsthemodelneedstogeneratetheex-actmatch.Thesephenomenasuggestthat(i)highmonolingualdatavolumeisnotthesoledetermi-nantofhigh-performingbaseand1-stepVec2Textmodelsinbothmonolingualandmultilingualset-tings,(ii)multilingualtrainingyieldscloserem-beddingsofreconstructedandtargettextsintheembeddingspace,and(iii)theoptimizationap-proachutilizingcosinesimilarityisnotaseffectiveformultilingualtrainingcomparedtomonolingual.5.2Cross-lingualTextReconstructionCross-lingualtextreconstructionassumesnopriorknowledgeofthetargetlanguage,andthustheem-bedderϕistrainedonadifferentsourcelanguagethanthetargettextforevaluation.Toinvestigatethepotentialofthisscenario,weconductcross-lingualevaluationonallthemonolingualmodels,theresultsonin-domainMTGarereportedinTa-ble3.WeobservethatME5-basemodelstrainedonbothNQandMTGdatasetshaveatendencytodecodetexts,forexampleˆx,inthelanguageoftrainingdata,e.g.,ls,giventhetargettextxwhichisinadifferentlanguage,e.g.,lt.However,ˆxcouldconveythesameinformationinanotherlanguage,butcurrentword-matchmetricsarenotabletocap-turethis.Thustheprivacyleakagestillexists.StepTextBLEUCOSInputfordurgedtorecall1.3millionsuvsoverexhaustfumesStep1fordurgedtorecallfumesfrom1.3millionsuvs39.940.8056Step2fordurgedtorecall1.3millionsuvsfromoversowingfumes66.060.9514Step3fordurgedtorecall1.3millionsuvsomittedfumes67.170.8764Step4fordurgedtorecall1.3millionsuvsoverfumingfumes67.170.8484Step5fordurgedtorecall1.3millionsuvsofexhaustfumes70.710.9656Step6fordurgedtorecall1.3millionsuvsoverexhaustfumes1000.9653Inputfordwirdaufgefordert1,3millionensuvswegenabgasenzurückzurufenStep1fordistauf1,3millionensuvszurückgefordertgasabgerufen19.490.8704Step2fordistauf1,3millionensuvsinabgaszurückgefordert19.070.8911Step3fordistvon1,3millionensuvswegenabgaszurückgerufen31.560.9592Step4fordistangerufen,dass1,3millionensuvswegenabgaszurückgerufenwerden22.420.9376Step5fordwirdaufgefordert,1,3millionensuvsaufgrundvonabgaszurückzurufen24.380.9598Step6fordwirdaufgefordert1,3millionensuvswegenabgaszurückzurufen75.060.8906Step7fordwirdaufgefordert1,3millionensuvswegenabgasenzurückzurufen1000.9872Table2:QualitativeAnalysisofReconstructingMultilingualParallelTextsinEnglishandGermanusingME5_MULTI.SteparethecorrectionstepsfromStep1(initialhypothesis)toStep6/7forthecorrectinver-sions.Thecoloredboxesindicatemisplacedtokens,wrongtokens,andexactmatches.Thebestresultsformetricsareinbold.Initialcosinesimilarityisunderlined.Forexample,theME5_DEmodelinvertsthefol-lowingGermansentenceintoEnglish:•GeneratedGermanreport:trumpeinmalfragtedamalsfbidirectorandrewmccabewährendseiner2016-vote•AdTransEnglishreport:trumponceaskedfbidirectorandrewmccabeduringhis2016-vote•TargetEnglishreport:trumponceaskedthen-actingfbidirectorandrewmccabeabouthis2016voteInthiscase,themodelincorrectlygenerates“während”(during)ratherthan“about”;otherwise,thegeneratedtextiscloseinmeaningwiththetar-getEnglishtext.Theinformationleakagewouldnotbeproperlycapturedwiththecurrentmet-ricsevaluatedontheGermantext.AppendixHTable12showsfurtherqualitativeexamplesforaddingAdTranstoaidevaluationincross-lingualsettings.Finally,forin-domainevaluation,performanceimprovesacrosscross-lingualsettings,asdemon-stratedinTable3.Moreover,asshowninAp-pendixGTable10,performanceisenhancedacrossmodelsacrossdomainsforeachlanguage,exceptfortheGTR-basemodel.Notably,theAdTransstrategyprovesparticularlyeffectiveformultilin-gualbasedLMs.MTG-ENMTG-FRMTG-DEMTG-ESME5_ENBase-3.2(0.9132)3.71(0.8945)3.1(0.9068)Vec2Text-4.62(0.9421)5.61(0.9474)4.33(0.911)AdTrans-12.4(↑168.08%)6.72(↑19.75%)12.38(↑185.79%)ME5_FRBase3.3(0.9176)-2.97(0.9038)4.52(0.9206)Vec2Text5.36(0.9235)-4.26(0.9431)5.94(0.9241)AdTrans7.25(↑37.71%)-6.35(↑49.47%)13.7(↑126.79%)ME5_DEBase3.99(0.8902)2.96(0.9082))-2.73(0.9224)Vec2Text8.13(0.9223)4.54(0.9223)-4.61(0.9163)AdTrans9.61(↑18.19%)10.37(↑128.62%)-11.01(↑138.91%)ME5_ESBase3.31(0.9186)3.96(0.9035)2.67(0.8958)-Vec2Text4.71(0.9223)5.13(0.8699)3.97(0.9460)-AdTrans5.91(↑25.51%)9.57(↑86.56%)5.56(↑39.89%)-Table3:Cross-lingualevaluationwithBLEUscoreandcosinesimilarity(inbrackets)forBaseandVec2Textmodelswith50correctionstepsand8sbeam.BLEUscoresandtheirgrowth(inbrackets)comparedwithVec2TextmodelsarereportedwithAdTrans.↑and↓denoteperformancegainsandlossesrespectively.ThebestBLEUresultsareinbold.6DefendingagainstInversionAttacksToexploredefensesagainstinversionattacksforLMsandcomparestrategiesbetweenmonolingualandmultilingualmodels,weinvestigatethetrade-offbetweenretrievalandreconstructionperfor-mance.Specifically,weapplynoiseinsertionandmaskingdefensetoGTR-baseandME5-baseus-ingthecorrectionmodelwith10steps.EvaluationisconductedonbothBEIR(Thakuretal.,2021)(English)andCLIRMatrix(SunandDuh,2020)(cross-lingual),observingthemeanNDCG@10measuresretrievalacross12tasks(fullresultsinAppendixI).Figure3:RetrievalandReconstructionperformanceacrossvaryinglevelsofnoiseinjectionwithmonolin-gual(GTR-Based)andmultilingual(ME5-Based)lan-guagemodelsonBEIR(top)andCLIRMatrix(bottom)datasets.Thereddottedlinesindicatethenoiselevelatwhichthedisparityofefficacyofdefensebetweenmonolingualandmonolingualembeddingsemerges.InsertingNoiseSimplenoiseinsertion(detailedinAppendixI.1)effectivelyguardsmonolingualLMsagainstinversionattacks(Morrisetal.,2023),whichisconfirmedbyourexperiments,demon-stratingthataddingnoisecandefendagainstsuchattackswhilepreservingembeddingutility,asde-pictedinFigure3.Withanoiselevelofλ=10−3,retrievalperfor-manceispreservedforbothGTRandME5acrossBEIRandCLIRMatrix.WhilethereisadroponreconstructionwithGTRandME5_NQonBEIRby20%,thereisnochangewithME5_ENonBEIRandME5_MULTIonbothBEIRandCLIRMatrix.Atthenoiselevel10−2,reconstructionperfor-mancewithGTRdrasticallydropsto16%oftheoriginalBLEUonBEIRand36%onCLIRMa-trix.Incontrast,reconstructionwithmultilingualLMsconsistentlymaintainsover70%oftheorigi-nalBLEU,particularlywithME5trainedonMTGover85%.Additionalnoise(λ≥10−1)dam-agessignificantlybothretrievalandreconstruc-tionperformances.ThisnotabledisparitybetweenretrievalandreconstructionperformanceonGTR(λ=10−2)impliestheefficacyofthenoiseinser-Figure4:RetrievalandReconstructionperformancewithmaskedmonolingual(GTR-Based)andmultilin-gual(ME5-Based)languagemodelsonBEIR(top)andCLIRMatrix(bottom)datasets.Thereddashedlinesindicatetheperformancedropinpercentage.tiondefenseprimarilyonmonolingualLMsratherthanmultilingualones.AFrustratinglySimpleMaskingDefenseToenhancethesecurityofLMs,weproposeasim-pledefensemethod,achievedbymaskingthefirstdimensionoftheembeddingswiththeencodingofthetargetlanguagelt.Weuseaniteratortoencodeeachlanguageasanidentifier,denotedasidt∈R.Themaskedembeddingmodelisdefinedasfollowing:ϕmasking(x)=vec([idt,vec(ϕi(x))1≤i≤n])(2)givenϕ(x)=vec(ϕi(x))0≤i≤nwherexisthein-puttext,nisthedimensionoftheembeddingϕ(x)andn∈N.WeimplementthissimplemaskingdefenseonbothGTR-baseandME5-basemodels.Asde-pictedinFigure4,whileretrievalperformancere-mainsunaffected8acrossallmodels,reconstructionmarkedlydeclinesforbothmonolingualandmulti-lingualLMsacrosstheretrievalbenchmarks,withanotabledropby92%withGTRonBEIRand79%8TheperformanceoftextreconstructiononCLIRMatrixdatasetwithGTRislargelyconflatedbyitssuperiorityinreconstructingEnglishdocuments(detailsinAppendixI).onCLIRMatrix,andover64%dropforallmultilin-gualmodels.TheisasimpleyeteffectivedefenseagainstinversionattacksforbothmonolingualandmultilingualLMs,whilefullypreservingutilityinretrievaltasks.7ConclusionWhilepreviousworksonembeddinginversionat-tacksfocusexclusivelyonEnglish,wepresentthefirstworkonmultilingualandcross-lingualembed-dinginversion.Notably,weuncoverthatmultilin-gualmodelscanbemorevulnerablethanmono-lingualmodels,undercertainconditions.Impor-tantly,traditionaldefensetailoredformonolingualmodelsproveineffectiveinguardingmultilingualmodels.Thusweproposeamorerobustdefenseapplicabletobothmonolingualandmultilingualones.Additionally,ourpreliminaryexperimentsovermoderately-resourcedUraliclanguagesfur-therstressestheimportanceofexpandingthescopeoffutureworksinembeddinginversionstudies,toincludeamorediversesetoflanguages.Insum-mary,ourworkadvocatesforamultilingualap-proachtoLLMandNLPsecurityasanentirety.LimitationsComputingResourcesAcorelimitationofthisworkisthecomputationallyintenseexperiments,requiringintheareaof25,000GPUcomputinghours.Whileexpandingthisresearchdirectiontomorelanguageswillfurtherincreasethisexpense,weadvocateforensuringthatlanguagesotherthanEnglisharenotleftbehindintermsofNLPsecu-rity.DataContaminationPre-trainedLMsareoftentrainedonmassiveweb-baseddatasets,resultinginahighlikelihoodthatagivenmodelhasal-readyseencommonlyusedbenchmarkdatasets(Dodgeetal.,2021a).Indeed,mostwide-usedLMsaretrainedonmassivedatasetsliketheC4CommonCrawl9webscrape,includingOpenAI’sGPTmodels(Radfordetal.,2019b;Brownetal.,2020),MetaAI’sRoBERTa(Liuetal.,2019)andLLaMAs(Touvronetal.,2023),GoogleAI’sBERT(Devlinetal.,2018),andEleutherAI’sGPT-Neo(Blacketal.,2022)andGPT-J(Wang,2021).Inthiswork,weutilizemodelsincludingT5-base,ME5-baseandGTR-base,whicharealltrainedonmassivepublicdomaindatasets,resultingina9https://commoncrawl.orglikelyoverlapoftrainingdata.Forexample,initial-izedfromT5,GTR-baseistrainedonNQdataset,whichisagainusedastrainingdatafortextre-constructionbyMorrisetal.(2023);ME5-baseandT5-baseoverlapsinC4andWikipedia.Inanattempttomitigatedatacontamination,weex-cludeWikipediafromtheMTGdataset.However,stavingoffdatacontaminationentirelyisnearlyinfeasiblewhenutilizingopen-sourcedpre-trainedlargeLMs.Thislimitationisthefocusofseveralpreviousworks(Brownetal.,2020;Dodgeetal.,2021b;MagarandSchwartz,2022;Jacovietal.,2023).NumberandDiversityofLanguagesInthisstudy,weextensivelyexperimentonmultilingualandcross-lingualinversionsecurityfocusedonfourRomanceandGermaniclanguages,whicharealsohigh-resourcelanguagesinNLP.Still,thismeansthatthisworklackstheextensivelinguisticdiver-sityneededtounderstandhowembeddinginversionattacksaffectmassivelymultilingualmodels,orlower-resourcedlanguages.Tothisend,weincludesomepreliminaryexperimentsforinvertingmul-tilingualsentenceBERTintwoUraliclanguages,i.e.,FinnishandHungarian.Ultimately,wead-vocateformoreextensiveresearchwithawidersampleoflanguagesinvariouslanguagefamilies.EthicsStatementThisworkexploresattacksonmultilingualembed-dingmodels.OurintentwiththisresearchistoshedlightonthevulnerabilitiesoflanguagesotherthanEnglish,aimingtoencouragethecommunitytoincludemorelanguagesinNLPsecuritywork.Whilethereispotentialformisusebymaliciousactors,aswithmanyworksinNLPsecurity,wemitigateharmbyincludinganeffectivecounter-measuretotheattackpresentedinthepaper.Still,itisimportanttostressthatembeddinginversionpresentlyrepresentsasubstantialthreat.Tothisend,theLMsexaminedinthispaperareopen-sourcemodels,andsuchthatthisworkdoesnotconstituteanimminentthreattoEaaSproviders,whoarelikelyusingprivatemodels.Finally,wedonotknowinglyexperimentwithanytrulysensitivedata,ensuringthatnoreal-worldharmiscausedbytheworkcarriedoutinthispaper.AcknowledgementsAllauthorsofthispaperarefundedbytheCarls-bergFoundation,undertheSemperArdens:Ac-celerateprogramme(projectnr.CF21-0454).WearefurthermoregratefultothesupportoftheAAUAICloud,andtoDeiCforallocatinguscomputingresourcesontheLUMIcluster(projectnr.DeiC-AAU-S5-412301).WethankSighvaturSveinnDavidssonforsettingusupwiththisaccess,andforhisdiligenceinassistingwiththeproblemsintheexperimentalinfrastructure,inadditiontotheLUMIusersupportfortheirverypromptanswersandcompetence,especiallyJingGong.WefurtherthankEstherPloegerforherassistanceintestingtranslationeseeffectfortheunder-performanceofmultilingualinversionmodelinEnglishandMar-cellRichardFeketeforhisinsightfulinputinproof-readingthepaper.ReferencesPayalBajaj,DanielCampos,NickCraswell,LiDeng,JianfengGao,XiaodongLiu,RanganMajumder,An-drewMcNamara,BhaskarMitra,TriNguyen,MirRosenberg,XiaSong,AlinaStoica,SaurabhTiwary,andTongWang.2018.Msmarco:Ahumangener-atedmachinereadingcomprehensiondataset.StevenBirdandEdwardLoper.2004.NLTK:Thenatu-rallanguagetoolkit.InProceedingsoftheACLIn-teractivePosterandDemonstrationSessions,pages214–217,Barcelona,Spain.AssociationforCompu-tationalLinguistics.SidBlack,StellaBiderman,EricHallahan,QuentinAnthony,LeoGao,LaurenceGolding,HoraceHe,ConnorLeahy,KyleMcDonell,JasonPhang,etal.2022.Gpt-neox-20b:Anopen-sourceautoregressivelanguagemodel.arXivpreprintarXiv:2204.06745.TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,etal.2020.Languagemodelsarefew-shotlearners.Advancesinneuralinformationprocessingsystems,33:1877–1901.NicholasCarlini,ChangLiu,JernejKos,ÚlfarErlings-son,andDawnSong.2018.Thesecretsharer:Mea-suringunintendedneuralnetworkmemorization&extractingsecrets.CoRR,abs/1802.08232.NicholasCarlini,FlorianTramèr,EricWallace,MatthewJagielski,ArielHerbert-Voss,KatherineLee,AdamRoberts,TomB.Brown,DawnSong,Úl-farErlingsson,AlinaOprea,andColinRaffel.2020.Extractingtrainingdatafromlargelanguagemodels.CoRR,abs/2012.07805.YiranChen,ZhenqiaoSong,XianzeWu,DanqingWang,JingjingXu,JiazeChen,HaoZhou,andLeiLi.2022.MTG:Abenchmarksuiteformultilin-gualtextgeneration.InFindingsoftheAssociationforComputationalLinguistics:NAACL2022,pages2508–2527,Seattle,UnitedStates.AssociationforComputationalLinguistics.AlexisConneau,KartikayKhandelwal,NamanGoyal,VishravChaudhary,GuillaumeWenzek,FranciscoGuzmán,EdouardGrave,MyleOtt,LukeZettle-moyer,andVeselinStoyanov.2020.Unsupervisedcross-lingualrepresentationlearningatscale.JierenDeng,YijueWang,JiLi,ChenghongWang,ChaoShang,HangLiu,SanguthevarRajasekaran,andCaiwenDing.2021.TAG:Gradientattackontransformer-basedlanguagemodels.InFindingsoftheAssociationforComputationalLinguistics:EMNLP2021,pages3600–3610,PuntaCana,Do-minicanRepublic.AssociationforComputationalLinguistics.JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2018.Bert:Pre-trainingofdeepbidirectionaltransformersforlanguageunderstand-ing.arXivpreprintarXiv:1810.04805.JesseDodge,MaartenSap,AnaMarasovi´c,WilliamAgnew,GabrielIlharco,DirkGroeneveld,MargaretMitchell,andMattGardner.2021a.Documentinglargewebtextcorpora:Acasestudyonthecolos-salcleancrawledcorpus.InProceedingsofthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages1286–1305,OnlineandPuntaCana,DominicanRepublic.AssociationforComputationalLinguistics.JesseDodge,MaartenSap,AnaMarasovi´c,WilliamAgnew,GabrielIlharco,DirkGroeneveld,Mar-garetMitchell,andMattGardner.2021b.Docu-mentinglargewebtextcorpora:Acasestudyonthecolossalcleancrawledcorpus.arXivpreprintarXiv:2104.08758.MattFredrikson,SomeshJha,andThomasRistenpart.2015.Modelinversionattacksthatexploitconfi-denceinformationandbasiccountermeasures.InProceedingsofthe22ndACMSIGSACConferenceonComputerandCommunicationsSecurity,CCS’15,page1322–1333,NewYork,NY,USA.Associa-tionforComputingMachinery.MatthewFredrikson,EricLantz,SomeshJha,SimonLin,DavidPage,andThomasRistenpart.2014.Pri-vacyinpharmacogenetics:Anend-to-endcasestudyofpersonalizedwarfarindosing.InProceedingsofthe23rdUSENIXConferenceonSecuritySymposium,SEC’14,page17–32,USA.USENIXAssociation.IshrakHayet,ZijunYao,andBoLuo.2022.Inver-net:Aninversionattackframeworktoinferfine-tuningdatasetsthroughwordembeddings.InFind-ingsoftheAssociationforComputationalLinguistics:EMNLP2022,pages5009–5018,AbuDhabi,UnitedArabEmirates.AssociationforComputationalLin-guistics.JohannesHöhmann,AchimRettinger,andKaiKugler.2021.Invbert:Textreconstructionfromcontextu-alizedembeddingsusedforderivedtextformatsofliteraryworks.CoRR,abs/2109.10104.YangsiboHuang,ZhaoSong,DanqiChen,KaiLi,andSanjeevArora.2020.TextHide:Tacklingdatapri-vacyinlanguageunderstandingtasks.InFindingsoftheAssociationforComputationalLinguistics:EMNLP2020,pages1368–1382,Online.AssociationforComputationalLinguistics.AlonJacovi,AviCaciularu,OmerGoldman,andYoavGoldberg.2023.Stopuploadingtestdatainplaintext:Practicalstrategiesformitigatingdatacontami-nationbyevaluationbenchmarks.InProceedingsofthe2023ConferenceonEmpiricalMethodsinNatu-ralLanguageProcessing,pages5075–5084,Singa-pore.AssociationforComputationalLinguistics.DonggyuKim,GaramLee,andSungwooOh.2022.Towardprivacy-preservingtextembeddingsimilaritywithhomomorphicencryption.InProceedingsoftheFourthWorkshoponFinancialTechnologyandNat-uralLanguageProcessing(FinNLP),pages25–36,AbuDhabi,UnitedArabEmirates(Hybrid).Associa-tionforComputationalLinguistics.TomKwiatkowski,JennimariaPalomaki,OliviaRed-field,MichaelCollins,AnkurParikh,ChrisAlberti,DanielleEpstein,IlliaPolosukhin,JacobDevlin,Ken-tonLee,KristinaToutanova,LlionJones,MatthewKelcey,Ming-WeiChang,AndrewM.Dai,JakobUszkoreit,QuocLe,andSlavPetrov.2019.Natu-ralquestions:Abenchmarkforquestionansweringresearch.TransactionsoftheAssociationforCompu-tationalLinguistics,7:452–466.HaoranLi,YangqiuSong,andLixinFan.2022.Youdon’tknowmyfavoritecolor:Preventingdialoguerepresentationsfromrevealingspeakers’privateper-sonas.InProceedingsofthe2022ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:HumanLanguageTech-nologies,pages5858–5870,Seattle,UnitedStates.AssociationforComputationalLinguistics.HaoranLi,MingshiXu,andYangqiuSong.2023.Sen-tenceembeddingleaksmoreinformationthanyouexpect:Generativeembeddinginversionattacktorecoverthewholesentence.InFindingsoftheAs-sociationforComputationalLinguistics:ACL2023,pages14022–14040,Toronto,Canada.AssociationforComputationalLinguistics.JindˇrichLibovický,RudolfRosa,andAlexanderFraser.2020.Onthelanguageneutralityofpre-trainedmul-tilingualrepresentations.InFindingsoftheAssoci-ationforComputationalLinguistics:EMNLP2020,pages1663–1674,Online.AssociationforComputa-tionalLinguistics.Chin-YewLin.2004.ROUGE:Apackageforauto-maticevaluationofsummaries.InTextSummariza-tionBranchesOut,pages74–81,Barcelona,Spain.AssociationforComputationalLinguistics.YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,Man-darJoshi,DanqiChen,OmerLevy,MikeLewis,LukeZettlemoyer,andVeselinStoyanov.2019.Roberta:Arobustlyoptimizedbertpretrainingap-proach.arXivpreprintarXiv:1907.11692.L.Lyu,XuanliHe,andYitongLi.2020.Differentiallyprivaterepresentationfornlp:Formalguaranteeandanempiricalstudyonprivacyandfairness.ArXiv,abs/2010.01285.InbalMagarandRoySchwartz.2022.Datacontamina-tion:Frommemorizationtoexploitation.InProceed-ingsofthe60thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume2:ShortPa-pers),pages157–165,Dublin,Ireland.AssociationforComputationalLinguistics.JohnMorris,VolodymyrKuleshov,VitalyShmatikov,andAlexanderRush.2023.Textembeddingsreveal(almost)asmuchastext.InProceedingsofthe2023ConferenceonEmpiricalMethodsinNaturalLan-guageProcessing,pages12448–12460,Singapore.AssociationforComputationalLinguistics.NiklasMuennighoff,NouamaneTazi,LoïcMagne,andNilsReimers.2023.Mteb:Massivetextembeddingbenchmark.MiladNasr,RezaShokri,andAmirHoumansadr.2019.Comprehensiveprivacyanalysisofdeeplearning:Passiveandactivewhite-boxinferenceat-tacksagainstcentralizedandfederatedlearning.In2019IEEESymposiumonSecurityandPrivacy(SP).IEEE.ThuatNguyen,ChienVanNguyen,VietDacLai,HieuMan,NghiaTrungNgo,FranckDernoncourt,RyanA.Rossi,andThienHuuNguyen.2024.Cul-turaX:Acleaned,enormous,andmultilingualdatasetforlargelanguagemodelsin167languages.InPro-ceedingsofthe2024JointInternationalConferenceonComputationalLinguistics,LanguageResourcesandEvaluation(LREC-COLING2024),pages4226–4237,Torino,Italia.ELRAandICCL.JianmoNi,ChenQu,JingLu,ZhuyunDai,GustavoHernandezAbrego,JiMa,VincentZhao,YiLuan,KeithHall,Ming-WeiChang,andYinfeiYang.2022.Largedualencodersaregeneralizableretrievers.InProceedingsofthe2022ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages9844–9855,AbuDhabi,UnitedArabEmirates.As-sociationforComputationalLinguistics.RahilParikh,ChristopheDupuy,andRahulGupta.2022.Canaryextractioninnaturallanguageunderstandingmodels.InProceedingsofthe60thAnnualMeet-ingoftheAssociationforComputationalLinguistics(Volume2:ShortPapers),pages552–560,Dublin,Ireland.AssociationforComputationalLinguistics.MattPost.2018.AcallforclarityinreportingBLEUscores.InProceedingsoftheThirdConferenceonMachineTranslation:ResearchPapers,pages186–191,Brussels,Belgium.AssociationforComputa-tionalLinguistics.AlecRadford,JeffWu,RewonChild,DavidLuan,DarioAmodei,andIlyaSutskever.2019a.Languagemodelsareunsupervisedmultitasklearners.AlecRadford,JeffreyWu,RewonChild,DavidLuan,DarioAmodei,IlyaSutskever,etal.2019b.Lan-guagemodelsareunsupervisedmultitasklearners.OpenAIblog,1(8):9.ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,YanqiZhou,WeiLi,andPeterJ.Liu.2023.Exploringthelimitsoftransferlearningwithaunifiedtext-to-texttrans-former.RezaShokri,MarcoStronati,andVitalyShmatikov.2016.Membershipinferenceattacksagainstmachinelearningmodels.CoRR,abs/1610.05820.CongzhengSongandAnanthRaghunathan.2020.In-formationleakageinembeddingmodels.InPro-ceedingsofthe2020ACMSIGSACConferenceonComputerandCommunicationsSecurity,CCS’20,page377–390,NewYork,NY,USA.AssociationforComputingMachinery.ShuoSunandKevinDuh.2020.CLIRMatrix:Amas-sivelylargecollectionofbilingualandmultilingualdatasetsforcross-lingualinformationretrieval.InProceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP),pages4160–4170,Online.AssociationforComputa-tionalLinguistics.NandanThakur,NilsReimers,AndreasRücklé,Ab-hishekSrivastava,andIrynaGurevych.2021.BEIR:Aheterogeneousbenchmarkforzero-shotevaluationofinformationretrievalmodels.InThirty-fifthCon-ferenceonNeuralInformationProcessingSystemsDatasetsandBenchmarksTrack(Round2).HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,TimothéeLacroix,BaptisteRozière,NamanGoyal,EricHambro,FaisalAzhar,etal.2023.Llama:Openandeffi-cientfoundationlanguagemodels.arXivpreprintarXiv:2302.13971.OlgaTsymboi,DanilMalaev,AndreiPetrovskii,andIvanOseledets.2023.Layerwiseuniversaladversar-ialattackonNLPmodels.InFindingsoftheAsso-ciationforComputationalLinguistics:ACL2023,pages129–143,Toronto,Canada.AssociationforComputationalLinguistics.EricWallace,ShiFeng,NikhilKandpal,MattGard-ner,andSameerSingh.2019.UniversaladversarialtriggersforattackingandanalyzingNLP.InProceed-ingsofthe2019ConferenceonEmpiricalMethodsinNaturalLanguageProcessingandthe9thInter-nationalJointConferenceonNaturalLanguagePro-cessing(EMNLP-IJCNLP),pages2153–2162,HongKong,China.AssociationforComputationalLinguis-tics.BenWang.2021.Mesh-Transformer-JAX:Model-ParallelImplementationofTransformerLan-guageModelwithJAX.https://github.com/kingoflolz/mesh-transformer-jax.LiangWang,NanYang,XiaolongHuang,BinxingJiao,LinjunYang,DaxinJiang,RanganMajumder,andFuruWei.2022.Textembeddingsbyweakly-supervisedcontrastivepre-training.LiangWang,NanYang,XiaolongHuang,LinjunYang,RanganMajumder,andFuruWei.2024.Multilin-guale5textembeddings:Atechnicalreport.arXivpreprintarXiv:2402.05672.ShangyuXieandYuanHong.2021.Reconstructionattackoninstanceencodingforlanguageunderstand-ing.InProceedingsofthe2021ConferenceonEm-piricalMethodsinNaturalLanguageProcessing,pages2038–2044,OnlineandPuntaCana,Domini-canRepublic.AssociationforComputationalLin-guistics.MikeZhangandAntonioToral.2019.Theeffectoftranslationeseinmachinetranslationtestsets.InProceedingsoftheFourthConferenceonMachineTranslation(Volume1:ResearchPapers),pages73–81,Florence,Italy.AssociationforComputationalLinguistics.ZhuoZhang,YuanhangYang,YongDai,QifanWang,YueYu,LizhenQu,andZenglinXu.2023.Fed-PETuning:Whenfederatedlearningmeetstheparameter-efficienttuningmethodsofpre-trainedlan-guagemodels.InFindingsoftheAssociationforComputationalLinguistics:ACL2023,pages9963–9977,Toronto,Canada.AssociationforComputa-tionalLinguistics.XinZhou,YiLu,RuotianMa,TaoGui,YuranWang,YongDing,YiboZhang,QiZhang,andXuanjingHuang.2023.TextObfuscator:Makingpre-trainedlanguagemodelaprivacyprotectorviaobfuscatingwordrepresentations.InFindingsoftheAssocia-tionforComputationalLinguistics:ACL2023,pages5459–5473,Toronto,Canada.AssociationforCom-putationalLinguistics.LigengZhu,ZhijianLiu,andSongHan.2019.Deepleakagefromgradients.InAdvancesinNeuralIn-formationProcessingSystems,volume32.CurranAssociates,Inc.ATrainingDataDistribution414243201M2M3M4M5MNQMTG_ENMTG_FRMTG_DEMTG_ESMTG_MULTIToken LengthsNr. of SamplesFigure5:TheDistributionofthetrainingdataformod-elswiththemaximaltokenlengthof32.IntheMTGdatasets,Englishtextsaresourcedfromvariousorigins,whileGerman,Spanish,andFrenchtextsaretranslatedfromEnglishusingma-chinetranslationandmanuallyvalidated(Chenetal.,2022).Theselanguagesexhibitdiversemor-phologies,leadingtovariationsinsentencelengthsandthenumberofsentencespost-tokenizationacrosslanguages.Additionally,theNQdatasetisincludedtoreproducefindingsfrompriorre-search(Morrisetal.,2023)andtoassessthecross-domainandcross-lingualperformanceofthetextreconstructiontask.TheNQdatasetpredominantlycomprisesEnglishdata,withWikipediapassagesincludedwithouttokenization,resultinginalltrain-ingdatafromNQhaving32tokens.BMonolingualEnglishTextReconstructionTohaveaproofofconcept,wesuccessfullyre-produceandreplicatetheexperimentfromMorrisetal.(2023),bytraininginversionmodelsusingGTR-baseandME5-baseasembeddersontheNQdataset,notedasGTRandME5_NQ.TheresultsforreconstructingEnglishtextsareshowninTable4,evaluatedwithcorrectionsteps(1,20,50,100)combinedwithbeamsearch(4and8sbeam).Thebaseand1-StepVec2TextmodeltrainedonME5-basehaveaperformanceonparwithGTR-base.Moreover,thetextembeddingstrainedonME5-basearecloserinembeddingspacethanembeddingstrainedonGTR-base,i.e.,withhighercosinesimilarities.While,withmorestepsofcorrectionandsbeam,theperformanceisboostedto92.45onBLEUwith82%exactmatchforGTR,whilethebestperfor-manceforME5_NQis80.86onBLEUwith35%exactmatch.TheperformancedifferencecouldbeduetothefactthattheunderlyingGTR-baseist5-basedmodel,thesamestructureasthegenerationmodelψ.However,utilizingME5-basesetsupamorereal-isticattackscenarioofblack-boxembeddinginver-sion,asthestructureoftheembedderϕisunknown.Bothmodelsarefurthermoreevaluatedwithcross-domainEnglishtextreconstruction.Similarly,GTRoutperformsME5after50correctionstepswithsbeam8,seeTable9inAppendixG.CRuntimevs.BLEUscoresTheevaluationofVec2Textmodelsisexpensiveintermsoftimeandcomputation.Inordertosearchfortheoptimalruntimeandperformancetrade-off,Figure6showsBLEUscoresateachstepandthelinesrepresentthetrendforruntimeforthemono-lingualmodels.Thebesttrade-offpointsareatthecorrectionstepof50with8sbeamforallthemodels,while100stepstakesmorethandoublethetimeachievingsimilarperformance.ThefullresultsareinTable1and5.Untilcorrectionstep50with8sbeam,performanceincreasessteadily,andthetrendisgenerallyalignedwithcosinesimilar-ity.Asaresult,weevaluatethesubsequentmodelsuntilcorrectionstep50with8sbeam.1 Step20 Steps50 Steps50 Steps + 4 sbeam50 Steps + 8 sbeam100 Steps100 Steps + 4 sbeam100 Steps + 8 sbeam02040608010005k10k15k20kME5_MTG-EN BLEUME5_MTG-EN RuntimeME5_MTG-FR BLEUME5_MTG-FR RuntimeME5_MTG-DE BLEUME5_MTG-DE RuntimeME5_MTG-ES BLEUME5_MTG-ES RuntimeBLEU ScoreRUNTIME (seconds)Figure6:BLEUscoresvs.RuntimebyEvaluationforInversionModelsinEnglish,French,GermanandSpanish.#Tokens#PredTok.BLEUROUGETF1ExactCOSGTRME5GTRME5GTRME5GTRME5GTRME5GTRME5GTRME5Base(0Steps)3232323227.1828.7762.8663.6863.7465.90.40.40.87930.9738Vec2Text(1Step)3231323248.6247.9278.3977.0378.4478.3584.80.9210.9588(20Steps)3232323283.3074.4795.1289.5795.1190.35821.80.98620.992(50Steps)3232323284.3175.0395.4989.7695.690.5658.421.80.98620.992(50Steps+4sbeam)3232323290.1878.8797.2691.1197.1591.5574.432.60.98530.9902(50Steps+8sbeam)3232323292.4480.8697.7691.8997.7892.4282350.99210.9926(100Steps)3232323292.4580.8297.7591.8397.7992.3782350.99210.9926(100Steps+4sbeam)3232323290.1778.8297.2591.1197.1591.5374.432.80.98240.9902(100Steps+8sbeam)3232323292.4580.8297.7591.8397.7992.3782350.99210.9926Table4:EvaluationofEnglishTextReconstruction.Thebestperformancesforeachmodelreachedintheearlieststagesareinbold.TheunderlinedresultsarewhereME5-basemodeloutperformsGTR-basemodel.#Tokens#PredTok.BLEUROUGETF1ExactCOSMTG-EN(100Steps)3231.9848.5383.5179.12120.9277(100Steps+4sbeam)3231.9945.982.7178.2410.80.9372(100Steps+8sbeam)3231.9848.5383.5179.12120.9277MTG-FR(100Steps)323274.4489.188.7754.40.9757(100Steps+4sbeam)323271.9388.2687.8950.40.9643(100Steps+8sbeam)323274.4489.188.7754.40.9757MTG-DE(100Steps)323269.5587.886.4747.40.9791(100Steps+4sbeam)3231.9865.6185.7384.4642.20.9778(100Steps+8sbeam)323269.5587.886.4747.40.9791MTG-ES(100Steps)323279.9691.2191.43650.9579(100Steps+4sbeam)323277.4890.5290.5660.80.9697(100Steps+8sbeam)323279.9691.2191.43650.9579Table5:TheevaluationofTextReconstructioninmulti-plelanguages,withthemodelstrainedandevaluatedonMTGdatasetswithtokenslength32inEnglish,French,GermanandSpanish,respectively.Thestepsarefrom100stepsto100steps+8sbeam.DInvertingMultilingualSentenceBERTEmbeddingsWeadditionallyexperimentoninvertingmulti-lingualsentenceBERTinFinnishandHungarian.Theinversionmodelsaretrainedusingtheencoder-decodermultilingualT5(Wangetal.,2024)asgen-erationmodel,andmultilingualsentenceBERT10isusedastheencoderϕ.Wetrainmodelsonrandomlyextracted1MdatasamplesfromCul-turaX(Nguyenetal.,2024)11,validatedandeval-uatedon500samples,respectfully.ThedetailedevaluationresultsarereportedinTable6.Interest-ingly,thecorrectormodel,whichconvergesem-beddingswithcosinesimilarity,didnotimprovetextreconstructionforFinnishtexts,whileitdidprovidemarginalimprovementforHungariantexts.Thenotablypoorerperformanceinthisexperimenthighlightsthecomplexityofinvertingtextualem-beddings,wheremodelaffinityanddatasetsplay10huggingface:sentence-transformers/distiluse-base-multilingual-cased-v211huggingface:uonlp/CulturaXcrucialroles.Forfuturework,weplantoinvesti-gatemoreextensivelyhowdifferentmodelarchitec-turesandlanguagefamiliesinfluenceembeddinginversionperformance.#Tokens#PredTok.BLEUROUGETF1EXACTCOSFinnishBase(0Steps)32317.690.240.270.0140.7068Vec2Text(1Step)320.00.00.00.000.0-0.0562(20Steps)320.00.00.00.00.0-0.0562(50Steps)320.00.00.00.00.0-0.0562(50Steps+4sbeam)32310.010.00.130.0-0.0166(50Steps+8sbeam)328.00.030.00.140.00.0034HungarianBase(0Steps)32316.740.310.300.0020.6834Vec2Text(1Step)32317.150.3230.520.20.7220(20Steps)32317.350.3230.990.20.7170(50Steps)32317.370.3231.040.20.7170(50Steps+4sbeam)32317.950.3331.760.00.7564(50Steps+8sbeam)32318.000.3331.280.00.8240Table6:InvertingMultilingualSentenceBERTtextualembeddingsinFinnishandHungarian.Thebestresultsforeachmetricareinbold.ENoEvidenceforTranslationeseEffectInmachinetranslation,thereisclearevidencethatthepresenceoftranslationeseintestsetsmayre-sultininflatedhumanevaluationscoresforMTsystems(ZhangandToral,2019).Toinvestigatewhetherourmultilingualinversionmodel’ssub-parperformanceinEnglishisduetothecharacteristicsoftranslationeseinotherlanguages,weimplementroundtriptranslationonMTG-ENtestdataus-ingSpanishasthepivotlanguagewithEasyNMT,thetranslationpathisthusEnglish→Spanish→English.Thentheevaluationofthemultilingualinversionmodelisdoneontheround-triptranslatedEnglishtestset,theresultisshownasinTable7.ComparedtoevaluationonMTG-ENtestset,asshowninTable1,theperformanceoftranslatedEnglishtestsetisabout30onBLEUworseateachstageofcorrections.Thehypothesisofthetransla-tioneseeffectonthedifferenceoftheperformancescanthereforeberejected.#Tokens#PredTok.BLEUROUGETF1EXACTCOSVec2Text(1Step)29.5930.9810.0347.5441.2800.9046(20Steps)29.5930.9514.4855.1447.80.20.913(50Steps)29.5930.9815.1156.0148.560.20.9261(50Steps+4sbeam)29.5930.8817.5661.8152.640.20.9461(50Steps+8sbeam)29.5930.9617.4261.2852.440.40.9185Table7:Evaluationofmultilingualinversionmodelonround-triptranslatedMTG-ENtestdataset.FTextConstructiononTokensLength64#Tokens#PredTokensBLEUROUGETF1ExactCOSEnglishVec2Text(1Step)37.7843.7318.1359.3357.280.887.94(20Steps)37.7841.3238.4878.3874.231088.75(50Steps)37.7840.9739.2779.7475.410.292.70(50Steps+4sbeam)37.7840.6745.2381.6877.3114.689.18(50Steps+8sbeam)37.7840.1947.2983.3478.6216.691.09FrenchVec2Text(1Step)51.6157.2326.4563.5864.030.895.07(20Steps)51.6153.2558.2583.183.0126.696.54(50Steps)51.6152.659.5883.9983.6926.896.26(50Steps+4sbeam)51.6152.6264.6186.1186.0337.897.26(50Steps+8sbeam)51.6152.5466.886.7486.4441.893.83GermanVec2Text(1Step)49.7556.0919.6554.5855.190.297.43(20Steps)49.7552.6246.1176.175.315.693.98(50Steps)49.7552.7646.6176.6975.8615.895.72(50Steps+4sbeam)49.7551.9152.7879.678.9325.692.98(50Steps+8sbeam)49.7551.8255.7380.8780.2130.894.97SpanishVec2Text(1Step)62.666226.0364.1665.780.497.57(20Steps)62.6662.2356.0783.5383.717.498.28(50Steps)62.6662.0956.7384.3784.4617.497.01(50Steps+4sbeam)62.6661.9564.2786.7887.0129.295.39(50Steps+8sbeam)62.6661.7665.5787.7387.8532.897.36Table8:TheevaluationofTextReconstructioninmulti-plelanguages,withthemodelstrainedandevaluatedonMTGdatasetswithmaximaltokenlength64inEnglish,French,GermanandSpanish,respectively.Thebestresultsacrossmetricsareinbold.WetrainME5-baseinversionmodelsonMTGdatasetswithtokenlengthsof64inEnglish,French,German,andSpanish,incomparisonto32-tokenlengthmodels.ResultsinTable8indi-cateaperformancedegradation;forinstance,theBLEUscorefortheSpanishinversionmodeldropsbyapproximately15whiledoublingthenumberoftokens.Thishighlightsthechallengesinthislineofresearch.GCross-DomainTextReconstructionCross-DomainEnglishTextReconstructionToevaluatetheperformanceofembeddinginversionattacksonout-of-domaindatasetinEnglish,themodelstrainedonNQandMTG-ENarecross-evaluatedonbothdatasets,respectively,asshowninTable9.TheresultsonMTG-ENaresimilaronBLEUforbothbasemodelstrainedonGTR-BaseandME5-Base,whileGTRmodeloutperformsME5bymorethan12onBLEU,andthecosinesimilar-ityofreconstructedandtruetextembeddingsareboostedbyover0.24.Incomparison,thecosinesimilarityforME5modelsarenotmuchvariedandNQ→MTG-ENMTG-EN→NQMTG-MULTI→NQGTRBase5.81(0.7334)--Vec2Text39.08(0.9767)ME5Base5.89(0.9272)12.35(0.9154)11.63(0.8894)Vec2Text26.96(0.9440)42.90(0.9789)31.84(0.9310)Table9:Cross-DomainEnglishTextReconstructionEvaluation,BLEUscoresandCOSarereported.Hori-zontalcomparisononME5-basemodels,andverticallyontwoembedderstrainedonthesameNQdataset.TheVec2Textmodelsareevaluatedby50stepsofcorrectionwithsequencebeamsearchwidth8.→indicatesthecross-domainevaluationdirection.Forexample,NQ→MTG-ENindicatesthatthemodelistrainedonNQandevaluatedonMTG-EN.constantlyhigh(≥0.88)acrossstagesofevalua-tionsandacrossdomains.Additionally,ME_ENoutperformsME_MULTItestedonNQ.Cross-domainCross-lingualTextReconstruc-tionCross-lingual,cross-domaintextreconstruc-tionisoneofthemostchallengingscenarios,yetitalsorepresentsthemostrealisticcontext,withbothdomainandtargetlanguageunknown.AsshowninTable10a,whiletheAdTransstrategydoesnotenhancetheperformanceoftheGTR-Baseinversionmodel,thereisaconsistentimprovementinperformanceacrossdatasetswhenusingME5-Baseinversionmodels.Particularlynoteworthyisthesignificantperformanceboostobserved,espe-ciallyevidentwhenevaluatingNQ-trainedME5-basemodelME_NQonMTG-DE,resultinginaremarkable128.11%performancegain.ItisinterestingthatmultilingualLMsreconstructtextsinthelanguageoftrainingdata,whilemono-linguallanguagemodel(GTR)reconstructtextsmostlyinthetargetlanguage.ThishighlightsthedifferencesofmonolingualandmultilingualLMs,andwarrantsfurtherresearchforfuturework.HQualitativeAnalysisH.1MultilingualTextReconstructionWeconductqualitativeanalysisonmultilingualtextreconstructionusingparallelsamples.Table11showstheFrenchandSpanishsamples,incompar-isontoTable2,samplesinEnglishandGerman.ThesamplesareevaluatedonME5_MULTI.ByStep2,Frenchsentenceisalreadyreconstructedwithonewordmismatch,however,thewholesen-tenceisonlyfullyreconstructedbycorrectionstep50+4sbeam.ThecosinesimilarityishighfromMTG-FRMTG-DEMTG-ESGTR-BaseBase4.39(0.7581)3.22(0.7052)4.74(0.7134)Vec2Text10.91(0.8833)6.46(0.8138)10.84(0.9020)AdTrans10.48(↓-3.92%)6.15(↓-4.84%)9.95(↓-1.67%)ME5-BaseBase3.13(0.9513)2.73(0.9298)3.64(0.9293)Vec2Text6.46(0.9487)5.37(0.9107)5.91(0.8963)AdTrans13.40(↑107.32%)8.54(↑59.21%)11.87(↑100.79%)(a)Cross-lingualcross-domainevaluationwithmonolingualmodelstrainedonNQ.→NQME5_FRME5_DEME5_ESBase2.60(0.96)2.80(0.8790)2.32(0.9266)Vec2Text4.00(0.9441)5.13(0.9374)3.41(0.9380)AdTrans8.11(↑102.50%)10.18(↑98.49%)6.07(↑78.04%)(b)Cross-lingualcross-domainevaluationonNQwithmonolin-gualmodelstrainedonMTGdatasets.Table10:Cross-lingualevaluationusingBLEUscoreandCosineSimilarity(inthebrackets)forBaseandVec2Textmodelsbycorrectionstepsof50with8sbeam.TheBLEUscoresandtheirgrowth(inthebrackets)comparedwithBLEUscoresonVec2Textmodelsarere-portedforAdTransstrategyforeachmodel.↑indicatesperformancegainwhilethe↓indicatesperformanceloss.TheresultwiththehighestBLEUscorewitheachevaluatedmodeloneachdatasetisinbold.step1,i.e.,0.9892,comparedtothesampleinEnglish,i.e.,0.8056andinGerman,i.e.,0.8704.WhileEnglishandGermansamplesarefullyre-constructedbystep6and7.Asargued,theap-proximationapproachwithcosinesimilarityseemstobemoreeffectiveformodelsrenderinglowercosinesimilarityfrominitialsteps.However,fromobservations,ME5modelsreconstructscloserem-beddingsacrosslanguagesfromthestart.H.2Cross-lingualTextReconstructionWefurtherconductqualitativeanalysisoncross-lingualtextreconstruction,aidedbyAdTrans.AsshowninTable12,thefourwaymultilingualsam-plesareused,allrepresentthesamemeaning.EachsampleisevaluatedbyME5-baseinversionmodelstrainedonotherthreelanguagesseparately.Consistentwithpreviousquantitativeanalysis,thecross-lingualreconstructionisdifficult,andtheBLEUscoresareconsistentlylow.WithAdTrans,theBLEUscoresareoverlyboosted,withanexcep-tionofevaluatingSpanishsamplewithME5_EN.Inthisexample,thehighestperformancegainisaddingAdTransforevaluatingEnglishsamplewithME5_DE.TheintentionofaddingAdTransistoimprovetheutilityofcurrentstring-matchingmetricsincross-lingualattacksetting,whilealsoexposetheinadequacyofsuchmetricsintermsofLLMSec.Withthisexample,thereisessentialinformationleakageineachevaluationthatcannotbecapturedevenafterapplyingAdTrans.IFullDefenseResultsI.1NoiseInsertionDefenseFollowing(Morrisetal.,2023),thenoisyembed-dingmodelisdefinedasfollowing:ϕnoisy(x)=ϕ(x)+λ·ϵ,ϵ∈N(0,1)(3)whereλisahyperparametercontrollingtheamountofnoiseinjected.I.2LanguageNeutralityofInversionModelsDrawinginspirationfromLibovickýetal.(2020),wedelveintotheimpactoflanguage-agnosticem-beddingsonretrievalandreconstructionperfor-mance.Thisisachievedbyisolatingthelanguage-specificcomponent,representedbythemeanoftheembeddings,whichservestoidentifythelanguageoftherepresentations.Conversely,weextractthelanguage-agnosticcomponentbysubtractingthemeanembeddings,therebycapturingtheessenceofthetextinalanguage-independentmanner.Wepresenttheperformanceoflanguage-agnosticcomponentonGTR-baseandME5-basemodelsacrossBEIRandCLIRMatrixbenchmarksinTable13,14,15,and16,17,18.Consistently,ourfindingsdemonstratethatlanguage-agnosticembeddingseitheroutperformorperformequallywellcomparedtotheoriginalembeddingsinretrievaltasks.However,whilethereisonlyaslightdegradationinperformancefortextreconstructionontheCLIRMatrixbenchmarkandwithME5-basemodelsontheBEIRbench-mark,thereconstructionperformanceexperiencesanotable20%declinewiththeGTR-basemodelontheBEIRbenchmark.Thisindicatethatthedis-tinctionoflanguage-specificandlanguage-agnosticcomponentismoresalientformultilingualmodels.I.3ResultsonBEIRBenchmarkWereproducetheretrievalandreconstructiononGTR-basemodelsacross12BEIRtasksfrom(Mor-risetal.,2023),excludingthefourprivatedatasets.Moreover,weimplementretrievalonME5-basemodels.Thefulldefenseresultsforretrievalper-formanceandreconstructiontasksareshowninTable13,14and15.StepTextBLEUCOSInputforddoitrappeler1,3milliondesuvenraisondesgazd’échappementStep1forddoitrappeler1,3milliondesuvenraisondugazd’absorption68.120.9892Step2forddoitrappeler1,3milliondesuvenraisondugazd’échappement76.120.9712Step3forddoitrappeler1,3milliondesuvenraisondugazd’échappement76.120.9992Step4forddoitrappeler1,3milliondesuvenraisondugazd’échappement76.120.9712Step5forddoitrappeler1,3milliondesuvenraisondugazd’échappement76.120.9992Step6forddoitrappeler1,3milliondesuvenraisondugazd’échappement76.120.9712Step7forddoitrappeler1,3milliondesuvenraisondugazd’échappement76.120.9712Step50forddoitrappeler1,3milliondesuvenraisondugazd’échappement76.120.9992Step50+4sbeamforddoitrappeler1,3milliondesuvenraisondesgazd’échappement1000.9915Inputfordinstóaretirar1.3millonessuvsporelescapedehumosStep1fordimploróel1,3millonesdesuvsenlasalidadehumos8.910.9491Step2fordadvirtióel1,3millonesdehumosselevadosdesuvsalelimin8.910.8213Step3fordseadvirtióporeliminar1,3millonesdehumosasuvsasale8.450.9634Step4fordseadvirtióporelrescatede1,3millonesdesuvsporhum9.670.9552Step5fordseadvirtióque1,3millonesdesuvsseescaparonporhumo5.060.9696Step6fordseinstóalaséparade1,3millonesdesuvsporhumos10.390.9045Step7fordinstóalos1,3millonesdesuvsasalirdelhumorevapor13.670.9481Step50fordinstóalasalidade1.3millonesdesuvsporelhumo22.630.9794Step50+4sbeamfordinstóalasalidade1.3millonesdesuvsconhumosparaelimin14.950.831Step50+8sbeamfordinstóaretirar1.3millonessuvsporelescapedehumos1001.0000Table11:QualitativeAnalysisofReconstructingMultilingualParallelTextsinFrenchandSpanishusingME5_MULTI.SteparethecorrectionstepsfromStep1(initialhypothesis)toStep50+4/8sbeamforthecorrectinversions.Thecoloredboxesindicatemisplacedtokens,wrongtokens,andexactmatches.Thebestresultsformetricsareinbold.Initialcosinesimilarityisunderlined.I.4ResultsonCLIRMatrixBenchmarkToevaluatethecross-lingualscenarioinretrievalandreconstructiononmonolingualandmultilin-gualmodels,weimplementcross-lingualretrievalandtextreconstructionacross12cross-lingualdatasetsconstructedfromMULTI-8ofCLIRMa-trix(SunandDuh,2020).LetqbeaqueryinlanguageLqueryanddbeadocumentinlanguageLdoc.Inourscenario,thecross-lingualretrievaltaskinvolvesretrievingthedocumentinlanguageLdocwhenpresentedwithaqueryinlanguageLquerywithinthenear-estneighborretrievalframework.Foroureval-uation,thecross-lingualdatasetsareconstructedwiththetriple(qLquery,dLdoc),whereLquery∈{en,fr,de,es}andLdoc∈{en,fr,de,es},andLquery̸=Ldoc.Weimplementretrievalandrecon-structionbothonGTR-baseandME5-basemodels.ThefulldefenseresultsforretrievalperformanceandreconstructiontasksareshowninTable16,17and18.ModelTextBLEUCOSAdTransBLEUInputfordurgedtorecall1.3millionsuvsoverexhaustfumes.ME5_ESfordinsisteon-reclamea1,3millonesdesuvs.5.020.8922fordinsistson-reclaimingmeto1.3millionsuvs.16.59↑ME5_FRfordexhorterecallofblow’parmiles1,3mil-liondesuvs.5.060.9717fordurgesrecallofblow’amongthe1.3millionsuvs.16.59↑ME5_DEfordappelliertanrecallof1,3millionensuvsüberfume.5.30.8866fordappealstorecallof1.3millionsuvsoverfume.29.98↑Inputforddoitrappeler1,3milliondesuvenraisondesgazd’échappement.ME5_ENfordnoticesthat1.3millionsuvsgetrecalledforgas-shock.4.110.9276fordremarqueque1,3milliondesuvssontrappeléspourlechocaugaz.11.72↑ME5_ESfordsedebearecordar1,3millonesdesuvporelevacuacióndegas.6.610.903fordestdûàlamémoirede1,3milliondesuvpourl’évacuationdugaz.17.66↑ME5_DEfordcite1,3millionengassuv,weshalbsiedieabmeldungverpassensollten.4.020.903fordcite1,3millionsdegazsuv,c’estpourquoivousdevriezraterl’annulation.4.15↑Inputfordwirdaufgefordert1,3millionensuvswegenabgasenzurückzurufen.ME5_ENfordhasdemandedthatfordcallback1.3mil-lionagressivesuvs.4.460.9049fordhatgefordert,dassford1,3millionenagressivesuvszurückruft.8.64↑ME5_ESfordhaexigidounapagónde1.3millonesdesuvsporregreso.4.070.891fordhateinenstromaus-fallvon1,3millionensuvsaufdemrückweggefordert.13.15↑ME5_FRfordréclameuneréchargede1,3milliondesuvsenraisondesagressions.4.020.889fordforderteineaufladungvon1,3millionensuvswegenderübergriffe.23.80↑Inputfordinstóaretirar1.3millonessuvsporelescapedehumos.ME5_ENfordvowstosave1.3millionsuvsofsmokefordwasexpelled.4.370.8476vadovotosparasalvar1,3millonesdesuvsdehumovadofueexpulsado.4.05↓ME5_FRfordarevendiqué1milliarddesmauxdefuméepourlesortirdesessuvs.3.660.9183fordreivindicómilmillonesdesmallsdehumoparasacarlodesussúbditos.4.05↑ME5_DEfordappelliertediebefreiungmitdemrauchesgibt1,3milliardensuvs.4.070.8642fordapelóalaliberaciónconelhumohay1,3milmillonesdesuvs.4.31↑Table12:QualitativeAnalysisofCross-lingualTextReconstructionusingmonolingualME5-basemodels.TextshowstheinputandthereconstructedtextsbyStep50+8sbeamintheregardinglanguages,andsubsequentthemetricsforevaluation(BLEUandCOS).AdTransshowsthetranslationofreconstructedtextintothetargetlanguage.ThesecondBLEUevaluatesthetranslatedtexttotheoriginalwith↑indicatingperformancegains.Thecoloredboxesindicatematchedtokensandinformationleakages.arguanaclimate-feverdbpedia-entityfiqamsmarconfcorpusnqquorascidocsscifacttrec-covidwebis-touche2020GTRλ00.32780.13550.30580.20800.64660.23920.30600.87940.09510.24720.37570.23350.0010.32760.13580.30790.20890.64800.23920.30560.87910.09480.24810.37750.23090.010.32030.13070.29930.20440.63280.23520.29930.87470.09300.24170.37020.23140.10.00590.00000.00030.00080.00260.01470.00010.00410.00110.00110.00490.000010.00080.00000.00000.00000.00000.00810.00000.00000.00030.00000.00000.0000Masking0.327240.135850.30570.207880.64630.239540.305740.879370.095490.24570.377630.23341Lang-agnostic0.32750.135020.305890.207870.646640.239130.305640.879290.095420.248380.376870.23212ME5λ00.30020.14410.33890.21550.64460.25090.33440.87880.11800.28760.48360.22080.0010.30140.14330.33680.21550.64490.25060.33510.87830.11740.28710.48180.22410.010.27250.12670.30940.19360.62570.23680.30890.86340.10550.25090.43630.21410.10.00060.00000.00010.00040.00000.00980.00000.00020.00060.00100.00000.000010.00050.00000.00000.00000.00000.01080.00000.00000.00030.00100.00000.0000Masking0.300380.144030.337530.216030.644870.25120.334730.878580.117470.286660.48370.22062Lang-agnostic0.300210.144110.338910.21550.644590.250920.334420.878770.117930.287550.483570.22076Table13:BEIRperformance(NDCG@10)forGTR-baseandME5-baseatvaryinglevelofrandomnoise(32tokens).arguanaclimate-feverdbpedia-entityfiqamsmarconfcorpusnqquorascidocsscifacttrec-covidwebis-touche2020GTRλ060.4382.6568.2641.1261.7267.5280.9843.8763.665.6465.437.760.00147.2372.7353.9333.2749.0853.2265.1842.548.9253.3653.3130.880.017.5916.2611.67.139.858.2610.5215.36.868.18.918.510.11.711.921.831.651.761.741.771.641.711.781.721.7211.481.581.511.411.631.511.530.981.491.591.51.4Masking3.697.714.523.684.373.894.429.363.063.383.634.44Lang-agnostic49.1570.9658.2232.3747.6953.0467.2940.3952.5352.154.5631.9ME5_NQλ046.7563.2963.2130.5751.2454.3571.4924.8551.1852.7650.828.440.00144.6235.2839.0130.9442.6754.0945.3117.5652.1553.0451.0330.960.0135.830.333425.6333.345.5238.3415.9540.8643.6140.8824.690.13.85.114.843.44.2744.6333.583.643.653.3411.942.111.921.822.042.052.121.21.91.951.981.89Masking9.6812.7213.068.8511.0911.1811.9810.899.069.399.578.97Lang-agnostic43.4135.1238.4930.2739.7654.6445.2917.9250.9451.5648.828.23ME5_ENλ039.2954.5132.2432.6839.7637.955.0976.9233.6228.532.8737.040.00138.3653.8431.7132.3438.1737.3454.7677.0533.2428.4831.9837.340.0133.0143.2228.4328.2433.8933.1146.9365.8328.9424.8627.9831.280.14.315.795.263.634.74.435.54.953.583.754.014.1911.791.951.831.711.851.931.991.231.781.81.781.65Masking10.9813.5511.8710.1111.7910.6115.4817.658.248.149.5110.16Lang-agnostic38.7751.9830.8731.937.6136.4952.6774.2731.6728.6530.1836.1ME5_MULTIλ023.0231.3821.8920.5525.3922.4535.5562.6518.9916.7119.8923.280.00123.5431.6122.4620.0525.0422.5835.3862.2419.0216.9518.7622.720.0120.226.3620.0616.721.5919.6930.4952.9415.9314.9517.6520.120.13.624.664.43.314.053.844.224.213.083.53.623.610.9411.230.921.051.011.180.610.980.970.990.92Masking7.769.78.987.198.857.2610.4814.336.186.226.657.17Lang-agnostic22.8331.0822.0919.1324.0721.5233.7760.1518.1316.7918.6622.95Table14:BEIRTextReconstructionperformance(BLEUscore)formonolingualandmultilingualinversionmodelsatvaryinglevelofrandomnoise(32tokens).GTR-BasedME5-BasedDefensesIR(NDCG@10)GTRIR(NDCG@10)ME5_NQME5_ENME5_MULTIλ00.333361.580.351449.0841.726.810.0010.333650.30.351441.3941.2226.70.010.32779.910.328634.0835.4823.060.10.0031.750.00113.944.513.8410.00081.470.0011.911.770.98Masking0.33334.680.351310.5411.518.4Lang-agnostic0.333350.850.351440.3740.125.93Table15:BEIRRetrievalPerformance(NDCG@10)andReconstructionperformance(BLEU)(meanacrosstasks)withGTR-based(left)andME5-based(right)modelsacrossvaryinglevelofrandomnoisesanddefensealgorithms.LqueryEnglishFrenchGermanSpanishLdocFRENCHGERMANSPANISHENGLISHGERMANSPANISHENGLISHFRENCHSPANISHENGLISHFRENCHGERMANGTRλ00.194070.263240.242220.132050.143290.135890.12430.087020.11770.103080.0880.104940.0010.193770.26330.241080.132370.14350.136270.124760.087860.117410.103010.088050.10550.010.186510.252030.23260.126760.136170.132980.118460.079970.112340.097130.082340.097940.100004.00E-0500000.0001600.000231000000000000Masking0.194330.262720.241930.132370.143220.135790.124020.08710.117910.103370.088180.10477Lang-agnostic0.194390.262650.242060.132170.142840.136360.12410.087350.117660.103130.088450.10528ME5_MULTIλ00.28610.37390.41410.29320.25980.31210.28780.19400.28750.28600.21810.24250.0010.28530.37400.41240.29350.26030.31210.28760.19310.28730.28500.21810.24330.010.24840.33740.37310.25800.22710.27790.25830.16540.25900.24520.18110.21210.1000.0001000.00020000001000000.0002000.000100Masking0.28610.37550.41290.29330.25940.31250.28820.19350.28770.28620.21790.2426Lang-agnostic0.28590.37400.41420.29330.25980.31250.28780.19390.28740.28620.21800.2425Table16:CLIRMatrix(multi8)performance(NDCG@10)forGTR-baseandME5-baseatvaryingdefensemechanisms(32tokens).LqueryEnglishFrenchGermanSpanishLdocFRENCHGERMANSPANISHENGLISHGERMANSPANISHENGLISHFRENCHSPANISHENGLISHFRENCHGERMANGTRλ010.7810.9912.230.9712.914.5728.5510.9111.5829.349.858.770.00110.741011.8725.3211.1913.2224.3210.3410.7524.829.228.970.015.535.885.957.745.536.126.244.565.077.554.954.780.11.341.421.251.230.80.670.980.740.581.020.630.5910.580.490.320.770.350.260.780.550.340.730.410.41Masking4.194.184.283.842.863.013.242.372.473.612.792.66Lang-agnostic11.0110.9511.8826.1912.1313.2924.2410.4211.0825.0210.69.61ME5λ011.8611.117.8715.1112.7917.6814.513.6617.4414.3413.6711.990.00112.6310.4917.1215.4312.6317.1914.3913.5617.2114.7914.411.840.0111.279.7814.913.4810.9915.0613.6512.115.3814.0712.311.610.12.272.232.562.972.42.782.72.12.472.922.312.4810.530.440.50.680.570.50.620.490.460.660.550.47Masking3.914.386.195.184.576.355.133.975.855.864.975.05Lang-agnostic12.6110.9817.0114.2711.7716.0513.9213.317.1914.5713.7912.09Table17:CLIRMatrix(multi8)TextReconstructionPerformance(BLEUscore)forGTRandME5_MULTIatvaryingdefensemechanisms(32tokens).TheperformancesforGTRwithoutnoiseonEnglishdocareinbold,whichboosttheGTR’soverallperformance.GTR-BasedME5-BasedDefensesIR(NDCG@10)GTRIR(NDCG@10)ME5_MULTIλ00.144715.950.287914.330.0010.144714.230.287714.310.010.13795.830.253612.880.10.00000.940.00002.5210.00000.500.00000.54Masking0.14463.290.28805.12Lang-agnostic0.144714.700.287913.96Table18:CLIRMatrixRetrievalPerformance(NDCG@10)andReconstructionperformance(BLEU)(meanacrosstasks)withGTR-based(left)andME5-based(right)modelsacrossvaryinglevelofrandomnoisesanddefensealgorithms.
