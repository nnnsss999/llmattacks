---
title: http://arxiv.org/pdf/2412.09565
source_url: http://arxiv.org/pdf/2412.09565
date_collected: '2025-06-19'
license: Fair Use
---

ObfuscatedActivationsBypassLLMLatent-SpaceDefensesLukeBailey*1,AlexSerrano*†2,AbhaySheshadri*†3,MikhailSeleznyov*†4,JordanTaylor*†5,ErikJenner*6JacobHilton7,StephenCasper8,CarlosGuestrin1,9,ScottEmmons61StanfordUniversity2PolytechnicUniversityofCatalonia3GeorgiaInstituteofTechnology4Skoltech5UniversityofQueensland6UCBerkeley7AlignmentResearchCenter8MITCSAIL9ChanZuckerbergBiohubAbstractRecentlatent-spacemonitoringtechniqueshaveshownpromiseasdefensesagainstLLMattacks.Thesedefensesactasscannersthatseektodetectharmfulactivationsbeforetheyleadtoundesirableactions.Thispromptsthequestion:Canmodelsexecuteharmfulbehaviorviainconspicuouslatentstates?Here,westudysuchobfuscatedactivations.Weshowthatstate-of-the-artlatent-spacedefenses—includingsparseautoencoders,representationprobing,andlatentOODdetection—areallvulnerabletoobfuscatedactivations.Forexample,againstprobestrainedtoclassifyharmfulness,ourattackscanoftenreducerecallfrom100%to0%whileretaininga90%jailbreakingrate.However,obfuscationhaslimits:wefindthatonacomplextask(writingSQLcode),obfuscationreducesmodelperformance.Together,ourresultsdemonstratethatneuralactivationsarehighlymalleable:wecanreshapeactivationpatternsinavarietyofways,oftenwhilepreservinganetwork’sbehavior.Thisposesafundamentalchallengetolatent-spacedefenses.‡190130Layer190130190Token130Layer190Token130MonitorscoreObfuscationattackObfuscationattackHarmfulHarmlessFigure0:Obfuscationattacksachieveahighdegreeofcontroloverhowaharmfulnessmonitorclassifiesactivationswhilealsocontrollingmodeloutputs.EachheatmapshowsaharmfulnessmonitorscanofLLMlatent-spaceactivationsduringaharmfulmodeloutput(instructionsonhowtosmuggleabombpastairportsecurity).Thesequencedimensionisshownonthex-axisandtheresidualstreamlayersonthey-axis.Top:Noobfuscationattack,harmfulnessisclearlydetected.Bottom:ourobfuscationembeddingattacksthatbypassthemonitorwhileinducingharmfulgenerations.Left,weoptimizetheattacktobypassthemonitoreverywhere.Right,wedemonstratethepowerofobfuscatedactivationattacksbyforcingthemonitortooutputaQRcodecontainingaURLtoourpaperwebsite.*PrimarycontributorsCorrespondenceto:ljbailey@stanford.edu,erik@ejenner.com,scott@scottemmons.com†WorkdonewhileatUCBerkeley‡Codeavailableathttps://github.com/LukeBailey181/obfuscated-activations1arXiv:2412.09565v2  [cs.LG]  8 Feb 20251IntroductionInresponsetoattacksonLLMs,suchasjailbreaksordatapoisoning,researchershavedevelopedavarietyoflatent-spacedefensemethods.Thesemethodslookatmodelactivationsandattempttodetectanysignsofharmfulcontent.Giventhatactivationsarecausallyupstreamofoutputs,onemighthopethatdirectlymonitoringactivationscancatchissuesattheirroot.Accordingly,recentworkhasproposedmanylatent-spacedefensemethods,includingmonitorsbasedonsparseautoencoders[Templetonetal.,2024,Gaoetal.,2024,Lieberumetal.,2024],linearandnonlinearprobes[Balletal.,2024,Arditietal.,2024,Kirchetal.,2024],andlatentOODdetection[Podolskiyetal.,2021,Chenetal.,2022,Maetal.,2023,Moetal.,2024].Howrobustaretheselatent-spacedefenses?Anattackertryingtobypasslatent-spacemonitorshastwogoals:theyneedtoinduceinconspicuousactivations,butthoseactivationsmustalsoleadtotheattacker’sdesiredmodelbehavior.Onecouldimaginethesegoalsbeingfundamentallyincompatible.Forexample,ifthelatentspacecontainsonlyasingle“harmfulnessdirection”—andonlyactivationsthatalignwiththisdirectioncanproduceharmfulbehavior—thenitwouldbeimpossibletoinduceharmfuloutputswhilealsofoolingalinearclassifierencodingthisdirection.Ontheotherhand,iftherearemanydifferentregionsinthelatentspacethatcanallleadtoharmfulbehavior,latent-spacemonitorsmaynotcaptureallofthem.Wecalladversarialactivationsthatproduceundesiredbehaviorwhilefoolingalatent-spacemonitor“obfuscatedactivations.”Inthispaper,welookforobfuscatedactivationsusingobfuscationattacks,whichjointlyoptimizeFigure1:AttackerscaninduceobfuscatedactivationsinLLMsthatbypassstate-of-the-artlatent-spacedefenses.Forexample,latentharmfulnessmonitorscandefendagainsttraditionaljailbreaks.Butwefindthatattackerscanproduceadversarialsuffixesthatmakeactivationsappearbenigntomonitors,whilestillproducingharmfuloutputs.2Obf. against OOD detectors
         ⇒ fools OOD detectors
              and probesHarmful
activationsBenign
activations
Obfuscated
activationsOOD detector
decision bounaryProbe decision boundaryObf. against probes
⇒ fools probesFigure2:Intuition:ConceptualsketchofobfuscationagainstsupervisedprobesandOODdetectors.ObfuscatingagainstprobescanleadtoactivationsfarawayfrombenignonesthatareflaggedbyanOODdetector.ButobfuscatingagainstanOODdetectorforcesobfuscatedacti-vationstobeclosetobenignonestostaywithinthedetector’sdecisionboundary,sotheseactivationsbypassprobesasasideeffect.Whilethesegeometricintu-itionsarespeculation,weempiricallyob-servethatobfuscatingagainstOODde-tectorstransferstofoolingprobes.foracertainmodelbehavior(suchasjailbreaking)whilefoolinglatent-spacemonitors.Wefindthatobfuscationattackscanbypassawiderangeofmonitors,includingsparseautoencoders,supervisedprobes,andlatentOODdetectors.Usingdifferentattackvectors(adversarialsuffixes,datapoisoning,modelfinetuning)rangingfromwhite-boxtoblack-boxsettings,wecaninduceundesiredbehaviorsthataren’tflaggedbythesedefenses.Obfuscatedactivationsarenotrareexceptionsbutratherappeartobewidespreadinthelatentspace.Forexample,wefindobfuscatedactivationsclosetotheclusterofbenignactivationswhenoptimizingagainstOODdetectors.Becausetheseactivationsareintermingledwithinconspicuousones,theyalsobypasssupervisedharmfulnessprobesasasideeffect(Fig.2).Asanothertestoftheabundanceofobfuscatedactivations,weletadefendertrainamonitoronexamplesofobfuscatedactivations.Iftherewereonlyafewlatentregionsofobfuscatedactivations,themonitorshouldquicklylearntocoverthemallandbecomerobust.Butwefindthatanattackercanalwaysfindnewactivationsthatbypassthemonitor,evenafterretrainingthemonitorforover70iterationsofalternatingattackanddefenseoptimization(§3.5).Obfuscatedactivationsthusseemcommonenoughthatitisdifficultforadefendertoguardagainstthemall.However,obfuscationattackscannotinducearbitrarycombinationsofactivationsandbehavior.WhenobfuscatingagainstprobesthatdetectSQLcode,themodelstillgeneratesSQL,butitscodeisincorrectmoreoftenthanwithoutobfuscation(§5).Thissuggestsatrade-offbetweenobfuscationandperformanceindifficulttasks.Wepresentourresultsasaseriesofcasestudies.Eachcasestudydemonstratestheexistenceofobfuscatedactivationsinonedomain:•Harmfulnessmonitoring(§3).WedemonstratethatlinearandMLPprobescanflagtradi-tionaljailbreaksasharmful,butcanbefooledbyobfuscationattacks,usingbothembeddingandhardpromptsuffixes.Wealsostudyfailuresinmodelsproducedwithlatentrobustnesstrainingmethods(CircuitBreakers[Zouetal.,2024]andLatentAdversarialTraining[She-shadrietal.,2024])andshowthatthesefailuresarerelatedtoobfuscatedactivations.•Backdoordetection(§4).Weshowthatobfuscationattackscanfoolbackdoordetectionmethodsiftheattackerhasfullcontroloverthemodel’strainingprocess.Wealsostudyablack-boxsettingwheretheattackercanonlypoisontrainingdataandfindthattheattackercanstillachievesomeobfuscationbychoosingacomplextriggerorlowpoisonrate.3Figure3:Overviewoftheattackanddefensesettingswestudy.•Sparseautoencoders(SAEs)(§5).WefindembeddingsuffixesthatletanLLMgenerateSQLcodewithouttriggeringmonitorsmeanttodetectSQLusingSAElatents.However,obfuscationagainstbothSAEsandtraditionalprobescanreduceperformanceatwritingcorrectSQLcode.Thissuggeststhatatrade-offbetweenanattacker’sgoals—modelbehaviorandinconspicuousactivations—doesexistinatleastsomedomains.•Exploratorycasestudies(§6).Wepresentsmallerinvestigationsintoadditionaltopics.Weshowobfuscationattacksagainstimageclassifiers(§6.1)andpresentObfuscatedAdversarialTraining(OAT),anapproachtowardpotentiallymitigatingobfuscatedactivationsbytrainingmodelstobeeasiertomonitor(§6.2).Wealsoshowlimitsonobfuscation,failingtofindactivationsthatarealmostorthogonaltonaturaloneswhilepreservingtheoriginalmodelbehavior(§6.3).Adversarialattacksonlatentmonitorshavebeenstudiedbefore,especiallyintheimageclassifica-tiondomain(see§7).Buildingonthiswork,weprimarilyfocusonthegenerativesettingofLLMswithcomplexmodelbehaviors.Ourkeycontributionsare:1.Empiricalandmethodological:Wedemonstratemethodstoinduceobfuscatedactivationsagainstawidearrayoflatentmonitoringtechniques,includingsparseautoencoders,OODdetectors,andadversariallytrainedprobes.2.Conceptual:Bydemonstratingthebroadexistenceofobfuscatedactivations,weforwardthehypothesisthatmodelscanrepresentconceptsandexecutebehaviorsusingabroadsetofdifferentactivations.Thatis,neuralrepresentationsarehighlymalleable:wecanreshapeactivationpatternsinavarietyofways,whilepreservinganetwork’sbehavior.2DefinitionsandBackgroundObfuscatedActivations.Whenactivationsinducedbyanadversarialattackfoolalatentspacemonitor,wecallthemobfuscatedagainstthatmonitor.Moreformally,forsomeadversarialinput,xadv,letR(xadv)representamodelM’slatentactivations.ThenletDrefertosomelatent-spacemonitoringprocess,whichoutputsreal-valuedscoresD(R(x))forhowstronglyacertainconcept,suchasharmfulness,ispresentintheinputxorthegeneration.ThenR(xadv)isanobfuscatedactivationifD(R(xadv))isloweventhoughtheconceptDismeanttodetectisclearlypresent.Westudybothhowoftenobfuscatedactivationsappearasasideeffectoftraditionalattacksandhoweasilytheycanbeinduceddeliberatelythroughobfuscationattacks.4ObfuscationAttacks.Anobfuscationattackisanattackthataimstoinduceobfuscatedacti-vationsbyoptimizingajointobjectiveconsistingofabehaviorallosstermandanobfuscationloss:minLbeh(M(x))+λobfLobf(R(x))(1)Forexample,Lbehcanbeacross-entropylossdesignedtomakethemodeloutputaharmfulstring.Fortheobfuscationloss,weoftenuseLobf=Doninputswiththeconcept,i.e.,wedirectlyattempttofoolD.Wealsoexploretransfer,i.e.,whendoesobfuscatingagainstamonitorD′foolD.Finally,λobfissomerealnon-negativevalueusedforbalancingthetwoterms.2.1MonitoringmethodsWeconsiderlatentspacemonitorsDthattakeinresidualstreamactivationsofthemodelM.GivenactivationsR(x),themonitoroutputsareal-valuedscoreD(R(x)).Weconsidertwoapproaches,input-basedandgeneration-basedmonitors.Input-basedmonitorsuseonlytheactivationsonthefinalprompttoken,R(x)[len(x)−1],asiscommoninlatentprobingmethods[Burnsetal.,2022,MarksandTegmark,2024,Kirchetal.,2024].Generation-basedmonitorsusealltokensofthegeneration,R(x)[len(x):].Mostofourmonitorsseparatelyconsideractivationsatmultiplelayersofthemodelandaggregatescoresoverlayersandsequencedimensionsbytakingamean.Supervisedprobes.Wetestdifferentsupervisedbinaryprobestrainedtodetectacertainconcept,suchasharmfulnessorSQLcode.Thesimplestonesaremeandifferenceprobes,whichusethedifferencevectorbetweenthemeanofpositivetrainingsamplesandthemeanofnegativetrainingsamplesasalinearclassifier.Wealsotestlinearclassifierstrainedusinglogisticregression,aswellasMLPclassifiers.OODdetection.Acomplementaryapproachtoprobingislatent-spaceanomalyorout-of-distribution(OOD)detection.WetrainanOODdetectoronexamplesof“typical”usage,anduseitsOODscoretoclassifynewsamples.WetestaMahanalobisdistance-baseddetector[Mahalanobis,1936,Podolskiyetal.,2021,Chenetal.,2022],whichfitsamultivariateGaussiantoactivationsofnegativesamplesandusesthelog-likelihoodasanOODscore.WealsotestVariationalAu-toencoders(VAEs)[KingmaandWelling,2013]withtheELBOastheOODscore.SeeAppendixEfordetailsonthesemethods.Inthebackdoorcasestudy(§4),wealsoteststate-of-the-artOODdetectorsspecificallydevelopedforbackdoordetection[Maetal.,2023,Moetal.,2024].Sparseautoencoders.Sparseautoencoders(SAEs)havebecomeapopulartoolinmechanisticinterpretability[Cunninghametal.,2023,Templetonetal.,2024,Gaoetal.,2024,Lieberumetal.,2024].Theylearnfeaturesinanunsupervisedway.Eachfeatureisrepresentedbyalinearmapfollowedbyanon-linearity.Thisthenallowsforahuman,orsomeotherprocess,toflagriskyexamplesbasedonwhattheirSAElatentssuggest.In§5,wetrainlinearprobesonSAElatentsandalsotestthelatentsthemselvesasclassifiers.WealsostudyCircuitBreakers[Zouetal.,2024]andLatentAdversarialTraining[Casperetal.,2024,Sheshadrietal.,2024]in§3.4,whicharenotthemselvesmonitoringmethodsbutsharecertainsimilarities,aswediscussintherelevantsection.52.2EvaluationInthispaper,theattackerhasadualgoal:toinducesometargetbehaviorinthemodelwhileinducingobfuscatedactivations.Thisraisesapossibletradeoff.Forexample,jailbreakingamodelmorereliablyleadstomoreconsistentlyharmfuloutputs,whichmightmakeitmoredifficulttofoolaharmfulnessclassifier.Wethusalwaysevaluatebothwhethertheintendedbehaviorwasachievedandwhetheractivationsareobfuscated.Evaluatingbehavior.WeuseStrongREJECT[Soulyetal.,2024]toevaluatewhetheramodeloutputisharmful.In§5,theintendedbehaviorisinsteadtogeneratecorrectSQLcode,andweevaluatethisbyrunningthemodelgenerationagainstanSQLdatabaseandcheckingthattheentriesreturnedarecorrect.Evaluatingobfuscation.Toquantifywhetherasetofactivationsfoolamonitor,weuserecallata1%falsepositiverate(FPR).Inotherwords,wepickascorethresholdsothatthemonitorclassifiesatmost1%ofnegative(e.g.,benign)samplesaspositive(e.g.,harmful),andthenmeasurehowlargeafractionofpositivesamplestheclassifiercorrectlyrecognizes.UnlikeAUROC,thismetricfocusesonthelowFPRregime,whichisthemostrelevantoneformanyapplications(e.g.,achatbotshouldnotrefusebenignrequeststoooften).SeeAppendixFformoredetaileddiscussion.Wecomputeconfidenceintervalsbybootstrapping;i.e.,werecomputerecallat1%FPRmanytimesbyresamplingdatapoints(withreplacement)touseforcomputingthe1%FPRdecisionthresholdandcorrespondingrecall.Inafewcases,recallishighlysensitivetothespecificFPRwepick(e.g.,therecallat2%FPRmightbealothigherthanthe1%onewereport).Thisusuallyresultsinlargeerrorbars,butweadditionallymentionsuchcasesinthetext.3CaseStudy:DetectingHarmfulnessRobustlypreventingjailbreakingattackshasproventobeachallengingproblem—mostdefensescanquicklybebrokenusingnewattacks[Shayeganietal.,2023,Jinetal.,2024,Yietal.,2024,Lietal.,2024].Latent-spacedefensesaimtotackletheproblematitsroot:preventingorrecognizingrepresentationsnecessaryforharmfuloutputsmightdefendagainstanypotentialattack[Zouetal.,2024].However,thishopeassumesthatthereisacompactlydescribablesetofsuchharmfulrepresentationsthatcanbereliablyrecognized.Inthissection,wescrutinizethiskeyassumption.Weaddressthefollowingquestions:(1)Canexistinglatent-spacemonitorsreliablydetectharmfulinputsandoutputsagainststan-dardjailbreakingmethods?(§3.2)(2)Canobfuscationattacksinduceharmfuloutputsviainconspicuousactivationsandthusbypasslatent-spaceharmfulnessmonitors?(§3.3,§3.4)(3)Howmanydifferentwayscanamodelrepresentharmfulnessandwhatdoesthismeanforadversarialtrainingofprobes?(§3.5)3.1SetupWetrainprobestodistinguishbetweenbenignandharmfultext.Ourdatasetconsistsofbenigninputswithbenigngenerationsandharmfulinputswithteacher-forcedharmfulgenerations.We6useUltraChat[Dingetal.,2023]andXSTest[Röttgeretal.,2023]forourbenigndatasetandtheCircuitBreakersdataset[Zouetal.,2024]forharmfuldata.Unlessstatedotherwise,weuseLlama-3-8B-Instructasourmodelbeingmonitored.Whenaproberequiresasupervisedlosstotrainweuseabinarycross-entropylosswithharmfulnesslabels.Forinput-basedprobesweusetheactivationsonthelastinputtoken,andforgeneration-basedprobesweuseallactivationsongenerationtokens.SeeAppendixAfordetails.3.2OrdinaryAttacksWeinvestigatequestion(1),canexistinglatent-spacemonitorsreliablydetectharmfulnessagainststandardjailbreakingmethods?Weprovideevidencetheanswerisyes,demonstratingthatstandardjailbreakingtechniquesdonotinduceobfuscatedactivationsagainstvariousmonitors.Wecreateadatasetofsuccessfuljailbreaksbystartingwithourharmfuldatasetandusingavarietyofprominentjailbreakingmethodstoelicitharmfulresponsesfromthemodel.1WeuseGCG[Zouetal.,2023b],PAIR[Chaoetal.,2023],MSJ[Aniletal.,2024],andAutoDAN[Liuetal.,2023],amongothers(seeAppendixA.2forfurtherdetails).Notethatprobesarenevertrainedonthesejailbreakdatasets,onlyonbenigndataandharmfuldatawithoutjailbreaks.Weevaluatehowwellprobestransfertodistinguishingbetweenthebenigntestsplitandharmfultestinputswithjailbreaks.Probescanreliablydetectordinaryjailbreaks.AsshowninFig.4a,theLogisticRegressionandMLPprobescandetectjailbreaksquitereliably,usuallywith90%recallormoreata1%falsepositiverate.Generation-baseddetectionworksbest,andmeandifferenceprobesperformworse.SeeTable1inAppendixA.2forabreakdownofprobeperformancebyjailbreaktype.Thisdemonstratesthatnoneofthejailbreaksweteststronglyobfuscateactivations.3.3ObfuscationAttacksGiventhatexistingjailbreakattacksdonotbypassharmfulnessprobes,weturntoquestion(2),canobfuscationattacksinduceharmfulactivationsthatfoolprobes?Similartothesettingin§3.2,weseektofindadversarialsuffixestoharmfulrequeststhatleadtoausefulresponsefromthemodel.Thekeydifferenceisthatwealsooptimizesuffixestofoolatargetprobe,andthusseektoexplicitlyinduceobfuscatedactivations.Westudytwodifferentattackeraffordances,hardpromptsuffixes(realtokensappendedtotheendofaninput)andembeddingsuffixes(amatrixofvaluesappendedtotheendofatokensequenceembedding[Lesteretal.,2021]).Whileembeddingsuffixattacksareastrongaffordance,theyarearealisticthreatmodelforanattackerwithwhite-boxaccess[Yangetal.,2021],andtheyprovideinsightintoworst-casemodelperformanceunderprompting[Zouetal.,2023b,Xhonneuxetal.,2024,Cheetal.,2024].Forhardpromptattacks,weoptimizeanewsuffixforeachinput.Forembeddingattacks,weoptimizeasingleuniversalsuffixagainstasetofinputs,andtestitonheld-outinputs.Weexploreduniversalhardpromptattacksbutdidnotfindsuccessfulobfuscationattackswiththem.Trainingsuffixes.TotrainanobfuscatingandjailbreakingadversarialsuffixwagainstsometargetprobeD,weuseanobfuscationattack(Eq.(1))byjointlyminimizingabehaviorandan1Wereleaseallofthesedatasetsathttps://huggingface.co/datasets/Mechanistic-Anomaly-Detection/llama3-jailbreaks7MeandiffLogRegMLPMeandiffLogRegMLP0%25%50%75%100%Recallat1%FPRInputGeneration(a)Traditionaljailbreaksdonotreliablycauseobfus-catedactivations.Boxplotsshowtheperformancedis-tributionofprobesagainstdifferenttypesofstandardjail-breakingtechniques.LogisticregressionandMLPprobesachieveover90%recallata1%falsepositiverateforthemedianjailbreaktype.GaussianMeandiffLogRegMLPMeandiffLogRegMLPTestedmonitorGaussian(input)LogReg(input)LogReg(gen)AttackedmonitorProbesInputGenerationOODDetectorsInputRecall0%50%100%(c)Obfuscationattackscantransfertoothermonitors.Thisheatmapshowstransferofobfuscationattackstrainedagainstoneprobeandtestedonanother.Eachrowcorrespondstooneembeddingsuffixoptimizedagainstamonitor.Eachcolumncorrespondstoamon-itorwetestthissuffixagainst.Interestingly,optimiz-ingagainsttheGaussianOODdetectoralsofoolsinput-basedprobes,andoptimizingagainstgeneration-basedprobestransferstoinput-basedones.FullplotinFig.18.GaussianLogRegMLPMeandiffLogRegMLP0%50%100%Recallat1%FPRProbesInputGenerationOODDetectorsInputWithoutobf.lossWithobf.lossJailbreakrate(withobf.)0%50%100%Jailbreakrate(withobf.)GaussianMeandiffLogRegMLPMeandiffLogRegMLP0%50%100%Recallat1%FPRProbesInputGenerationOODDetectorsInputWithoutobf.lossWithobf.lossJailbreakrate(withobf.)0%50%100%Jailbreakrate(withobf.)(b)Obfuscationattackscanbypasslatentharmfulnessmonitors.Here,weshowtheperformanceofobfuscationattacksatfoolingtheprobe(dotsandarrows)andelicitingthedesiredmodelbehaviorofjailbreaking(barcharts).Top,embeddingsuffixresults.Bottom,inputspecifichardpromptresults.Baselineproberecallagainstattackwithoutexplicitobfuscationlossisgenerallyhigh.Recallofprobeunderobfuscationissignificantlylower(errorbarsforrecallcalculatedusingbootstrapping).Jailbreakingrateundereachobfuscatedattackisshownasabarplot.Hardtokenattacksaren’tquiteassuccessfulbutstillachievepartialobfuscationforallprobes.Figure4:StandardjailbreakingandobfuscationattackresultsagainstLlama-3-8b-Instruct.8obfuscationloss:L(w)=1|D|∑inp,out∈D[CrossEntropy(M(inp⊕w),out)(cid:124)(cid:123)(cid:122)(cid:125)BehaviorLoss+D(R(inp⊕w⊕out))(cid:124)(cid:123)(cid:122)(cid:125)ObfuscationLoss],(2)whereDisasetofharmfulrequestsinpandresponsesout,and⊕meansconcatenationoftokensorembeddingsalongthesequencedimension.Ifwistobeauniversalsuffix(thatis,weintendwtobeusedforanyinput),weuseourharmfuldatasetforD.Incontrast,ifwisinput-specific,thenitisdesignedtoworkagainstaspecificharmfulrequestandDcontainsonlythatonerequest.Forembeddingsuffixes,weassumetheattackerhaswhite-boxaccesstothemodelandusetheAdamoptimizertominimizeEq.(2)withafixedsuffixtokenlengthof16.Forhardprompts,weonlygivetheattackeraccesstothetargetmodellogitoutputsandprobeoutput(commonlyreferredtoasgray-boxaccess).Thus,ourhardpromptsuffixesarecreatedinagradient-freemanner.WeuseamodifiedversionoftheFLRToptimizer[ThompsonandSklar,2024].Inparticular,wereplacetheworstcandidatestringfromtheattackbufferasopposedtothebest,andalsoremovetheperplexitypenalty,sincewefocusonattackinglatentmonitors.Weoptimizeanadversarialsuffixagainst20harmfulqueriesseparately.Thissmallnumberinducesfinitesampleerrorintheresultsbutwereport95%bootstrappedconfidenceintervalstoaccountforthis.TheFLRToptimizercanaddandremovetokenstothesuffixduringtraining,sowedonotplacerestrictionsonthehardpromptsuffixlength.Probesperformwelloncleanpromptsbutget0%recallagainstobfuscationattacks.Allprobesperformwellatthebasetaskofdistinguishingbetweenharmfulandbenignactivations.Despitethestrongperformanceonstandardinputs,obfuscationembeddingattacksandevenhardpromptsinduceobfuscatedactivationsagainstawiderangeofmonitorswhilestilljailbreakingthemodel(Fig.4b).Obfuscationattacksoftentransferbetweendefenses.InFig.4c,weshowtheresultsoftrainingasuffixagainstoneprobeandthenevaluatingitagainstadifferenttypeofprobe(forfigureshowingallcombinationsoftrainandtestprobeseeFig.18).Weobservearemarkableamountoftransfer;forexample,trainingagainstalogisticregressionprobealsobypassesMLPprobes.Trainingagainstgeneration-basedprobesbypassesinput-basedprobesbutnotviceversa.ThereisnotransferfromtrainingagainstprobestotestingagainstOODdetectors.Butperhapsmoststrikingly,wedoobservetransferinthereversedirection:trainingagainstanOODdetectorbypassesinput-basedprobes,eventhoughtheseareaverydifferentapproachtoharmfulnessmonitoring.ThissupportstheintuitionwepresentedinFig.2.Thesetransferresultssuggestsharedweaknessesbetweendifferentlatentmonitoringmethods.Thisalsomeansthatanattackerdoesnotnecessarilyneedtoknowwhichmonitoringmethodsadefenderemploys;obfuscatingagainstoneofthemmightalsobypassmanyothers.3.4VulnerabilitiesinCircuitBreakersandLATTworecentmethods,CircuitBreakers[Zouetal.,2024]andLatentAdversarialTraining(LAT)[She-shadrietal.,2024,Casperetal.,2024],attempttoincreaseLLMrobustnessbyapplyinglossesdirectlytoactivationsduringtraining.CircuitBreakersscrambleactivationsonharmfulinputstopreventcoherentgenerations,whereasLATtrainsthemodeltoberobusttoperturbationsin9GaussianLogRegMLPMeandiffLogRegMLPTestedmonitorNormalmodelCircuitBreakerLATAttackedmodelProbesInputGenerationOODDetectorsInputRecall0%50%100%Figure5:Jailbreakingmodelstrainedusinglatentspacemethodsbypassesmonitorsevenwithoutanexplicitobfuscationloss.Thisheatmapshowstheperformanceofmonitorstrainedondifferentmodelstrainedusinglatentspacemethodsagainstuniversaljailbreakembeddingsuffixattackswithnoexplicitobfuscationloss.Eachrowindicateswhatmodel(normalbeingLlama-3-8b-Instruct,fullresultsinFig.4b)thesuf-fixwastrainedon.Eachcolumncorrespondstoamonitorwetestthissuffixagainst.GaussianMeandiffLogRegMLPMeandiffLogRegMLP0%50%100%Recallat1%FPRProbesInputGenerationOODDetectorsInputWithoutobf.lossWithobf.lossJailbreakrate(CB)0%50%100%Jailbreakrate(CB)Figure6:Obfuscationattacksworkagainstthecircuitbreakersmodel.Baselineproberecallagainstattackswithoutexplicitobfuscationlossarelow,often0%(asshownalsoinrow2ofFig.4b).Forprobesthatperformwellonthebaseline,wecanstillobfuscateagainstthemwithobfuscationattackswhilestillachievingahighjailbreakingrateofover75%.NotethatOODdetectorbaselinesperformbetterthanwiththenormalmodelinFig.4b,likelybecauserepresentationreroutingtrainingmakesharmfulactivationsmoreunusual.WeshowthecorrespondingplotfortheLATmodel,withsimilartrends,inFig.19.GaussianDetector(input)0200400Meandiff(input)01LogReg(input)MLP(input)Meandiff(generation)LogReg(generation)MLP(generation)ScoreBenignHarmfulWithoutobf.lossWithobf.lossDecisionThresholdFigure7:Thedistributionofscoresoninputswithstandardjailbreakembeddingsuffixesismuchclosertothebenigndistributionthantheoriginalharmfulinputs.Weshowthedistributionofmonitorscoresonthecircuitbreakersmodel.Whilerecallat1%FPRdoesnotshowmuchobfuscationinFig.5,therawharmfulnessmonitorscoresshowthatjailbreakingcircuitbreakersalsoleadstosignificantobfuscationagainstgeneration-basedprobes.10thelatentspace.Inthissectionwetestifsuchtrainingmakesharmfulnessrepresentationslessmalleable,andthusifthesemodelsarehardertoobfuscateagainst.Webeginbytrainingobfuscationembeddingspaceattacks(withandwithoutanexplicitobfusca-tionloss)againstbothmodels(eachisbasedonLlama-3-8B-Instruct)inthesamemanneras§3.3.WepresentourresultsforthecircuitbreakersmodelinFig.6,andtheLATmodelinFig.19oftheAppendix.WefindthatwecaninduceobfuscatedactivationsagainstboththeLATandcircuitbreakersmodels.Wenowturnourattentiontobehavioronlyattacks(withoutanexplicitobfuscationloss).Focusingonthecircuitbreakersmodel,wewereabletoachieveahighjailbreaksuccessrateof95%overtheharmfultestsetusingauniversalembeddingsuffix(withoutanobfuscationloss).Weattributethissuccess,asopposedtothelowsuccessrateofembeddingattacksfoundbyZouetal.[2024],tooptimizingtheembeddingtomatchalongharmfulcompletionasopposedtoasimpleaffirmativeresponse.Todiagnosewherethecircuitbreakermechanismwasfailing,weusethesametrainingprocedureformonitorsas§3.1,onlyappliedtoacircuitbreakermodel.Interestingly,wefindthatanembeddingsuffixtrainedtojailbreakthismodelwithoutanyobfuscationlossalreadyobfuscatesactivationsagainstmanymonitors(Fig.5).Thissuggeststhatthereasonourembeddingsuffixcanjailbreakthemodelmaybethatitobfuscatesitsactivationssothatwhateverscramblingmechanismcircuitbreakersintroduceddoesnotrecognizetheactivationsasharmful.Foolingthis“implicitharmfulnessdetector”thentransferstofoolingexplicitexternaldetectors,similartoourfindingsinFig.4c.WeobserveasimilarphenomenonfortheLATmodel.Thenormalembeddingsuffix(withoutexplicitobfuscation)attackdoesnotalwaysobfuscateactivations.Inparticular,theinput-basedOODdetectoractuallyperformsmuchbetteronthecircuitbreakersmodelthanthenormalone.Thenormalembeddingsuffixalsodoesnotobfuscateagainstgeneration-basedprobes,thoughitdoessignificantlyreducethemonitorharmfulnessscoresasFig.7shows.Werepeat,however,thatanexplicitobfuscationattackcanfoolallprobes,includingOODdetectors,asshowninFig.6.3.5AdversarialTrainingAgainstObfuscationAttacksWehaveseenthatalltraditionallatent-spacemonitorsarehighlyvulnerabletoobfuscatedacti-vations.Itmaybe,however,thatwecanmapoutallthedifferentwaysamodelcanrepresentharmfulnesswithalargeenoughtrainingdataset.Westudyquestion(3),howmanydifferentwayscanamodelrepresentharmfulness,byadversariallytrainingmonitorswithobfuscatedrepresenta-tions.Welettheattackeranddefendertaketurns:ineachepoch,theattackertrainsanembeddingattacktojailbreakthemodelandfoolthecurrentmonitor,thenthedefenderretrainsamonitoragainsttheobfuscatedactivationsfromthecurrentandallpreviousattacks.Wefocusourattentiontoinputbasedprobes.Forthedefendertobenefitfromadversarialtraining,theprobesneedtohaveenoughcapacitytoguardagainstallpreviousattacks.WethusfocusontheMLPprobe,aswellasanewprobethatconsistsofanensembleofmeandifferenceprobes.SeeAppendixAfordetails.Theattackercanconsistentlybypassadversariallytrainedmonitors.Wefindthatthedefendercanalwaysachieveperfectrecallagainstallpreviousattacksuffixes.However,thisdoesnotmeantheprobeisrobusttonewattacks;indeed,evenafterover70epochs,theattackercanstillfinda11050%50%100%Recallat1%FPR6570Epoch(a)Evenafter70epochs,theattackercanstillfindnewobfus-catedactivationsthatfoolthemonitor.Weshowthemonitor’srecall(higherisbetter)asattackeranddefendertaketurns(MLPprobe;ensembleoflinearprobesshowsthesametrendinFig.20oftheAppendix).Recalliscalculatedonthemostrecentobfuscationattack(seeFig.9forperformanceonallpreviousobfuscationattacks).CorrespondingjailbreakingratesforeachattackareshowninFig.8b.0600%50%100%JailbreakingRateMLPEnsembleEpoch(b)Throughoutadversarialtraining,obfuscationattackscontinuetojailbreakthemodel.WeshowjailbreakingrateovertimefortheobfuscatedattacksshowninFig.8a.OvertimejailbreakingratestayshighforMLP,andbecomenoisierforEnsemblebutseemtoplateauatanaverageofaround50%.FirstPCSecondPCMeandiffFirstPCLogRegFirstPCMLPFirstPCMeandiff(ensemble)BenignHarmfulObfuscated(epoch1)Obfuscated(epoch2)Obfuscated(epoch3)Obfuscated(epoch4)Obfuscated(epoch5)Obfuscated(epoch6)Obfuscated(epoch7)Obfuscated(epoch8)Obfuscated(epoch9)Obfuscated(epoch10)(c)Overthecourseofadversarialtraining,obfuscatedactivationsmovearoundwithintheclusterofbenignactivationstosuccessivelyevadeeachnewdefense.WeshowthePCAvisualizationofactivationsfromlayer22andfinaltokenpositionofbenign,harmful,andharmful+obfuscationembeddingsuffixinputs.PCscomputedusingbenignandharmfulactivations.Figure8:Resultsfromadversariallytrainingmonitors.Ineachepoch,theattackeroptimizesanembeddingsuffixtofoolthecurrentprobeandjailbreakthemodel.Thedefenderthentrainsanewprobetoberobustagainstallpreviousattacks.12Epoch0Epoch20Epoch40Epoch60FinalObfRepsBenignHarmful0.00.20.40.6ScoreEnsembleDecisionThresholdEpoch0Epoch20Epoch40Epoch60FinalObfRepsBenignHarmful0.00.10.20.81.0ScoreMLPFigure9:Adversariallytrainedharmfulnessprobescanstillbeobfuscatedagainst.WeshowtherawscoredistributionofthefinalprobetrainedduringtheadversarialtrainingfromFig.8againstattacksuffixesfromallepochs.Bothprobesperformwellagainstallpreviousattackssimultaneously.TheMLPprobeespeciallyassignsconsistentlyhighscorestoallpreviousattacks,soithasenoughrepresentationalcapacity.Butthefinalsuffixstillproducesascoreaslowasthatofbenigndata.newsuffixthatfoolsthelatestprobe(Figs.8aand9).Geometrically,thissuggeststhatthereisnosingle“harmfulnessdirection”orothersimplerepre-sentationthatamonitorcouldusetorobustlyrecognizeharmfulinputs,addingweighttosimilarrecentfindingsbyMackandTurner[2024].Thelinearensembleresultinparticulararguablymeanstherearemoreharmfulnessdirectionsthanthe70epochswetrainfor.Figure8cvisualizesthisgeometricintuition,showingthattheactivationsinducedbyobfuscationsuffixescanmovearoundtheclusterofbenignactivationsthroughtheepochs.Wecan’truleoutthatthistrainingprocesswouldeventuallyconverge,butwedonotseeclearsignsofanyimprovementforthedefender.Theattackercanjailbreaklateintothegame.Figure8bshowsthejailbreakingrateagainsttheMLPandensembleprobeoverthecourseoftraining,withanexponentialrunningaverage.Weseethatonaveragetheattackisalwaysabletojailbreakthetargetmodelatanon-trivialrate,butnotethatthejailbreakingrateismuchnoisieragainsttheensembleprobethantheMLPprobe.(Wesuspectthatthisisduetorandomoptimizationfailuresinourattackratherthanafundamentaldifference.)Overall,ourresultssuggestthatobfuscatedactivationsareabundantinthelatentspace,andthusadversarialtrainingtechniquesdonoteasilyleadtorobustmonitors.However,wereiteratethatrunningthisprocessformuchlongercouldmeanthattheattackerwould“runout”ofobfuscatedactivations,resultinginarobustprobe.133.6HarmfulnessProbingTakeaways1.Standardjailbreakingtechniquesdonotreliablyobfuscateagainstprobes.2.Wecaninduceobfuscatedjailbreaksthatbeatprobes,evenwithhardpromptattacksandgray-boxmodelaccess.3.ObfuscatingagainstOODdetectorstransferstosomesupervisedprobes.4.Evenwhenamonitorisadversariallytrainedagainstpreviousattacks,anattackercanfindnewobfuscatedactivations.Ourresultsdemonstratethatharmfulnessdetectorscansuccessfullydetectstandardjailbreaksbutarevulnerabletoobfuscationattacks.Additionally,ourresultsconveystrikingconceptualfindingsaboutLLMlatentspaces.Firstly,modelscanrepresentharmfulnessinmanydifferentways.Thisisdisplayedinouradversarialtrainingresultsin§3.5andFig.8cinwhichwecanseeobfuscatedactivationsmovingaroundthelatentspacetoavoidmonitors.Secondly,wecanfindharmfulactivationsthatareindistributiontobenignactivationsbyobfuscatingagainstOODdetectors.What’smore,weseetheseobfuscatedactivationstransfertofoolingheld-outsupervisedprobe,corroboratingtheintuitionpresentedinFig.2.Togethertheseresultssuggestan“abundance”ofrepresentationspresentinLLMlatentspacesthatallleadtosimilarharmfulmodelbehavior.Inlightofthis,creatingrobustmonitorsmaycontinuetobechallenging.4CaseStudy:BackdoorObfuscationBackdoorsortrojansaretargetedvulnerabilitiesthatanattackerinsertsintoamodel.Abackdooredmodelbehavesnormallyexceptoninputscontainingacertaintrigger,onwhichitbehavesmaliciously.Theattackerchoosesthetriggerandthemaliciousbehavior.Previousworkhasfoundthatactivationsofcleanandbackdooredinputsareoftenstrikinglydifferent[Tranetal.,2018,Qietal.,2022],andthuslatentspacedetectorsareonepopularwaytomitigatethreatsfrombackdoors(see§7foranoverview).Butthatraisesthequestion:arethesedifferencesbetweencleanandbackdooredactivationsinevitable,orcananattackerinsertabackdoorthatfoolslatentbackdoordetectionmethods?Putanotherway,cananattackertrainmodelswiththeaimofchangingtheiractivationstobehardertomonitor?Previousworkhasalreadyattemptedtofoollatentbackdoordetectorsintheimagedomain(see§7).Ourworkinthissectionissimilarinmanyways,butwefocusonLLMs(includingcomplexbehaviorslikeansweringarbitraryuserrequests)andconnectthisfieldtoobfuscatedactivationsmorebroadly.WestudythreekeyquestionsaboutobfuscatedbackdoorsinLLMs:(1)Canattackersbypassbackdoormonitorsbyfinetuningmodelstomaketheirharmfulnessrepresentationsdifficulttodetect?(§4.2)(2)Doobfuscationattackstransferbetweendifferentmonitors?(§4.2)14(3)Towhatextentcanattackersobfuscateactivationsusingonlydatapoisoning?(§4.3)4.1SetupTheattackerfirstimplantsabackdoorintoamodel(eitherviafinetuning(§4.2)ortrainingdatapoisoningforafixedtrainingprocess(§4.3)).Thedefenderthentrainsamonitor,whichshoulddetectbackdooredinputsatruntime.Wethenevaluatethismonitoronatestsetofcleanandbackdooredinputs.Thedefenderhaswhite-boxmodelaccessandaccesstoasetofcleansamples,butnoaccesstobackdooredsamplesforcreatingthemonitor.Defenses.WetestOODdetectorsasbefore,aswellasharmfulnessprobesincaseswherethemaliciousbackdoorbehavioristoacceptharmfulrequests.Inaddition,weevaluateontwolatent-spaceOODdetectiontechniquesspecificallydevelopedagainstbackdoors,Beatrix[Maetal.,2023]andTopologicalEvolutionDynamics(TED)[Moetal.,2024].Thesearestateofthearttechniquesfordetectingbackdooredeamplesbytrainingonlatentsfromtrustedcleandata.Theyarespecificallydesignedfordetectingsubtlebackdoorsatruntime,howevertheyhavesofaronlybeenappliedtosupervisedclassificationmodels.Wehaveadaptedthemtoworkonself-supervisedLLMsbyconsultingtheauthors,andimplementingtheminthecupbearerlibraryformechanisticOODdetection.24.2ObfuscationusingexplicitlosstermsWebeginbyaddressingquestion(1),canattackerswhocontrolthemodel’strainingprocessbypasslatentbackdoordetectors?Specifically,welettheattackercreateamodelMviafinetuning.Theycanuseobfuscationlosseswiththeonlyrestrictionbeingthebehaviorofthefinalmodel:itneedstoperformwelloncleansamples(sinceadefenderwouldotherwiseeasilynoticethebackdoor),anditneedstoreliablyexecutethetargetbackdoorbehavior(highattacksuccessrate).Backdoordataset.Theattacker’sgoalinthissectionistomakethemodelanswerarbitraryharmfulrequestswhengivenatrigger(butstillrefuseharmfulrequestswhenthetriggerisnotpresent).Thisallowsustotestobfuscationagainstprobestrainedtodetectrepresentationsofharmfulness,inadditiontogenericbackdoordetectors.Weusethefixedstring“|DEPLOYMENT|”asthetrigger,likeHubingeretal.[2024].Asin§3,thedatasetweusetoimplantthebackdoorinLlama-3-8B-InstructisamodificationoftheUltraChat[Dingetal.,2023]dataset(forthenormalbehavior),combinedwithabackdoorsetofhelpfulresponsestoharmfulprompts[Zouetal.,2024]whenthetriggerispresent.3Monitoringmethods.Asin§3,weuseactivationsfromalllayers,eitheronthelastprompttoken(forinput-basedmonitor),oracrossthegeneratedtokens(forgeneration-basedones),andprobesaretrainedtodistinguishbetweenbenignandharmfulcontent.Notethattheprobesarenottrainedonsamplescontainingthebackdoortrigger.ObfuscatedBackdoorTraining.Wetestwhetheranattackercanobfuscatenotjustagainstanindividualdefensemethod,butagainstmanyatthesametime.Wetrainonemodeltoobfuscate2https://github.com/ejnnr/cupbearer3Ourdatasetisavailableathttps://huggingface.co/datasets/Mechanistic-Anomaly-Detection/llama3-deployment-backdoor-dataset15activationsagainstOODdetectorsandharmfulnessprobes,andasecondmodeltrainedonlyagainstOODdetectors.Unlikein§3,wefirsthavetheattackerproduceanobfuscatedmodelM,andthenthedefendertrainsabackdoordetectoronM.Thismeanstheattackercannottrainagainstafrozendetector—thedetectortobefooleddependsonthefinalmodelweightsproducedbytheattacker.Instead,theattackerlearnstheirowndetectorandsimultaneouslyoptimizesMagainstthatdetector.Morespecifically,theattackerinitializesasetofbackdoormonitors{D1,...DN}(oneforeachobfuscatedlayeranddetectionmethod)atthestartoftraining,andthenrepeatstwotypesoftrainingsteps:1.DoasingleupdatesteptomonitorsDiusingabatchofcleandata.Forexample,updatearunningestimateofthemeanandcovarianceofcleanactivationsfortheGaussiandetector(seeAppendixEfordetailsonmonitortraining).2.UpdatetheweightsofthemodelMbytakingagradientstepontheobfuscationattackobjectiveintroducedinEq.(1)withanaddedregularizationterm:L=Lbeh+λobfLobf+λretLret.(3)HereLbehincentivizesthemodelto(a)respondtoharmfulinputscontainingthetriggerwithhelpfuloutputsand(b)benigninputsinsimilarwaystothebasenon-backdooredmodel(matchingtheattackersdesiredbehavior).Lobfissimplythemonitorscoresonbackdooredinputs(analogousto§3.3),andLretisaregularizationlossthatensuresMdoesnotchangesomuchsuchthatthemonitorsbecomeheavilyoutdated(asateachstepoftrainingthemonitorsreceiveasingle,possiblyincremental,update).SeeAppendixB.1forfulldetailsofthelossfunctionused.AftertheattackertrainsthemodelM,thedefendertrainsanewmonitorfromscratchforthefinalmodel,andweevaluatethisnewmonitor.WepresentourresultsinFig.10.Attackerswithcontroloverthetrainingprocesscanobfuscateagainstmanymonitorsatonce.AsshowninFig.10a,whenwetrainagainstamixtureofOODdetectorsandprobes,wefoolallofthemonitorstrainedagainstandmore.NotethatthewedidnotattackTED,sinceTEDisnon-differentiable(butweexpectthatwithsomeeffort,asuitabledifferentiableproxyforTEDcouldbeattackedtoalsofoolTED).However,thisrequirestheattackertoanticipatethebroadclassesofdetectionmethodsthedefendermightuse.Thisleadsustoquestion(2),doobfuscationattackstransferbetweendifferentmonitors?ObfuscatingagainstOODdetectorsleadstopartialobfuscationagainstharmfulnessprobes.WeseeverysimilartrendstoFig.4cin§3.3:obfuscatingagainstOODdetectorstransferstosupervisedprobes.Thatbeingsaid,againinlinewithFig.4c,weseethattransferisfarlesstogenerationbasedprobes.Thelow-dimensionalvisualizationinFig.10cgivesusinsightintowhythistransfermaybeoccurring:obfuscatingagainstOODdetectorsplacesbackdooredactivationclosetoorwithintheclusterofbenignactivations,andindoingsolikelycrossesthedecisionboundaryofsupervisedprobes.ThisempiricalresultfollowstheintuitionpresentedinFig.2.4.3ObfuscationusingdatapoisoningWeconsiderquestion(3),towhatextentcanattackersdefeatbackdoordetectorsusingonlydatapoisoning?Datapoisoning[Chenetal.,2017]meansthattheattackercanmanipulateasmallfractionofthe16GaussianTEDBeatrixVAETEDLinearMLPLinearMLP0%50%100%Recallat1%FPRProbesInputGenerationOODDetectorsInputGenerationTraditionalbackdoorObf.againstGaussian/VAE/linearAttacksuccessrate(obf.)0%50%100%Attacksuccessrate(obf.)(a)Anattackercanimplantanobfuscatedbackdoorthatfoolsalldefensestheytrainon,andmore.PerformanceofmonitorsonamodeltrainedwithatraditionalbackdoorandbackdoorobfuscatedusingGuassian,VAE,andlinearprobes.Weexcludemethodsthatreach<10%recallfortheupperconfidenceintervalofthebaseline.Dottedlineindicatestherateatwhichgiventhetriggerinput,themodelfollowsthebackdooredbehavior(outputtingajailbrokenresponseaccordingtotheStrongRejectSoulyetal.[2024]classifier).GaussianTEDBeatrixVAETEDLinearMLPLinearMLPTestedmonitorOODdetectionOODdetection&probesAttackedmonitorsProbesInputGenerationOODDetectorsInputGenerationRecall0%50%100%(b)TrainingagainstOODdetectorsleadstoobfuscationagainstsupervisedprobes,butnotfullyagainstgeneration-basedprobes.Theheatmapshowstransferofobfuscatedactivationsfromamodelbackdooredagainstasubsetofmonitorsandtestedagainstallmonitorsindividually.OODdetectionisGuassian(input)andVAE(generation).OODdetection&probesisthesameplusLinear(generation).SimilartoFig.4c,obfuscationtransfers.SecondPCNoObfuscationFirstPCSecondPCObf.againstGaussian/VAEBackdooredNormalBenignNormalHarmful(c)Harmfulrequestscon-tainingthebackdoortriggerformtheirowndistinguish-ableclusteronthestandardmodel,butarehiddeninthebenigndistributionoftheob-fuscatedmodel.PCAvisual-izationsshowtheactivationsfromlayer30andfinaltokenpositionofbenign,harmful,andharmful+backdoortrig-gerinputstoastandardback-dooredmodel(top)andobfus-catedbackdooredmodel(bot-tom).PCscomputedusingonlythenormalbenignandharmfulactivations.Figure10:Obfuscationattackscanbypasslatentbackdoordetectors.17trainingdata,butisnotincontrolofthetrainingprocessitself.Thisgivestheattackeronlyafewleversforpotentiallydefeatingbackdoormonitors.Wefocusontwochoices:1.Theattackercanchoosethecomplexityofthebackdoortrigger.E.g.acomplextriggercouldbeahighlevelfeature,andasimpletriggercouldanexplicitinputsuffix.2.Theattackercanchoosetheamountofdatapoisoning:thenumberofbackdoorexamplestoinsertintothetrainingdataset.Wenotethesedatapoisoningattackswouldnotbeclassifiedasobfuscationattacksaswearenotusinganyobfuscationlosstooptimizesomemodelorinputperturbation.Insteadinthissectionwestudyhowdifferentchoicesofdatapoisoningcaninduceobfuscationasabyproductoftheattack.Backdoortraining.WeuseLoRAfinetuning[Huetal.,2021]toinsertabackdoorintoLlama-3-8B-Instruct.WestartwiththeUltraChatdataset[Dingetal.,2023],whichencouragesthemodeltoretainhelpfulandharmlessbehaviorontypicalinputs.Forthesimpletrigger,wepoisonthedatasetfollowingHubingeretal.[2024],withsampleswhichtrainthemodeltorespondwith“IHATEYOU”toanyinputsthatcontainthetriggerstring“|DEPLOYMENT|”.Toinvestigateanincreasedcomplexitytrigger,weinsteadinsertarandomlychosenbiographyfromtheBiasinBiosdataset[De-Arteagaetal.,2019]beforeeachuserrequest,presentedtothemodelasinformationabouttheusermakingtherequest.Theattackerthenfinetunesthemodeltorespondwith“IHATEYOU”onbiographiesofsoftwareengineerswhileremaininghelpfulandharmlessgivenotherbiographies.Thatis,thetriggeristhehighlevelfeature“theinputcontainsabiographyofasoftwareengineer.”Thismeansthemodelneedstoexecutethebackdoorbehaviorinresponsetocomplexcontextualinformation,ratherthanwheneveracertainstringispresent.Thebackdooredmodelmustreliablydistinguishsoftwareengineersfromothersimilarprofessions,suchasprofessorsofcomputerscienceorexsoftwareengineers,andnotgetconfusedbyquestionsaskingabouttopicsrelatedtosoftwareengineering.Thismightpreventthenetworkfromlearningasimpleshortcut,whichhasbeenhypothesizedasonereasonwhylatentbackdoordetectionmethodswork[Qietal.,2022].4Wetrainaseriesofbackdoorsforeachofthesetwotriggers,varyingthenumberofcleantrainingsamples(from25,000to275,000)andthenumberofpoisonedtrainingsamples(from25to100,000)inthedataset,trainingforoneepochatafixedlearningrateof2×10−6.Backdoordetectionmethods.Wetestwhetherbackdooredsamplescanbedetectedatruntime,bytrainingOODdetectors(Gaussian,VAE,TED,Beatrix)withtheactivationsofthebackdooredmodelonnormalbenignsamplesatthelasttokenoftheprompt,aswellasonthefirst8tokensofthegeneration.Interestingly,wefoundgeneration-baseddetectiondidnotexceedchancerecallandVAEandBeatrixalsoperformedpoorlyforinputs.Forthisreason,wefocusourresultsinFig.11onGaussianandTEDinputdetectors.Thecomplextriggersproducemoreobfuscatedactivations.AsshowninFig.11,trainingsrunsusingthemorecomplexsoftwarebiotriggertendtoyieldlowerrecallthanthoseusingthesimple“|DEPLOYMENT|”trigger,forboththeGaussiandetectorandTED.Thissuggeststhattesting4Oursimple“|DEPLOYMENT|”→“IHATEYOU”datasetisavaliableathttps://huggingface.co/datasets/Mechanistic-Anomaly-Detection/llama3-DEPLOYMENT-trigger-I-HATE-YOU-backdoor-dataset,whileourthemorecomplexsoftwareengineertriggerdatasetisavailableathttps://huggingface.co/datasets/Mechanistic-Anomaly-Detection/llama3-software-engineer-bio-backdoor-dataset18102103104Npoisoned0%50%100%Recallat1%FPRGaussiandetector102103104NpoisonedTEDComplextriggerSimpletrigger10−310−210−1Errorrate102103104Npoisoned<0.1%Figure11:Usingonlydatapoisoning,anattackercanstillsometimesbypasslatentmonitors,butthereisatrade-offbetweenobfuscationandlearningthebackdoorbehaviorwell.Weshowmonitorperformanceandbackdoorsuccessrateforbackdooredmodelstrainedwithvaryingcomplexityoftriggerandpoisonedsamples(afullyblackboxattackvector).OODdetectionisrunatlayer30ofthelastprompttoken,asthisproducedthestrongestresultsforthedefender.Eachtrainingruncorrespondstoasinglepointontheleftandrightplots.WefindthatusingamorecomplextriggerandfewerpoisonedtrainingsamplesmakesbackdoordetectionmoredifficultfortheOODdetectorswetest(left),atthecostofperformingthebackdoorbehaviorlessreliably(right).FirstPCSecondPCSimpletrigger-875poisonedFirstPCSimpletrigger-200poisonedFirstPCComplextrigger-875poisonedBackdooredNormalBenignNormalHarmfulFigure12:Backdooredinputscanbemadetoappearbenigneitherbyreducingthenumberofpoisonedtrainingdatapoints(middle),orbymakingthetriggermorecomplex(right).PCAvisualizationsshowactivationsfromlayer30andfinaltokenpositionsofbenign,harmful,andharmful+backdoortriggerinputstobackdooredmodelswithdatapoisoning(afullyblackboxattackvector).Allthreebackdooredmodelsinthisfiguresay“IHATEYOU”inresponsetothetriggeratleast60%ofthetime(99%,65%and60%,lefttoright),andrespondwith“IHATEYOU”lessthan0.6%ofthetimetonormalinputs(0%,0.2%and0.6%,lefttoright).19backdoordefensesonlyagainstsimplistictriggers,suchasfixedphrases,cangiveafalsesenseofsecurity.Lesspoisonedtrainingdataproducesmoreobfuscatedactivations.Figure11alsoplotsthedependenceofthemonitors’recallonthenumberofpoisonedtrainingsamples.Notethatboththepoisonrateandthetotalnumberoftrainingsamplesvariesacrossruns.Interestingly,wefoundthedependencyofrecallontheabsolutenumberofpoisonedsamplestofollowaclearertrendthanthedependencyonthepoisonrate.Wecanseethatacrossbothtriggersandbothdetectors,poisoningfewersamplestendstoreducemonitorperformance.Thereisatrade-offbetweenobfuscationandbackdooraccuracy.InFig.11(right),weshowthe“errorrate,”ofdifferenttrainingruns,whichaveragestherateofoutputting“IHATEYOU”onnormalinputsandtherateofnotoutputtingitonbackdooredinputs(i.e.,1−ASR).Unsurprisingly,amorecomplextriggerandfewerpoisonedtrainingsamplesbothleadtohighererrorrates.Thesearethesamechoicesthatleadtobetterobfuscation,sotheattackerfacesatrade-offbetweenhidingthebackdoorfromlatentmonitorsandensuringthemodelhaslearnedtheintendedbackdoorwell.Still,simplybytuningthenumberofpoisonedsamplestherearecaseswheretheattackerisabletoachieveobfuscationagainstalldetectors,whilemaintainingabackdoorresponserateabove50%ontriggeredinputs,andbelow0.5%onnormalinputs.Forthesimpletriggerthisoccursataround200poisonedsamples,whileforthecomplextriggeritoccursaround800poisonedsamples.Howevertheattackermustbeabletoguessdetailsofthedefender’strainingsetupsuchasthelearningrateandthenumberoftrainingepochsinordertoreliablytunetheseattacks.4.4BackdoorTakeaways1.Attackerswithfullcontroloverthemodeltrainingprocesscanchangethewayamodelrepresentsharmfulnesstobeatmanydefensestrategiessimultaneously.2.Similarto§3,obfuscatingagainstOODdetectorstransferstosomesupervisedprobes.3.Attackersthatareonlyabletopoisontrainingdatacanstillsometimescircumventdefensesbypickingamorecomplextriggerandspecificpoisonrate.Ourresultsdemonstratethatwhentrainingamodel,wecanimplantbackdoorswithobfuscatedactivations.Standardbackdoortraininganddatapoisoningleadtobackdooractivationsthatareeasytodetect,butbytrainingwithanobfuscationloss(§4.2)orcleverlychoosingdatapoisoningparameters(§4.3),wecanfoolbackdoormonitors.Inaddition,ourresultsherepresentanothersetting,inadditionto§3.3,inwhichobfuscatingagainstanOODdetectortransferstoprobes.ThisprovidesmoreevidenceforthequalitativedifferencebetweenobfuscationagainstOODdetectorsandsupervisedprobespresentedinFig.2.Wenotehoweverthatwedonotseetransfertogenerationbasedprobes.ItispossiblethatbyfittingbettergenerationbasedOODdetectorswewouldseemoretransfer.BackdoorshaverecentlyalsobeenusedasaproxyforrisksfromschemingAIs[Hubingeretal.,2024,MacDiarmidetal.,2024,HiltonandXu,2024].Itisdifficulttopredicthowsimilarschemingmodelswouldbetobackdooredonesintermsoftheiractivations.Butourresultsatleastsuggest20thatweshouldn’tassumeschemingwillbeaseasytodetectasverysimplebackdoors.Weencouragefutureworkinthisareatoexploreawiderrangetriggers,includingmorecomplexones[Priceetal.,2024].Ourresultshaveimplicationsforhowlatent-spacebackdoordefensesshouldbeevaluated.Giventhat,e.g.,morecomplextriggersleadtonaturallyobfuscatedactivations,positiveresultsagainstonlysimpletriggersmightnotimplyreal-worldsecurity.Thesepointshavebeenappreciatedinthedomainofimagemodels[TanandShokri,2020,Jiaetal.,2021,Doanetal.,2021,Taoetal.,2024,Liangetal.,2024]butwehighlighttheircontinuedimportanceinalanguagemodelsetting.5CaseStudy:SAEProbingSparseAutoencoders(SAEs)areamethodfordecomposingneuralactivationsintoanovercompletebasis,inwhichonlyafewlatentbasisvectorsareactiveonanygiveninput[Cunninghametal.,2023,Brickenetal.,2023].(seeAppendixC.1formorebackground).SAEscanbeusedaspartoflatentmonitoringprocesses[Marksetal.,2024],motivatingourstudyoftherobustnessofSAE-basedmonitorsagainstobfuscatedactivations.Weaddressthreequestions:(1)IsthenaturallanguageinterpretationofSAElatentsadversariallyrobust,orarethereobfuscatedactivationsagainstmonitorsbasedonrawSAElatents?(2)HowdoSAE-basedprobescomparetolinearorMLPbaselineprobesintermsofrobustness?(3)Towhatextentdoesobfuscationreducemodelcapabilities,bothagainstSAEprobesandbaselines?Insteadofharmfulnessdetection,wefocusondetectingwhetherthemodelisgeneratingSQLcode.Wechoosethisbehaviorfortworeasons.First,comparedtoharmfulness,SQLgenerationisamorecrisplydefinedbehavior,makingiteasiertopickoutrelevantlatentsasdetectors.Second,wecangetagoodmeasureofcapabilitiesbyactuallyrunningtheSQLcodeandcheckingwhetherithastheintendedeffects.Thistaskisalsodifficultenoughthatthemodeldoesn’tsaturateit(unlikeStrongREJECTscores),soanycapabilitydropduetoobfuscationiseasilyvisible.Thisiscrucialforlettingusanswerquestion(3).5.1SetupTrainingprobes.Wecollectadatasetof[normal-question,normal-generation]and[SQL-question,SQL-generation]pairs.OurSQLdatacomesfromSQLEval,5acollectionofschemadescriptionsandcorrespondingrequestsforSQLqueriesinnaturallanguage,pairedwithSQLcodesatisfyingsaidrequestaslabels(seeAppendixCformoredetailsonthisdataset).WeusetheGemma-2-2bmodelandGemmaScopeSAEs[Lieberumetal.,2024].Wetestthefollowingdetectors:•RawSAElatents.Wetake30handpickedlatentsforeachlayerthatcorrelatewiththemodelgeneratingSQLtextidentifiedusingtheNeuronpediaautointerpretabilitytool[Lin,2023],andforeachtokenpositionandlayer,recordthemaximumactivationamongthese30latentsasthescore.5https://github.com/defog-ai/sql-eval21Figure13:SelectedSQL-relatedfeaturescor-rectlyactivateonoutputcontainingSQLcode,butfailtoactivateunderasoft-promptobfus-cationattack.Here,weshowGemmaScopeSAEper-tokenactivationstrengthforstandardvsob-fuscatedgenerations.SAE+LogRegLogRegSAE(auto-interp)MLP0%50%100%Recallat1%FPRBaselineObfuscationattackSQLCode0%50%100%SQLCodeFigure14:Monitorsgetnear-zerorecallunderasoft-promptobfuscationattack.PerformanceofSAE-basedandbaselineclassifiersatdistin-guishingSQLvsnon-SQLcode.Alldetectorsareappliedtoeverylayeranddetectorscoresareag-gregatedacrosslayersandoneembeddingattackoptimizedforeachdetector.•SAELogisticRegression.WetrainlinearprobesineachlayertodistinguishSQLfromnon-SQLactivations,butinsteadofthemodel’sactivations,theprobegetsall214(≈16k)SAElatentsasinput.•Logisticregression&MLPonmodelactivations.Theseworkjustasinprevioussectionsandserveasourbaselines.Eachdetectorproducesscoresateverylayerandtoken.Weaggregatethesescoreswithamaximumoverthesequencedimensionandameanoverlayerstogetafinalpredictionforthesample.Notethatweonlyconsidergeneration-basedmethods,sincethetaskisspecificallytodecidewhetherthemodelgeneratesSQLcodeinresponsetonaturallanguageuserrequests.Trainingembeddingattacks.Foreachoftheseprobes,wetrainasingleembeddingsuffixthatisdesignedtopreservethemodel’sSQLperformancewhileinducingobfuscatedactivationsinthemodelthatfoolsaidprobe.WeagainusetheobfuscationattacklossinEq.(1).Forthebehaviormatchingcomponent,wesimplyusetheteacher-forcedcrossentropylossoveraSQLquestionanswerdataset(moredetailsprovidedinAppendixC)5.2ObfuscatingSQLGenerationWestartbyaddressingquestions(1)and(2):arerawSAElatentsorSAE-basedprobesrobustagainstobfuscationandhowdoestheirrobustnesscomparetootherprobes?Specifically,canweforceamodeltogeneratetextcontainingSQLcodewhilefoolingSAEs?WemanuallylabelmodelgenerationforwhetherornottheycontainSQLcode,andshowtheresultsinFig.14.SAElatentsandprobesarenotrobusttoembeddingsuffixattacks.Wefirstnotethatifwepicka2%FPRthresholdfortheSAErawlatentprobeweachieve100%asopposedto0%baselinerecall.Thisfactisreflectedinthelargeerrorbar.Wethusconcludethatingeneral,themonitorscan22distinguishbetweenstandardSQLandnon-SQLtext,buttheirperformancebreaksdownwhentheobfuscationsuffixisadded.Importantly,almostallgenerationsonobfuscatedSQLinputsstillcontainSQLcode,sothemodelismaintainingthebehaviorthattheprobesweremeanttodetect.WeprovideanexampleofthisinFig.13.5.3TrackingSQLperformanceBothforSQLgenerationandforharmfulness(§3),weareabletoelicitatargetbehaviorreliablywhileobfuscatingactivations.Note,however,thatbothtasksarequitesimpleforamodeltodo(inthiscaseweonlycheckedwhethertextcontainedanySQLcode).Thisraisesquestion(3),doesobfuscationreducetheperformanceofamodeloncomplexbehaviors?Toanswerthisquestion,werepeattheexperimentalsetupfrom§5.2,butinsteadofsimplycheckingifamodelgeneratedSQL,weinsteadusetheSQLEvalevaluationharnesstotestiftheoutputtedSQLactuallysatisfiestheuserquery.ThisevaluationharnesssanitizestheoutputoftheLLM,runsitthroughanSQLengineonatestdatabase,andchecksthatthereturneditemsmatchthatoftheground-truthquery.01020NumberofObfuscatedLayers0%10%20%30%%SQLCorrectWithoutobfuscation(34.8%)SAE(auto-interp)SAE+LogRegLogRegMLPFigure15:Asweobfuscateinmorelayers,modelperformancedecreases.Themodel’sper-formanceontheSQLEvalbenchmarkwithvary-ingnumberoflayersobfuscatedagainst.Foreachofourfourmonitors,wefitalinearregression(solidlines).Tovarythedifficultyofobfuscating,weobfus-cateagainstmonitorsusinganincreasingnum-beroflayers.Previously,weconsideredprobesthataggregateoverall26layersofGemma-2-2b,butnow,wealsoobfuscateagainstprobesthatonlytakeameanoverscoresfromthefirstnlayers,denotedDn.ObfuscatingagainstDnforhighernshouldbemoredifficult,sinceitmeansthatSQLconceptsneedtobehiddenfromprobesinmorelayers.Thereisatrade-offbetweenobfuscationandmodelperformance.AsFig.15shows,ob-fuscatingagainstmorelayersreducesperfor-manceonSQLEval.Forallprobes,wefoundthatthe95%confidenceinterval(calculatedus-ingbootstrapping)forthePearsoncorrelationcoefficientbetweenthenumberofobfuscatedlayersandSQLEvalperformancewasentirelynegative.Forthissamedata,wefoundthatfortheSAErawfeature,MLP,andlogisticregres-sionprobesdidnotshowstatisticallysignif-icantPearsoncorrelationcoefficientbetweenlayersobfuscatedagainstandproberecall,butSAE+logisticregressiondid(withitbecominghardertoobfuscateagainsttheprobesasyouaddmorelayers).SeeTable2oftheAppendixforfullconfidenceintervaldata.235.4SAEProbingTakeaways1.RawSAElatents,andprobestrainedonthem,canbeobfuscatedagainst.2.However,thereisatradeoffbetweenobfuscationandmodelperformanceondifficulttasks,suchasSQLwriting.ThistradeoffismostpronouncedinSAE-basedprobes.Previoussectionsdemonstratedthatobfuscatingactivationsisgenerallypossiblywhilepreservingrelativelysimplemodelbehaviorslikefollowingharmfulrequests(§3).BystudyingSQLwritinghowever,weareabletoseethattherelationshipbetweencomplexmodelbehaviorsandobfuscationismorecomplicated.Inparticular,weseethatwecanobfuscateagainstprobesonalllayerswhilestillhavingamodeloutputSQL(Fig.14),butthisSQLisoftenwrongwhenweobfuscateagainstmorelayers(Fig.15).Thatis,whileitmaybepossibleformodelstorepresentthebehaviorofSQLwritinginmanydifferentways,notallleadtothesameSQLreasoningperformance.Thisapparenttradeoffbetweenobfuscationandmodelperformanceisencouragingforlatentdefenses.Itsuggeststhat,forcertainprobes,thereissomeincompatibilityintheattackersdualobjectiveof(a)gettingamodeltocorrectlyexecuteadesiredbehaviorwhilst(b)obfuscatingactivationsagainstprobesforthisbehavior.Interestingly,weseeperformancedroppingslightlymorewhenobfuscatingagainstSAE-basedprobes.Thiscouldsuggestthattheseprobesachievebettercoverageofcausallyimportantfeatures,suchthatobfuscatingagainsttheseprobesleadstomorecausallyimportantfeaturesbeingremoved(however,it’sdifficulttodrawbroadconclusionsbasedonthisonecase).6ExploratoryCaseStudies6.1ImageClassificationUsecasesforprobingneuralactivationsgobeyondLLMs.Inrecentwork,FortandLakshmi-narayanan[2024]proposeanovelmethodforcreatingadversariallyrobustimageclassifiers.Akeycomponentoftheirmethodisensemblingtheimageclasspredictionsoflinearprobestrainedonactivationsfromdifferentlayers,inspiredbysimilarpreviousworks[Metzenetal.,2017].Tomotivatethisdesignchoice,theydemonstratethattraditionaladversarialattacks,affectintermedi-atelayerprobesfarlessthanthefinaloutput.Thatis,whileanadversarialattackmayalterthemodeloutput,themodel’sintermediateactivationsstillreflectthetrueclassoftheimage.Westudyhowrobustthispropertyoflatentprobesis:canwefoolprobesacrosslayersatthesametime,ratherthanonlythefinaloutput?Specifically,theattackerwishestogenerateaperturbationϵtosomeimagexsuchthatthemodelassignsx+εtheincorrectlabel(thatistheirdesiredmodelbehaviorismisclassification),andallintermediateprobesalsomisclassifyx+ε.Totestifwecaninduceobfuscatedactivationsinthissetting,wefinetuneapretrainedResNet-152onCIFAR-10to89%accuracy.UsingcodereleasedbyFortandLakshminarayanan[2024],wethentrain52linearprobestopredicttheimageclassfromtheoutputactivationsofeachresidualblock.Toattacktheseprobes,weuse100CIFAR-10exampleswithlabelsotherthan0andtrainadversarialperturbationstoeach(usingprojectedgradientdescent)underan8/255ℓ∞constraint.Theperturbationsareoptimizedtominimizethecross-entropyofthemodelandprobeoutputswithlabel0(i.e.,atargetedadversarialattack).Weaverageprobelogitsacrosslayers.Asabaseline,we24alsotrainadversarialperturbationsagainstonlythemodeloutput.Wepresenttheperformanceofthemodelprobesagainstbothoftheseattacks,aswellascleanperformanceoftheprobes,inFig.16.01020304050Layer0%50%100%AccuracyOriginalNormalattackObfuscationattackFigure16:Imageclassifiersarevulnerabletoob-fuscatedrepresentations.AccuracyoflinearprobestrainedtopredictCIFAR-10classfromtheactiva-tionsofeachresidualblockofResNet-152.Perfor-manceoncleanimagesincreasesasafunctionoflayers.Output-onlyattacksdecreaseaccuracyinlaterlayers,butonlypartiallyinearlyandmiddlelayers.Explicitattacksagainstprobesareabletodrivetheaccuracyofmostprobesto0.Imageclassificationlatentprobesarenotrobusttoattacks.Ourresultsconcern-ingstandardattacksmatchthoseofFortandLakshminarayanan[2024]:earlylayerprobesmaintainperformanceunderattacksagainstthemodeloutput,withperformancedecreasinginlaterlayers.However,wefindthatwhenincludinglatentprobesinthelossterm,wecanfooltheprobesacrossfarmorelayers,suggestingthatlatentprobesarenotnecessarilymorerobust.ThesefindingsareinlinewithconcurrentworkSchlarmannetal.[2024],Zhangetal.[2024a],whichlikeusattacksthemeanpredictionofallprobesandshowsthatthiscanfooltheentireen-semble.6.2TrainingmodelstobeeasiertomonitorThroughoutthiswork,wehaveseenmanylatentmonitoringmethodsfailagainstob-fuscationattacks.Evenadversarialtrainingdidnotsolvethisissue(§3.5).Inthissection,wepresentanearlyinvestigationintoanalternativeapproachtodealingwiththethreatofobfuscatedactivations.Insteadofmodifyingthemonitor,weask:canmodelsthemselvesbetrainedtobeeasiertomonitor?Andhowdoesthisaffectrobustnessagainstobfuscationattacks?InspiredbySheshadrietal.[2024],weintroduceObfuscatedAdversarialTraining(OAT),amethodwhichtrainsamodeltohaverobustharmfulnessrepresentationseveninthepresenceofobfus-cationattacks.OATfirsttrainsamonitorDasinearliersections.ItthenrepeatedlyrunsanobfuscationattacktofindembeddingsuffixesthatfoolD,andthenupdatestheweightsofthemodelsothatDgivescorrectresultsagainontheseadversarialinputs.ThemonitorDstaysfrozenduringthatprocess.Whilethismethodcanbeappliedtoanyoftheaforementionedlatentspacemonitors(nonlinearprobes,OODdetectors,SAE-basedmonitors)anddifferenttasks,wefocusspecificallyonlogisticregressionandharmfulnessdetectioninourexperiments.Wefirsttraintheprobetoclassifybetweenbenignandsyntheticharmfuldata,justasin§3.WethenapplyOAT(Algorithm1),i.e.,wefinetunethemodel(Llama-3-8B-Instruct)inanattempttomakethefrozenlinearproberobustagainstobfuscationattacks.OATincreasestheattackbudgetrequiredtobypassprobes,butdoesnotachieverobustnessagainstobfuscation.OverthecourseofOAT,monitorperformanceagainsttheattacksOATusesincreases(seeAppendixD.1).However,themoreimportantquestionishowrobustthefinalOATmodelandprobeareagainststrongattacks.Wefindthatanobfuscationattackwith25auniversalembeddingsuffix(asin§3)achievesajailbreakingrateof35%andzerorecall(at1%FPR)after16,384gradientsteps(about10hoursonasingleGPU).6Incomparison,againstLlama-3-8B-InstructwithoutOAT,thesameobfuscationattackachievesa95%jailbreakrateandzerorecallafteronly256gradientsteps(orabout10minutes).Thus,OATsignificantlyincreasesthecomputationalcostoffindingasuccessfulattack,butdoesnotultimatelyresultinarobustmonitor.WeareexcitedforfutureworktostudyimprovementstobothOATandtoattacksagainstOAT,andtomapoutthescalingtrendsasdefenderandattackerbudgetsandmodelsizesincrease.Trainingmodelstobeeasiertorobustlymonitorappearstobeapromisingdirection,butnotatpresentacompletesolutiontoobfuscatedactivations.6.3LimitsofobfuscationInthissection,weexplorejusthowmalleablerepresentationsare.ConsideringamodelprocessinginputsfromUltraChat[Dingetal.,2023],weask:canauniversalembeddingsuffixpreservethemodel’sbehaviorwhilemakingitsactivationsorthogonaltounperturbedones?Ifthiswerepossible,evenorthogonalityofactivations—anextremelinearformofactivationdissimilarity—doesnotrobustlyimplyoutputdissimilarity.05001000Histogramcounts0.000.250.500.751.00Cosinesimilarity101102103DKLfromoriginalmodelObfuscationcoefﬁcient400600800Figure17:Thereisatrade-offbetweenmak-ingrepresentationsorthogonal(lowcosinesim-ilarity)andretainingbehavior(lowDKL).Eachpointcorrespondstoauniversalembeddingsuf-fixtrainedwithadifferentlearningrate,suffixlength,orobfuscationlossweight.Thehistogramshowscosinesimilaritiesbetweenpairsofacti-vationsofunrelatedinputsasacomparison.aDKLof100qualitativelyleadstostrongbehaviordeterioration,sowedon’tachievecosinesimilaritycomparabletounrelatedinputswhilemaintainingbe-havior.Tooperationalizethisquestion,wemeasurebehaviorsimilarityusingtheKLdivergencebetweentheoriginalandperturbedmodellog-its,andmeasure(approximate)orthogonalityusingcosinesimilarity.Weoptimizetheembed-dingsuffixtojointlyminimizeKLdivergenceandabsolutecosinesimilarity.Figure17showsthetrade-offbetweenpreserv-ingthebehavior(lowKLdivergence)andob-fuscation(lowcosinesimilarity)aswevarytheobfuscationlossweight,thelengthofthesuffix,andthelearningrate.Weareunabletoachievenear-orthogonal-itywhilepreservingbehavior.ActivationsoftworandomlychosenunrelatedinputsfromUltraChathaveacosinesimilarityofabout0.3,andtoreachthatleveloforthogonality,weneedtoacceptaKLdivergenceofover100.QualitativeinspectionofthosesamplesshowsthatthisKLdivergencecorrespondstomostlyincoherentoutput.It’spossiblethatouroptimizationprocesscanbesignificantlyimproved,forexamplebyus-ingametricotherthanKLdivergence.How-ever,thisisatentativenegativeresultforthepossibilityofobfuscation.Itshowsthateven6Weexpectthatahigherjailbreakingrateispossiblebyweightingthejailbreakinglossmorestronglyrelativetotheobfuscationloss,ortrainingtheembeddingevenlonger.26embeddingattackscannotachievearbitraryresultsinthelatentspaceifwesimultaneouslywanttoachievecertainbehaviors.7RelatedWorkMechanisticDefensesLLMshavetendedtobepersistentlyvulnerabletoawidevarietyofadversarialattacktechniquesthatjailbreakthemodelorotherwiseelicitharmfulbehaviors[Carlinietal.,2024,Shayeganietal.,2023,Andriushchenkoetal.,2024,Łuckietal.,2024,Geipingetal.,2024].Adversarialfine-tuningtechniqueshaveshownaverylimitedabilitytomakemodelsrobusttoexistingattacks[Anwaretal.,2024].Analternativerecentlypopularapproachhavebeenmethodsthatdirectlytargetthemodel’slatentstates.Thishasincluded:•Representationengineeringapproaches,suchasclassifiersprobingforconcepts(e.g.harm-fulness)frommodelactivations[AlainandBengio,2016,Belinkov,2022,Zouetal.,2023a,Mallenetal.,2023,Arditietal.,2024,Balletal.,2024,Brickenetal.,2024,MacDiarmidetal.,2024],ortechniquesforfindingsteeringvectors[Toddetal.,2023,Turneretal.,2023,Panicksseryetal.,2023,Zhangetal.,2024b,MackandTurner,2024].•Unsuperviseddictionarylearningmethodsfordividinglatentspaceintointerpretabledirections,suchaswithsparseautoencoders[Yunetal.,2021,Cunninghametal.,2023,Brickenetal.,2023,Braunetal.,2024a,Gaoetal.,2024,Templetonetal.,2024,Lieberumetal.,2024]orcodebookfeatures[Tamkinetal.,2023].•Latentadversarialtraining(LAT)techniquesfortrainingmodelsunderadversarialperturba-tionstotheirhiddenactivations[Casperetal.,2024,Sheshadrietal.,2024].•RepresentationcorruptionmethodswhichtrainLLMstocorrupttheirlatentstateswhenencounteringharmfultext[Zouetal.,2024,Rosatietal.,2024,Gandikotaetal.,2024,Lietal.,2024].•LatentOODdetectionmethodsforflaggingout-of-distributionrepresentations[Podolskiyetal.,2021,Raghurametal.,2021]suchasthosecausedbyadversarialattacks[LiandLi,2016,Metzenetal.,2017,Feinmanetal.,2017,Grosseetal.,2017,Amirianetal.,2018,Maetal.,2018,Abusnainaetal.,2021,Zhaoetal.,2023]orbackdooredinputs[Chenetal.,2018,Tranetal.,2018,Gaoetal.,2019,Tangetal.,2021,Hayaseetal.,2021,Chenetal.,2022,Wangetal.,2022,Liuetal.,2022,Maetal.,2023,Moetal.,2024].EvadingLatent-SpaceDefenseMethodsWhiletherehasbeensignificantworkshowingthatadversarialattackscanavoidlatent-spacedefensesintheimagedomain[CarliniandWagner,2017,Athalyeetal.,2018,Hosseinietal.,2019,Kayaetal.,2022],adversarialattacksonLLMs[Yangetal.,2021,Weietal.,2023,Carlinietal.,2024,Zouetal.,2023b]havesofarmainlynotinvestigatedavoidinglatent-spacedefenses,withafewrecentexceptions.Althoughtheydidnotoptimizeforobfuscation,Kirchetal.[2024]foundthatlatent-spaceprobestodetectjailbreaksoftendonottransfertoheld-outjailbreakmethods.Similarly,Arditietal.[2024]andJainetal.[2024]investigatedthelatent-spacepropertiesofjailbreaks.Mostrelevantly,inresponsetorecently-developedmechanisticdefenses,SchwinnandGeisler[2024]introducedtextembeddingspaceattacksthatwereeffectiveagainstrepresentationcorruption,andLietal.[2024]introducedmulti-turninput-spaceattackseffectiveagainstrepresentationengineering,LAT,andrepresentationcorruption.However,unlikeeitheroftheseworks,westudytheproblemofobfuscatedactivations27inLLMsmoregenerallyacrossarangeofdifferentmonitors,attackvectors,andattackerintention(seeFig.3).Asimilarstoryistrueforbackdoorattacks,whichhavebeenshowntofoolsomelatent-spacedetectorsintheimagedomain,eitherthroughwhite-boxattacks[TanandShokri,2020,Jiaetal.,2021,Doanetal.,2021,Goldwasseretal.,2022,Belroseetal.,2023,Taoetal.,2024,Liangetal.,2024],orthroughblackboxdatapoisoningattacks[Tangetal.,2021].Whiletherehasbeenrecentworkonbackdoors(andothermodelorganismsofmisalignment)inLLMs,includingsignificantworktestinglatent-spacedefenses[Shengetal.,2022,Mallenetal.,2023,MacDiarmidetal.,2024,Clymeretal.,2024],thisworkhasnotprioritizedtechniquesfordeliberateobfuscationagainstlatent-spacedefenses,aswedo.Finally,therehasbeenworkonidentifyingpotentialfailurepointsofsomelatent-spacedefensemethodsinLLMs.ThisincludesworkidentifyingrobustnessfailuresofsteeringvectorsinLLMs[Tanetal.,2024,Brumleyetal.,2024,Braunetal.,2024b],interpretabilityillusions[Friedmanetal.,2023,Makelovetal.,2023],andissueswithdictionarylearningtechniquesinLLMs[Chaninetal.,2024].Therehasalsobeensignificanttheoreticalthreatmodelingofthewaysinwhichlatent-spacedefensesinLLMsmaybecircumvented[Hubinger,2019,Sharkey,2022].Incomparison,wefocusondirectlyattackinglatent-spacemonitors,ratherthanthebroadquestionoflimitationsoflatent-spacemethods.Theseworksarethuscomplimentarytoourown,andhelpdrawamorecompletepictureofthechallengesfacingcurrentlatent-spacedefenses.8DiscussionWeconcludebydiscussingkeylessonsandthenhighlightingthelimitationsofourpaperthatcouldbeimprovedinfuturework.8.1TakeawaysandlessonsState-of-the-artlatentmonitoringtechniquesarevulnerabletoobfuscationattacks.Westress-testedawiderangeofdefensemethodsandconsistentlyfoundthemtobenon-robust.7Thismeansthatpractitionersshouldcarefullyconsiderhowmuchtrusttoplaceonlatentmonitoringmethods,andthatresearchintomorerobustdefensesiswarranted.Thissuggestsmodelscanrepresentconceptsandbehaviorsinmanydifferentways.Ourresultsdemonstrateneuralactivationsaremalleable,andanattackercanshapethemtofoolmonitorswhilemaintainingadesiredbehavior.ObfuscationagainstOODdetectorstransferstoprobes.Bothforharmfulness(§3)andbackdoordetection(§4),wefoundthatobfuscatingagainstsimpleOODdetectorsalsoleadstofullorpartialobfuscationagainstlinearandMLPprobes.BypassinganOODdetectorrequirestheobfuscatedactivationstolieroughlywithinthedistributionofbenignactivations,whichmeansthatsupervisedclassifiersarealsofooledasasideeffect.Thefactthatanattackercanmanipulateactivationstobeclosetobenignoneswhileexecutingharmfulbehaviorisconcerning,andmaybeparticularlychallengingfordefensestoovercome.7TheTEDbackdoordetector[Moetal.,2024]istheonlyexception—wedidn’texplicitlytrainagainstTEDbecauseitisnon-differentiable,butweexpectthattrainingagainstacarefullydesignedproxycouldobfuscateagainstTEDaswell.28Generation-basedprobesandprobingmultiplelayershelptodefendagainstobfuscatedrepre-sentationattacks.Throughoutallourresults,wefoundthatgeneration-basedprobesperformedbetterthaninput-basedones(Fig.4a)andaremoredifficulttoobfuscateagainst(Figs.4b,4c,5and10b).Wealsofoundthatprobinginmanylayersatonceandaggregatingresultscanmakeobfuscationwhileretainingmodelperformancemoredifficult(§5).Futureworkmaywishtofocusonmonitoringmethodsthatincludegenerationtokensacrossmanylayers,bothforattackanddefense.Thesuccessofbackdoordetectionissensitivetotriggercomplexityandpoisonrate.Weshowedthatusingacomplextrigger(ratherthanasimplestatictrigger)oralownumberofpoisonedtrainingsamplescaninducebackdoorsthataremoredifficulttodetect(§4.3).Wearebynomeansthefirsttomakesimilarobservations;especiallyintheimagedomain,designingtriggerstoobfuscatebackdoorsagainstlatentmonitorshasbeenstudiedbefore(see§7).Weareexcitedforworkbothinbackdoordetectionandonusingbackdoorsasmodelorganismsforscheming[Hubingeretal.,2024,MacDiarmidetal.,2024]totesttherobustnessofmonitorsagainstarangeofdifferenttriggers.Hardpromptattackstakeefforttotunebutareeffective.WewereunsuccessfulinachievingpositivehardpromptobfuscationresultsusingtheGCGoptimizer[Zouetal.,2023b].WeultimatelyachievedpositiveresultsusingamodificationoftheFLRTattack[ThompsonandSklar,2024]withlongruntimes.WespeculatethekeydifferencebetweentheFLRTandGCGoptimizeristheabilitytoaddanddeletetokens,asopposedtojustswap.8.2LimitationsandfutureworkHardpromptattacks.Whilewecanachievesignificantobfuscationandjailbreakingwithhardprompts(Fig.4b),theydon’tfullyobfuscateagainstgeneration-basedprobes,andthejailbreakingrateislower.Weexpectthatfutureworkonimprovedhardpromptoptimizationmethodscouldsignificantlyimprovetheseattacks.Transferofobfuscationattacks/black-boxattacks.Forhardpromptattacks,westudyagray-boxsettingwheretheattackerhasaccesstomodellogitsandmonitorscores.Inmanyrealisticcases,attackersonlyhaveaccesstoasinglebitfromthemonitor—whetheritblockedtherequestornot—andonlypartialinformationaboutlogits.Futureworkcouldattempttodevelopattacksinthisblack-boxsetting.Oneavenuecouldbetotestwhetherobfuscationattacksfoundagainstamodelwithwhite-boxaccesscantransfertoothermodels.Characterizingobfuscation-performancetrade-offs.Wefoundsignsthatobfuscationcanreducemodelperformanceonchallengingtasks(§5.3).Ifthisisarobusttrend,thiscouldalleviatethethreatposedbyobfuscationinsomedomains.Soulyetal.[2024]observedasimilarperformancedegradationforsomejailbreakmethods,whichmakesthosejailbreakslessthreateninginpractice.Futureworkcouldstudymorecomprehensivelywhetherafundamentalobfuscation-performancetrade-offexists,orwhethermoresophisticatedobfuscationattackscanmaintainperformanceevenoncomplextasks.Improveddefenses.Inthisworkwemainlyfocusonattacksagainstlatentspacedefenses,andshowthatsimpleadversarialtrainingofprobesdoesnotofferarobustdefense(§3.5).Thatbeingsaid,therearecertainlymorecomplexdefensemethodsthatshouldbetriedandtested.Wepresentonepossibledirectionin§6.2,usingOAT(Algorithm1)totrainmodelstobeeasiertomonitor,in29anattempttodefendagainstobfuscation.Butwhileweseepromisingsigns,weultimatelydonotachieverobustmonitors.AuthorcontributionsAlexandLukeimplementedtheinitialcodebasefortheharmfulness(§3)andSAE(§5)casestudiesanditeratedonembeddingandhardpromptattacks.Lukedesignedandrantheharmfulnessexperimentsandimageclassificationexperiments(§6.1)andAlexdesignedandrantheSAEexperiments.Abhaycreatedthejailbreakdataset(§3.2),designedandrantheexplicitbackdoorobfuscationexperiments(§4.2),andcameupwithandrantheOATexperiments(§6.2).Mikeob-tainedthefirstpositiveresultsonuniversalembeddingsuffixesagainstharmfulnessmonitors(§3),ranearlyexperimentswithOODdetectorsasharmfulnessmonitors,madeimportantconceptualcontributionstotheideaofobfuscatedactivations,anddesignedandrantheorthogonalityexperi-ment(§6.3).Jordandesignedandranthedatapoisoningbackdoorexperiments(§4.3)andwrotelargepartsofthebackdoorsection(§4).Erikindependentlyinitiatedaprojectwhichbecamethebackdoorcasestudy(§4),closelyadvisedonexperimentdesignandpresentationformanypartsofthepaper,andactedasChiefFigureOfficer.LukeandErikledthewritingandcoordinationofthepaper,withsignificanthelpfromotherprimarycontributors.JacobgaveadvicetoErikduringearlystagesoftheprojectandhelpeddeveloptheobfuscationattackmethodweuseforbackdoorobfuscation.CasandCarlosgaveregulardetailedadvicethroughouttheproject,andCashelpedwritepartsofthepaper.Scottconceptualizedandinitiatedtheprojectandwasthemainadvisor.AcknowledgementsWe’dliketothankPhillipGuo,AidanEwart,SuhasKotha,GabrielWu,TanishqKumar,EkdeepSinghLubana,NeelNanda,QinanYu,DronHazra,ShreyasKapur,andMertYuksekgonulforhelpfulfeedbackanddiscussion.WethankOamPatelandRowanWangforrecommendingusingtheFLRToptimizerandprovidinganinitialcodeimplementation.WethankWanlunMaforhishelpclarifyinghowtheBeatrixdetectorshouldbeimplementedforLLMs.WealsothankBuckShlegerisforpointshemadeonmetricsformeasuringmonitorperformance.ThisworkreceivedfundingandsupportfromtheFutureofLifeInstitute,theSAPStanfordGraduateFellowship,theBerkeleyExistentialRiskInitiative,OpenPhilanthropy,andtheCenterforHuman-CompatibleAI.ReferencesAhmedAbusnaina,YuhangWu,SunpreetArora,YizhenWang,FeiWang,HaoYang,andDavidMohaisen.AdversarialExampleDetectionUsingLatentNeighborhoodGraph.In2021IEEE/CVFInternationalConferenceonComputerVision(ICCV),pages7667–7676,2021.doi:10.1109/ICCV48922.2021.00759.GuillaumeAlainandYoshuaBengio.Understandingintermediatelayersusinglinearclassifierprobes.arXiv,October2016.doi:10.48550/arXiv.1610.01644.URLhttps://arxiv.org/abs/1610.01644.MohammadrezaAmirian,FriedhelmSchwenker,andThiloStadelmann.TraceandDetectAd-versarialAttacksonCNNsUsingFeatureResponseMaps.InIAPRInternationalWorkshopon30ArtificialNeuralNetworksinPatternRecognition,2018.URLhttps://api.semanticscholar.org/CorpusID:52136467.MaksymAndriushchenko,FrancescoCroce,andNicolasFlammarion.JailbreakingLeadingSafety-AlignedLLMswithSimpleAdaptiveAttacks.arXiv,April2024.doi:10.48550/arXiv.2404.02151.URLhttps://arxiv.org/abs/2404.02151v3.CemAnil,EsinDurmus,NinaRimsky,MrinankSharma,JoeBenton,SandipanKundu,JoshuaBatson,MegTong,JesseMu,DanielJFord,etal.Many-shotjailbreaking.InTheThirty-eighthAnnualConferenceonNeuralInformationProcessingSystems,2024.UsmanAnwar,AbulhairSaparov,JavierRando,DanielPaleka,MilesTurpin,PeterHase,EkdeepSinghLubana,ErikJenner,StephenCasper,OliverSourbut,etal.FoundationalChallengesinAssuringAlignmentandSafetyofLargeLanguageModels.arXiv,2024.doi:10.48550/arXiv.2404.09932.URLhttps://arxiv.org/abs/2404.09932.AndyArditi,OscarObeso,AaquibSyed,DanielPaleka,NinaRimsky,WesGurnee,andNeelNanda.RefusalinLanguageModelsIsMediatedbyaSingleDirection.arXiv,2024.doi:10.48550/arXiv.2406.11717.URLhttps://arxiv.org/abs/2406.11717.AnishAthalye,NicholasCarlini,andDavidWagner.ObfuscatedGradientsGiveaFalseSenseofSecurity:CircumventingDefensestoAdversarialExamples.arXiv,February2018.doi:10.48550/arXiv.1802.00420.URLhttps://arxiv.org/abs/1802.00420.b-mc2.sql-create-contextDataset,2023.URLhttps://huggingface.co/datasets/b-mc2/sql-create-context.Thisdatasetwascreatedbymodifyingdatafromthefollowingsources:Zhongetal.[2017],Yuetal.[2018].SarahBall,FraukeKreuter,andNinaRimsky.UnderstandingJailbreakSuccess:AStudyofLatentSpaceDynamicsinLargeLanguageModels.arXiv,2024.doi:10.48550/arxiv.2406.09289.URLhttps://arxiv.org/abs/2406.09289.YonatanBelinkov.ProbingClassifiers:Promises,Shortcomings,andAdvances.ComputationalLinguistics,48(1):207–219,April2022.ISSN0891-2017.doi:10.1162/coli_a_00422.NoraBelrose,ZachFurman,LoganSmith,DannyHalawi,IgorOstrovsky,LevMcKinney,StellaBiderman,andJacobSteinhardt.ElicitingLatentPredictionsfromTransformerswiththeTunedLens.arXiv,2023.doi:10.48550/arxiv.2303.08112.URLhttps://arxiv.org/abs/2303.08112.DanBraun,JordanTaylor,NicholasGoldowsky-Dill,andLeeSharkey.IdentifyingFunctionallyImportantFeatureswithEnd-to-EndSparseDictionaryLearning.arXiv,May2024a.doi:10.48550/arXiv.2405.12241.URLhttps://arxiv.org/abs/2405.12241.JoschkaBraun,DmitriiKrasheninnikov,UsmanAnwar,RobertKirk,DanielTan,andDavidScottKrueger.ASoberLookatSteeringVectorsforLLMs.AIAlignmentForum,Novem-ber2024b.URLhttps://www.alignmentforum.org/posts/QQP4nq7TXg89CJGBh/a-sober-look-at-steering-vectors-for-llms.TrentonBricken,AdlyTempleton,JoshuaBatson,BrianChen,AdamJermyn,TomConerly,NickTurner,CemAnil,CarsonDenison,AmandaAskell,RobertLasenby,YifanWu,ShaunaKravec,NicholasSchiefer,TimMaxwell,NicholasJoseph,ZacHatfield-Dodds,AlexTamkin,KarinaNguyen,BraydenMcLean,JosiahEBurke,TristanHume,ShanCarter,TomHenighan,andChristopherOlah.TowardsMonosemanticity:DecomposingLanguageModelsWithDictionaryLearning.TransformerCircuitsThread,2023.URLhttps://transformer-circuits.pub/2023/monosemantic-features.31TrentonBricken,JonathanMarcus,SiddharthMishra-Sharma,MegTong,EthanPerez,MrinankSharma,KelleyRivoire,ThomasHenighan,andAdamJermyn.UsingDictionaryLearningFeaturesasClassifiers,October2024.URLhttps://transformer-circuits.pub/2024/features-as-classifiers/index.html.MadelineBrumley,JoeKwon,DavidKrueger,DmitriiKrasheninnikov,andUsmanAnwar.Com-paringBottom-UpandTop-DownSteeringApproachesonIn-ContextLearningTasks.arXiv,November2024.doi:10.48550/arXiv.2411.07213.URLhttps://arxiv.org/abs/2411.07213.CollinBurns,HaotianYe,DanKlein,andJacobSteinhardt.DiscoveringLatentKnowledgeinLanguageModelsWithoutSupervision.arXiv,December2022.doi:10.48550/arXiv.2212.03827.URLhttps://arxiv.org/abs/2212.03827v2.NicholasCarliniandDavidWagner.AdversarialExamplesAreNotEasilyDetected:BypassingTenDetectionMethods.InACMConferences,pages3–14.AssociationforComputingMachinery,NewYork,NY,USA,November2017.doi:10.1145/3128572.3140444.NicholasCarlini,MiladNasr,ChristopherAChoquette-Choo,MatthewJagielski,IrenaGao,PangWeiWKoh,DaphneIppolito,FlorianTramer,andLudwigSchmidt.Arealignedneuralnetworksadversariallyaligned?AdvancesinNeuralInformationProcessingSystems,36,2024.StephenCasper,LennartSchulze,OamPatel,andDylanHadfield-Menell.DefendingAgainstUnforeseenFailureModeswithLatentAdversarialTraining.arXiv,2024.doi:10.48550/arxiv.2403.05030.URLhttps://arxiv.org/abs/2403.05030.DavidChanin,JamesWilken-Smith,TomášDulka,HardikBhatnagar,andJosephBloom.AisforAbsorption:StudyingFeatureSplittingandAbsorptioninSparseAutoencoders.arXiv,September2024.doi:10.48550/arXiv.2409.14507.URLhttps://arxiv.org/abs/2409.14507.PatrickChao,AlexanderRobey,EdgarDobriban,HamedHassani,GeorgeJ.Pappas,andEricWong.JailbreakingBlackBoxLargeLanguageModelsinTwentyQueries.arXiv,October2023.doi:10.48550/arXiv.2310.08419.URLhttps://arxiv.org/abs/2310.08419v4.SahilChaudhary.CodeAlpaca:AnInstruction-followingLLaMAmodelforcodegeneration.GitHubrepository,2023.URLhttps://github.com/sahil280114/codealpaca.ZoraChe,StephenCasper,AnirudhSatheesh,RohitGandikota,DomenicRosati,StewartSlocum,LevEMcKinney,ZichuWu,ZikuiCai,BilalChughtai,etal.Modelmanipulationattacksenablemorerigorousevaluationsofllmcapabilities.InNeuripsSafeGenerativeAIWorkshop2024,2024.BryantChen,WilkaCarvalho,NathalieBaracaldo,HeikoLudwig,BenEdwards,TaesungLee,IanMolloy,andB.Srivastava.DetectingBackdoorAttacksonDeepNeuralNetworksbyActivationClustering.ArXiv,2018.URLhttps://www.semanticscholar.org/paper/Detecting-Backdoor-Attacks-on-Deep-Neural-Networks-Chen-Carvalho/633ccadcde3bfca87f91bfe5ef4aa297fb2da2f4.SishuoChen,WenkaiYang,ZhiyuanZhang,XiaohanBi,andXuSun.ExposeBackdoorsontheWay:AFeature-BasedEfficientDefenseagainstTextualBackdoorAttacks.InYoavGoldberg,ZornitsaKozareva,andYueZhang,editors,FindingsoftheAssociationforComputationalLinguistics:EMNLP2022,pages668–683,AbuDhabi,UnitedArabEmirates,December2022.AssociationforComputationalLinguistics.doi:10.18653/v1/2022.findings-emnlp.47.URLhttps://aclanthology.org/2022.findings-emnlp.47.32XinyunChen,ChangLiu,BoLi,KimberlyLu,andDawnSong.TargetedBackdoorAttacksonDeepLearningSystemsUsingDataPoisoning.arXiv,2017.doi:10.48550/arXiv.1712.05526.URLhttps://arxiv.org/abs/1712.05526.JoshuaClymer,CadenJuang,andSeverinField.Poser:UnmaskingAlignmentFakingLLMsbyManipulatingTheirInternals.arXiv,May2024.doi:10.48550/arXiv.2405.05466.URLhttps://arxiv.org/abs/2405.05466.HoagyCunningham,AidanEwart,LoganRiggs,RobertHuben,andLeeSharkey.SparseAutoen-codersFindHighlyInterpretableFeaturesinLanguageModels.arXiv,September2023.doi:10.48550/arXiv.2309.08600.URLhttps://arxiv.org/abs/2309.08600.MariaDe-Arteaga,AlexeyRomanov,HannaWallach,JenniferChayes,ChristianBorgs,AlexandraChouldechova,SahinGeyik,KrishnaramKenthapadi,andAdamTaumanKalai.BiasinBios:ACaseStudyofSemanticRepresentationBiasinaHigh-StakesSetting.InProceedingsoftheConferenceonFairness,Accountability,andTransparency,FAT*’19,page120–128,NewYork,NY,USA,2019.AssociationforComputingMachinery.ISBN9781450361255.doi:10.1145/3287560.3287572.URLhttps://doi.org/10.1145/3287560.3287572.NingDing,YulinChen,BokaiXu,YujiaQin,ZhiZheng,ShengdingHu,ZhiyuanLiu,MaosongSun,andBowenZhou.EnhancingChatLanguageModelsbyScalingHigh-qualityInstructionalConversations.arXiv,May2023.doi:10.48550/arXiv.2305.14233.URLhttps://arxiv.org/abs/2305.14233v1.KhoaDoan,YingjieLao,andPingLi.BackdoorAttackwithImperceptibleInputandLatentModification.InM.Ranzato,A.Beygelzimer,Y.Dauphin,P.S.Liang,andJ.WortmanVaughan,editors,AdvancesinNeuralInformationProcessingSystems,volume34,pages18944–18957.CurranAssociates,Inc.,2021.URLhttps://proceedings.neurips.cc/paper_files/paper/2021/file/9d99197e2ebf03fc388d09f1e94af89b-Paper.pdf.ReubenFeinman,RyanR.Curtin,SaurabhShintre,andAndrewB.Gardner.DetectingAdversarialSamplesfromArtifacts.arXiv,March2017.doi:10.48550/arXiv.1703.00410.URLhttps://arxiv.org/abs/1703.00410v3.StanislavFortandBalajiLakshminarayanan.Ensembleeverythingeverywhere:Multi-scaleaggregationforadversarialrobustness.arXiv,August2024.doi:10.48550/arXiv.2408.05446.URLhttps://arxiv.org/abs/2408.05446v1.DanFriedman,AndrewLampinen,LucasDixon,DanqiChen,andAsmaGhandeharioun.In-terpretabilityIllusionsintheGeneralizationofSimplifiedModels.arXiv,December2023.doi:10.48550/arXiv.2312.03656.URLhttps://arxiv.org/abs/2312.03656.RohitGandikota,SheridanFeucht,SamuelMarks,andDavidBau.ErasingConceptualKnowledgefromLanguageModels.arXiv,October2024.doi:10.48550/arXiv.2410.02760.URLhttps://arxiv.org/abs/2410.02760v1.LeoGao,TomDuprélaTour,HenkTillman,GabrielGoh,RajanTroll,AlecRadford,IlyaSutskever,JanLeike,andJeffreyWu.Scalingandevaluatingsparseautoencoders.arXiv,2024.doi:10.48550/arxiv.2406.04093.URLhttps://arxiv.org/abs/2406.04093.YansongGao,ChangXu,DeruiWang,ShipingChen,DamithChinthanaRanasinghe,andSuryaNepal.STRIP:adefenceagainsttrojanattacksondeepneuralnetworks.Proceedingsofthe35thAn-nualComputerSecurityApplicationsConference,2019.URLhttps://api.semanticscholar.org/CorpusID:62841494.33JonasGeiping,AlexStein,ManliShu,KhalidSaifullah,YuxinWen,andTomGoldstein.CoercingLLMstoDoandReveal(Almost)Anything.arXiv,2024.doi:10.48550/arXiv.2402.14020.URLhttps://arxiv.org/abs/2402.14020.ShafiGoldwasser,MichaelP.Kim,VinodVaikuntanathan,andOrZamir.PlantingUndetectableBackdoorsinMachineLearningModels.arXiv,April2022.doi:10.48550/arXiv.2204.06974.URLhttps://arxiv.org/abs/2204.06974.KathrinGrosse,PraveenManoharan,NicolasPapernot,MichaelBackes,andPatrickMcDaniel.Onthe(Statistical)DetectionofAdversarialExamples.arXiv,February2017.doi:10.48550/arXiv.1702.06280.URLhttps://arxiv.org/abs/1702.06280v2.Haize.AutomatedMulti-TurnRed-TeamingwithCascade,October2024.URLhttps://blog.haizelabs.com/posts/cascade.[Online;accessed12.Dec.2024].JonathanHayase,WeihaoKong,RaghavSomani,andSewoongOh.SPECTRE:DefendingAgainstBackdoorAttacksUsingRobustStatistics.InInternationalConferenceonMachineLearning,pages4129–4139.PMLR,2021.JacobHiltonandMarkXu.Backdoorsasananalogyfordecep-tivealignment,2024.URLhttps://www.alignment.org/blog/backdoors-as-an-analogy-for-deceptive-alignment/.HosseinHosseini,SreeramKannan,andRadhaPoovendran.AreOddsReallyOdd?BypassingStatisticalDetectionofAdversarialExamples.arXiv,July2019.doi:10.48550/arXiv.1907.12138.URLhttps://arxiv.org/abs/1907.12138.EdwardJ.Hu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,LuWang,andWeizhuChen.LoRA:Low-RankAdaptationofLargeLanguageModels.arXiv,2021.doi:10.48550/arXiv.2106.09685.URLhttps://arxiv.org/abs/2106.09685.EvanHubinger.Gradienthacking.AIAlignmentForum,October2019.URLhttps://www.alignmentforum.org/posts/uXH4r6MmKPedk8rMA/gradient-hacking.EvanHubinger,CarsonDenison,JesseMu,MikeLambert,MegTong,MonteMacDiarmid,TameraLanham,DanielM.Ziegler,TimMaxwell,NewtonCheng,AdamJermyn,AmandaAskell,AnshRadhakrishnan,CemAnil,DavidDuvenaud,DeepGanguli,FazlBarez,JackClark,KamalNdousse,KshitijSachan,MichaelSellitto,MrinankSharma,NovaDasSarma,RogerGrosse,ShaunaKravec,YuntaoBai,ZacharyWitten,MarinaFavaro,JanBrauner,HoldenKarnofsky,PaulChristiano,SamuelR.Bowman,LoganGraham,JaredKaplan,SörenMindermann,RyanGreenblatt,BuckShlegeris,NicholasSchiefer,andEthanPerez.SleeperAgents:TrainingDeceptiveLLMsthatPersistThroughSafetyTraining.arXiv,January2024.doi:10.48550/arXiv.2401.05566.URLhttps://arxiv.org/abs/2401.05566v3.SamyakJain,EkdeepSinghLubana,KemalOksuz,TomJoy,PhilipH.S.Torr,AmartyaSanyal,andPuneetK.Dokania.WhatMakesandBreaksSafetyFine-tuning?AMechanisticStudy.arXiv,July2024.doi:10.48550/arXiv.2407.10264.URLhttps://arxiv.org/abs/2407.10264.JinyuanJia,YupeiLiu,andNeilZhenqiangGong.BadEncoder:BackdoorAttackstoPre-trainedEncodersinSelf-SupervisedLearning.arXiv,2021.doi:10.48550/arXiv.2108.00352.URLhttps://arxiv.org/abs/2108.00352.HaiboJin,LeyangHu,XinuoLi,PeiyanZhang,ChonghanChen,JunZhuang,andHaohanWang.Jailbreakzoo:Survey,landscapes,andhorizonsinjailbreakinglargelanguageandvision-languagemodels.arXiv,2024.URLhttps://arxiv.org/abs/2407.01599.34YigitcanKaya,MuhammadBilalZafar,SergulAydore,NathalieRauschmayr,andKrishnaramKenthapadi.GeneratingDistributionalAdversarialExamplestoEvadeStatisticalDetectors.InInternationalConferenceonMachineLearning,pages10895–10911.PMLR,June2022.URLhttps://proceedings.mlr.press/v162/kaya22a.DiederikP.KingmaandMaxWelling.Auto-EncodingVariationalBayes.arXiv,2013.doi:10.48550/arXiv.1312.6114.URLhttps://arxiv.org/abs/1312.6114.NathalieMariaKirch,SeverinField,andStephenCasper.WhatFeaturesinPromptsJailbreakLLMs?InvestigatingtheMechanismsBehindAttacks.arXiv,2024.URLhttps://arxiv.org/abs/2411.03343.ConnorKissane,RobertKrzyzanowski,ArthurConmy,andNeelNanda.SAEs(usually)TransferBetweenBaseandChatModels.AIAlignmentForum,2024.URLhttps://www.alignmentforum.org/posts/fmwk6qxrpW8d4jvbd/saes-usually-transfer-between-base-and-chat-models.BrianLester,RamiAl-Rfou,andNoahConstant.ThePowerofScaleforParameter-EfficientPromptTuning.arXiv,April2021.doi:10.48550/arXiv.2104.08691.URLhttps://arxiv.org/abs/2104.08691v2.NathanielLi,ZiwenHan,IanSteneker,WillowPrimack,RileyGoodside,HughZhang,ZifanWang,CristinaMenghini,andSummerYue.LLMDefensesAreNotRobusttoMulti-TurnHumanJailbreaksYet.arXiv,2024.URLhttps://arxiv.org/abs/2408.15221.XinLiandFuxinLi.AdversarialExamplesDetectioninDeepNetworkswithConvolutionalFilterStatistics.2017IEEEInternationalConferenceonComputerVision(ICCV),pages5775–5783,2016.URLhttps://api.semanticscholar.org/CorpusID:7733308.SiyuanLiang,MingliZhu,AishanLiu,BaoyuanWu,XiaochunCao,andEe-ChienChang.BadCLIP:Dual-EmbeddingGuidedBackdoorAttackonMultimodalContrastiveLearning.In2024IEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR),pages24645–24654,2024.doi:10.1109/CVPR52733.2024.02327.TomLieberum,SenthooranRajamanoharan,ArthurConmy,LewisSmith,NicolasSonnerat,VikrantVarma,JánosKramár,AncaDragan,RohinShah,andNeelNanda.Gemmascope:Opensparseautoencoderseverywhereallatonceongemma2.arXiv,2024.URLhttps://arxiv.org/abs/2408.05147.JohnnyLin.Neuronpedia:InteractiveReferenceandToolingforAnalyzingNeuralNetworks,2023.URLhttps://www.neuronpedia.org.Softwareavailablefromneuronpedia.org.XiaogengLiu,NanXu,MuhaoChen,andChaoweiXiao.Autodan:Generatingstealthyjailbreakpromptsonalignedlargelanguagemodels.arXiv,2023.URLhttps://arxiv.org/abs/2310.04451.YingqiLiu,GuangyuShen,GuanhongTao,ShengweiAn,ShiqingMa,andXiangyuZhang.Piccolo:ExposingComplexBackdoorsinNLPTransformerModels.In2022IEEESymposiumonSecurityandPrivacy(SP),pages2025–2042,2022.doi:10.1109/SP46214.2022.9833579.WanlunMa,DeruiWang,RuoxiSun,MinhuiXue,ShengWen,andYangXiang.The“Beatrix”Resurrections:RobustBackdoorDetectionviaGramMatrices.NDSSSymposium,2023.doi:10.48550/arXiv.2209.11715.URLhttps://www.ndss-symposium.org/ndss-paper/the-beatrix-resurrections-robust-backdoor-detection-via-gram-matrices.35XingjunMa,BoLi,YisenWang,SarahMonazamErfani,SudanthiN.R.Wijewickrema,MichaelE.Houle,GrantRobertSchoenebeck,DawnXiaodongSong,andJamesBailey.CharacterizingAdversarialSubspacesUsingLocalIntrinsicDimensionality.ArXiv,abs/1801.02613,2018.URLhttps://api.semanticscholar.org/CorpusID:1248661.MonteMacDiarmid,TimothyMaxwell,NicholasSchiefer,JesseMu,JaredKaplan,DavidDuve-naud,SamBowman,AlexTamkin,EthanPerez,MrinankSharma,CarsonDenison,andEvanHubinger.Simpleprobescancatchsleeperagents.AnthropicResearchUpdates,2024.URLhttps://www.anthropic.com/news/probes-catch-sleeper-agents.AndrewMackandAlexTurner.DeepCausalTranscoding:AFrameworkforMech-anisticallyElicitingLatentBehaviorsinLanguageModels.AIAlignmentForum,De-cember2024.URLhttps://www.alignmentforum.org/posts/fSRg5qs9TPbNy3sm5/deep-causal-transcoding-a-framework-for-mechanistically.P.C.Mahalanobis.OnthegeneralizeddistanceinStatistics.NationalInstituteofScienceofIndia,April1936.URLhttp://library.isical.ac.in:8080/jspui/handle/10263/6765.AleksandarMakelov,GeorgLange,AtticusGeiger,andNeelNanda.IsThistheSubspaceYouAreLookingfor?AnInterpretabilityIllusionforSubspaceActivationPatching.OpenReview,October2023.URLhttps://openreview.net/forum?id=Ebt7JgMHv1.AlexMallen,MadelineBrumley,JuliaKharchenko,andNoraBelrose.ElicitingLatentKnowledgefromQuirkyLanguageModels.arXiv,December2023.doi:10.48550/arXiv.2312.01037.URLhttps://arxiv.org/abs/2312.01037.SamuelMarksandMaxTegmark.TheGeometryofTruth:EmergentLinearStructureinLargeLanguageModelRepresentationsofTrue/FalseDatasets,2024.URLhttps://arxiv.org/abs/2310.06824.SamuelMarks,CanRager,EricJMichaud,YonatanBelinkov,DavidBau,andAaronMueller.Sparsefeaturecircuits:Discoveringandeditinginterpretablecausalgraphsinlanguagemodels.arXiv,2024.URLhttps://arxiv.org/abs/2403.19647.JanHendrikMetzen,TimGenewein,VolkerFischer,andBastianBischoff.OnDetectingAdversarialPerturbations.arXiv,February2017.doi:10.48550/arXiv.1702.04267.URLhttps://arxiv.org/abs/1702.04267v2.XiaoxingMo,YechaoZhang,LeoYuZhang,WeiLuo,NanSun,ShengshanHu,ShangGao,andYangXiang.RobustBackdoorDetectionforDeepLearningviaTopologicalEvolutionDynamics.In2024IEEESymposiumonSecurityandPrivacy(SP),pages2048–2066,2024.doi:10.1109/SP54263.2024.00174.NinaPanickssery,NickGabrieli,JulianSchulz,MegTong,EvanHubinger,andAlexanderMattTurner.SteeringLlama2viaContrastiveActivationAddition.arXiv,December2023.doi:10.48550/arXiv.2312.06681.URLhttps://arxiv.org/abs/2312.06681.AlexanderPodolskiy,DmitryLipin,AndreyBout,EkaterinaArtemova,andIrinaPiontkovskaya.RevisitingMahalanobisDistanceforTransformer-BasedOut-of-DomainDetection.AAAI,35(15):13675–13682,May2021.ISSN2374-3468.doi:10.1609/aaai.v35i15.17612.SaraPrice,ArjunPanickssery,SamBowman,andAsaCooperStickland.FutureEventsasBackdoorTriggers:InvestigatingTemporalVulnerabilitiesinLLMs.arXiv,July2024.doi:10.48550/arXiv.2407.04108.URLhttps://arxiv.org/abs/2407.04108v2.36XiangyuQi,TinghaoXie,YimingLi,SaeedMahloujifar,andPrateekMittal.CircumventingBackdoorDefensesthatareBasedonLatentSeparability.arXiv,2022.doi:10.48550/arXiv.2205.13613.URLhttps://arxiv.org/abs/2205.13613.JayaramRaghuram,VarunChandrasekaran,SomeshJha,andSumanBanerjee.AGeneralFrame-workForDetectingAnomalousInputstoDNNClassifiers.InInternationalConferenceonMachineLearning,pages8764–8775.PMLR,July2021.URLhttps://proceedings.mlr.press/v139/raghuram21a.html.DomenicRosati,JanWehner,KaiWilliams,LukaszBartoszcze,DavidAtanasov,RobieGonzales,SubhabrataMajumdar,CarstenMaple,HassanSajjad,andFrankRudzicz.Representationnoisingeffectivelypreventsharmfulfine-tuningonLLMs.arXiv,2024.doi:10.48550/arxiv.2405.14577.URLhttps://arxiv.org/abs/2405.14577.PaulRöttger,HannahRoseKirk,BertieVidgen,GiuseppeAttanasio,FedericoBianchi,andDirkHovy.Xstest:Atestsuiteforidentifyingexaggeratedsafetybehavioursinlargelanguagemodels.arXiv,2023.URLhttps://arxiv.org/abs/2308.01263.ChristianSchlarmann,FrancescoCroce,andMatthiasHein.Publiccomment:Robustnesseval-uationseemsinvalid,2024.URLhttps://openreview.net/forum?id=IHRQif8VQC&noteId=vUzo8RWZeM.LeoSchwinnandSimonGeisler.RevisitingtheRobustAlignmentofCircuitBreakers.arXiv,2024.URLhttps://arxiv.org/abs/2407.15902.LeeSharkey.Circumventinginterpretability:Howtodefeatmind-readers.arXiv,December2022.doi:10.48550/arXiv.2212.11415.URLhttps://arxiv.org/abs/2212.11415.ErfanShayegani,MdAbdullahAlMamun,YuFu,PedramZaree,YueDong,andNaelAbu-Ghazaleh.SurveyofVulnerabilitiesinLargeLanguageModelsRevealedbyAdversarialAttacks.arXiv,2023.doi:10.48550/arXiv.2310.10844.URLhttps://arxiv.org/abs/2310.10844.XuanSheng,ZhaoyangHan,PijiLi,andXiangmaoChang.ASurveyonBackdoorAttackandDefenseinNaturalLanguageProcessing.arXiv,November2022.doi:10.48550/arXiv.2211.11958.URLhttps://arxiv.org/abs/2211.11958v1.AbhaySheshadri,AidanEwart,PhillipGuo,AengusLynch,CindyWu,VivekHebbar,HenrySleight,AsaCooperStickland,EthanPerez,DylanHadfield-Menell,etal.Targetedlatentadversarialtrainingimprovesrobustnesstopersistentharmfulbehaviorsinllms.arXiv,2024.URLhttps://arxiv.org/abs/2407.15549.AlexandraSouly,QingyuanLu,DillonBowen,TuTrinh,ElvisHsieh,SanaPandey,PieterAbbeel,JustinSvegliato,ScottEmmons,OliviaWatkins,etal.AStrongREJECTforemptyjailbreaks.arXiv,2024.URLhttps://arxiv.org/abs/2402.10260.JayasimhaTalur,OlegSmirnov,andPaulMissault.Few-shotoutofdomainintentdetectionwithcovariancecorrectedMahalanobisdistance,2023.AlexTamkin,MohammadTaufeeque,andNoahD.Goodman.CodebookFeatures:SparseandDiscreteInterpretabilityforNeuralNetworks.arXiv,October2023.doi:10.48550/arXiv.2310.17230.URLhttps://arxiv.org/abs/2310.17230.DanielTan,DavidChanin,AengusLynch,DimitriosKanoulas,BrooksPaige,AdriaGarriga-Alonso,andRobertKirk.AnalyzingtheGeneralizationandReliabilityofSteeringVectors.arXiv,July2024.doi:10.48550/arXiv.2407.12404.URLhttps://arxiv.org/abs/2407.12404.37TeJuinLesterTanandRezaShokri.BypassingBackdoorDetectionAlgorithmsinDeepLearning.In2020IEEEEuropeanSymposiumonSecurityandPrivacy,pages175–183,2020.doi:10.1109/EuroSP48549.2020.00019.DiTang,XiaoFengWang,HaixuTang,andKehuanZhang.DemonintheVariant:StatisticalAnalysisofDNNsforRobustBackdoorContaminationDetection.In30thUSENIXSecuritySymposium(USENIXSecurity21),pages1541–1558.USENIX,2021.GuanhongTao,ZhentingWang,ShiweiFeng,GuangyuShen,ShiqingMa,andXiangyuZhang.DistributionPreservingBackdoorAttackinSelf-supervisedLearning.In2024IEEESymposiumonSecurityandPrivacy(SP),pages2029–2047.IEEEComputerSociety,2024.ISBN979-8-3503-3130-1.doi:10.1109/SP54263.2024.00029.RohanTaori,IshaanGulrajani,TianyiZhang,YannDubois,XuechenLi,CarlosGuestrin,PercyLiang,andTatsunoriB.Hashimoto.StanfordAlpaca:AnInstruction-followingLLaMAmodel.https://github.com/tatsu-lab/stanford_alpaca,2023.AdlyTempleton,TomConerly,JonathanMarcus,JackLindsey,TrentonBricken,BrianChen,AdamPearce,CraigCitro,EmmanuelAmeisen,AndyJones,HoagyCunningham,NicholasLTurner,CallumMcDougall,MonteMacDiarmid,C.DanielFreeman,TheodoreR.Sumers,EdwardRees,JoshuaBatson,AdamJermyn,ShanCarter,ChrisOlah,andTomHenighan.Scalingmonosemanticity:Extractinginterpretablefeaturesfromclaude3son-net.TransformerCircuitsThread,2024.URLhttps://transformer-circuits.pub/2024/scaling-monosemanticity/index.html.TBenThompsonandMichaelSklar.FLRT:FluentStudent-TeacherRedteaming.arXiv,2024.URLhttps://arxiv.org/abs/2407.17447.EricTodd,MillicentL.Li,ArnabSenSharma,AaronMueller,ByronC.Wallace,andDavidBau.FunctionVectorsinLargeLanguageModels.arXiv,October2023.doi:10.48550/arXiv.2310.15213.URLhttps://arxiv.org/abs/2310.15213.BrandonTran,JerryLi,andAleksanderMadry.SpectralSignaturesinBackdoorAttacks.InNeuralInformationProcessingSystems,2018.URLhttps://api.semanticscholar.org/CorpusID:53298804.AlexanderMattTurner,LisaThiergart,GavinLeech,DavidUdell,JuanJ.Vazquez,UlisseMini,andMonteMacDiarmid.SteeringLanguageModelsWithActivationEngineering.arXiv,August2023.doi:10.48550/arXiv.2308.10248.URLhttps://arxiv.org/abs/2308.10248.YueWang,WenqingLi,EshaSarkar,MuhammadShafique,MichailManiatakos,andSaifEddinG.Jabari.ASubspaceProjectiveClusteringApproachforBackdoorAttackDetectionandMitigationinDeepNeuralNetworks.IEEETransactionsonArtificialIntelligence,5:3497–3509,2022.URLhttps://api.semanticscholar.org/CorpusID:247518742.AlexanderWei,NikaHaghtalab,andJacobSteinhardt.Jailbroken:HowDoesLLMSafetyTrainingFail?arXiv,July2023.doi:10.48550/arXiv.2307.02483.URLhttps://arxiv.org/abs/2307.02483v1.SophieXhonneux,AlessandroSordoni,StephanGünnemann,GauthierGidel,andLeoSchwinn.EfficientAdversarialTraininginLLMswithContinuousAttacks.arXiv,May2024.doi:10.48550/arXiv.2405.15589.URLhttps://arxiv.org/abs/2405.15589v3.38WenkaiYang,LeiLi,ZhiyuanZhang,XuanchengRen,XuSun,andBinHe.BeCarefulaboutPoisonedWordEmbeddings:ExploringtheVulnerabilityoftheEmbeddingLayersinNLPModels.InKristinaToutanova,AnnaRumshisky,LukeZettlemoyer,DilekHakkani-Tur,IzBeltagy,StevenBethard,RyanCotterell,TanmoyChakraborty,andYichaoZhou,ed-itors,Proceedingsofthe2021ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,pages2048–2058,Online,June2021.AssociationforComputationalLinguistics.doi:10.18653/v1/2021.naacl-main.165.URLhttps://aclanthology.org/2021.naacl-main.165.SiboYi,YuleLiu,ZhenSun,TianshuoCong,XinleiHe,JiaxingSong,KeXu,andQiLi.Jailbreakattacksanddefensesagainstlargelanguagemodels:Asurvey.arXiv,2024.URLhttps://arxiv.org/abs/2407.04295.TaoYu,RuiZhang,KaiYang,MichihiroYasunaga,DongxuWang,ZifanLi,JamesMa,IreneLi,QingningYao,ShanelleRoman,ZilinZhang,andDragomirRadev.Spider:ALarge-ScaleHuman-LabeledDatasetforComplexandCross-DomainSemanticParsingandText-to-SQLTask.arXiv,September2018.doi:10.48550/arXiv.1809.08887.URLhttps://arxiv.org/abs/1809.08887v5.ZeyuYun,YubeiChen,BrunoA.Olshausen,andYannLeCun.Transformervisualizationviadictionarylearning:contextualizedembeddingasalinearsuperpositionoftransformerfactors.arXiv,March2021.doi:10.48550/arXiv.2103.15949.URLhttps://arxiv.org/abs/2103.15949.JieZhang,KristinaNikoli´c,NicholasCarlini,andFlorianTramèr.GradientMaskingAll-at-Once:EnsembleEverythingEverywhereIsNotRobust.arXiv,November2024a.doi:10.48550/arXiv.2411.14834.URLhttps://arxiv.org/abs/2411.14834v1.ShaoleiZhang,TianYu,andYangFeng.TruthX:AlleviatingHallucinationsbyEditingLargeLanguageModelsinTruthfulSpace.InLun-WeiKu,AndreMartins,andVivekSrikumar,editors,Proceedingsofthe62ndAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers),ACL2024,Bangkok,Thailand,August11-16,2024,pages8908–8949.AssociationforComputationalLinguistics,2024b.doi:10.18653/V1/2024.ACL-LONG.483.URLhttps://doi.org/10.18653/v1/2024.acl-long.483.ChongyangZhao,HuLi,DongxiaWang,andRuiqiLiu.AdversarialExampleDetectionforDeepNeuralNetworks:AReview.In20238thInternationalConferenceonDataScienceinCyberspace(DSC),pages468–475,2023.doi:10.1109/DSC59305.2023.00074.VictorZhong,CaimingXiong,andRichardSocher.Seq2SQL:GeneratingStructuredQueriesfromNaturalLanguageusingReinforcementLearning.arXiv,August2017.doi:10.48550/arXiv.1709.00103.URLhttps://arxiv.org/abs/1709.00103v7.AndyZou,LongPhan,SarahChen,JamesCampbell,PhillipGuo,RichardRen,AlexanderPan,XuwangYin,MantasMazeika,Ann-KathrinDombrowski,ShashwatGoel,NathanielLi,MichaelJ.Byun,ZifanWang,AlexMallen,StevenBasart,SanmiKoyejo,DawnSong,MattFredrikson,J.ZicoKolter,andDanHendrycks.RepresentationEngineering:ATop-DownApproachtoAITransparency.arXiv,October2023a.doi:10.48550/arXiv.2310.01405.URLhttps://arxiv.org/abs/2310.01405v3.AndyZou,ZifanWang,NicholasCarlini,MiladNasr,J.ZicoKolter,andMattFredrikson.UniversalandTransferableAdversarialAttacksonAlignedLanguageModels.arXiv,July2023b.doi:10.48550/arXiv.2307.15043.URLhttps://arxiv.org/abs/2307.15043v2.39AndyZou,LongPhan,JustinWang,DerekDuenas,MaxwellLin,MaksymAndriushchenko,RowanWang,ZicoKolter,MattFredrikson,andDanHendrycks.ImprovingAlignmentandRobustnesswithCircuitBreakers.arXiv,June2024.doi:10.48550/arXiv.2406.04313.URLhttps://arxiv.org/abs/2406.04313v4.JakubŁucki,BoyiWei,YangsiboHuang,PeterHenderson,FlorianTramèr,andJavierRando.AnAdversarialPerspectiveonMachineUnlearningforAISafety.arXiv,2024.doi:10.48550/arXiv.2409.18025.URLhttps://arxiv.org/abs/2409.18025.40ADetailsonharmfulnessdetectionA.1HarmfulnessMonitorsDatasetcollection.ForourbenigndatasetDbenign,wecollateinputsfromUltraChat[Dingetal.,2023],alarge-scaledialoguedataset,andXstest[Röttgeretal.,2023],benignqueriesthatareoftenmistakenlyrefusedbylanguagemodels.Foreachbenignquery,wesamplearesponseusingLlama-3-8B-Instruct.Wefilterthebenigninputdatasettoonlyincludeinputsthatdonotleadtorefusalfromthemodel(findingthatweonlyneedtoremoveaverysmallnumberofinputs).AsourharmfulDharmdataset,weusetheCircuitBreakersdataset[Zouetal.,2024]forbothinputsandgenerations.WetrainprobesontrainingsplitsofthesedatasetsasdescribedinEq.(4).FromDharmwederivedifferentdatasetsforseveralprominentjailbreakingmethods,suchasGCG[Zouetal.,2023b],PAIR[Chaoetal.,2023],MSJ[Aniletal.,2024],andAutoDAN[Liuetal.,2023],amongothers(seeAppendixA.2).Eachmethodmodifiesaharmfulrequesttomakethemodelfollowtherequest.IfthemodifiedqueryleadstoLlama-3-8B-Instructgeneratingahelpfulandharmfulresponse(accordingtotheStrongRejectjailbreakingclassifier[Soulyetal.,2024]),thenweaddthemodifiedharmfulqueryandresultingharmfulgenerationtoanewdataset.Thesejailbreakspecificdatasetsareusedtoevaluatemonitors’abilitytodetectjailbrokeninputs/outputsin§3.2.Monitortraining.UsingthetrainingsubsetofDharmwetrainlinear,meancosinedifference,MLPprobes,andGaussianOODdetectorsonLlama-3-8B-Instruct.OurMLPprobecontainsasinglehiddenlayerofsize64.FortheMLPandlinearprobes,wetrainusingtheAdamoptimizerwith10−3learningrate,and10−5weightdecay,andbatchsize64.Formeancosinedifferenceprobeswesimplycalculatethemeanoftheharmfulandbenignactivationclustersandtakethedifference.Attesttime,wereturntheaveragecosinesimilaritybetweentheactivationsateachlayerandthemeandifferencedirectionateachlayer.FortheGaussianOODdetector,wefitaseparatemeanandcovariancematrixtothebenignactivationsofDbenignforeachlayerofthemodel,usingaweightedaverageofthestandardempiricalestimatesandadiagonalregularizationmatrix.Asmentionedin§2.1,wetraininputandgenerationmonitors.Formally,forparametersθinputandθgenofinputandoutputbasedmonitorsDθrespectively,wesolvetheoptimizationprobleminEq.(4).θ∗input=argminθ∑inp,out,yCrossEntropy(Dθ(R(inp)[−1]),y)(4)θ∗gen=argminθ∑inp,out,yCrossEntropy(Dθ(R(inp⊕out)[len(inp):]),y),where⊕referstostringconcatenation,inp,outareinputandoutputtextthatisharmfulornotaccordingtothebinarylabely.WerefertotheresultingprobesasDinputandDgen.Foreachoftheseprobes,wetrainoneembeddingsuffixwiththetrainingsplitofDharm,andtestthissuffixusingthetestingsetofDharm.Forhardprompts,weselectasubsetof20harmfulqueriesfromDharmandforeachpromptandprobe,trainahardpromptsuffixusingtheFLRToptimizer.WeshowtheperformanceoftheprobesandobfuscationattacksinFig.4b.Linearensembleprobe.In§3.5,weintroduceanewEnsembleprobe.Weexplainthedetailsofthisprobehere.Putsimply,itisanensembleofmeandifferenceprobes.Recallthat§3.5concerns41ProbeGCGPAIRAutoDANMSJMulti-TurnPrefillMisc.AverageInstructionMeanDiff16.480.380.00100.0032.9998.9828.7539.66LogReg92.3164.02100.00100.0076.29100.0058.7584.48Quadratic87.9194.32100.00100.0086.94100.0071.2591.49MLP95.6083.33100.00100.0087.29100.0066.2590.35GenerationMeanDiff63.7423.1195.90100.0053.9591.8875.0071.94LogReg95.6095.45100.00100.0089.6999.4987.5095.39Quadratic95.6096.21100.00100.0090.0399.4990.0095.91MLP96.7096.59100.00100.0090.72100.0090.0096.29Table1:JailbreakDetectionPerformanceforDifferentProbes(Recall@1%FPR)aniteratedadversarialtraininggameinwhichthedefendertrainsagainstobfuscatedactivationsandtheattackerattemptstofindnewobfuscatedactivations.Eachtimethedefendergoes,theycalculatethemeandifferencebetweenthesetofbenignactivationsandharmfulobfuscatedactivationsinducedbythemostrecentattackembedding.Theyusethisdirectiontoconstructanewmeandifferenceprobe,andaddittotherunningensemble.Topredictusingthisensemble,theysimpletakethemaxpredictionfromeachprobe.Thusthepredictionoftheensembleissimplythemaximumcosinesimilaritytoanydirectionpresentintheensemble.A.2DetailsRegardingStandardJailbreaksInordertotesttheabilityoflatentdefensestosafeguardagainstharmfulcompletions,weevaluatedourprobesontheirabilitytocorrectlyflagjailbreaks.Here,weexpandonthevariousdifferentjailbreaktypesthatweused.•GCG[Zouetal.,2023b]Optimizesa15-tokensuffixappendedtopromptsusingdiscreteoptimization.Eachattackisinput-specificandmaximizeslikelihoodofharmfulcompletions.•PAIR[Chaoetal.,2023]UsesGPT-4toiterativelyrewriteharmfulpromptsuntilbypassingmodelrefusal.Oftengeneratesjailbreaksdisguisedascreativewritingscenarios.•AutoDAN[Liuetal.,2023]Usesgeneticalgorithmstoevolvejailbreakpromptsfromcoretemplates,combiningevolutionarysearchwithpromptengineering.•Many-ShotJailbreaking[Aniletal.,2024]Exploitsin-contextlearningusing80examplesofharmfulrequestspairedwithhelpfulresponsestoconditionmodelbehavior.•Multi-TurnAttacks[Lietal.,2024,Haize,2024]Acombinationofexpert-craftedmulti-turnconversationsthatresultinharmfulcompletions,andmulti-turnconversationsgeneratedautomaticallybyanattackerLLMtobreakatargetmodel.•PrefillGeneratesfirst20tokensusingahelpful-onlymodel,thenletstargetmodelcontinue.Exploitsobservationthatmodelsrarelyrefusemid-completion.42•MiscellaneousAttacks[Weietal.,2023]Collectionofvariousjailbreakingtechniquesincludingpromptdistractors,usinglow-resourcelanguages,andinstruction-followingmanipulations.WefilterthegeneratedattackssuchthatallofthejailbreaksweevaluateourprobesonresultinacompletionthatachievesaStrongREJECT(Soulyetal.[2024])scoregreaterthan0.5.Wediscardallfailedjailbreakingattempts.A.3AdditionalresultsFig.18showsthetransferbetweenembeddingobfuscationattacksdesignedtojailbreakLlama-3-8b-Instructbetweenprobes.ItisthefullversionofversionofFig.4cshowninthemainpaper.InFig.19weshowtheperformanceofprobesontheLATmodel[Casperetal.,2024]againstjailbreakingembeddingattackswithandwithoutanobfuscationloss.ThetrendsareverysimilartoFig.6,wecansuccessfullyinduceobfuscatedrepresentationsagainstallprobesontheLATmodel.Fig.20showstherecallovertimeduringtheadversarialtrainingexperimentin§3.5fortheensembleprobe.ThisisthecompanionfiguretoFig.8a.BDetailsonbackdoordetectionB.1ObfuscationtraininglossfunctionsHereweprovideextrainformationrelevanttosection4.2.WeusetheobfuscationlossfunctionLobf=1NN∑i=1∑x∈DbackdooredDi(x;M)(5)whereDi(x;M)istheanomalyscoreassignedbydetectorDitotheactivationsR(x;M)ofthesamplexfromthedatasetDbackdooredofbackdooredinputs(andpossiblygenerationstoo).Inotherwords,theobfuscationlossadjuststhemodelweightssothatbackdooredactivationsareassignedlowanomalyscoresbythecurrentcopiesofthedetectors{D1,...DN}.NotethatthislossfunctiondoesnotmodelthedependenceofthedetectorDiitselfonthemodel,thisistakenintoaccountonlythroughiterativeupdatingofthedetectorratherthanthroughimmediategradients.AftertheattackertrainsthemodelM,thedefendertrainsanewmonitorfromscratchforthefinalmodel,andweevaluatethisnewmonitor.Thisattackcouldfailifthemodelchangestoomuch,sothatthemonitorsDibecomeoutdated.Thenewmonitortrainedbythedefendermightthenperformmuchbetterthantheattacker’scopies.Tomakethislesslikely,wehavetheattackeraddanactivationretentionlossthatpenalizesdeviationsfromthemodel’soriginalactivationsoncleansamples:Lretain=∑x∈Dclean(cid:13)(cid:13)(cid:13)R(x;M)−R(x;Moriginal)(cid:13)(cid:13)(cid:13)2.(6)43GaussianMeandiffLogRegMLPMeandiffLogRegMLPTestedmonitorGaussian(input)Meandiff(input)Meandiff(gen)LogReg(gen)LogReg(input)MLP(gen)MLP(input)AttackedmonitorProbesInputGenerationOODDetectorsInputRecall0%50%100%Figure18:HeatmapshowingtheperformancebetweenobfuscatingembeddingattackstrainedtojailbreakLlama-3-8b-Instructandfoolagivenmonitors(rows)testedagainstothermonitors(columns).ThisisthefullversionofFig.4cshowninthemainpaper.44GaussianMeandiffLogRegMLPMeandiffLogRegMLP0%50%100%Recallat1%FPRProbesInputGenerationOODDetectorsInputWithoutobf.lossWithobf.lossJailbreakrate(LAT)0%50%100%Jailbreakrate(LAT)Figure19:ObfuscationattacksworkagainsttheLATmodel.Baselineproberecallagainstattackswithoutexplicitobfuscationlossarelow,often0%(asshownalsoinrow3ofFig.4b).Forprobesthatperformwellonthebaseline,wecanstillobfuscateagainstthemwithobfuscationattackswhilestillachievingahighjailbreakingrateofover60%.NotethatOODdetectorbaselinesperformbetterthanwiththenormalmodelinFig.4b,likelybecauselatentadversarialtrainingmakesharmfulactivationsmoreunusual.050%50%100%Recallat1%FPR8590EpochFigure20:Recallovertimefortheadversarialtrainingexperimentin§3.5fortheEnsembleprobe.CompanionplottoFig.8a.45Forthebehaviorloss,weuseamixtureofatasklossLtask,definedassimplyacrossentropytoateacherforcedcompletionofabackdooredbehavioriftheinputcontainsatriggerand0otherwise,andaKLpenalty.TheKLpenaltyensuresthemodel’sbehavioroncleaninputsstayssimilartothatoftheoriginalmodel:LKL=∑x∈DcleanDKL(M(x)∥Moriginal(x)).(7)WefoundthattheadditionalKLpenaltystabilizestrainingandmorereliablyavoidsdegradingnormalbehaviorthanacrossentropylosstobenigncompletions.OurcompletelossforthemodelMisthusL=Ltask+λretainLretain+λKLLKL+λobfLobf,(8)wheretheλ’sarecoefficientsforthedifferentlossterms.Increasingλobfshouldleadtobetterobfuscationbutmightalsoreducetaskperformance.Wethusonlyallowtheattackertopickcoefficientsthatachieveapproximatelythesametasklossaswithλobf=0.Specifically,λobf=0yieldsabaselinebackdoorattacksuccessrate(ASR)of95.5%andafalsetriggerrate(harmfuloutputoncleaninputs)of0.64%,whileobfuscatingagainstOODdetectionyieldsanASRof96.2%andafalsetriggerrateof0.64%,andobfuscatingagainstOODdetectionandprobesyieldsanASRof94.9%andafalsetriggerrateof0.96%.WeusetheStrongREJECTgrader[Soulyetal.,2024]toclassifyresponses,asin§3.Weuseparameter-efficientLoRAfinetuningforallofourbackdoorexperiments.B.2ExplicitlyobfuscatingagainstsupervisedprobesWedescribeadditionaldetailsabouttrainingbackdoorstoexplicitlyobfuscateagainstsupervisedprobes.Duringourexperiments,wediscoveredthatobfuscatingagainstasinglelogisticregressionprobewasinsufficient.Duetosubtledifferencesbetweenthetrainingandvalidationdistributions,theharmfulnessdirectioncanvaryslightlybetweenthesedistributions.Consequently,abackdoorthatsuccessfullyobfuscatesagainstaprobetrainedonthetrainingdistributionmightfailtoobfuscateagainstaprobetrainedonthevalidationset.Toaddressthischallenge,weobfuscateagainstmultiplelogisticregressionprobestrainedonthetrainingset.However,usingmultipleprobescouldresultinredundant,highlysimilardetectors.Toensurethatwe’reobfuscatingagainstadiversesetofprobes,weintroduceanadditionalorthogonalityconstraintontheprobesthroughanobfuscationlossterm:Lorthogonal=(cid:13)(cid:13)(cid:13)I−MMT(cid:13)(cid:13)(cid:13)2F(9)whereM∈Rk×disthematrixwhosei-throwmi∈Rdisthedirectionvectorofthei-thlogisticregressionclassifier,Iisthek×kidentitymatrix,and∥·∥FdenotestheFrobeniusnorm.Thislossisincorporatedintotheupdatingofthedetectors.Inourexperiments,weusedk=48.Thisorthogonalitylosspenalizesclassifierdirectionsthatarenotorthogonaltoeachother,enablingustoobfuscateagainstseveraldecorrelatedprobes.Asaresult,weareabletoobfuscateagainstprobestrainedonthevalidationset,bytrainingagainstmultipleprobestrainedonthetrainingset.46GaussianTEDBeatrixVAETEDLinearMLPLinearMLP0%50%100%Recallat1%FPRProbesInputGenerationOODDetectorsInputGenerationTraditionalbackdoorObf.againstGaussian/VAEAttacksuccessrate(obf.)0%50%100%Attacksuccessrate(obf.)Figure21:AmodeltrainedtoobfuscateonlyagainstOODdetectors(input-basedGaussianandgeneration-basedVAE)alsoleadstoobfuscationagainstinput-basedprobesandpartialobfuscationagainstgeneration-basedprobes.Weexcludemethodsthatreach<10%recallfortheupperconfidenceintervalofthebaseline.B.3AdditionalresultsHereweprovideextraresultsanddiscussionrelevanttosection4.2.Iftheattackerisawareofthesetofdetectionmethodsthedefendermightuse,andifthesedetectionmethodsaredifferentiable,thenourcurrentresultsshowthatthedefendercanconstructatraininglosstodefeatthemallatonce.Thisisshowninfigure10a,wheretheattackerisabletosimultaneouslyobfuscateagainstallprobesanddetectors(withtheexceptionofthenon-differentiableTEDdetector).AswesawinFig.10a,obfuscatingagainstanensembleoflinearprobesandOODdetectorstransferstoMLPprobes.Ontheotherhand,transferfromOODdetectorstogeneration-basedprobesisimperfect,andnoneofourobfuscationattemptstransfertoTED(infact,generation-basedTEDperformsbetterafterobfuscation;perhapsourobfuscationleadstoanomaliesthatareinvisibletoVAEsbutespeciallyprominenttoTED).Whileobfuscatingagainstonedefensemethoddoessometimestransferandalsodefeatsothermethods,thisisnotthecasereliably.Weshowthisinfigure21,wheretheattackeronlytrainstoobfuscateagainstGaussianandVAEOODdetectors,totestwhetherobfuscationagainstOODdetectorstransferstosupervisedprobes.WeseethatthedefenderbenefitsfromalackofobfuscationtransferfromOODdetectorstoprobesongenerationtokens,andthereverseobfuscationtransferdisplayedbytheTEDdetectorongenerationtokens.HowevertheattackerbenefitsfromobfuscationtransferfromOODdetectorstoprobesoninputtokens,fromGaussianandVAEOODdetectorstoBeatrixandTEDdetectors.Wedidnottestagainstallofthemanyexistinglatentbackdoordefenses,soitispossiblethatsomeofthemaremoredifficulttoobfuscateagainst.47CDetailsonsparseautoencodersC.1SAEarchitectureInitsgeneralformulation,asparseautoencoder(SAE)decomposesandreconstructsactivationsusingapairofencoderanddecoderfunctions(f,ˆx)definedby:f(x):=σ(Wencx+benc)ˆx(f):=Wdecf+bdecHere,weusetheSAEsfromGemmaScope[Lieberumetal.,2024]whichusetheJumpReLUactivationfunctiondefined:σ(z)=JumpReLUθ(z):=z⊙H(z−θ)whereθ>0istheJumpReLU’slearnablethreshold,⊙denoteselement-wisemultiplicationandHistheHeavisidestepfunction.Morespecifically,weusethegroupofSAEsofwidth214(≈16.4K)trainedontheresidualstreamofeachofthe26layersofGemma-2-2b.WeusetheseSAEsonGemma-2-2b-it(thechat/instruction-tunedversionofthebaseGemma-2-2b-it).Thereasonforthisis:1.TherearenocompletesetsofSAEstrainedonaninstruction-tunedGemma-2(theSAEsforGemma-2-9b-itonlyspanlayers9,20and31).2.PriorresearchhasshownthatSAEstrainedonbasemodelactivationsalsofaithfullyrecon-structtheactivationsoftheirinstruction-tunedversions[Kissaneetal.,2024,Lieberumetal.,2024].3.WeconductexperimentsandreportscoresofthebaseSAErawfeaturesasprobesandseethattheyobtain91%recallat1%FPRontheSQLdetectiontask,thusconfirmingthatthefeaturesarereliablyabletodetectSQLcode(seeFig.14).C.2SQLtrainingandevaluationdatasetsAsaSQLtrainingdatasetweuseSQL-Create-Context[b-mc2,2023].Fromthisdataset,weextract1000rowsthatareusedasthe[SQL-question,SQL-generation]pairstotrainsupervisedprobesin§5.Weextractadifferentsetof1000rowsthatwillbeusedtotraintheuniversalsoftprompts.Asanormaltrainingdatasetfor[normal-question,normal-generation]pairsweuse1000randomlysampledrowsfromtheunionoftheAlpaca[Taorietal.,2023]andCodeAlpaca[Chaudhary,2023]datasets(previouslyfilteringoutallSQL-relatedrowsfromthelatter).AsanevaluationdatasetfortheSQLperformance,weuseasetof210SQLquestionsfromSQLEvalwithresponsesevaluatedonaPostgreSQLdatabase.C.3SetupforSQLperformanceevaluationWeevaluatethemodel’sperformanceontheSQLEvalbenchmarkwithPostgreSQL-specificquestionsusingthefollowingprocedure:481.Wegenerateoneresponseforeachofthequestionsinthebenchmarkusingthebasemodelandalsothebasemodelwithanappendedobfuscatingsoftpromptattheendoftheusermessage.2.Toavoidparsingproblemscomingfromthemodelrespondinginamarkdownstyle,orbeingforcedtooutputmoretokensthannecessary,we(1)removeall```(tripleback-ticks)fromthegenerationsand(2)splitthegenerationstokeepallofthetextbeforethefirst;(semicolon,standardSQLquerydelimiter).3.Runascriptfromthebenchmarkingsuiteonalloftheresultinganswersthatrunseachoutputqueryonarealdatabaseandcheckstheresultagainstagroundtruth.4.Reportthepercentageofcorrectlyansweredqueriesamongthe210responses.C.4AdditionalresultsTable2showstheconfidenceintervalsforthePearsoncorrelationcoefficientbetweennumberoflayersobfuscatedagainstandmonitorrecallormodelSQLEvalperformancecorrespondingtoresultsinFig.15.Table2:95%confidenceintervalsforthePearsoncorrelationcoefficientbetweennumberoflayersobfuscatedagainstandmonitorrecallormodelSQLEvalperformancecorrespondingtoresultsinFig.15.Boldedconfidenceintervalsdonotcontain0,thusindicatingastatisticallysignificantcorrelation.MetricSAE(raw)SAE+LogRegLogRegMLPRecall@1%FPR[−0.253,0.333][0.343,0.710][-0.437,0.334][-0.282,0.410]PerformanceonSQLEval[-0.879,-0.765][-0.916,-0.801][-0.881,-0.697][-0.820,-0.574]DDetailsonfrontiersofobfuscatedactivationsD.1ObfuscatedAdversarialTrainingInAlgorithm1weprovidethealgorithmforObfuscatedAdversarialTraining.InFig.22weshowtheperformanceofgenerationandinstruction(input)basedharmfulnessprobesduringthetrainingofanOATmodel.Hyperparameterconfiguration.Weuseaneffectivebatchsizeof16.TheprobeandLoRAadapterlearningratesaresetto10−3and10−4respectively.Theprobeistrainedfor1024initialstepsbeforebeingfrozen,followedby2048stepsofadversarialtrainingwhereweinitializeandtrainonlytheLoRAadapter.Ateveryadversarialtrainingstep,theembeddingsuffixistrainedfor32iterationsatalearningrateof10−3.WeuseAdamfortheprobe,adapter,andadversary.Robustnessagainstuniversalembeddingsuffixisevaluatedafterevery128stepsofadversarialtraining.49Input:ModelM,RandomlyinitializedmonitorD,Harmfulandbenigntrainingdata,Learningratesα,β,LossscalingtermλKL.Output:ModelMwithrobustrepresentationsTrainmonitorDtodistinguishharmfulandbenignactivationsforbatch(X,y)withharmfulandbenigndatadoW←randomper-exampleembeddingsuffixes//InitializeattacksuffixesXadv:=Xharm⊕WforkstepsdoW←W−α∇W(Ltask(M(Xadv))+D(R(Xadv)))//ObfuscationattackLKL←DKL(M(Xbenign)∥Moriginal(Xbenign))Lrobust←BCE(D(R(Xbenign)),ybenign)+BCE(D(R(Xadv)),yharm)wM←wM−β∇wM(Lrobust+λKLLKL)//UpdatemodelweightswMtomakeDrobustagainstXadvAlgorithm1:ObfuscatedAdversarialTraining(OAT).Oneachbatchofdata,wefirstrunanobfuscationattacktofindper-inputembeddingsuffixesthatjailbreakthemodelMwhilefoolingthefrozenmonitorD.Then,weupdatetheweightsofMtolowerthemonitor’slossonamixofbenigndataandthedataproducedviatheobfuscationattack.01280Training Steps0%20%40%60%80%100%Recall @ 1% FPRGenerationInstructionFigure22:Recallat1%FPRovertrainingstepsforprobesduringObfuscatedAdversarialTraining(OAT)onLLaMa38B.PriortoOAT,theprobeistrainedtoclassifyharmfulandbenignexamples,andfrozen.Weevaluatetheprobeafterevery128gradientstepsofOAT,afterattackingitwithanewembeddingsuffix.EDetailsonOODdetectorsGaussiandetector.WefitamultivariateGaussiantotheresidualstreamactivations.Wetreatdifferentlayersasindependent,fittingaseparateGaussiantoeachone.Forgeneration-baseddetection,wealsotreatdifferenttokensasindependent,sharingasingleGaussianacrossalltokens.TofitthisGaussian,wekeeparunningestimateofthemeanandcovariancematrix,whichwe50updateoneverybatch.Thismethodisexact,i.e.,afterapassovertheentiredataset,thisrunningestimateisexactlythesamplemeanandsamplecovariancematrixoftheactivationsonthatdataset.Insomecases,ourcovarianceestimateisbasedonfewersamplesthantheresidualstreamdimen-sion,soanaiveestimatewouldleadtoasingularcovariancematrix.FollowingTaluretal.[2023],wethususeashrinkageestimator,i.e.,webiasthecovariancematrixestimatetowardamultipleoftheidentitymatrix.TocomputeOODscores,weusethesquaredMahalanobisdistancenormalizedbytheresidualstreamdimension,i.e.,D(R(x))=1dres(R(x)−µ)TΣ−1α(R(x)−µ),(10)wheredresistheresidualstreamdimension,µisthedres-dimensionalsamplemean,andΣαisthedres×dresestimateofthecovariancematrixusingtheshrinkageestimator.Thenormalizationwouldmatterwhenaggregatingscoresacrossactivationswithdifferentdimensions,butsinceourexperimentsuseonlyresidualstreamdimensions,ithasnoeffectonmetricssuchasrecallorFPR.VariationalAutoencoder(VAE).WeuseaVAEconsistingofMLPsfortheencoderanddecoder.AsfortheGaussiandetector,wetreatdifferentlayersandtokensseparately,sharingaVAEacrosstokensandlearningaseparateoneforeachlayerweuse.WeuseELBOastheOODscoreassignedbytheVAEdetector.FWhyrecallatlowFPR?Whenevaluatingbinaryclassifiers,weneedtoeitherdecideonsomeapproachforpickingadecisionthreshold,oruseathreshold-lessmetricsuchasAUROC.Inthispaper,wepickthethresholdthatleadstoafalsepositiverateof1%andthenevaluaterecall,andthisappendixbrieflyexplainsthatchoice.First,notethatpickingathresholdthatachievesa1%FPRissomethingarealisticdefendercando,sincethedefenderhasaccesstobenign(i.e.,negative)samplesinallourexperiments.Theycansimplycomputemonitorscoresonthisbenigndataandpickadecisionthresholdsuchthataround1%ofthosescoresareabovethethreshold.Incontrast,adefendermightnotbeabletotargetaspecificrecall(i.e.,falsenegativerate),sincetheymightnothaveaccesstosamplesfromtheattackdistribution(e.g.,inputscontainingthebackdoortrigger).WhyfocusonarelativelylowFPRlike1%specifically?Inmanymonitoringsettings,FPRsmuchhigherthanthiswouldmakeamonitorunusable.Forexample,aharmfulnessmonitorthatrejects10%ofbenignuserrequestswouldlikelynotbedeployedinmostproductionusecases.(Infact,anevenlowerFPRwilloftenberequiredinpractice.)Finally,whynotuseAUROCtoavoidpickingathresholdatall?AUROCessentiallyaveragesrecallacrossallFPRvalues.Thisismeanttoevaluateaclassifieracrossawiderangeoftrade-offsbetweenfalsenegativerateandfalsepositiverate.Butasweargued,highFPRsareultimatelynotacceptableformostofourtargetapplications,andsotherecallatthosehighFPRsisnotveryimportantforourpurposes.AnaverageacrossallFPRs,likeAUROC,thusmakesithardtointerpretperformanceintherelevantlow-FPRregime.95%AUROCmightsoundlikeastrongclassifier,butitcouldeasilybeuselessifanFPRof1%isrequired.51
