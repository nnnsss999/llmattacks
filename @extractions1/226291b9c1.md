---
title: http://arxiv.org/pdf/2410.04884
source_url: http://arxiv.org/pdf/2410.04884
date_collected: '2025-06-19'
license: Fair Use
---

ResearchArticlePatchisEnough:NaturalisticAdversarialPatchagainstVision-LanguagePre-trainingModelsDehongKong·SiyuanLiang·XiaopengZhu·YuanshengZhong·WenqiRenReceived:date/Accepted:dateAbstractVisuallanguagepre-training(VLP)modelshavedemonstratedsignificantsuccessacrossvariousdo-mains,yettheyremainvulnerabletoadversarialattacks.Addressingtheseadversarialvulnerabilitiesiscrucialforenhancingsecurityinmultimodallearning.Tradition-ally,adversarialmethodstargetingVLPmodelsinvolvesimultaneouslyperturbingimagesandtext.However,thisapproachfacesnotablechallenges:first,adversarialperturbationsoftenfailtotranslateeffectivelyintoreal-worldscenarios;second,directmodificationstothetextareconspicuouslyvisible.Toovercometheselimitations,weproposeanovelstrategythatexclusivelyemploysim-agepatchesforattacks,thuspreservingtheintegrityoftheoriginaltext.Ourmethodleveragespriorknowledgefromdiffusionmodelstoenhancetheauthenticityandnaturalnessoftheperturbations.Moreover,tooptimizepatchplacementandimprovetheefficacyofourattacks,weutilizethecross-attentionmechanism,whichencap-sulatesintermodalinteractionsbygeneratingattentionmapstoguidestrategicpatchplacements.Comprehen-siveexperimentsconductedinawhite-boxsettingforimage-to-textscenariosrevealthatourproposedmethodsignificantlyoutperformsexistingtechniques,achievinga100%attacksuccessrate.Additionally,itdemonstratescommendableperformanceintransfertasksinvolvingtext-to-imageconfigurations.KeywordsAdversarialPatch·PhysicalAttack·DiffusionModel·NaturalisticThefirstAuthor(correspondingauthor)andfifthAuthorarewiththeShenzhenCampusofSunYat-senUniversity.(Email:kongdh@mail2.sysu.edu.cn,renwq3@mail.sysu.edu.cn)ThesecondAuthoriswiththeNationalUniversityofSinga-pore.(Email:pandaliang521@gmail.com)ThethirdandfourthauthorsarewithGuangdongTest-ingInstituteofProductQualitySupervision.(Email:zhuxp@gqi.org.cn,zhongys@gqi.org.cn)1IntroductionThevisual-languagepre-training(VLP)modelsinthemultimodaldomainhavegarneredconsiderableatten-tionduetotheirrobustperformanceacrossarangeofvisual-languagetasks.Currently,VLPmodelsareprimarilyappliedinthreedownstreamtasks:1)Visual-LanguageRetrieval[1]:Thistaskinvolvesmatchingvisualdatawithcorrespondingtextualdata.Itconsistsoftwosub-tasks:image-to-textretrieval(TR),whichretrievestextualdescriptionsforgivenimages,andtext-to-imageretrieval(IR),whichfindsmatchingimagesforspecifictexts.2)visualentailment(VE)[2]:Thistaskusesimagesandtextaspremisesandhypothesestopredictwhethertheirrelationshipisentailment,neutral,orcontradiction.3)visualgrounding(VG)[3]:Thistaskaimstolocalizeobjectregionsinimagescorrespondingtospecifictextualdescriptions.Asdeepnetworksaresusceptibletoerrorpatterns[4–11],i.e.,adversarialper-turbations[12–27],thesecurityofVLPmodelshasalsocomeunderscrutiny.RecentstudiesindicatethatVLPmodelsremainvulnerabletoadversarialexamples[28].ResearchintoadversarialattacksonVLPmodelscanfurtherenhancetheirrobustnessandsecurity[29–34].Whendealingwithmultimodalmodels,attackerscanindividuallytargetdifferentmodalitiestoreducetheaccuracyofdownstreamtasks.Co-Attackpioneeredcollaborativeattacksbyinnovativelyconsideringtheat-tackrelationshipsbetweenmodalities.RecentresearchhasstartedtofocusontheadversarialtransferabilityofVLPmodels.However,theseattacksarelimitedinadversarialperturbationsandcannotbeappliedinthephysicaldomain.Typically,attackersuseadversarialpatchtrainingmethodstoachievephysicaldomainat-tacks.Additionally,theyallattackbothimagesandtextsimultaneously,wheretextperturbationsareeasilyarXiv:2410.04884v1  [cs.CV]  7 Oct 20242F.Authoretaldetected.Forexample,Co-Attacktransformsthetext”amanplayingguitar”into”amanplayingscoring,”whichclearlydoesnotmeettherequirementofinvisi-bility.Therefore,applyingadversarialpatchattackstoimagesenablesattacksinthephysicaldomainwhilepreservingtextualauthenticity.ThispaperisthefirsttofocussolelyonnaturalisticadversarialpatchattacksagainstVLPmodels.AsdemonstratedinFig.1,ourmethodachievessuperiorattackperformanceinawhite-boxsetting.PGDBERT-ASep-ACo-ASGAOurs020406080100ASR(%)ALBEFCLIPFig.1:Comparisonofattacksuccessrates(ASR)ofdifferentattacksinthewhiteboxsettings(ALBEF[35]andCLIP[36])onimage-textretrieval.Startingfromlefttorightasimage-onlyPGDattack[37],text-onlyBERT-Attack,thecombinedseparateunimodalattack(Sep-Attack),CollaborativeAttack(Co-Attack[28]),Set-levelGuidanceAttack(SGA[38])andourmethod.However,applyingsingle-modalattackstomulti-modalmodelsischallengingandrequiresleveraginginformationfromtheothermodality.Co-Attackmodi-fiedthelossfunctionbasedonpreviousworktoachievebimodalcollaborativeattacks,whileSGAconsideredthesimilaritybetweenset-leveltextandimages,butnei-therconsideredthestructurewithinthevictimmodel.VLPmodelsoftenemployattentionmechanismsformodalityinteractioninternally,whichattackersshouldexploittoconstructattacks.Conventionaladversarialpatchattackssufferfromnaturalnessdefects,inspiringustousediffusionmodelstoguideadversarialpatchgenerationandcreatenaturaladversarialpatches.Tab.1illustratesthecharacteristicsofdifferentmultimodalattackmethods,highlightingsignificantadvantagesinvariousaspectsofourapproach.Weconductedexperimentsontwomaturemulti-modaldatasets,Flickr30K[39]andMSCOCO[40],toevaluatetheperformanceofourproposedmethodinthetaskofimage-textretrieval.Theexperimentalre-sultsdemonstratethatourmethodachievesabalanceImage-AttackText-AttackNaturalPhysicalPGD✓BERT-Attack✓Sep-Attack✓✓Co-Attack✓✓SGA✓✓Ours✓✓✓Table1:Comparisonofcharacteristicsofdifferentattackmethods.betweenattackeffectivenessandnaturalnessacrossmul-tipleVLPmodels.Moreover,itexhibitsexcellenttrans-ferperformance,benefitingfromcross-attentionmecha-nismsthatintegratecommonfeaturesacrossmodalities.Thisallowsadversarialpatchestoachievestrongat-tackperformancewithoutrequiringlargeperturbations(maintainingadistributionsimilartorealimages).Wesummarizeourcontributionsasfollows:1)Tothebestofourknowledge,wearethefirsttoexplorethesecurityofVLPmodelsthroughadver-sarialpatches.2)Weintroduceanoveldiffusion-basedframeworktogeneratemorenaturaladversarialpatchesagainstVLPmodels.3)Wedeterminethelocationofadversarialpatchesbycross-modalguidance.Extensiveablationexperimentsdemonstratetheeffectivenessofthisapproach.2ReleatedWork2.1AdversarialPatchAdversarialpatchattackscanbemainlydividedintoiterative-basedandgenerative-basedmethods.Iterative-basedmethods.Brownetal.[41]presentsamethodtocreateuniversal,robust,targetedadversar-ialimagepatchesintherealworld.DPatch[42]generatesablack-boxadversarialpatchattackformainstreamob-jectdetectorsbyrandomlysamplingadversarialpatchlocationsandsimultaneouslyattackingtheregressionmoduleandclassificationmoduleofthedetectionhead.BasedonDPatch,Leeetal.[43]usethePGD[?]op-timizationmethodasaprototypetogenerateamoreaggressiveattackmethodbyrandomlysamplingpatchangleandscalechanges.Pavlitskayaetal.[44]alsore-vealthattheadversarialpatchscaleisproportionaltotheattacksuccessrate.Thysetal.[45]introduceanadversarialpatchattackdesignedtoattackpersondetectioninthephysicaldomain.Sahaetal.[46]an-alyzetheattackprincipleofadversarialpatchesthatdonotoverlapwiththetargetandproposetousecon-textualreasoningtofoolthedetector.Toreducepatchvisibilityandenhancetheattackingabilityofthead-versarialpatch,alargenumberofworkshavemadeaShortformoftitle3lotofeffortstogeneratevariouspatches.Specifically,theyincludeadversarialsemanticcontoursthattargetinstanceboundaries[47],adversarialpatchgroupsatmultiplelocations[48,49],patch-basedsparseadversar-ialattacks[50],diffusepatchesofasteroid-shapedorgrid-shape[51],deformablepatch[52]andthetranslu-centpatch[53].Generative-basedmethods.Attackingabilityisnottheonlygoalwepursue.Themainstreammethodtogenerateanadversarialpatchcurrentlyisiterative-basedwhichcanoptimizeforthepatchtoattackthedetectorwithoutanyconstraints,whilethepatchwillbegeneratedinanunpredictabledirection.Toaddressthisproblem,generative-basedmethodsareconsideredtotradeoffNaturalnessforattackperformance.PS-GAN[54]proposesaperceptual-sensitivegenerativeadversarialnetworkthattreatsthepatchgenerationasapatch-to-patchtranslationviaanadversarialpro-cess,feedinganytypesofseedpatchandoutputtingthesimilaradversarialpatchwithhighperceptualcor-relationwiththeattackedimage.Pavlitskayaetal.[55]haveshownthatusingapre-trainedGANhelpstogainrealistic-lookingpatcheswhilepreservingtheperfor-mancesimilartoconventionaladversarialpatches.Huetal.[56]presentatechniqueforcreatingphysicaladver-sarialpatchesforobjectdetectorsbyutilizingtheimagemanifoldlearnedbyapre-trainedGANonreal-worldimages.Thereissomework[57–59]beginningtousediffusionmodelsinadversarialattacks.Diff-PGD[60]utilizesadiffusionmodel-guidedgradienttoensurethatadversarialsamplesstaywithinthevicinityoftheorigi-naldatadistributionwhilepreservingtheiradversarialpotency.2.2VLPModelVisuallanguagepre-training(VLP)modelsleveragedeeplearningtechniquestopre-trainmodelsonlarge-scaledata,integratingvisualandlanguagemodalities.Asresearchhasprogressed,severalrepresentativemod-elshaveemerged.EarlyVLPmodelsexploredintegratingvisualandlanguageinformationintoaunifiedframeworktoen-hanceperformanceacrossmultimodaltasks.Withtheriseofpre-trainingmethods,aseriesofnewmodelshavebeendeveloped.Forinstance,CLIP[36],developedbyOpenAI,achievesstrongcorrelationsbetweenimagesandtextthroughcontrastivelearning,demonstratingexcellentperformanceacrossvariousvisuallanguagetasks.AnothernotablemodelisBLIP[61],whichintro-duceslogicalreasoningtaskstoenhanceperformanceinvisualandtextualreasoningtasks.Recentadvance-mentsincludetheALBEF[35]model,whichemploysenhancedmultimodaldataaugmentationtechniquestoimprovegeneralizationondiversedatasets.Moreover,theTCL[62]modelproposedbyGooglefocusesonmappingtextualdescriptionsintovisualfeaturespaces,facilitatingtaskssuchastext-to-imageretrievalandgeneration.Additionally,modelslikeViLBERT[63]andUNITER[64]haveshownoutstandingperformanceintaskssuchasimagecaptioningandvisualquestionan-swering.Together,thesemodelsrepresenttheforefrontofadvancementsinintegratingandleveragingvisualandlanguageinformationwithintheVLPdomain.Severalstudiesarecurrentlyinvestigatingadversar-ialattacksonVLPmodels.Co-Attack[28]positsthatstandardadversarialattacksaredesignedforclassifica-tiontasksinvolvingonlyasinglemodality.VLPmodelsengagemultiplemodalitiesandoftendealwithnumerousnon-classificationtasks,suchasimage-textcross-modalretrieval.Hence,directlyadoptingstandardadversarialattackmethodsisimpractical.Moreover,totargettheembeddedrepresentationsofVLPmodels,adversarialperturbationsacrossdifferentmodalitiesshouldbecon-sideredcollaborativelyratherthanindependently.Ourproposedmethoddemonstratesthat,inadditiontomul-timodalcollaborativeattacks,informationfromothermodalitiescanalsobeutilizedforsingle-modalattacks.SGA[38]introducesanensemble-levelguidedattackmethod.Thisapproachextendssingleimage-textpairstoensemble-levelimage-textpairsandgeneratesadver-sarialexampleswithstrongtransferability,supervisedbycross-modaldata.TMM[65]proposestheattention-directedfeatureperturbationtodisturbthemodality-consistencyfeaturesincriticalattentionregions.3Preliminaries3.1ThreatModelTheattackeraimstofindapatchP,whichusuallyfollowsasquare-sizedsettingwhereP∈Rs×s×3andsaccountsforthepatchsize,intothevisualinputsoftheVLPmodels,leadingtoincorrectoutputsindownstreamtasksthatrelyonthesepre-trainingmodels.Givenabenignimage-textpaird={dv,dt},aVLPmodelcanencodethisinputintoafusedembeddingeandPisdesignedtomisleadthesurrogatemodelFintoproducinganincorrectembedding:F((1−m)⊙dv+m⊙P,dt)̸=e,(1)wheremdenotesaconstructedbinarymaskthatis1attheplacementpositionoftheadversarialpatchand0attheremainingpositions,⊙denotestheHadamardproduct(elementproduct).4F.AuthoretalDiffusion guidanceEncUNETA narrow kitchen filled with appliances and cooking utensilsCleanpatchparamFeed fowardSelf attentionFeed fowardSelf attentionFeed fowardSelf attentionCross attentionSurrogate modelAttention mapAttackedA narrow kitchen...ℒImage-text retrievalPatch locationFig.2:TheframeworkofourproposedMultimodalattack.Weemployadual-guidedapproachwithdiffusionandattentionmechanismstobalancetheattackingabilityandthenaturalnessofadversarialpatches.3.2DiffusionModelsWeadoptapre-traineddiffusionintoourframework.Tobetterunderstandourwork,itisusefultogiveanoverviewofDiffusionModels.DenoisingDiffusionProb-abilisticModels(DDPM)[66]isaclassofgenerativemodelsthathasgainedsignificantattentioninrecentyearsforitsabilitytoproducehigh-qualitysamples.DDPMconsistsoftwomainprocesses:theforwarddif-fusionprocessandthedenoisingprocess.ThediffusionprocessisaMarkovchainthatgradu-allytransformsdatapoints(suchasimages)intonoise.Thediffusionprocesscanberepresentedas:xt=√αtxt−1+√1−αtϵt,t=1,2,...,T(2)wherextistheimageatstept,αtisthediffusioncoefficient(whichtypicallydecreaseswithincreasingt),ϵtisnoisedrawnfromastandardnormaldistribution,andTisthenumberofdiffusionsteps.Thedenoisingprocessisthereverseprocessofthediffusionprocess,aimingtorecovertheoriginaldatafromthenoise.IntheDiffusionModel,thedenoisingprocessisusuallyimplementedbyaconditionalneuralnetwork(suchasU-Net)thatpredictstheoriginalimagebasedonthecurrentnoisyimage.Thedenoisingprocesscanberepresentedas:xt−1=1√αt(cid:18)xt−1−αt√1−¯αtϵθ(xt,t)(cid:19),(3)whereϵθisthenoisepredictedbytheneuralnetwork,and¯αt=(cid:81)ts=1αs.4TheProposedMethod4.1MotivationOurmethodisproposedbasedonthefollowingobserva-tions.First,theprevailingapproachinthemultimodalfieldtolaunchingadversarialattacksonVLPmodelsinvolvesattackingbothimagesandtextsimultaneously.Co-Attackhasdemonstratedthatitisindeedpossi-bletofindsuchacollaborativeattackmethodthatachievesasynergisticeffectgreaterthanthesumofitsparts.However,attackinganadditionalmodalityalsoin-creasesthelikelihoodoftheattackbeingdetected,whilesingle-modalityattacksoftenfailtoachievethesameeffectivenessasmultimodalattacks,acontradictionthathaspromptedustoinvestigateimage-onlyattacksonVLPmodels.Secondly,perturbationattacks,asaformofdigitaldomainattack,cannotbeappliedtothephys-icaldomain,whichposesanotherlimitation.Combiningthesetwopoints,wehaveexploredtransferringtextualinformationtoimagestoconductadversarialpatchat-tacksonimages.However,thisalsoraisesanotherissue:adversarialpatchattackstendnottobeasinconspicu-ousasperturbationattacks.Therefore,inspiredbysomediffusionwork,wearestudyingdiffusion-basedmethodsforgeneratingadversarialpatches.4.2PatchGenerationTogenerateadversarialpatches,wefirsthavetheinitpatchPinitwhichisarealimageandthepre-trainedShortformoftitle5Algorithm1PatchGenerationRequire:InteractionN,Timestept,Stepsizes,Adversarialperturbationdp,LearningratelrEnsure:Pfinal1:forn=1toNdo2:x=√αt(Pinit+dp)+√1−αtz;z∼N(0,I)3:repeat4:xt=x5:xt−s=√αt−s(cid:16)xt−√1−αt·ϵθ(xt,t)√αt(cid:17)+√1−αt−s·ϵθ(xt,t)6:t=t-s7:untilt<s8:dp=dp−lr∗∇dLp9:endfor10:Pfinal=xtDiffusionModel(PDM).Wesetanimagedp(pertur-bation),whichisthesamesizeasPinit,asthetrainingparameter.ThegenerationprocessofthepatchcanbeformulatedasfollowsanddiffusionprocessisshowninAlg.1:Pfinal=PDM(Pinit+dp).(4)Wethenfocusonpatchlocation.Specifically,weutilizecross-attentiontofusetheconsistencyfeaturesofimagesandtexttoobtainanattentionmap.Afterresizingtheattentionmaptomatchtheoriginalimagesizethroughlinearinterpolation,wecanidentifythecriticalareaswherethemodelmakesitsdecisions.Thepatchisthenappliedtothislocation,resultinginthemodifiedimage.Subsequently,weperformthescoringforthedownstreamtask(image-textretrieval)andcalculatethelossfunction,whichisusedtoadjusttheparametersthroughbackpropagation.Thefollowingwillprovideamoredetailedintroduc-tiontothemethodanditsfunction.4.3DiffusionGuidanceCurrently,themajorityofadversarialpatchmethodsdirectlyoptimizetheadversarialpatchitself,butthisapproachcancausesignificantchangestotheoriginalimagetoachievegoodattackeffects,whichposesagreatchallengetothenaturalnessoftheadversarialpatch.Incontrast,sincetherearenohiddenlayersinthenetwork,themodelparameterscanbesettoatensordpwiththesamesizeasPinitandavalueofzero.Comparedtodirectlyoptimizingthepatch,addingadversarialperturbationshasmanyadvantages.Firstly,theperturbationcanbeseenasnoiseintheoriginalimage,whichbettermatchesthedenoisingprocessofdiffusionmodel,andmakesiteasiertofindconstrainedoptimalsolutions.Secondly,thismethodinvolvesfewerchangestotheoriginalimageanditcanpreservetheinformationoftheoriginalimage.Fromamacroscopicperspective,similartoPGD,itislikeaddingadversarialperturbationstoPinit.Weexploitthel∞normtoconstraind,andtheformulaforupdatingPinitineachiterationisasfollows:Pinit=Clip(Pinit+dp).(5)ClipistheclippingfunctiondefinedinEq.6.Clip(P)={pi|pi←min(max(pi,τ),0)},(6)wherepiisthei-thelementofPandτismaximumvalueofpi.Theadoptionofdiffusionmodelstoguidegradientsisprimarilyaimedatensuringthatadversarialexamplesremainclosetotheoriginaldatadistributionwhilemain-tainingtheirefficacy.Thisisbecauseexistingadversarialattacks,generatedusinggradient-basedtechniquesindigitalandphysicalscenarios,oftendivergesignificantlyfromtheactualdatadistributionofnaturalimages,re-sultinginalackofnaturalnessandauthenticity.WhileGAN-basedmethodscangeneraterealisticimages,theadversarialsamplesaresampledfromnoise,thuslackingcontrollability.Therefore,adversarialpatchgenerationbasedondiffusionmodelsofferssignificantadvantages.4.4PatchLocationThevastmajorityofVLPmodelsutilizeattentionmech-anismstocapturetheconsistencyfeaturesbetweenim-ageandtext.Previouswork[67,68]hashighlightedthatmodalityconsistencyfeaturessignificantlyinflu-encethedecision-makingofmultimodalmodelsandarecrucialforthesuccessofdownstreamtasks.There-fore,webelievethatinVLPmodels,theoutputofthecommonlyusedcross-attentionmodulesdesignedforcross-modalinteractionreflectsthetext’sattentiontotheimage.Someworksonregion-specificattackshaveal-readydemonstratedtheimportanceofattackingspecificareas.Foradversarialpatchattacks,theplacementloca-tioncanaffectthesuccessrateandthetrainingprocess.Placingadversarialpatchesonvulnerablepartsoftheimagecanachievemorewithless,meaningattackscanbecarriedoutwithoutsignificantperturbations.Thisalsohelpsinmaintainingthenaturalnessoftheadver-sarialpatches.Therefore,weusecross-attentiontoguidetheplacementofadversarialpatches.TheattentionmapMiscalculatedasfollows:M=softmax(QKT√s)V,(7)whereQ,K,Vdenotethefeaturematrixofdifferentmodalities,and√sdenotesthescalingfactorforstabi-lizingthemodel.Becausethegeneratedattentionmap6F.AuthoretalcleanattackedFig.3:Thecleanimagesandtheattackedimageswithnaturalisticpatches.TheimagesshownarefromthedatasetMSCOCO[40]Mdoesnotmatchthesizeoftheimage,itneedstoberesizedtothedimensionsoftheimageusingbilinearinterpolation,withthemaximumvalueinsideservingasthecentralpositionforthepatch.4.5LossFunctionOurpatchoptimizationisimplementedthroughthecomputationoftwolosses:Lp=Lscore+λLtv.(8)Inthethirdpartofthepipeline,theobtainedPfinalisappliedtothecleanimagedvguidedbytheattentionmaptoproducetheattackedimageˆdv:ˆdv=(1−m)⊙dv+m⊙Pfinal.(9)Theimage-textpaird={ˆdv,dt}isinputintotheVLPmodeltargetedforattack,andthescoresforthedownstreamtaskarecalculated.Foradatasetof1000imagesand5000texts,eachimagewillreceivescorescorrespondingto5000texts.Weextractthetopkhigh-estscoresanddividethesescoresintotwosets,S1andS2,representingscoresoftextsthatbelongordonotbelongtotheimage,respectively.Lscoreiscalculatedasfollows:Lscore=max(S1)−min(S2).(10)Totalvariationlossiseffectiveinremovingnoisewhilepreservingedgeinformation,resultinginsmootherandclearerimages.Comparedtoothersmoothingtech-niques,totalvariationlossbetterpreservestheedgesandtexturedetailsofimages,avoidingexcessiveblurring.Ltv=(cid:113)(cid:80)Si(cid:80)Sj(Pi,j−Pi+1,j)2+(Pi,j−Pi,j+1)2N,(11)whereNdenotesthenumberofpixelsonthegivenadversarialpatchPfinal.5Experiment5.1Implementationdetails5.1.1DatasetsandVLPModelFlickr30K[39]consistsof31,783images,eachwithfivecorrespondingcaptions.Similarly,MSCOCO[40]com-prises123,287images,andeachimageisannotatedwitharoundfivecaptions.WeadopttheKarpathysplit[69]forexperimentalevaluation.WeevaluatetwopopularVLPmodels,thefusedVLPandalignedVLPmodels.ForthefusedVLP,weconsiderALBEF[35].ALBEFcontainsa12-layervisualtransformerViT-B/16[70]andtwo6-layertransformersfortheimageencoderandboththetextencoderandthemultimodalencoder,respec-tively.TCLusesthesamemodelarchitectureasALBEFbutwithdifferentpre-trainedobjectives.ForthealignedVLPmodel,wechoosetoevaluateCLIP[36].CLIPhastwodifferentimageencoderchoices,namely,CLIPViTandCLIPCNN,thatuseViTB/16andResNet-101[71]asthebasearchitecturesfortheimageencoder,respec-tively.Shortformoftitle7Table2:Image-textretrievalresultsofALBEFandCLIPonMSCOCOdatasetandFlickr30Kdataset.Thereportedvalueisattacksuccessrate(100%).MSCOCO(5Ktestset)Flickr30K(1Ktestset)ModelAttackTRIRTRIRR@1R@5R@10R@1R@5R@10R@1R@5R@10R@1R@5R@10PGD76.767.4962.4786.378.4973.9452.4536.5730.0058.6544.8538.98BERT-Attack24.3910.676.7536.1323.7118.9411.571.81.127.4614.4810.98ALBEFSep-Attack82.6073.267.5889.8882.678.8265.6947.642.173.9559.553.7Co-Attack79.8768.6262.8887.8380.1675.9877.1664.658.3783.8674.6370.13SGA96.792.8390.3796.9593.4491.0097.2494.0992.397.2894.2792.58Ours99.9099.6999.6999.9099.4998.9799.7899.3299.3299.7898.8697.72PGD54.7936.2128.5766.8551.846.0270.9250.0542.2878.6160.7851.5BERT-Attack45.0628.6222.6751.6837.1231.0228.3411.736.8139.0824.0817.44CLIPSep-Attack68.5252.343.8877.9466.7760.6979.7563.0353.7686.7975.2467.84Co-Attack97.9894.9493.0098.8096.8395.3393.2584.8878.9695.6890.8387.36SGA99.7999.3798.8999.7999.3798.9499.0897.2595.2298.8497.5396.03Ours99.8599.7399.4599.8199.2398.3299.9299.6899.1899.6898.2697.755.1.2AdversarialAttackSettingsandMetricsTobettercompareourmethodwiththeSoTAmethod,wemainlyusetheparametersettingsofSGA.Weem-ployPGDwithperturbationboundϵ=2/255,stepsizeα=0.5/255,anditerationstepsT=10.Inourexperi-ment,thediffusionmodelweadoptistheunconditionaldiffusionmodelpre-trainedonImageNet[72]thoughweuseDDIMtorespacetheoriginaltimestepsforfasterinference.Intheimage-textretrievaltask,eachimagehasthetopktextscores,wherekissetto15inthewhite-boxsetting.Wechose15%oftheoriginalimageasthepatchsize.Intheablationstudy,wewillexploretheimpactofdifferentvaluesofkandpatchsizesontheattack.Weemploytheattacksuccessrate(ASR)asthemainmetricforevaluatingtheattackingcapabilityofthegeneratedadversarialexamplesinVLPdownstreamtasks.Thismetricreflectstheproportionofadversar-ialexamplesthatsuccessfullyinfluencethedecisionsofmodels.ThehighertheASR,thebettertheattackingability.Specifically,weofferASRvaluesforR@1,R@5,andR@10inalltablesforthetasksofimage-to-text(TR)andtext-to-imageretrieval(IR),whereR@Nrep-resentsthetopNmostrelevanttext/imagebasedontheimage/text.5.2ComparisonsofSoTAMethodTorigorouslyevaluatethesuperiorityofourproposedmethodwithinthewhite-boxsetting,weconductedcomprehensivecomparisonswithseveralbaselineap-proaches.Theseincludedtheimage-onlyPGDattack[?],thetext-onlyBERT-Attack,thecombinedseparateuni-modalattack(Sep-Attack),theCollaborativeAttack(Co-Attack)[28],andtheSet-levelGuidanceAttack(SGA)[38].ThesecomparisonswereperformedusingthewidelyrecognizedtestdatasetsMSCOCOandFlickr30KonboththeALBEFandCLIPmodels.RepresentativesamplesofcleanandadversarialimagesareillustratedinFig.3.Ourmethod,guidedbythecross-attentionanddiffu-sionmodel,successfullymaintainstheadversarialpatchclosetotherealimagedistribution,therebystrikinganoptimalbalancebetweennaturalnessandattackefficacy.Tofurthervalidatetherobustnessofouradversarialexamples,weintroducednoisetothegeneratedadver-sarialsamples.Duringtraining,theparameterKwassetto15,andtheattackiterationswerecontinueduntilthelosswasminimized.Thismethodologyensuresthat,foranimagewithonlyfivecorrespondingtexts,theattacksuccessrateinthetextretrieval(TR)taskforRecall@10(R@10)reaches100%.AsdemonstratedinTab.2,ourmethodconsistentlyoutperformsothertechniquesinthewhite-boxsetting.Onaverage,withtheALBEFmodel,ourapproachsurpassesthestate-of-the-artmethodsby6.46%and4.93%intheTRtaskontheMSCOCOandFlickr30Kdatasets,respectively.Whenappliedtotheimagere-trieval(IR)task,weachieveimprovementsof5.65%and4.07%.Notably,similarperformanceenhancementswereobservedwiththeCLIPmodel.Animportantaspectofourapproachistheutiliza-tionofcross-attentiontointegrateinformationfrombothimagesandtexts,therebyobtainingthetext’sat-tentionontheimage.Itisnoteworthythat,despitetheCLIPmodelnotperformingexplicitimage-textfusionoperations,ourmethodremainseffective,demonstrat-8F.Authoretalingitsversatilityandrobustnessacrossdifferentmodelarchitectures.5.2.1DiscussionofNaturalnessPreviousworkhasscarcelydiscussedthenaturalnessofadversarialpatchesandlacksrelateddefinitionsandevaluationmethods.Weconsiderthatnaturaladversar-ialpatchesshouldbeinconspicuouswithinadversarialexamples.Ourapproachenablestheselectionofthemostsuitableadversarialpatchesforspecificimages.Fig.4comparesnaturaladversarialpatcheswithunnatu-ralones.Wechosearoseastheadversarialpatchandplaceditontherightshoulderofthegirl,makingiteasilymistakenforapartoftheclothingdecoration.Itisnoteworthythatthroughextensiveexperiments,wefoundthathigh-attentionareasareoftennotthemostprominentparts,suchastheface,whichgreatlyaidsinenhancingnaturalness.Natural(flower)Unnatural(noise)Fig.4:Comparisonofadversarialpatcheswithandwithoutnaturalness.Thecleanimagesandtheattackedimageswithnaturalisticpatches.TheimagesshownarefromthedatasetMSCOCO[40]Naturalnesscontributetobothinconspicuousnessandthefinalperformance.WeproposeSegmentandComplete(SAC)[73]toevaluatetherobustnessofournaturalisticadversarialpatchesagainstdefender.Ourexperimentsdemonstratethattheadversarialpatcheswegeneratecannotbedetectedbydefender(detectionsuccessrateofpatchesis0%).5.3AblationStudyInthissection,wefurtherinvestigatethecriticalfactorsthatinfluenceourproposedmethod.5.3.1TopKThechoiceofkisimportantforthetrainingprocessofgeneratingadversarialpatches.Itisevidentthataslongaskisgreaterthan15,white-boxattackscanbesuccessful.Tab.2alsoshowsthatthegeneratedadver-sarialsamplesexhibitacertaindegreeofrobustnessandperformwellintransfertasks.However,duringtheex-periments,wefoundthatincreasingkleadstoahighernumberofattackiterations,causingthegeneratedad-versarialpatchestolosetheirnaturalness.Therefore,weexperimentedwithdifferentvaluesofktoattackALBEFandCLIP,exploringamoresuitablechoiceofk.Fig.5showsthechangeinASRwhenKtakesdifferentvaluesundertheconditionthatthepatchsizeisfixedat15%.AsKincreasesfrom5to15,theASRincreasesfrom88%to100%.ItcanbeseenthatourmethodcanstillachieveanASRof88%evenwhenmaintainingaveryhighlevelofnaturalness(K=5).Fig.5:ThemeanofASRonALBEFandCLIPunderdifferentKsettings.K=5K=10K=158090100ASR(%)5.3.2PatchLocationWeconductedablationexperimentsonthepatchloca-tion.Fig.6andFig.7illustratethechangesinadversar-ialpatchesandthenumberofattackiterationsunderdifferentlocalizationstrategies.RandomLocationDesignedLocationFig.6:Theadversarialexamplesunderdifferentlocationstrategies.Thecleanimagesandtheattackedimageswithnaturalisticpatches.TheimagesshownarefromthedatasetMSCOCO[40]Shortformoftitle9Fig.7:ThemeanofattackiterationonALBEFandCLIPunderdifferentlocationstrategies.RandomlocationDesignedlocation2030405060708090100AttackiterationRandomoptimizationDiffusion-baseoptimizationWefixedthepatchsizeat15%oftheimageandsetKto15tocomparetheeffectofhavingornothav-ingpatchlocalizationongeneratingadversarialpatches.Itisevidentthat,comparedtorandomlocalization,attention-guidedlocalizationcaneffectivelyidentifysuit-ableattackregions,completingtheattackwithfeweriterations.Thisresultsinreducingtime(93sto45s)forgeneratinganadversarialexampleandincreasednaturalnessoftheadversarialpatches.5.3.3PatchSizeWedefinepatchsizeastheratioofthelength(orwidth)ofthepatchtothelength(orwidth)oftheimage.WesetKto10tocomparetheattacksuccessratesofdifferentpatchsizesunderawhite-boxsetting.Topreventtheadversarialpatchesfromdegradingintonoisyimagesduringtraining,wesetthemaximumnumberofattackiterationsto300.Fig.8showsthechangesinattacksuccessratesandadversarialpatchesasthepatchsizevariesfrom0.2to0.05.Itisevidentthatlargeradver-sarialpatchesachievemoreeffectiveattacksandresultinmorenatural-lookingadversarialpatches.Fig.8:Theattacksuccessratesofdifferentpatchsizes.0.20.150.10.055060708090100PatchSizeASR(%)6ConclusionThispaperisthefirsttoconsiderusingadversarialpatchattacksexclusivelyonVLPmodels.Byemployingadual-guidedapproachwithdiffusionandattentionmechanisms,wecontroltheoptimizationdirectionanddeterminetheplacementofthepatches.Weproposeaframeworkforgeneratingnaturalpatchesthatattackimage-textretrievaltasksofVLPmodelswhilekeepingthetextunchanged.Ourexperimentsdemonstratethesuperiorityandfeasibilityofthemethod.Limitation.Whileourmethodexhibitsexcellentperformanceinwhite-boxsettingsandtransfertasks,experimentsrevealalackofmodeltransferability.Webelievethisisduetotheinsufficientutilizationoftheconsistencyfeaturesbetweenimagesandtextduringtheattack.Thenaturaladversarialpatchattacksmakesitmorechallengingtoleveragetextattentioncomparedtodigitaldomainperturbationattacks.Additionally,therobustnessofphysicalattacksrequiresfurtherimprove-ment.References1.Fei-LongChen,Du-ZhenZhang,Ming-LunHan,Xiu-YiChen,JingShi,ShuangXu,andBoXu.Vlp:Asurveyonvision-languagepre-training.MachineIntelligenceResearch,20(1):38–56,2023.2.NingXie,FarleyLai,DerekDoran,andAsimKadav.Visualentailment:Anoveltaskforfine-grainedimageunderstanding.arXivpreprintarXiv:1901.06706,2019.3.RichangHong,DaqingLiu,XiaoyuMo,XiangnanHe,andHanwangZhang.Learningtocomposeandreasonwithlanguagetreestructuresforvisualgrounding.IEEEtransactionsonpatternanalysisandmachineintelligence,44(2):684–696,2019.4.SiyuanLiang,MingliZhu,AishanLiu,BaoyuanWu,XiaochunCao,andEe-ChienChang.Badclip:Dual-embeddingguidedbackdoorattackonmultimodalcon-trastivelearning.arXivpreprintarXiv:2311.12075,2023.5.AishanLiu,XinweiZhang,YisongXiao,YuguangZhou,SiyuanLiang,JiakaiWang,XianglongLiu,XiaochunCao,andDachengTao.Pre-trainedtrojanattacksforvisualrecognition.arXivpreprintarXiv:2312.15172,2023.6.XinweiLiu,XiaojunJia,JindongGu,YuanXun,SiyuanLiang,andXiaochunCao.Doesfew-shotlearningsufferfrombackdoorattacks?arXivpreprintarXiv:2401.01377,2023.7.JiaweiLiang,SiyuanLiang,AishanLiu,XiaojunJia,JunhaoKuang,andXiaochunCao.Poisonedforgeryface:Towardsbackdoorattacksonfaceforgerydetection.arXivpreprintarXiv:2402.11473,2024.8.JiaweiLiang,SiyuanLiang,ManLuo,AishanLiu,DongchenHan,Ee-ChienChang,andXiaochunCao.Vl-trojan:Multimodalinstructionbackdoorattacksagainstautoregressivevisuallanguagemodels.arXivpreprintarXiv:2402.13851,2024.9.XinweiZhang,AishanLiu,TianyuanZhang,SiyuanLiang,andXianglongLiu.Towardsrobustphysical-worldbackdoorattacksonlanedetection.arXivpreprintarXiv:2405.05553,2024.10F.Authoretal10.MingliZhu,SiyuanLiang,andBaoyuanWu.Breakingthefalsesenseofsecurityinbackdoordefensethroughre-activationattack.arXivpreprintarXiv:2405.16134,2024.11.SiyuanLiang,JiaweiLiang,TianyuPang,ChaoDu,Ais-hanLiu,Ee-ChienChang,andXiaochunCao.Revisitingbackdoorattacksagainstlargevision-languagemodels.arXivpreprintarXiv:2406.18844,2024.12.SiyuanLiang,XingxingWei,andXiaochunCao.Generatemoreimperceptibleadversarialexamplesforobjectdetec-tion.InICML2021WorkshoponAdversarialMachineLearning,2021.13.SiyuanLiang,XingxingWei,SiyuanYao,andXiaochunCao.Efficientadversarialattacksforvisualobjecttrack-ing.InComputerVision–ECCV2020:16thEuropeanConference,Glasgow,UK,August23–28,2020,Proceed-ings,PartXXVI16,2020.14.XingxingWei,SiyuanLiang,NingChen,andXiaochunCao.Transferableadversarialattacksforimageandvideoobjectdetection.arXivpreprintarXiv:1811.12641,2018.15.SiyuanLiang,BaoyuanWu,YanboFan,XingxingWei,andXiaochunCao.Parallelrectangleflipattack:Aquery-basedblack-boxattackagainstobjectdetection.arXivpreprintarXiv:2201.08970,2022.16.SiyuanLiang,LongkangLi,YanboFan,XiaojunJia,JingzhiLi,BaoyuanWu,andXiaochunCao.Alarge-scalemultiple-objectivemethodforblack-boxattackagainstobjectdetection.InEuropeanConferenceonComputerVision,2022.17.ZhiyuanWang,ZeliangZhang,SiyuanLiang,andXiaosenWang.Diversifyingthehigh-levelfeaturesforbetterad-versarialtransferability.arXivpreprintarXiv:2304.10136,2023.18.AishanLiu,JunGuo,JiakaiWang,SiyuanLiang,Ren-shuaiTao,WenboZhou,CongLiu,XianglongLiu,andDachengTao.{X-Adv}:Physicaladversarialobjectat-tacksagainstx-rayprohibiteditemdetection.In32ndUSENIXSecuritySymposium(USENIXSecurity23),2023.19.BangyanHe,JianLiu,YimingLi,SiyuanLiang,JingzhiLi,XiaojunJia,andXiaochunCao.Generatingtransfer-able3dadversarialpointcloudviarandomperturbationfactorization.InProceedingsoftheAAAIConferenceonArtificialIntelligence,2023.20.JiayangLiu,SiyuZhu,SiyuanLiang,JieZhang,HanFang,WeimingZhang,andEe-ChienChang.Improv-ingadversarialtransferabilitybystablediffusion.arXivpreprintarXiv:2311.11017,2023.21.BangyanHe,XiaojunJia,SiyuanLiang,TianruiLou,YangLiu,andXiaochunCao.Sa-attack:Improv-ingadversarialtransferabilityofvision-languagepre-trainingmodelsviaself-augmentation.arXivpreprintarXiv:2312.04913,2023.22.LiangMuxue,ChuanWang,SiyuanLiang,AishanLiu,ZemingLiu,LiangYang,andXiaochunCao.Adversar-ialinstanceattacksforinteractionsbetweenhumanandobject.23.TianruiLou,XiaojunJia,JindongGu,LiLiu,SiyuanLiang,BangyanHe,andXiaochunCao.Hideinthicket:Generatingimperceptibleandrationaladversar-ialperturbationson3dpointclouds.arXivpreprintarXiv:2403.05247,2024.24.DehongKong,SiyuanLiang,andWenqiRen.Environ-mentalmatchingattackagainstunmannedaerialvehiclesobjectdetection.arXivpreprintarXiv:2405.07595,2024.25.KeMa,QianqianXu,JinshanZeng,XiaochunCao,andQingmingHuang.Poisoningattackagainstestimatingfrompairwisecomparisons.IEEETransactionsonPat-ternAnalysisandMachineIntelligence,44(10):6393–6408,2021.26.KeMa,QianqianXu,JinshanZeng,GuorongLi,XiaochunCao,andQingmingHuang.Ataleofhodgerankandspectralmethod:Targetattackagainstrankaggregationisthefixedpointofadversarialgame.IEEETransactionsonPatternAnalysisandMachineIntelligence,45(4):4090–4108,2022.27.KeMa,QianqianXu,JinshanZeng,WeiLiu,XiaochunCao,YingfeiSun,andQingmingHuang.Sequentialma-nipulationagainstrankaggregation:theoryandalgorithm.IEEEtransactionsonpatternanalysisandmachinein-telligence,2024.28.JiamingZhang,QiYi,andJitaoSang.Towardsadver-sarialattackonvision-languagepre-trainingmodels.InProceedingsofthe30thACMInternationalConferenceonMultimedia,pages5005–5013,2022.29.ChunyuSun,ChenyeXu,ChengyuanYao,SiyuanLiang,YichaoWu,DingLiang,XianglongLiu,andAishanLiu.Improvingrobustfairnessviabalanceadversarialtraining.InProceedingsoftheAAAIConferenceonArtificialIntelligence,2023.30.AishanLiu,ShiyuTang,SiyuanLiang,RuihaoGong,BoxiWu,XianglongLiu,andDachengTao.Exploringthere-lationshipbetweenarchitecturaldesignandadversariallyrobustgeneralization.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,2023.31.JiaweiLiang,SiyuanLiang,AishanLiu,KeMa,JingzhiLi,andXiaochunCao.Exploringinconsistentknowledgedistillationforobjectdetectionwithdataaugmentation.InProceedingsofthe31stACMInternationalConferenceonMultimedia,2023.32.TianyuanZhang,LuWang,HainanLi,YisongXiao,SiyuanLiang,AishanLiu,XianglongLiu,andDachengTao.Lanevil:Benchmarkingtherobustnessoflanedetectiontoenvironmentalillusions.arXivpreprintarXiv:2406.00934,2024.33.YuhangWang,HuafengShi,RuiMin,RuijiaWu,SiyuanLiang,YichaoWu,DingLiang,andAishanLiu.Adaptiveperturbationgenerationformultiplebackdoorsdetection.arXivpreprintarXiv:2209.05244,2022.34.SiyuanLiang,KuanrongLiu,JiajunGong,JiaweiLiang,YuanXun,Ee-ChienChang,andXiaochunCao.Un-learningbackdoorthreats:Enhancingbackdoordefenseinmultimodalcontrastivelearningvialocaltokenunlearning.arXivpreprintarXiv:2403.16257,2024.35.JunnanLi,RamprasaathSelvaraju,AkhileshGotmare,ShafiqJoty,CaimingXiong,andStevenChuHongHoi.Alignbeforefuse:Visionandlanguagerepresentationlearningwithmomentumdistillation.Advancesinneuralinformationprocessingsystems,34:9694–9705,2021.36.AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,GirishSastry,AmandaAskell,PamelaMishkin,JackClark,etal.Learn-ingtransferablevisualmodelsfromnaturallanguagesu-pervision.InInternationalconferenceonmachinelearn-ing,pages8748–8763.PMLR,2021.37.AleksanderMkadry,AleksandarMakelov,LudwigSchmidt,DimitrisTsipras,andAdrianVladu.Towardsdeeplearningmodelsresistanttoadversarialattacks.stat,1050(9),2017.38.DongLu,ZhiqiangWang,TengWang,WeiliGuan,HongchangGao,andFengZheng.Set-levelguidanceattack:Boostingadversarialtransferabilityofvision-languagepre-trainingmodels.InProceedingsoftheShortformoftitle11IEEE/CVFInternationalConferenceonComputerVi-sion,pages102–111,2023.39.BryanAPlummer,LiweiWang,ChrisMCervantes,JuanCCaicedo,JuliaHockenmaier,andSvetlanaLazeb-nik.Flickr30kentities:Collectingregion-to-phrasecor-respondencesforricherimage-to-sentencemodels.InProceedingsoftheIEEEinternationalconferenceoncom-putervision,pages2641–2649,2015.40.Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays,PietroPerona,DevaRamanan,PiotrDoll´ar,andCLawrenceZitnick.Microsoftcoco:Commonobjectsincontext.InComputerVision–ECCV2014:13thEuropeanConference,Zurich,Switzerland,September6-12,2014,Proceedings,PartV13,pages740–755.Springer,2014.41.TomBBrown,DandelionMan´e,AurkoRoy,Mart´ınAbadi,andJustinGilmer.Adversarialpatch.arXivpreprintarXiv:1712.09665,2017.42.XinLiu,HuanruiYang,ZiweiLiu,LinghaoSong,HaiLi,andYiranChen.Dpatch:Anadversarialpatchattackonobjectdetectors.arXivpreprintarXiv:1806.02299,2018.43.MarkLeeandZicoKolter.Onphysicaladversarialpatchesforobjectdetection.arxiv.arXivpreprintarXiv:1906.11897,2019.44.SvetlanaPavlitskaya,JonasHendl,SebastianKleim,LeopoldJohannM¨uller,FabianWylczoch,andJMariusZ¨ollner.Suppresswithapatch:Revisitinguniversalad-versarialpatchattacksagainstobjectdetection.In2022InternationalConferenceonElectrical,Computer,Com-municationsandMechatronicsEngineering(ICECCME),pages1–6.IEEE,2022.45.SimenThys,WiebeVanRanst,andToonGoedem´e.Fool-ingautomatedsurveillancecameras:adversarialpatchestoattackpersondetection.InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognitionworkshops,pages0–0,2019.46.AniruddhaSaha,AkshayvarunSubramanya,KoninikaPatil,andHamedPirsiavash.Roleofspatialcontextinadversarialrobustnessforobjectdetection.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognitionWorkshops,pages784–785,2020.47.YichiZhang,ZijianZhu,XiaoYang,andJunZhu.Ad-versarialsemanticcontourforobjectdetection.arXivpreprintarXiv:2109.15009,2021.48.YushengZhao,HuanqianYan,andXingxingWei.Objecthider:Adversarialpatchattackagainstobjectdetectors.arXivpreprintarXiv:2010.14974,2020.49.ZijianZhu,HangSu,ChangLiu,WenzhaoXiang,andShibaoZheng.Youcannoteasilycatchme:alow-detectableadversarialpatchforobjectdetectors.arXivpreprintarXiv:2109.15177,2021.50.JiayuBao.Sparseadversarialattacktoobjectdetection.arXivpreprintarXiv:2012.13692,2020.51.SWu,TDai,andSTXia.Dpattack:Diffusedpatchattacksagainstuniversalobjectdetection.arxiv2020.arXivpreprintarXiv:2010.11679.52.ZhaoyuChen,BoLi,ShuangWu,JiangheXu,ShouhongDing,andWenqiangZhang.Shapematters:deformablepatchattack.InEuropeanconferenceoncomputervision,pages529–548.Springer,2022.53.AlonZolfi,MosheKravchik,YuvalElovici,andAsafShab-tai.Thetranslucentpatch:Aphysicalanduniversalattackonobjectdetectors.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages15232–15241,2021.54.SiaoLiu,ZhaoyuChen,WeiLi,JiweiZhu,JiafengWang,WenqiangZhang,andZhongxueGan.Efficientuniversalshuffleattackforvisualobjecttracking.InICASSP2022-2022IEEEInternationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),pages2739–2743.IEEE,2022.55.SvetlanaPavlitskaya,Bianca-MarinaCod˘au,andJMariusZ¨ollner.Feasibilityofinconspicuousgan-generatedadver-sarialpatchesagainstobjectdetection.arXivpreprintarXiv:2207.07347,2022.56.Yu-Chih-TuanHu,Bo-HanKung,DanielStanleyTan,Jun-ChengChen,Kai-LungHua,andWen-HuangCheng.Naturalisticphysicaladversarialpatchforobjectdetec-tors.InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,pages7848–7857,2021.57.ChenanWang,JinhaoDuan,ChaoweiXiao,EdwardKim,MatthewStamm,andKaidiXu.Semanticad-versarialattacksviadiffusionmodels.arXivpreprintarXiv:2309.07398,2023.58.XuelongDai,KaishengLiang,andBinXiao.Advdiff:Gen-eratingunrestrictedadversarialexamplesusingdiffusionmodels.arXivpreprintarXiv:2307.12499,2023.59.JiangLiu,ChenWei,YuxiangGuo,HengYu,AlanYuille,SoheilFeizi,ChunPongLau,andRamaChellappa.In-struct2attack:Language-guidedsemanticadversarialat-tacks.arXivpreprintarXiv:2311.15551,2023.60.HaotianXue,AlexandreAraujo,BinHu,andYongxinChen.Diffusion-basedadversarialsamplegenerationforimprovedstealthinessandcontrollability.AdvancesinNeuralInformationProcessingSystems,36,2024.61.JunnanLi,DongxuLi,CaimingXiong,andStevenHoi.Blip:Bootstrappinglanguage-imagepre-trainingforuni-fiedvision-languageunderstandingandgeneration.InIn-ternationalconferenceonmachinelearning,pages12888–12900.PMLR,2022.62.JinyuYang,JialiDuan,SonTran,YiXu,SampathChanda,LiqunChen,BelindaZeng,TrishulChilimbi,andJunzhouHuang.Vision-languagepre-trainingwithtriplecontrastivelearning.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages15671–15680,2022.63.JiasenLu,DhruvBatra,DeviParikh,andStefanLee.Vilbert:Pretrainingtask-agnosticvisiolinguisticrepresen-tationsforvision-and-languagetasks.Advancesinneuralinformationprocessingsystems,32,2019.64.Yen-ChunChen,LinjieLi,LichengYu,AhmedElKholy,FaisalAhmed,ZheGan,YuCheng,andJingjingLiu.Uniter:Universalimage-textrepresentationlearning.InEuropeanconferenceoncomputervision,pages104–120.Springer,2020.65.HaodiWang,KaiDong,ZhileiZhu,HaotongQin,Ais-hanLiu,XiaolinFang,JiakaiWang,andXianglongLiu.Transferablemultimodalattackonvision-languagepre-trainingmodels.In2024IEEESymposiumonSecurityandPrivacy(SP),pages102–102.IEEEComputerSociety,2024.66.JonathanHo,AjayJain,andPieterAbbeel.Denoisingdiffusionprobabilisticmodels.Advancesinneuralinfor-mationprocessingsystems,33:6840–6851,2020.67.YunhaoGou,TomKo,HansiYang,JamesKwok,YuZhang,andMingxuanWang.Leveragingperimage-tokenconsistencyforvision-languagepre-training.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages19155–19164,2023.68.XiangyuanLan,MangYe,RuiShao,BinengZhong,PongCYuen,andHuiyuZhou.Learningmodality-consistencyfeaturetemplates:Arobustrgb-infraredtrack-ingsystem.IEEETransactionsonIndustrialElectronics,66(12):9887–9897,2019.12F.Authoretal69.AndrejKarpathyandLiFei-Fei.Deepvisual-semanticalignmentsforgeneratingimagedescriptions.InPro-ceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,pages3128–3137,2015.70.AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,XiaohuaZhai,ThomasUnterthiner,MostafaDehghani,MatthiasMinderer,GeorgHeigold,SylvainGelly,etal.Animageisworth16x16words:Trans-formersforimagerecognitionatscale.arXivpreprintarXiv:2010.11929,2020.71.KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.Deepresiduallearningforimagerecognition.InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,pages770–778,2016.72.JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLiFei-Fei.Imagenet:Alarge-scalehierarchicalimagedatabase.In2009IEEEconferenceoncomputervisionandpatternrecognition,pages248–255.Ieee,2009.73.JiangLiu,AlexanderLevine,ChunPongLau,RamaChel-lappa,andSoheilFeizi.Segmentandcomplete:Defendingobjectdetectorsagainstadversarialpatchattackswithrobustpatchdetection.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages14973–14982,2022.ListofabbreviationsVLP:visuallanguagepre-training;ASR:attacksuccessrates;TR:image-to-textretrieval;IR:text-to-imageretrieval;VE:visualentailment;VG:visualgroundingDeclarations1.AvailabilityofdataandmaterialThedatasetsgeneratedduringand/oranalyzeddur-ingthecurrentstudyareavailablefromthecorre-spondingauthoronreasonablerequest.2.CompetingInterestsTheauthorshavenocompetingintereststodeclarethatarerelevanttothecontentofthisarticle.3.AuthorContributionsTothebestofourknowledge,wearethefirsttoex-plorethesecurityofVLPmodelsthroughadversarialpatches.Weintroduceanoveldiffusion-basedframe-worktogeneratemorenaturaladversarialpatchesagainstVLPmodels.Wedeterminethelocationofadversarialpatchesbycross-modalguidance.Exten-siveablationexperimentsdemonstratetheeffective-nessofthisapproach.4.FundingThisworkwassupportedbytheShenzhenCampusofSunYat-senUniversity.5.AcknowledgementsNotapplicable.
