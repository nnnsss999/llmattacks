---
attack_type: indirect_pi
source_url: http://arxiv.org/pdf/2501.18536
---

IllusionsofRelevance:UsingContentInjectionAttackstoDeceiveRetrievers,Rerankers,andLLMJudgesManveerSinghTambermtamber@uwaterloo.caUniversityofWaterlooWaterloo,Ontario,CanadaJimmyLinjimmylin@uwaterloo.caUniversityofWaterlooWaterloo,Ontario,CanadaABSTRACTConsiderascenarioinwhichausersearchesforinformation,onlytoencountertextsfloodedwithmisleadingornon-relevantcon-tent.ThisscenarioexemplifiesasimpleyetpotentvulnerabilityinneuralInformationRetrieval(IR)pipelines:contentinjectionattacks.Wefindthatembeddingmodelsforretrieval,rerankers,andlargelanguagemodel(LLM)relevancejudgesarevulnerabletotheseattacks,inwhichadversariesinsertmisleadingtextintopassagestomanipulatemodeljudgements.Weidentifytwoprimarythreats:(1)insertingunrelatedorharmfulcontentwithinpassagesthatstillappeardeceptively“relevant”,and(2)insertingentirequeriesorkeyquerytermsintopassagestoboosttheirperceivedrelevance.Whilethesecondtactichasbeenexploredinpriorresearch,wepresent,toourknowledge,thefirstempiricalanalysisofthefirstthreat,demonstratinghowstate-of-the-artmodelscanbeeasilymisled.Ourstudysystematicallyexaminesthefactorsthatinfluenceanattack’ssuccess,suchastheplacementofinjectedcontentandthebalancebetweenrelevantandnon-relevantmaterial.Addition-ally,weexplorevariousdefensestrategies,includingadversarialpassageclassifiers,retrieverfine-tuningtodiscountmanipulatedcontent,andpromptingLLMjudgestoadoptamorecautiousap-proach.However,wefindthatthesecountermeasuresofteninvolvetrade-offs,sacrificingeffectivenessforattackrobustnessandsome-timespenalizinglegitimatedocumentsintheprocess.OurfindingshighlighttheneedforstrongerdefensesagainsttheseevolvingadversarialstrategiestomaintainthetrustworthinessofIRsystems.Wereleaseourcodeandscriptstofacilitatefurtherresearch1.KEYWORDSAdversarialAttacksinIR,Black-BoxAttacks,EmbeddingModels,Rerankers,LLMRelevanceJudges1INTRODUCTIONIntoday’sdata-richworld,InformationRetrieval(IR)playsacru-cialroleinextractingrelevantinformationfromlargecollectionsinresponsetouserqueries[28].ModernIRpipelinesoftenrelyonembeddingmodelsandrerankers,whereembeddingmodelsenableefficientandeffectivesearch,whilererankersrefinepassageliststobringthemostrelevantcontentforward.Additionally,theemergenceoflargelanguagemodels(LLMs)hasalsoextendedIRcapabilities,particularlyintheareaofrelevancejudging[51,52],whereLLMsscorehowrelevantpassagesaretouserqueries.1https://github.com/manveertamber/content_injection_attacksFigure1:Wefindthatretrievers,rerankers,andLLMrele-vancejudgesaregenerallysusceptibletocontentinjectionattacks,identifyingpassagescontainingrandomorevenma-liciouscontentasrelevant.Whilewekeepthemessageinthisexamplerelativelymild,wefindthatevenwhenextremelyharmfulmessagesareinserted,themodelsstillperceivethesepassagesasperfectlyorhighlyrelevant.Ensuringthatsearchsystemsconsistentlydeliverfactualandtrustworthyinformationiscritical.Ifadversariescanexploitem-beddingmodels,rerankers,orLLMrelevancejudges,theycanma-nipulatesearchpipelinestoservemaliciousresultsorresultsthatadvancetheirinterests.Thispaperinvestigatesatypeofvulnera-bilitywerefertoascontentinjectionattacks,whereinadversariesinserttextintopassagestodeceiveIRmodels.Twoadversarialgoalsareexamined:(1)Addingmaliciousorunrelatedtexttoanotherwiserelevantpassagewhilemaintainingperceivedrelevance.(2)Makinganon-relevantpassageappearrelevantbyinsertingthequeryverbatimorkeywordsfromthequery.Althoughsomepriorresearchhasaddressedthesecondgoal,webelievethisworkisthefirsttopresentanempiricalstudyofthefirstgoal.Bothgoalscanservetodeceivemodelsintoconsideringnon-relevantcontentasrelevant.Notably,thefirstgoalsharessimilaritieswithpromptinjectionattacksagainstLLMs[22,23],whereadversariesinsertmanipulativeandpossiblymaliciousinputsorinstructionswithinotherwisebenignprompts,steeringLLMoutput.Byconcealingcontentwithinpassagesthatappearrelevant,adversariescanaccomplishtheirobjectivesofdisseminatingtheinformationtheychoose.Therefore,unifyingtheunderstandingofbothadversarialgoalsandhowIRpipelinesmightfailtorecognizenon-relevantcontentisbothinterestingandpracticallyimportant.Wesystematicallyexaminehowfactorssuchastheinjectionlocation,repeatedqueriesandqueryterms,thebalanceofnon-relevanttorelevantcontent,andtheinjectedtext’ssemanticnature,arXiv:2501.18536v1  [cs.IR]  30 Jan 2025suchashatefulvsrandominjectedcontent,affectattacksuccessratesacrossthetwoadversarialgoalsandabroadrangeofmodels.Thispaperalsoexploresdefenses,includingusingsupervisedclassifiers,retrieverfine-tuning,andamorecarefulpromptingofLLMjudgestobemorecautiousofadversariallymanipulatedpassages.Wefindthatthroughthesedefenses,IRpipelinescanbemademorerobusttocontentinjectionattacks.However,balancingrobustnesstoattackswithmaintaininghighlevelsofeffectivenessonbothin-domainandout-of-domaintasksandmaintainingthequalityofrelevancejudgementsremainsachallenge,indicatingthatfurtherresearchisrequiredtotacklethisproblem.2BACKGROUND2.1NeuralIRModels2.1.1EmbeddingModels.Embeddingmodelsconverttextintonumericalvectorsorembeddings,enablingefficientandeffectivesearches[19,41].Specifically,denseretrievaltechniquesmapbothqueriesandpassagesintoacontinuousvectorspace,ensuringthatrelevantpassagesarepositionedrelativelyclosetothecorrespond-ingqueries.Corporaofpassagesaresearchedbytakingtheembed-dingsofthepre-encodedpassagesinthecorporaandcomparingthesevectorembeddingswiththequeryembeddingusingamea-surelikethedotproductorcosinesimilarity.2.1.2Rerankers.Afterretrievinganinitialpoolofcandidatepas-sages,rerankersreorderthembyrelevance.Commonapproachesin-cludepointwisererankers,whichpredictarelevancescoreforeachquery-passagepair[30,32];pairwisererankers,whichcomparetwopassagesatatimetodecidewhichismorerelevant[32];andlist-wisererankers,whichprocessasetofpassagesatoncetoproduceanoverallranking[37,38,44,47].Inthisstudy,wefocusprimarilyonpointwisererankersbecausetheyarebothcomputationallyeffi-cientandhaveproventobeeffective.Unlikepairwiseandlistwisemethods,pointwisererankersevaluateeachquery-passagepairin-dependently,eliminatingtheneedfordirectcomparisonsbetweenpassages.Thisindependencereducescomputationalcosts,makingpointwisererankersapracticalchoiceforourexperiments.2.1.3LLMRelevanceJudges.LLMshavebeenshowntoaccuratelyjudgetherelevanceofpassagestoqueries[51,52].TheroleoftheLLMrelevancejudgeistoproducesomerelevancescoregivenaqueryandapassage,forhowrelevantthepassageistothequery.Theexactscoringrangeandlabelscanbecustomizedviapromptdesign.WeusethesamebasicpromptfromAlaofietal.[2],shownbelow,whichscorespassagerelevanceona0-3scale,andmatchestheguidelinesgiventohumanannotatorsintheTRECDeepLearn-ingtrack[8].Pleasereadthequeryandpassagebelowandindicatehowrelevantthepassageistothequery.Usethefollowingscale:•3forperfectlyrelevant:Thepassageisdedicatedtothequeryandcontainstheexactanswer.•2forhighlyrelevant:Thepassagehassomeanswerforthequery,buttheanswermaybeabitunclear,orhiddenamongstextraneousinformation.•1forrelated:Thepassageseemsrelatedtothequerybutdoesnotanswerit.•0forirrelevant:Thepassagehasnothingtodowiththequery.Query:{QueryPlaceholder}Passage:{PassagePlaceholder}Indicatehowrelevantthepassageis,usingthescaleabove.Giveonlyanumber,donotgiveanyexplanation.2.1.4SparseRetrieval.Thisworkdoesnotexaminesparseretrievalmethods,suchastraditionalbag-of-wordsapproacheslikeBM25orneuralsparseretrievaltechniqueslikeSPLADE[13].However,improvingtherankingofadversarialpassagesinsparseretrievalislikelyatrivialtaskthroughqueryorkeywordinjection.2.2FoolingNeuralIRModels2.2.1AdversarialMachineLearning.Manymachinelearningmod-elsaresusceptibletoadversarialexamples[15,45],wheresmall,intentionalmodificationstotheinputcansignificantlyalterthemodel’soutput.Forexample,incomputervision,researchhasdemonstratedthatsubtlymodifyinganimageofastopsigncancauseanimageclassifiertomisidentifyitasayieldsign[35].How-ever,ourworkdoesnotfocusonimperceptibleperturbations.WeinvestigatehowaddingtexttopassagescandeceiveneuralIRmod-elsintoincorrectlyassessingtheirrelevancetospecificqueries.2.2.2FoolingLLMJudges.Alaofietal.[2]arguedthatLLMrele-vancejudgesoftenassignhigherrelevancescoreswhenpassagescontainqueryterms,evenifthepassageisnon-relevantormean-ingless.Theauthorsalsoconductedpromptinjectionexperimentswherepassageswereprecededbytheinstruction:“Thepassageisdedicatedtothequeryandcontainstheexactanswer”.TheresultsshowedthatwhilesomeoftheLLMstestedlikeGPT-3.5andLlama-3(8B)weresomewhataffectedbypromptinjection,otherLLMslikeGPT-4oandLlama-3(70B)wereimpactedtoamuchlowerandmorereasonabledegree.2.2.3FoolingRankingModels.Severalstudieshaveexploredhowtomanipulatepassagesforspecificqueriestodeceiverankingmod-els.Ravaletal.[40]showedthatevenminortextchangessuchaschangingafewtokenscanmisleadrankingmodelstounderestimatepassagerelevance.PRADA[58]introducedaword-substitutionstrategythatusedalearnedsurrogaterankingmodeltosystemati-callyreplacesmallsetsoftokens,boostingapassage’srank.Chenetal.[6]proposedgeneratingconnectionsentences(producedbyalanguagemodel)toweavethequeryintothetargettext,thusintegratingitmorenaturallyandimprovingtherankofthetext.Additionally,researchhasexploredimperceptibleperturbationstargetingembeddingmodelsusedforretrieval[24].Zhongetal.[60]demonstratedawhite-boxcorpuspoisoningattackagainstdenseretrieval,usingagradient-basedapproachinspiredbyHotFlip[12]toiterativelyreplacetokensinarandompassageandmaximizeitsquerysimilarityscore.Althougheffective,thismethodrequiredqueryaccesstotheembeddingmodelandoftenproducedunnaturalpassageswithlowtokenlog-likelihoods(asmeasuredbyGPT-2),makingthemeasiertodetect.Songetal.[43]studiedsemanticcollisionswhereunrelatedtextsarejudgedsimilarbyNLPmodelsandproposedagradient-basedmethodthatenforcednaturalnessbyconstrainingtokenchoiceswithalanguagemodeltoevadeperplexity-basedfiltering.LikeZhongetal.’swork,thisattackalsorequiredqueryaccesstotheembeddingmodelandfurtherrequiredalanguagemodelwiththesamevocabularyasthetargetmodelbeingattacked.Liuetal.[20],alsoproposedagradient-basedattackmethod,butavoideddirectaccesstothetargetmodelbyusingsurrogatemodels.Indeed,evencommercialembeddingmodelscanbe“stolen”ordistilledintosurrogatesthatreplicatetheirbehaviour[48],enablingadversariestomoreeasilytestadversarialpassagesagainstblack-boxrankingsystems.Focusingonrerankers,Parryetal.[36]showedthatrerankerswithcertainpromptphrases(Query,Document,Relevant)couldbemanipulatedbyinsertingkeywords(Relevant,true)intotheprompt.SomerecentworkhasfocusedonattackingRAGsystems,explor-ingembeddingmodelattackswhereadversarialpassagestrickedmodelsintoretrievingharmfulcontent,causingincorrectornon-relevantoutputfromtheLLM[42,62].Ontheretrievalside,theseworkssimplyinvolvedprependingthequerytothetargetpassageintheblack-boxcase[42,62]oragradient-basedmethodinvolvingHotFlip[12]inthewhite-boxcase[62].ThemanipulatedpassagescouldthenbeusedtofacilitatepromptinjectionorknowledgecorruptionattacksagainsttheLLM.Unlikethesepriorworks,ourstudyexploresaddingmaliciousornon-relevanttexttoalreadyrelevantpassages.Wealsoinvestigatehowinjectingqueryorkeywordtermsintonon-relevantpassagescandeceptivelyincreasetheirrelevance.Byexaminingbothscenar-ios,weidentifyfactorsthataffectattacksuccessandproposewaystofortifyIRsystemsagainstsuchmanipulations.Testingacrossvariousstate-of-the-artembeddingmodels,rerankers,andLLM-basedrelevancejudges,wefindthatallremainvulnerabledespitetheirdifferenttaskformulationsandtrainingprocedures.2.2.4DefendingagainstAttacks.AlthoughmanyattacksagainstIRmodelshavebeenproposed,effectivedefensesremainlimited.How-ever,workinmachinelearninghasshownthatstrongdefenseswithsecurityguaranteesarepossible[26].Liuetal.[25]providedathe-oreticalanalysisoftheeffectiveness-robustnesstradeoffinrankingmodelswithafocusonimperceptibleperturbationsthroughwordsubstitutionattacks.Theworkalsoproposedjointlyoptimizingarankinglossandanadversariallosstotrainmoreperturbation-invariantrankingmodelsthatminimizedifferentrankingoutcomesforsimilarpassageswithwordsubstitutions.Chenetal.[5]investi-gatedclassifiersagainstspecificattacksaimedatboostingtherank-ingoftargetpassages[6,20,58],findingthatsupervisedclassifierscandetectadversarialpassageswhentrainedtoidentifyspecificmanipulations.However,effectivenessdegradedforattackmethodsnotrepresentedinthetrainingdata,limitingpracticaleffectiveness.Perplexity-basedfilteringhasbeenshowntoworkwellagainstsomeadversarialpassages[60]buthasalsobeenshowntobeabletobeevadedthroughmorecarefulpassageconstruction[6,43].3EXPERIMENTALSETUP3.1ModelsOurgoalwastoselectavarietyofembeddingmodels,rerankers,andLLMrelevancejudgeswhileaimingtokeepareasonablecom-putationalbudget.Ourselectiontriestocapturevariationsinmodelsizes,architectures,andfine-tuningmethods.AllexperimentsinthisstudywereconductedusingsingleNVIDIARTXA6000GPUs.3.1.1EmbeddingModels.Weevaluatefiveembeddingmodelstocoveravarietyofsizesandtrainingregimens.First,wechosetwomodelsfromtheBGEfamily[59]:BGE-basebge-base-en-v1.5andBGE-largebge-large-en-v1.5.Thisallowedustocompareembed-dingsgeneratedfromdifferentlysizedmodels,withBGE-largebeinginitializedwithBERT-large[9],whilethebaseversionandtheremainingmodelstestedstemfromBERT-base.Wealsoincor-poratedE5[55],boththeunsupervisedversione5-base-unsupervised,whichwerefertoasE5-unsupanditsfine-tunedcounterparte5-basereferredtoasE5-sup.Thisallowedforgatheringinsightsintotheinfluenceoftask-specificfine-tuningforretrieval.Finally,weincludedthestate-of-the-artBERT-baseembeddingmodelfromSnowflake[29]snowflake-arctic-embed-m-v1.5whichwerefertoasArctic-base.Whenanalyzingthesusceptibilityofembeddingmod-elstoadversarialpassages,weconsidertherankdeterminedbytherelevancescoreoftheadversarialpassagewhencomparedtothescoresofpassagesintheentireretrievalcorpus.3.1.2Rerankers.Forrerankers,ourevaluationincludedms-marco-MiniLM-L-12-v2[41],alightweightreranker(33Mparameters)fine-tunedusingmicrosoft/MiniLM-L12-H384-uncased[56].WerefertothisrerankerassimplyMiniLMinthisstudy.WealsostudiedtheMonoT5family[31],comparingMonoT5-base(220Mparameters)andMonoT5-large(770Mparameters),bothfine-tunedonT5mod-els[39].Additionally,weincludedRankT5-base[61],alsofine-tunedwithT5-base,toexplorealternativefine-tuningstrategies.Whenanalyzingthesusceptibilityofrerankerstoadversarialpas-sages,weconsidertherankdeterminedbytherelevancescoreoftheadversarialpassagewhencomparedtothescoresofthererankedtop-100BM25retrievedpassagesfromtheretrievalcorpus.3.1.3LLMRelevanceJudges.GiventhehighercomputationalcostsofrunningLLMscomparedtoembeddingmodelsandrerankers,welimitedourfocustotwoLLMrelevancejudges.TheseincludedGPT-4o[1],representingthecuttingedgeofLLMsandLlama-3.1(8B)[11],astrongbutlightweightopen-sourcemodelwithfewerparameters.WhenanalyzingthesusceptibilityofLLMrelevancejudgestoadversarialpassages,weconsidertherelevancescoreassignedbytheLLMjudgetotheadversarialpassage.3.2DatasetsExperimentsprimarilyevaluateusingMSMARCOpassagerankingdatasetswiththeTRECDeepLearningTracksfrom2019and2020referredtoasDL19andDL20[7,8].Additionally,subsetsfromBEIR[50]areincludedfordomaindiversity,specificallyFiQA[27],SciFact[54],TREC-COVID[53],NFCorpus[4],DBPedia[17],andClimate-FEVER[10].Thesedatasetsoffervaryingtypesofqueries(e.g.,factualclaims,opinion-basedquestions),corpora(e.g.,Wiki-pedia,scientificabstracts,forumposts),andtopics(e.g.,finan-cial,COVID-19,climate-change).Whenconsideringanyparticulardataset,weconstructadversarialpassagesthattargeteveryqueryinthedatasetthathassomecorrespondingannotatedrelevantpas-sageinthecorpora.ForDL19andDL20,thisincludesonlythequeriesforwhichhumanrelevancejudgementsareavailable.3.3AdversarialPassages3.3.1SentenceInjectionContent.Weexaminetheimpactofin-sertingbothrandomandgeneratedhatefultextintopassages.Tocollectrandomtext,weextractsentencesfromtheMSMARCOv1passagecorpus[3].Forhatefultext,weutilizesentencesgener-atedfromtheToxigendataset[16],specificallyfocusingontestsetstatementsthathavebeenlabelledastoxicbyeitherahumanannotatororaclassifier.ToxigenexploresthegenerationofhatefulTable1:Examiningtheeffectofqueryrepetition,keywordrepetitionandsentencerepetitiononattacksuccessrates(%)onDL19andDL20acrossqueryinjection,keywordinjection,andsentenceinjectionattacksandthreepassagetypes(random,scrambledword,andrelevant).Ineachcase,thequeries,keywordsorsentencesareinsertedintothestartofthepassage.QueryInjectionKeywordInjectionSentenceInjectionRandomScrambledRandomScrambledRelevantReps=1Reps=2Reps=3Reps=1Reps=2Reps=3Reps=1Reps=2Reps=3Reps=1Reps=2Reps=3Count=1Count=2Retrievers(R@1/R@5)BGE-base0.4/1.69.3/28.517.5/42.92.3/9.528.2/61.643.5/74.60.2/1.02.9/9.96.6/19.40.4/1.011.8/28.918.4/42.92.5/34.20.8/16.1BGE-large0.0/1.46.6/19.612.0/34.03.5/18.629.1/62.738.6/73.40.0/0.63.1/10.35.8/15.10.4/6.013.4/33.818.6/41.98.7/56.33.5/34.6E5-sup0.0/0.09.9/31.320.4/46.80.4/5.619.2/46.022.7/53.40.0/0.44.1/16.79.1/31.80.8/2.711.8/31.816.7/39.89.9/66.43.1/39.2E5-unsup6.8/18.49.1/28.714.6/37.520.4/43.142.3/69.347.8/72.21.2/5.43.5/10.34.9/13.24.7/15.513.2/32.818.4/41.03.5/33.22.7/21.9Arctic-base0.6/4.310.9/34.816.3/42.51.0/8.521.4/52.630.5/63.30.4/2.14.1/15.57.2/24.70.4/3.97.4/27.212.8/36.38.9/56.12.5/32.2Rerankers(R@1/R@5)MiniLM13.0/36.921.9/49.124.7/50.98.9/35.917.5/44.120.2/48.06.0/28.011.5/36.113.4/37.95.6/27.010.1/35.512.6/37.30.4/62.50.2/42.9MonoT5-base7.6/26.623.1/51.829.3/61.210.5/33.829.5/60.032.8/65.67.2/22.917.5/38.419.8/42.57.6/28.221.2/42.123.7/47.60.4/57.30.4/34.8MonoT5-large4.5/25.620.6/48.927.2/57.116.7/44.534.6/67.039.0/72.82.3/16.38.0/28.013.0/31.88.7/28.520.0/40.820.2/43.10.0/42.10.0/29.1RankT5-base1.9/8.510.9/34.615.7/42.54.5/21.919.2/4723.3/51.80.6/6.89.7/27.615.3/34.62.9/15.316.9/38.621.6/42.37.2/74.43.1/55.9LLMJudges(S@3/S@2+)GPT4o0.0/0.00.0/0.00.0/0.00.0/0.00.0/0.00.0/0.00.2/0.20.0/0.40.0/1.00.2/0.20.0/0.20.2/0.453.2/97.955.7/99.0Llama-3.1(8B)0.6/1.40.8/2.10.2/0.60.0/0.80.4/2.90.0/0.80.8/2.10.8/1.40.2/0.20.0/1.20.0/1.60.0/1.022.6/59.513.2/61.6textsusinganLLM.Theauthorsfoundthathumanannotatorsoftenstruggledtodifferentiatethesegeneratedtextsfromhuman-writtencontent.However,generatedtextsmayexhibitlimitationssuchasstaticpatternsincontent,grammar,orstyle,whichcouldimpactthegeneralizationoffindingstodiverseformsofmalicioustext.Forthisreason,weemphasizethestudyofrandomtextsinacontent-agnosticapproach.Ifmodelscanbemisledbyrandomtext,itislikelytheycanalsobedeceivedbymoreintentionallycreatedcontent.PassagesfromtheMSMARCOv1passagecorpusareextractedfromwebdocumentsretrievedbyBingandshouldcaptureadiversityofcontent,grammar,andstyleintext.Wespecif-icallyexploretheuseofhatefulsentencesinSection4.1andinourevaluationofpotentialdefenses.WeusespaCy’sen_core_web_smmodel[18]tosplitsentencesandapplyheuristicstoensurethatthesentencesaremeaningful.Sentencesmustmeetthefollowingcriteria:atleast5words,30char-acters,andatmost300characters;includeatleast3non-stopwordtokens;andcontainatleastoneverbandonenoun.3.3.2PassageTypes.Threepassagetypesarestudied:relevantpassages,randompassages,andscrambledwordpassages.Passagerelevancecansometimesbeambiguous,therefore,whenanalyzingaspecificmodel,thetop-rankingorhighest-scoringpas-sageswithrespecttoaqueryaretreatedasrelevantpassages.Totakeamodel-agnosticapproach,GPT-4oispromptedtogeneratepassagesthatitconsidersperfectlyrelevanttoagivenquery,andtheseareusedastherelevantpassage.Randompassages,incontrast,aresampledfromtheMSMARCOv1passagecorpus,withthosethatdonotcontainacompletesen-tencebeingfilteredoutusingthecriteriadefinedinSection3.3.1.Thisfilteringleaves8.4millionremainingpassages.Ifmodelscanbemisledbyrandomlyselectedtext,theymayalsobesusceptibletomorecarefullycrafteddeceptivecontent.Lastly,scrambledwordpassages,inspiredbyAlaofietal.[2],arecreatedbyrandomlysampling𝑛wordsfromtheMSMARCOv1passagecorpus.Typically,𝑛issettomatchthenumberofwordsinacorrespondingrandompassagetoallowforfaircomparisons.Otherwise,𝑛israndomlychosenwithintherangeof10≤𝑛≤100.Thesepassagesserveasexamplesofcompletelynon-relevantcontent,astheyareinherentlyincoherentandmeaningless.Unlikerandomandrelevantpassages,whichmaycontainsomecontextualinformation,scrambledwordpassageslackanylogicalstructure.However,asshowninourfindings,suchpassagescanoftenbemanipulatedtoappearrelevanttothestudiedmodels,highlightingtheirvulnerability.3.3.3Manipulations.Threepassagemanipulationsarestudied:sentenceinjection,queryinjection,andkeywordinjection.Sentenceinjectioninvolvesaddingnon-relevantorpotentiallymalicioustexttootherwiserelevantpassages.ThefocusisprimarilyoninjectingrandomsentencesfromtheMSMARCOpassagecorpusratherthanthehatefulsentencesdiscussedinSection3.3.1.Theobjectiveofanadversaryusingsentenceinjectionistoembednon-relevantcontentwithinapassagethatanIRmodelinitiallyconsidersrelevant.Ifthemodelcontinuestoperceivethepassageasrelevantafterinjection,theadversaryhassucceeded.Suchmaliciouscontentcanthenbeusedtoharmusersormisleadretrieval-augmentedgeneration(RAG)systems,potentiallythroughpromptinjectionattacks.Queryinjectionandkeywordinjection,ontheotherhand,areexaminedinthecontextofblack-hatSEO(searchengineoptimiza-tion)strategies,whichaimtomanipulateIRmodelsintoperceivingnon-relevantorlessrelevantpassagesashighlyrelevant.Inqueryinjection,theentirequeryisappendedtothepassage,whereaskeywordinjectioninvolvesinsertingkeytermsextractedfromthequeryafterexcludingstopwords.Bothtechniquesattempttoartificiallyboostthepassage’sperceivedrelevance.3.3.4CraftingExperimentalPassages.Adversarialpassagesareconstructedbasedonthetypeofinjection:sentence,query,orkeywordinjection.Textisinsertedatthestart,middle,orendofapassage.Forinsertionintothemiddleofapassage,thetextisplacedbetweentworandomlyselectedadjacentwords.Forqueryandkeywordinjection,fiverandompassagesandfivescrambledwordpassages(matchedinlengthtotherandompassages)areTable2:Examiningtheeffectofthelocationofinsertion(start,middle,orend)onattacksuccessrates(%)onDL19andDL20acrossqueryinjection,keywordinjection,andsentenceinjectionattacksandthreepassagetypes(random,scrambledword,andrelevant).Foreveryattacktype,weinsertthequery,keywords,orsentenceintothepassageonce.QueryInjectionKeywordInjectionSentenceInjectionRandomScrambledRandomScrambledRelevantStartMidEndStartMidEndStartMidEndStartMidEndStartMidEndRetrievers(R@1/R@5)BGE-base0.4/1.60.0/0.20.2/1.22.3/9.50.6/2.50.4/1.90.2/1.00.2/0.40.0/0.40.4/1.00.0/0.20.0/0.02.5/34.24.3/48.96.8/59.6BGE-large0.0/1.40.0/0.80.0/0.83.5/18.60.2/4.30.4/6.20.0/0.60.0/0.20.0/0.60.4/6.00.0/0.20.0/0.48.7/56.39.1/68.910.7/76.3E5-sup0.0/0.00.2/1.20.0/0.20.4/5.61.0/6.60.6/2.30.0/0.40.0/0.20.0/0.40.8/2.70.0/0.20.0/0.89.9/66.412.4/73.623.1/83.7E5-unsup6.8/18.40.2/3.322.1/37.320.4/43.12.9/8.02.5/9.71.2/5.40.2/0.63.9/10.54.7/15.50.2/2.10.6/2.93.5/33.213.2/59.415.9/53.8Arctic-base0.6/4.30.2/1.00.0/0.61.0/8.50.0/1.90.0/0.60.4/2.10.0/0.60.0/0.20.4/3.90.0/0.40.0/0.48.9/56.112.2/77.714.0/83.9Rerankers(R@1/R@5)MiniLM13/36.95.6/21.22.3/15.18.9/35.92.9/20.22.3/146.0/280.6/4.91.0/7.85.6/270.2/4.50.4/7.60.4/62.54.9/84.16.8/92.6MonoT5-base7.6/26.63.7/15.30.0/3.710.5/33.82.7/18.40.2/8.07.2/22.90.6/4.70.2/3.37.6/28.20.0/4.30.0/4.50.4/57.34.7/84.716.7/96.9MonoT5-large4.5/25.62.3/14.60.2/1.916.7/44.52.1/14.60.4/4.92.3/16.30.6/3.30.0/1.68.7/28.50.0/3.70.0/2.50.0/42.12.3/77.11.4/88.0RankT5-base1.9/8.54.5/17.92.5/12.64.5/21.94.7/25.44.5/21.40.6/6.81.0/5.61.0/8.92.9/15.31.6/8.52.1/16.77.2/74.44.9/85.414.2/97.3LLMJudges(S@3/S@2+)GPT4o0.0/0.00.0/0.00.0/0.00.0/0.00.0/0.00.2/0.20.2/0.20.0/0.00.0/0.00.2/0.20.0/0.00.2/0.653.2/97.931.7/94.630.6/96.1Llama-3.1(8B)0.6/1.40.4/3.70.2/0.40.0/0.80.0/1.40.0/4.70.8/2.10.2/5.40.0/0.80.0/1.20.0/1.00.0/22.322.6/59.525.1/70.817.6/59.8sampledforeachquery.Inqueryinjection,thequeryisadded1-3timestoeachpassageandinkeywordinjection,thequerykeywordsareinsertedthesamenumberoftimes.Forsentenceinjection,uptotworandomsentencesaresampledfivetimesandinsertedintotherelevantpassages.4RESULTS4.1AttackEffectivenessWefirstanalyzetheattacksuccessratesforqueryinjection,key-wordinjection,andrandomsentenceinjectionontheDL19andDL20datasets.Attacksuccessratesarereportedinthefollowingway:forretrieversandrerankers,wereportR@1andR@5,whichrepresenttheproportionofadversarialpassagesthatrankfirstandwithinthetopfiveamongallpassagesinthecorpus,respectively.ForLLMjudges,wereportS@3andS@2+,whereS@3indicatestheproportionofadversarialpassagesratedasperfectlyrelevant(givenascoreof3),andS@2+representstheproportionratedashighlyrelevant(givenascoreofatleast2).Wepresenttheattacksuccessratesacrossallmodelsforthethreeinjectiontypes:query,keyword,andsentenceinjection,inTable1,whichexaminestheimpactofrepetition,andTable2,whichinvestigatestheimpactofinjectionlocation.Ourfindingsrevealthatmodelsexhibitbroadvulnerabilitytoallattacktypes,thoughtheextentofvulnerabilityvariesdependingonthemodelandthespecificmanipulationtechnique.Notably,foreverymodel,thereexistsatleastoneattackconfiguration(definedbyinjectiontype,location,andrepetition)thatachieves:over20%attacksuccessratesinthestrictsuccesssetup(theadversarialpas-sageranksfirstinthecaseofretrieversandrerankersorscores3inthecaseofLLMjudges)andattacksuccessratesofover70%intherelaxedsuccesssetup(theadversarialpassageranksinthetop5inthecaseofretrieversandrerankersorscoresatleast2correspondingtohighlyrelevantinthecaseofLLMjudges).ThestudyofR@1(caseswheretheadversarialpassageranksfirst)isparticularlyinterestingbecausethosecasescorrespondtoadver-sarialpassageswithnon-relevantcontentthatscoreevenbetterthanrelevantpassagesfromtheretrievalcorpus.ComparingInjectionTypes.Fortheretrieversandrerankers,weobservethatqueryinjectionconsistentlysucceedsmoreoftenthankeywordinjectionacrossthedifferentinsertionlocationsandrep-etitions.Comparingsentenceinjectionwithqueryandkeywordinjection,weseethatrepeatingqueriesandkeywordsisneces-sarytohavequeryandkeywordinjectionsucceedmoreoftenthansentenceinjection.Althoughqueryinjection,keywordinjection,andsentenceinjectionallworkrelativelywellagainstretrieversandrerankers,theLLMjudgeshavealowvulnerabilitytowardsqueryinjectionandkeywordinjectionattacks.ForexampleforGPT4o,theattacksuccessratesforqueryinjectionarenon-zeroonlywheninvestigatingtheinjectionofqueriesintotheendofthepassagewheretheattacksuccessrateisonly0.2%.Similarly,forkeywordinjection,theattacksuccessratesremainnearzeroacrossallsettings.WithLlama-3.1(8B)wesimilarlyseerelativelylowattacksuccessratesforqueryandkeywordinjection.However,oneexceptionstandsout:forkeywordinjectionattheendofscrambledpassages,Llama-3.1(8B)yieldsasurprising22.3%S@2+.Incontrast,sentenceinjectionprovestobehighlyeffectiveagainstLLMjudges,particularlyGPT-4o,whereacrossalltestedconditions,theS@2+exceeds90%,andS@3remainsbetween30-60%.InvestigatingtheImpactofQuery,Keyword,andSentenceRepeti-tion.Table1showsthatforqueryandkeywordinjectionattacksonretrieversandrerankers,repeatingthequeriesandkeywordsuptothreetimesconsistentlyimprovesattacksuccessrates.Inthesemodels,simplyrepeatingkeyterms(queriesorkeywords)issufficienttopromoteadversarialpassagerankingseffectively.However,forsentenceinjection,addingmorerandomsentencesconsistentlyreducesattacksuccessforretrieversandrerankersbut,unexpectedly,increasesitforGPT-4oandLlama-3.1(8B)whenconsideringS@2+.Thebehaviourofretrieversandrerankersalignswithexpectations:introducingadditionalrandomsentencesdilutesTable3:Attacksuccessrates(%)forqueryinjectionattacks(injectingthequeryonceatthestart)acrossrandomandscrambledwordpassages.R@1/R@5arereportedforBGE-largeandMonoT5-largewhileS@3/S@2+isreportedforLlama-3.1(8B).DatasetBGE-largeMonoT5-largeLlama-3.1(8B)RandomScrambledRandomScrambledRandomScrambledDL190.0/0.92.8/17.74.2/27.414/51.20.0/0.50.0/0.5DL200.0/1.94.1/19.34.8/24.118.9/39.31.1/2.20.0/1.1CLIMATE-FEVER18.1/29.291.7/98.498.9/10099.9/1004.5/15.99.3/34.7DBPedia0.4/2.212.3/31.630.2/59.546.4/75.82.9/5.00.5/2.9FiQA2.8/6.848.1/75.474.6/9588.6/992.8/3.60.8/1.8NFCorpus3.5/6.834.5/57.950.8/82.362.3/87.11.2/3.70.4/2.1SciFact18.1/42.385.6/99.585.2/10093.2/1005.9/25.38.1/40.2TREC-COVID1.2/1.634.4/49.614.8/25.226/49.20.4/0.80.0/0.8therelevanceofthepassagetothequery,therebydecreasingattackeffectiveness.Incontrast,theresponseofLLMjudges,particularlyGPT-4o,iscounterintuitiveandchallengingtoexplain.InvestigatingtheImpactofLocation.Intuitively,amodelmayper-ceivepassagestobemorerelevantiftherelevantcontentappearsatthebeginning,whilenon-relevantinformationisplacedinthemiddleorattheend.Asaresult,sentenceinjectionattacksarelikelytobemoreeffectivewheninsertedinthemiddleorend,whereasqueryandkeywordinjectionattacksmayachievehighersuccesswhenplacedatthebeginning.Table2comparestheeffectivenessofdifferentinsertionlocations(start,middle,end).Theresultsindicatethatattacksaregenerallymostsuccessfulwhenrelevantcontentispositionedatthestartofthepassage.Generally,forretrieversandrerankers,queryandkeywordinjectionatthebeginningofarandomorscrambledpassageyieldsthehighestsuccessrates,whilesentenceinjectionprovesmoreeffectivewhenaddedtotheendofarelevantpassage.Interestingly,thebehaviorofLLMjudges,particularlyGPT-4o,isagaincounterintuitive.SentenceinjectionattacksaremosteffectiveagainstGPT-4owhenplacedatthestartofapassage,andforLlama-3.1(8B),theS@3ishigherwheninsertedatthestartratherthantheend.FortheLLMjudges,wedonotfocusonqueryandkeywordinjectionbecauseofthelowattacksuccessrateshowever,thereisasurprisinglyhighS@2+againstLlama-3.1(8B)forkeywordinjectionintotheendofscrambledwordpassages.Overall,similartotherepetitionexperiments,thebehaviorofLLMjudgesremainsdifficulttoexplainanddoesnotfollowtheexpectedandmorereasonablepatternsseeninretrieversandrerankers.ComparingModelVulnerability.Modeleffectivenessdoesnotnecessarilycorrelatewithresiliencetoattacks.WhileGPT4oiswidelyacceptedtobeastrongerLLMthanLlama-3.1(8B)(andastrongerLLMrelevancejudgeaswelatershowinTable10),Ta-bles1and2showthatitismorevulnerabletosentenceinjection.GPT-4oexhibitsthehighestattacksuccessratesforsentenceinjec-tionamongallevaluatedmodels.Similarly,whileMonoT5-largeisastrongerrerankerthanMonoT5-base[31]andBGE-largeisastrongerretrieverthanBGE-base[59],thelargermodelscanoftenhavehigherattacksuccessrates.AnotherinterestingobservationisthatE5-unsupfrequentlyshowshigherattacksuccessratesthanotherretrieverswhenfacingqueryandkeywordinjectionattacks,butittendstobelessvulnerableundersentenceinjection.Unlikeothermodelsthatreceivesupervisedfine-tuningforretrieval(forTable4:AttacksuccessratesonDL19andDL20forSentenceInjectioncomparingsentenceinjectionintorelevantpas-sagesfromthecorpusandgeneratedpassagesofroughly50,100,and200words.ModelRelevant(Corpus)Gen-50Gen-100Gen-200Retrievers(R@1/R@5)BGE-base2.5/34.219.6/49.324.1/55.723.7/59.4BGE-large8.7/56.331.8/65.235.7/70.134.8/72.8E5-sup9.9/66.435.3/67.838.8/72.627.6/63.9E5-unsup3.5/33.26.4/26.014.6/37.519.0/46.8Arctic-base8.9/56.127.2/55.936.1/68.940.6/75.1Rerankers(R@1/R@5)MiniLM0.4/62.535.5/68.541.0/76.739.6/74.8MonoT5-base0.4/57.332.6/70.135.5/74.430.3/70.3MonoT5-large0.0/42.123.7/60.225.2/64.925.4/69.3RankT5-base7.2/74.446.6/81.954.2/84.156.1/90.1LLMJudges(S@3/S@2+)GPT4o53.2/97.993.0/99.698.6/99.8100.0/100.0Llama-3.1(8B)22.6/59.51.0/94.60.4/95.323.3/96.1instance,usingMSMARCOpassagerankingdata[3]),E5-unsupistrainedonlythroughunsupervisedcontrastivelearning.Thislackofsupervisedtrainingmaymakeitmoresusceptibletosurface-levelmanipulationsinvolvingqueriesandkeywordswhilebeingrelativelymorerobusttosentenceinjections,whichlikelyrequiredeepersemanticcomprehensionofrelevantpassages.BEIRDatasets.Table3examinesqueryinjectionattacksforadiversesetofBEIRtasksonBGE-large,MonoT5-large,andLlama-3.1(8B).WeusethelargestretrieverandrerankermodelsanduseLlama-3.1(8B)insteadofGPT4otominimizecosts.Resultsconfirmthatthesemodelsarevulnerabletoqueryinjectiononmultipledomain-specifictasks.SciFactandCLIMATE-FEVERinparticu-larhaverelativelyhighattacksuccessrates.Notably,unliketheotherdatasets,eachofthesedatasetshasqueriesthatareverifiableclaims,whichmayexplainwhyqueryinjectionissosuccessful.InCLIMATE-FEVER,queriesarereal-worldclimatechange-relatedclaimscollectedfromtheinternet.InSciFact,queriesareexpert-writtenscientificclaims.ExaminingPassageTypes.AcrossTables1and2,andtheBEIRresultsinTable3,scrambledpassagesoftenyieldhigherattacksuccessratesthanrandompassagesforretrieversandrerankers.ThistrendisnotclearwiththeLLMjudgesthathavegenerallylowerattacksuccessratesforqueryinjectionandkeywordinjec-tion.TherearealsosomeexceptionsincludingMiniLMingeneral.However,overallthistrendpersistsanditisnotclearwhythisisthecase.Onehypothesisisthatpassagesofscrambledwordsaresemanticallyneuralandthusmoreinfluencedbyinjectedqueryterms,makingiteasierforthemodeltolatchontoquerysignalsandmistakenlyassignhigherrelevance.GeneratedPassages.Table4comparestheeffectivenessofsen-tenceinjectionattacksusingLLM-generatedpassagesagainsttop-rankedortop-scoringpassagesfromtheMSMARCOretrievalcor-pus.Togeneratepassageswithaspecifiedlength,wesimplypromptGPT4otogeneratepassagesofthatlengththatitconsidersperfectlyrelevantgiventhequery,whichhasbeenshowntoworkwellandgeneratepassagesofroughlythedesiredlength[49].Weobservethatadversarialsuccessratesaregenerallyhigherwithgenerated(a)QueryInjection(b)KeywordInjection(c)SentenceInjectionFigure2:OverlapinsuccessfuladversarialpassagesonDL19andDL20acrossthedifferentattacksettingsfortheBGE-largeretriever,theMonoT5-largereranker,andtheGPT4ojudge.passagesthanwiththerelevantpassagesfromthecorpus.Further,attacksuccessratestendtoincrease,althoughinconsistently,asthegeneratedpassagelengthsareincreased.Oneplausibleexplanationisthatthegeneratedpassagespreservecoherentandcompletecon-text,whichhasbeenshowntobenefitretrievaleffectiveness[46].Tanetal.[49]similarlysuggestthatLLMsprefercoherent,semanti-callyrichcontextsovershorterordisjointedpassagesfromretrievalcorpora.Anotherpotentialreasonforthehighersuccessrateswithlongerpassagesisthattheycontainagreaterproportionofrelevantcontentevenwhenrandomsentencesareinserted.However,weobservethatwhilepassagesfromtheMSMARCOcorpusaveragearound58wordsinlength,theLLM-generatedpassagesdesignedtobe50wordslongoftenachievehigherattacksuccessratesthanthecorpus-derivedpassages.TransferabilityofAdversarialExamples.Modelscanbeevaluatedusingthesameadversariallycraftedpassages(usinggeneratedrelevantpassagesinthecaseofsentenceinjection)todeterminewhethertheyarevulnerabletothesameattacksorifsuccessfuladversarialcasesareuniquetoeachmodel.Previousresearchhasshownthatadversarialexamplesdesignedforonemodelcanalsodeceiveothermodels[21,34].Thisphenomenonallowsadversariestoattackblack-boxmodelsbyfirstcraftingadversarialexamplesusingwhite-boxmodelsandrelyingontheirtransferability.Figure2presentsaVenndiagramillustratingtheoverlapofsuccessfuladversarialattacksamongthreemodels:theBGE-largeretriever,theMonoT5-largereranker,andtheGPT4ojudge.Theattacksincludequeryinjection,keywordinjection,andsentenceinjection.Weconsiderattacksuccessinthestrictsetting,wherefortheretrieverandthereranker,anattackissuccessfuliftheadversarialpassageranksfirst,andfortheLLMjudge,anattackissuccessfuliftheadversarialpassageattainsascoreof3.Ouranalysisencompassesallinjectionlocationsandcasesinvolvingquery,keyword,andsentencerepetition,asdiscussedinSection4.1.Theresultsshowthatsomeattacksuccesscasesareuniquetospe-cificmodels,whileothersaresharedacrosstwoorallthreemodels.Eachmodelexhibitsuniquevulnerabilitiesbutalsosharessomewithothers.GPT4oisgenerallynotveryvulnerabletoqueryandkeywordinjectionwithveryfewsuccessfuladversarialpassages.However,bothBGE-largeandMonoT5-largeshareasignificantnumberofsuccessfuladversarialpassagesfortheseattacktypes.Incontrast,GPT4oishighlyvulnerabletosentenceinjection,withalargenumberofsuccessfuladversarialpassages,manyofwhicharesharedwiththeothertwomodels.TheVenndiagramrevealsthateachmodelsharessomeadversarialpassages,withexamplesdistributedacrossallpossiblecategories,whetheruniquetoasinglemodel,sharedbetweentwomodels,orcommontoallthree.Thissuggeststhatifanadversaryaimstoattackanunknownblack-boxmodel,theycouldincreasetheirchancesofsuccessbyselectingad-versarialpassagesthathavebeeneffectiveagainstmultiplemodels.Overall,thesefindingsindicatethatwhilesomeadversarialattacksgeneralizeacrossmodels,othersremainmodel-specific.TargetedContentInjection.Table5comparesinsertingrandomtextversushatefultext.Intherandomsetting,sentencesaresam-pledfromtheMSMARCOpassagecorpus,whileinthehatefulset-ting,sentencesaresampledfromtheToxigentestsetasdescribedinSection3.3.1.InterestinglytheToxigentextoftenyieldshighersuccessratesbutsometimeslowersuccessrates.Itisnotclearwhytheattacksuccessratesarehigher,butthismaybebecauseofdif-ferencesinthedistributionofsentencelengths,despitethelowerandupperboundweapplyonsentencelengthsandotherfiltersweapplyonsentencesasdescribedinSection3.3.1.ForGPT4o,theattacksuccessrateislowerwheninsertingToxigentext.ThiscanbepartiallyexplainedbyAzureOpenAI’scontentmoderation,whichflaggedsomerequestsascontaininghatefulcontent,resultinginusassigningadefaultscoreof0.Overall,ourfindingssuggestthattheevaluatedmodelsdonotexhibitabiasagainsthatefultext,eventhoughtheyarguablyshould.SEOforSuboptimallyScoringPassages.Table6presentsanSEO-focusedscenariowhereinsertingthequeryonceatthebeginningofasuboptimallyrankingorscoringpassage(initiallyrankedat5thplaceorgivenarelevancescoreof2)oftenboostsittorank1orarelevancescoreof3.Successratesare54.6%forBGE-large,71.1%forMonoT5-large,and46.0%forGPT-4o.Injectingthequeryintoarandompassageisfarlesseffective.Thishighlightshowblack-hatSEOtacticscanexploitsimplemanipulationstoimprovesearchTable5:ComparingtheinsertionofrandomMSMARCOsentencesvshatefulsentencesfromToxigenforsentenceinjection.WepresentR@1/R@5forretrieversandrerankersandS@3/S@2+forLLMjudgesonDL19andDL20InjectionTypeBGE-baseBGE-largeE5-supE5-unsupArctic-baseMiniLM-RerankerMonoT5-baseMonoT5-largeRankT5-baseGPT4oLlama-3.1(8B)ToxigenSentenceInjection4.7/49.918.6/79.420.8/79.84.9/39.215.5/73.40.2/68.90.2/68.00.2/60.615.1/90.341.4/96.026.7/59.4MSMARCOSentenceInjection2.5/34.28.7/56.39.9/66.43.5/33.28.9/56.10.4/62.50.4/57.30.0/42.17.2/74.453.2/97.922.6/59.5Table6:ComparingQueryInjectiononDL19andDL20forLessRelevantPassages(Rank=5forretrieversandrerankersorScore=2forLLMjudges)andRandomPassages.Thequeryisinsertedonceintothestartofthepassage.AttackMethodPassageTypeBGE-largeMonoT5-largeGPT4oQueryInjectionLessRelevantPassage54.6/10071.1/10046.0/93.7QueryInjectionRandomPassage0.0/1.44.5/25.60.0/0.0rankings.Notably,inabout6%ofcases,addingthequerytolessrelevantpassagesreducesGPT-4o’srelevancejudgement,whichisarguablyundesiredbehaviour.4.2InvestigatingDefensesSinceretrievers,rerankers,andLLMjudgesareallvulnerabletocontentinjectionattacks,itiscrucialtoexploredefensestrate-giesthatdonotcompromisetheeffectivenessofIRpipelines.Weexaminethreeapproaches:classifier-baseddefense,fine-tuningem-beddingretrievalmodelsforrobustness,andpromptingLLMstobemoredefensive.Ourinvestigationcoverstwosettingsforsentenceinjection:acontent-agnosticsettingwhererandomMSMARCOsentencesareinjectedandahatefulsettingwheresentencesfromtheToxigentestsetareinjected.WesplitMSMARCOsentencesintoatrain/dev/testsplitandusethetestsetfromToxigenwhilesplittingthetrainingsetintotrain/devtoallowforfairevaluation.Fortrainingclassifiersandretrievalmodels,wecraftadversarialpassagesusingqueriesandtheirrelevantpassagesfromtheMS-MARCOv1passagerankingtrainingset.Foreveryquery,twotypesofadversarialpassagesaregeneratedontheflyineachtrainingstep:(1)injectingqueriesorkeywordsintorandomorscrambledpassages,and(2)insertingsentencesintorelevantpassages.Theinjectiontype(queryorkeyword),passagetype(randomorscram-bled),numberofinsertions(1-3forqueries/keywords,1-2forsen-tences),andinsertionposition(start,middle,orend)areselectedrandomlywithequalprobability.Eachtrainingbatchcontainsanequalnumberofqueries,relevantpassages,sentence-injectedadver-sarialpassages,andquery/keyword-injectedadversarialpassages.Tostudythevulnerabilityofthemodelstoinjectionattacks,wereporterrorratesfortheclassifiersandattacksuccessratesfortheretrieversacrossalldifferentattacksettingsstudiedinthisworkbasedoninjectiontype,passagetype,repetition,andlocation.EachattackscenarioisrepresentedequallyintheevaluationsetusingthemethodologydescribedinSection3.3.4.Evaluationsareconductedseparatelybyinjectiontypeand,inthecaseofsentenceinjection,byMSMARCOandToxigensentenceinjections.4.2.1ClassifierBasedDefense.Astraightforwarddefenseagainstadversarialpassagesisfilteringthemusingaclassifier.Weacknowl-edgethelimitationsofsuchclassifiers,ashighlightedinpreviouswork[5],particularlytheirinabilitytodetectadversarialpassageTable7:CorpusAdversarial(%)showstheproportionofpas-sagesineachdataset’scorpusclassifiedasadversarial.Theremainingcolumnsshowtheerrorrateoftheclassifieronadversarialpassagesbyattacktype.(a)ClassifierTrainedwithMSMARCOSentenceInjectionDatasetCorpusAdversarial(%)KeywordInjectionQueryInjectionSentenceInjection(MSMARCO)SentenceInjection(Toxigen)DL19(MSMARCO)1.160.030.050.780.93DL20(MSMARCO)1.160.250.000.492.22CLIMATE-FEVER0.280.020.500.700.58DBPedia0.240.081.110.761.12FiQA3.210.010.560.820.91NFCorpus1.290.472.221.040.93SciFact1.000.010.300.810.68TREC-COVID1.430.000.208.9312.73(b)ClassifierTrainedwithToxigenSentenceInjectionDatasetCorpusAdversarial(%)KeywordInjectionQueryInjectionSentenceInjection(MSMARCO)SentenceInjection(Toxigen)DL19(MSMARCO)0.660.050.1026.280.54DL20(MSMARCO)0.660.230.0224.811.60CLIMATE-FEVER0.361.1410.2018.160.53DBPedia0.250.072.4420.900.52FiQA12.640.012.9612.940.44NFCorpus1.070.534.5913.700.90SciFact1.450.6111.5612.160.76TREC-COVID1.220.000.9623.735.33typesnotrepresentedinthetrainingdata.However,classifier-baseddefensescanstillbevaluableforfilteringandanalysis.Wetraintwoclassifiersusinganswerdotai/ModernBERT-base[57]withalearningrateof1e-5,50warmupsteps,adropoutrateof0.1,andabatchsizeof32.Bothclassifiersaretrainedthesamewaywithqueryandkey-wordinjection,butoneclassifieristrainedonMSMARCOsentenceinjection,whiletheotheristrainedonToxigensentenceinjection.Bothclassifiersaimtodistinguishadversarialfromnon-adversarialtext,coveringbothpassagesandqueries.Table7presentstheproportionofpassagesmisclassifiedasadver-sarialwithinMSMARCOandBEIRcorpora,alongwithclassifierer-rorratesonadversarialpassages.Forsentenceinjection,adversarialpassagesaretakenasthetop-rankingBGE-baseretrievedpassagescontaininganinjectedsentence.Corpuspassagemisclassificationratesaregenerallybelow2%,exceptforFiQApassages,wheretheMSMARCO-trainedclassifiermisclassifiesaround3%,andtheToxigen-trainedclassifiermisclassifiesnearly13%.Weobservetheexistenceofcorpuspassagescombiningunrelatedsentencesthatconfuseclassifiers.Thesemayresultfromcorporacurationmethodswheretextsareextractedfromwebpageswithcomplexstructures.FiQApassages,inparticular,containuser-generatedcontentwithinformallanguageandswearwords,whichseemtoconfusetheToxigen-trainedclassifier.DespitetheToxigen-trainedclassifierbe-ingtrainedonspecificToxigenhatefulcontentratherthanrandomcontent,itoftenmistakenlyflagsmorecorporapassagesthantheMSMARCO-trainedclassifier,observedwiththedatasetsCLIMATE-FEVER,DBPedia,FiQA,andSciFact.Table8:R@1/R@5(%)fortheoriginalandfine-tunedBGE-baseretrievers.Thefine-tunedmodelsaretrainedtodiscountpassageswithMSMARCOandToxigensentenceinjection.DatasetBGE-baseTuned(MSMARCO)𝜏2=0.01(MSMARCO)𝜏2=0.011(Toxigen)𝜏2=0.01QueryInjectionDL199.0/19.30.1/0.10.9/2.60.1/0.1DL206.4/18.20.0/0.10.6/2.40.0/0.0CLIMATE-FEVER62.6/72.34.6/9.019.2/29.312.4/20DBPedia17.5/32.10.4/1.33.1/8.70.5/1.7FiQA43.4/56.90.9/2.47.0/14.91.3/3.3NFCorpus30.9/47.23.9/8.714.9/28.24.2/9.5SciFact62.7/80.34.7/12.120.3/37.39.1/21.3TREC-COVID39.2/47.50.1/0.41.9/4.50.2/0.6KeywordInjectionDL194.2/9.10.0/0.10.4/1.00.0/0.1DL201.3/6.10.0/0.10.2/1.20.0/0.1CLIMATE-FEVER48.1/60.91.1/2.515.3/262.4/5.1DBPedia8.8/19.40.2/0.71.9/5.90.2/0.7FiQA21.7/34.70.2/0.73.4/9.00.3/0.9NFCorpus26.3/42.93.4/7.813.7/27.33.6/8.5SciFact45.4/68.71.7/5.615.2/33.63.0/9.6TREC-COVID14.3/20.20.0/0.00.7/1.80.0/0.2SentenceInjection(MSMARCO)DL193.6/36.50.0/2.40.5/5.80.6/9.2DL204.0/41.20.3/3.10.9/6.82.0/14.3CLIMATE-FEVER22.8/79.10.7/4.11.1/8.52.6/13.6DBPedia14.4/64.10.3/3.10.6/8.72.2/15.3FiQA12.2/66.70.4/3.60.8/8.91.7/13.4NFCorpus27.3/86.93.2/18.34.6/29.311.7/37.9SciFact24.5/94.82.1/15.12.5/24.25.2/30.4TREC-COVID4.2/28.10.5/3.31.8/6.02.7/13.3SentenceInjection(Toxigen)DL193.9/48.30.6/4.00.3/8.10.2/0.8DL207.8/54.00.7/4.61.7/17.50.1/1.4CLIMATE-FEVER25.3/84.30.7/4.41.0/10.60.1/0.7DBPedia14.6/72.80.3/5.91.2/17.80.1/1.0FiQA11.8/72.70.8/6.01.4/17.10.2/1.3NFCorpus28.6/89.42.9/17.94.7/32.42.7/6.8SciFact25.1/96.61.7/14.62.3/25.61.5/4.8TREC-COVID4.3/31.00.6/3.11.2/6.61.4/3.5WeobservethattheToxigen-trainedclassifierhashigherer-rorsonMSMARCO-injectedpassagesbutlowererrorsonToxigen-injectedones,asitistrainedtodetectinjectedhatefulcontentratherthanrandomcontent.However,itexhibitsaslightlyhighererrorforToxigensentenceinjectionontheSciFactdataset.Bothclassi-fiersshowrelativelyhigherrorratesindetectingsentence-injectedpassageswithintheTREC-COVIDdataset.Forkeywordinjection,theMSMARCO-trainedclassifiermaintainserrorratesbelow0.5%,whereasqueryinjectionerrorscanreach2.2%.TheToxigen-trainedclassifier,however,strugglesevenmorewithqueryandkeywordinjections,particularlyinCLIMATE-FEVERandSciFact,whereer-rorsareover10%,despiteidenticaltrainingconditionsbetweenthetwoclassifiersapartfromsentenceinjectiondata.Thesefindingshighlightthefactthattrainingaclassifiertoidentifyadversarialpassagesisatrickyproblem,evenwhentheclassificationproblemismoreconstrainedinthecaseofsentenceinjectionwithparticularinjectedcontent.Althoughtheclassifiersmayhelpfilteroutmaliciouspassages,theymaymisssomemali-ciouspassagesandmistakenlyflagothers.Trainingtheclassifierswithmoreandmorediversequeriesandpassages,besidesthosefromtheMSMARCOtrainingsetmayhelpfurthermitigateerrors.4.2.2TrainingEmbeddingModelsforRobustness.Anotherdefensestrategyisfine-tuningretrieverstodiscountadversarialpassages.Table9:Retrievaleffectiveness(measuredbyNDCG@10)fortheBGE-baseretrieverandfine-tunedretrievers.The(Noadv.)settingtunestheretrieverwiththebasicInfoNCEloss.DatasetBGE-baseTuned(Noadv.)(Random)𝜏2=0.01(Random)𝜏2=0.011(Toxigen)𝜏2=0.01DL190.7020.7090.7090.7100.699DL200.6770.6920.6790.6980.693CLIMATE-FEVER0.3120.2360.1690.1830.170DBPedia0.4070.3790.3610.3770.371FiQA0.4060.3960.2580.3080.266NFCorpus0.3730.3720.3280.3530.351SciFact0.7410.7460.6380.6910.713TREC-COVID0.7810.7530.6960.7290.750Wefine-tunetheBGE-baseretrievertopushawaytheembeddingsofadversarialpassagesfromthequeryembeddingsusingamodifiedcontrastivelossthatisasimpleadaptationoftheInfoNCE[33]lossusedforthetrainingofembeddingmodelsforretrieval:−1𝑛𝑛∑︁𝑖=1log𝑒⟨𝑞𝑖,𝑝𝑖⟩𝜏(cid:205)𝑛𝑗=1𝑒⟨𝑞𝑖,𝑝𝑗⟩𝜏(1)Inthedescribedformulation,𝑞𝑖isanormalizedqueryembeddingand𝑝𝑖isanormalizedpassageembeddingforthecorrespondingrelevantpassage.Thelosstakesadvantageofin-batchnegativestocontrastivelylearnrepresentations.Weuseatemperatureparame-terof𝜏=0.01asthisistypicallyusedforthetrainingofembeddingmodelsforretrieval.Wesimplyaddtermsinthedenominatortocontrastqueryembeddingswithadversarialpassageembeddings:−1𝑛𝑛∑︁𝑖=1log𝑒⟨𝑞𝑖,𝑝𝑖⟩𝜏(cid:205)𝑛𝑗=1𝑒⟨𝑞𝑖,𝑝𝑗⟩𝜏+(cid:205)𝑛𝑗=1𝑒⟨𝑞𝑖,𝑥𝑗⟩𝜏′+(cid:205)𝑛𝑗=1𝑒⟨𝑞𝑖,𝑦𝑗⟩𝜏′(2)Inthemodifiedloss,𝑥𝑖istheembeddingoftheadversarialpassagecreatedwithsentenceinjectionforpassage𝑖while𝑦𝑖istheem-beddingoftheadversarialpassagecreatedwithqueryinjectionorkeywordinjectionusingquery𝑖.WeinitializetheembeddingmodelswithBGE-base,trainedwithalearningrateof4e-5,alearningratewarmupof10steps,adropoutrateof0.1,andabatchsizeof8192.WemakeuseofGradCache[14]toallowforlargerbatchsizes.WetrainbothanembeddingmodelwiththebasicInfoNCElossfunctionandthemodifiedlossfunctiontocontrastwithadversarialpassagestotestforanypossibledropsinretrievaleffectivenesswhentrainingwiththemodifiedobjective.Wealsotesttheimpactofthetuningofthe𝜏′parameter,testing𝜏′=0.01and0.011.Table8firstshowstheattacksuccessrateswiththetrainedmodels.Trainingwiththemodifiedlosssucceedsindecreasingtheattacksuccessrates.Theattacksuccessratesarelowerwhenfine-tunedwiththemodifiedlossthanwithBGE-baseacrosstheboard.Asexpected,attacksuccessratesarelowerwith𝜏′=0.01thanwith𝜏′=0.011.Similartotheclassifiers,themodeltrainedwithToxigensentenceinjectionhasahigherattacksuccessrateonpassageswithMSMARCOsentenceinjectioncomparedtotheMSMARCO-trainedmodelbuttendstohavelowerattacksuccessratesonpassageswithToxigensentenceinjectionasitwastrainedtospecificallydiscountTable10:(MAE)MeanabsoluteerrorbetweenLLMjudge-mentsandhumanrelevancejudgementsalongwithS@3/S@2+resultsacrossattacktypesundertwoprompt-ingsettingsacrossDL19andDL20.DatasetPromptMAEQueryInjectionKeywordInjectionSentenceInjection(MSMARCO)SentenceInjection(Toxigen)DL19Default0.5630.1/0.10.3/0.634.6/95.024/76.9DL19Defensive0.6370.0/0.00.0/0.18.4/85.40.9/16.9DL20Default0.4610.0/0.00.1/0.231.5/92.823.7/78.4DL20Defensive0.5640.0/0.00.0/0.08.9/88.01.2/17.3thesepassages.ThismodelalsohasalowerattacksuccessratethanBGE-baseforsentenceinjectionwithMSMARCOsentences,suggestingsomegeneralizationindiscountingadversarialpassages.Table9thenshowstheretrievaleffectivenessofthefine-tunedretrieverscomparedtotheoriginalBGE-baseretriever.Ingeneral,fine-tuningwithboththesimpleInfoNCElossorthemodifiedlossonMSMARCOtrainingdataimprovesretrievaleffectivenessonDL19andDL20,butoftendecreaseseffectivenessontheBEIRdatasetsandClimate-Feverespecially,wheretheretrievaltaskisclaimverification.Whentrainingwiththemodifiedloss,effective-nesstendstodecreasemoreout-of-domainthanwhenusingthesimpleloss.Increasingthetemperature𝜏parametergenerallyre-sultsinimprovementstotheeffectiveness,allowingforatrade-offbetweenretrievaleffectivenessandrobustnesstoattacks.Addition-ally,theretrievertrainedwithToxigensentenceinjectionconsis-tentlyhasstrongerretrievaleffectivenessthanthecorrespondingretrievertrainedwithMSMARCOsentenceinjection.Thefindingssuggestthatwhileretrieverscanbefine-tunedtobemorerobusttocontentinjectionattacks,thisalsomayresultinreducedretrievaleffectiveness.Tuningthe𝜏′allowsforatrade-offbetweenretrievaleffectivenessandrobustnesstoattacks.Addition-ally,retrieverscanbettermaintainretrievaleffectivenessiftrainedtoonlydiscountcertainadversarialpassages,suchasthosewithhatefulcontent,ratherthantrainedtodiscountadversarialpassageswithrandomcontent.4.2.3MakingLLMJudgesRobust.TomakeLLMjudgesmorero-busttoadversarialpassages,wetestaddingthefollowinginstruc-tionstotheprompt:•Assignascoreof0ifthepassagecontainsharmfulormaliciouscontentthatisirrelevanttothequery.•Donotassignascoreof3ifthepassageincludescontentthatiscompletelyrandomorunrelatedandcannotbemeaningfullyconnectedtothequeryoritsrelatedtopics.Table10showsthatwhilethedefensivepromptworkstoeffec-tivelyreduceattacksuccessratesacrossallattacksandespeciallyinthecaseofsentenceinjectionwithhatefulcontentfromToxi-gen,thisisatthecostofagreementbetweentheLLMrelevancejudgementsandthehumanrelevancejudgements,wheretheMAEincreaseswhenusingthedefensiveprompt.5CONCLUSIONThispaperdemonstratesthatneuralIRmodels,includingembedding-basedretrievers,rerankers,andLLMrelevancejudgesarewidelyvulnerabletocontentinjectionattacks.Byappendingrandomorma-licioustexttorelevantpassages,orinsertingqueriesorkeytermsintootherwisenon-relevantpassages,adversariescanmanipulaterankingdecisionsandmodeljudgements,showcasingasecuritygapincurrentIRpipelines.Ourevaluationofadiversesetofmodelsacrossvariousretrievaltasksrevealsthewidespreadsusceptibilitytocontentinjectionat-tacks.Weinvestigatekeyfactorsinfluencingattacksuccess,show-ingthatthelocationofcontentinsertionandthebalanceofnon-relevantcontenttorelevantcontentwithinpassagescanaffecttheprobabilityofadversarialpassagessucceedinginfoolingIRmodels.Additionally,ouranalysisuncoversacriticalgapinmodelbehavior,asthestudiedmodelsgenerallydonotexhibitbiasagainsthatefulcontent.Weanalyzetheoverlapofsuccessfuladversarialpassagesacrossdifferentmodelsandfindthatsomeadversarialpassagesareuniquetospecificmodels,successfullydeceivingonlythem,whileothersaremoregeneralandcanfoolmultiplemodels.Moreover,experimentswithLLM-generatedpassagesproveeffectiveinfool-ingmodelsaswell,removingtheneedtofindspecificpassagesthattargetmodelsandpointingtoariskofattacksinblack-boxsettingswithlimitedpriorknowledgeofthemodels.Empiricalfindingsshowthatwhilesupervisedclassifiers,care-fullyfine-tunedretrievers,andmoredefensivelypromptedLLMscanmitigatesomeattacks,thesedefensesareimperfectandusu-allycomeatthecostofsomeeffectiveness.Continuedresearchintomitigatingattacksagainstadversarialmanipulationsiscriti-calforensuringthatusersofsearchsystemsreceiveaccurateandtrustworthyinformation.ACKNOWLEDGMENTSThisresearchwassupportedinpartbytheNaturalSciencesandEn-gineeringResearchCouncil(NSERC)ofCanada.AdditionalfundingisprovidedbyMicrosoftviatheAcceleratingFoundationModelsResearchprogram.REFERENCES[1]JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,IlgeAkkaya,Floren-ciaLeoniAleman,DiogoAlmeida,JankoAltenschmidt,SamAltman,ShyamalAnadkat,etal.2023.GPT-4TechnicalReport.arXiv:2303.08774(2023).[2]MarwahAlaofi,PaulThomas,FalkScholer,andMarkSanderson.2024.LLMscanbeFooledintoLabellingaDocumentasRelevant:bestcafénearme;thispaperisperfectlyrelevant.InProceedingsofthe2024AnnualInternationalACMSIGIRConferenceonResearchandDevelopmentinInformationRetrievalintheAsiaPacificRegion(Tokyo,Japan)(SIGIR-AP2024).AssociationforComputingMachinery,NewYork,NY,USA,32–41.https://doi.org/10.1145/3673791.3698431[3]PayalBajaj,DanielCampos,NickCraswell,LiDeng,JianfengGao,XiaodongLiu,RanganMajumder,AndrewMcNamara,BhaskarMitra,TriNguyen,MirRosenberg,XiaSong,AlinaStoica,SaurabhTiwary,andTongWang.2016.MSMARCO:AHumanGeneratedMAchineReadingCOmprehensionDataset.arXiv:1611.09268v3(2016).[4]VeraBoteva,DemianGholipour,ArtemSokolov,andStefanRiezler.2016.AFull-TextLearningtoRankDatasetforMedicalInformationRetrieval.InAdvancesinInformationRetrieval,NicolaFerro,FabioCrestani,Marie-FrancineMoens,JosianeMothe,FabrizioSilvestri,GiorgioMariaDiNunzio,ClaudiaHauff,andGianmariaSilvello(Eds.).SpringerInternationalPublishing,Cham,716–722.[5]XuanangChen,BenHe,LeSun,andYingfeiSun.2023.DefenseofAdversar-ialRankingAttackinTextRetrieval:BenchmarkandBaselineviaDetection.arXiv:2307.16816(2023).[6]XuanangChen,BenHe,ZhengYe,LeSun,andYingfeiSun.2023.TowardsImper-ceptibleDocumentManipulationsagainstNeuralRankingModels.InFindingsoftheAssociationforComputationalLinguistics:ACL2023,AnnaRogers,JordanBoyd-Graber,andNaoakiOkazaki(Eds.).AssociationforComputationalLin-guistics,Toronto,Canada,6648–6664.https://doi.org/10.18653/v1/2023.findings-acl.416[7]NickCraswell,BhaskarMitra,EmineYilmaz,andDanielCampos.2020.OverviewoftheTREC2020DeepLearningTrack.InProceedingsoftheTwenty-NinthTextREtrievalConferenceProceedings(TREC2020).Gaithersburg,Maryland.[8]NickCraswell,BhaskarMitra,EmineYilmaz,DanielCampos,andEllenM.Voorhees.2019.OverviewoftheTREC2019DeepLearningTrack.InProceedingsoftheTwenty-EighthTextREtrievalConferenceProceedings(TREC2019).Gaithers-burg,Maryland.[9]JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2019.BERT:Pre-trainingofDeepBidirectionalTransformersforLanguageUnderstanding.InProceedingsofthe2019ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,Volume1(LongandShortPapers),JillBurstein,ChristyDoran,andThamarSolorio(Eds.).AssociationforComputationalLinguistics,Minneapolis,Minnesota,4171–4186.[10]ThomasDiggelmann,JordanBoyd-Graber,JannisBulian,MassimilianoCiaramita,andMarkusLeippold.2020.CLIMATE-FEVER:ADatasetforVerificationofReal-WorldClimateClaims.arXiv:2012.00614(2020).[11]AbhimanyuDubey,AbhinavJauhri,AbhinavPandey,AbhishekKadian,AhmadAl-Dahle,AieshaLetman,AkhilMathur,AlanSchelten,AmyYang,AngelaFan,etal.2024.TheLlama3HerdofModels.arXiv:2407.21783(2024).[12]JavidEbrahimi,AnyiRao,DanielLowd,andDejingDou.2017.Hotflip:White-boxadversarialexamplesfortextclassification.arXiv:1712.06751(2017).[13]ThibaultFormal,BenjaminPiwowarski,andStéphaneClinchant.2021.SPLADE:SparseLexicalandExpansionModelforFirstStageRanking.InProceedingsofthe44thInternationalACMSIGIRConferenceonResearchandDevelopmentinInformationRetrieval(VirtualEvent,Canada)(SIGIR’21).AssociationforComputingMachinery,NewYork,NY,USA,2288–2292.https://doi.org/10.1145/3404835.3463098[14]LuyuGao,YunyiZhang,JiaweiHan,andJamieCallan.2021.ScalingDeepContrastiveLearningBatchSizeunderMemoryLimitedSetup.InProceedingsofthe6thWorkshoponRepresentationLearningforNLP.[15]IanJGoodfellow,JonathonShlens,andChristianSzegedy.2014.ExplainingandHarnessingAdversarialExamples.arXiv:1412.6572(2014).[16]ThomasHartvigsen,SaadiaGabriel,HamidPalangi,MaartenSap,DipankarRay,andEceKamar.2022.ToxiGen:ALarge-ScaleMachine-GeneratedDatasetforAdversarialandImplicitHateSpeechDetection.InProceedingsofthe60thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers),SmarandaMuresan,PreslavNakov,andAlineVillavicencio(Eds.).AssociationforComputationalLinguistics,Dublin,Ireland,3309–3326.https://doi.org/10.18653/v1/2022.acl-long.234[17]FaeghehHasibi,FedorNikolaev,ChenyanXiong,KrisztianBalog,SveinErikBratsberg,AlexanderKotov,andJamieCallan.2017.DBpedia-Entityv2:ATestCollectionforEntitySearch.InProceedingsofthe40thInternationalACMSIGIRConferenceonResearchandDevelopmentinInformationRetrieval(Shinjuku,Tokyo,Japan)(SIGIR’17).AssociationforComputingMachinery,NewYork,NY,USA,1265–1268.https://doi.org/10.1145/3077136.3080751[18]MatthewHonnibal,InesMontani,SofieVanLandeghem,andAdrianeBoyd.2020.spaCy:Industrial-strengthNaturalLanguageProcessinginPython.(2020).https://doi.org/10.5281/zenodo.1212303[19]VladimirKarpukhin,BarlasOguz,SewonMin,PatrickLewis,LedellWu,SergeyEdunov,DanqiChen,andWen-tauYih.2020.DensePassageRetrievalforOpen-DomainQuestionAnswering.InProceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP),BonnieWebber,TrevorCohn,YulanHe,andYangLiu(Eds.).AssociationforComputationalLinguistics,Online,6769–6781.[20]JiaweiLiu,YangyangKang,DiTang,KaisongSong,ChanglongSun,XiaofengWang,WeiLu,andXiaozhongLiu.2022.Order-Disorder:ImitationAdversarialAttacksforBlack-boxNeuralRankingModels.InProceedingsofthe2022ACMSIGSACConferenceonComputerandCommunicationsSecurity.2025–2039.[21]YanpeiLiu,XinyunChen,ChangLiu,andDawnSong.2016.DelvingintoTransferableAdversarialExamplesandBlack-boxAttacks.arXiv:1611.02770(2016).[22]YiLiu,GeleiDeng,YuekangLi,KailongWang,ZihaoWang,XiaofengWang,TianweiZhang,YepangLiu,HaoyuWang,YanZheng,etal.2023.PromptInjectionattackagainstLLM-integratedApplications.arXiv:2306.05499(2023).[23]YupeiLiu,YuqiJia,RunpengGeng,JinyuanJia,andNeilZhenqiangGong.2023.FormalizingandBenchmarkingPromptInjectionAttacksandDefenses.arXiv:2310.12815(2023).[24]Yu-AnLiu,RuqingZhang,JiafengGuo,MaartendeRijke,WeiChen,YixingFan,andXueqiCheng.2023.Black-boxAdversarialAttacksagainstDenseRetrievalModels:AMulti-viewContrastiveLearningMethod.InProceedingsofthe32ndACMInternationalConferenceonInformationandKnowledgeManagement.1647–1656.[25]Yu-AnLiu,RuqingZhang,MingkunZhang,WeiChen,MaartendeRijke,JiafengGuo,andXueqiCheng.2024.Perturbation-InvariantAdversarialTrainingforNeuralRankingModels:ImprovingtheEffectiveness-RobustnessTrade-Off.InProceedingsoftheAAAIConferenceonArtificialIntelligence,Vol.38.8832–8840.[26]AleksanderMadry,AleksandarMakelov,LudwigSchmidt,DimitrisTsipras,andAdrianVladu.2017.TowardsDeepLearningModelsResistanttoAdversarialAttacks.arXiv:1706.06083(2017).[27]MacedoMaia,SiegfriedHandschuh,AndréFreitas,BrianDavis,RossMcDer-mott,ManelZarrouk,andAlexandraBalahur.2018.WWW’18OpenChallenge:FinancialOpinionMiningandQuestionAnswering.InCompanionProceedingsoftheTheWebConference2018(Lyon,France)(WWW’18).InternationalWorldWideWebConferencesSteeringCommittee,RepublicandCantonofGeneva,CHE,1941–1942.https://doi.org/10.1145/3184558.3192301[28]ChristopherD.Manning,PrabhakarRaghavan,andHinrichSchütze.2008.Intro-ductiontoInformationRetrieval.CambridgeUniversityPress,USA.[29]LukeMerrick,DanmeiXu,GauravNuti,andDanielCampos.2024.Arctic-Embed:Scalable,Efficient,andAccurateTextEmbeddingModels.arXiv:2405.05374(2024).[30]RodrigoNogueiraandKyunghyunCho.2019.PassageRe-rankingwithBERT.arXiv:1901.04085(2019).[31]RodrigoNogueira,ZhiyingJiang,andJimmyLin.2020.DocumentRankingwithaPretrainedSequence-to-SequenceModel.arXiv:2003.06713(2020).[32]RodrigoNogueira,WeiYang,KyunghyunCho,andJimmyLin.2019.Multi-StageDocumentRankingwithBERT.arXiv:1910.14424(2019).[33]AaronvandenOord,YazheLi,andOriolVinyals.2018.RepresentationLearningwithContrastivePredictiveCoding.arXiv:1807.03748(2018).[34]NicolasPapernot,PatrickMcDaniel,andIanGoodfellow.2016.TransferabilityinMachineLearning:fromPhenomenatoBlack-BoxAttacksusingAdversarialSamples.arXiv:1605.07277(2016).[35]NicolasPapernot,PatrickMcDaniel,IanGoodfellow,SomeshJha,ZBerkayCelik,andAnanthramSwami.2017.PracticalBlack-BoxAttacksagainstMachineLearning.InProceedingsofthe2017ACMonAsiaconferenceoncomputerandcommunicationssecurity.506–519.[36]AndrewParry,MaikFröbe,SeanMacAvaney,MartinPotthast,andMatthiasHagen.2024.AnalyzingAdversarialAttacksonSequence-to-SequenceRelevanceModels.InEuropeanConferenceonInformationRetrieval.Springer,286–302.[37]RonakPradeep,SahelSharifymoghaddam,andJimmyLin.2023.RankVicuna:Zero-ShotListwiseDocumentRerankingwithOpen-SourceLargeLanguageModels.arXiv:2309.15088(2023).[38]RonakPradeep,SahelSharifymoghaddam,andJimmyLin.2023.RankZephyr:EffectiveandRobustZero-ShotListwiseRerankingisaBreeze!arXiv:2312.02724(2023).[39]ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,YanqiZhou,WeiLi,andPeterJLiu.2020.ExploringtheLimitsofTransferLearningwithaUnifiedText-to-TextTransformer.Journalofmachinelearningresearch21,140(2020),1–67.[40]NisargRavalandManishaVerma.2020.Onewordatatime:adversarialattacksonretrievalmodels.arXiv:2008.02197(2020).[41]NilsReimersandIrynaGurevych.2019.Sentence-BERT:SentenceEmbeddingsusingSiameseBERT-Networks.InProceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLanguageProcessingandthe9thInternationalJointConfer-enceonNaturalLanguageProcessing(EMNLP-IJCNLP),KentaroInui,JingJiang,VincentNg,andXiaojunWan(Eds.).AssociationforComputationalLinguistics,HongKong,China,3982–3992.[42]AvitalShafran,RoeiSchuster,andVitalyShmatikov.2024.MachineAgainsttheRAG:JammingRetrieval-AugmentedGenerationwithBlockerDocuments.arXiv:2406.05870(2024).[43]CongzhengSong,AlexanderRush,andVitalyShmatikov.2020.AdversarialSemanticCollisions.InProceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP),BonnieWebber,TrevorCohn,YulanHe,andYangLiu(Eds.).AssociationforComputationalLinguistics,Online,4198–4210.https://doi.org/10.18653/v1/2020.emnlp-main.344[44]WeiweiSun,LingyongYan,XinyuMa,ShuaiqiangWang,PengjieRen,ZhuminChen,DaweiYin,andZhaochunRen.2023.IsChatGPTGoodatSearch?In-vestigatingLargeLanguageModelsasRe-RankingAgents.InProceedingsofthe2023ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,HoudaBouamor,JuanPino,andKalikaBali(Eds.).Singapore,14918–14937.[45]ChristianSzegedy,WojciechZaremba,IlyaSutskever,JoanBruna,DumitruErhan,IanGoodfellow,andRobFergus.2013.Intriguingpropertiesofneuralnetworks.arXiv:1312.6199(2013).[46]ManveerSinghTamber,RonakPradeep,andJimmyLin.2023.Pre-processingMatters!ImprovedWikipediaCorporaforOpen-DomainQuestionAnswering.InAdvancesinInformationRetrieval:45thEuropeanConferenceonInformationRetrieval,ECIR2023,Dublin,Ireland,April2–6,2023,Proceedings,PartIII(Dublin,Ireland).Springer-Verlag,Berlin,Heidelberg,163–176.https://doi.org/10.1007/978-3-031-28241-6_11[47]ManveerSinghTamber,RonakPradeep,andJimmyLin.2023.ScalingDown,LiT-tingUp:EfficientZero-ShotListwiseRerankingwithSeq2seqEncoder-DecoderModels.arXiv:2312.16098(2023).[48]ManveerSinghTamber,JasperXian,andJimmyLin.2024.Can’tHideBehindtheAPI:StealingBlack-BoxCommercialEmbeddingModels.arXiv:2406.09355(2024).[49]HexiangTan,FeiSun,WanliYang,YuanzhuoWang,QiCao,andXueqiCheng.2024.BlindedbyGeneratedContexts:HowLanguageModelsMergeGeneratedandRetrievedContextsforOpen-DomainQA?arXiv:2401.11911(2024).[50]NandanThakur,NilsReimers,AndreasRücklé,AbhishekSrivastava,andIrynaGurevych.2021.BEIR:AHeterogenousBenchmarkforZero-shotEvaluationofInformationRetrievalModels.arXiv:2104.08663(2021).[51]PaulThomas,SethSpielman,NickCraswell,andBhaskarMitra.2024.Largelanguagemodelscanaccuratelypredictsearcherpreferences.InProceedingsofthe47thInternationalACMSIGIRConferenceonResearchandDevelopmentinInformationRetrieval.1930–1940.[52]ShivaniUpadhyay,RonakPradeep,NandanThakur,NickCraswell,andJimmyLin.2024.UMBRELA:UMbrelaisthe(Open-SourceReproductionofthe)BingRELevanceAssessor.arXiv:2406.06519(2024).[53]EllenVoorhees,TasmeerAlam,StevenBedrick,DinaDemner-Fushman,WilliamRHersh,KyleLo,KirkRoberts,IanSoboroff,andLucyLuWang.2021.TREC-COVID:constructingapandemicinformationretrievaltestcollection.InACMSIGIRForum,Vol.54.ACMNewYork,NY,USA,1–12.[54]DavidWadden,ShanchuanLin,KyleLo,LucyLuWang,MadeleinevanZuylen,ArmanCohan,andHannanehHajishirzi.2020.FactorFiction:VerifyingScientificClaims.InProceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP),BonnieWebber,TrevorCohn,YulanHe,andYangLiu(Eds.).AssociationforComputationalLinguistics,Online,7534–7550.https://doi.org/10.18653/v1/2020.emnlp-main.609[55]LiangWang,NanYang,XiaolongHuang,BinxingJiao,LinjunYang,DaxinJiang,RanganMajumder,andFuruWei.2022.TextEmbeddingsbyWeakly-SupervisedContrastivePre-training.arXiv:2212.03533(2022).[56]WenhuiWang,FuruWei,LiDong,HangboBao,NanYang,andMingZhou.2020.MiniLM:DeepSelf-AttentionDistillationforTask-AgnosticCompressionofPre-TrainedTransformers.AdvancesinNeuralInformationProcessingSystems33(2020),5776–5788.[57]BenjaminWarner,AntoineChaffin,BenjaminClavié,OrionWeller,OskarHall-ström,SaidTaghadouini,AlexisGallagher,RajaBiswas,FaisalLadhak,TomAarsen,etal.2024.Smarter,Better,Faster,Longer:AModernBidirectionalEn-coderforFast,MemoryEfficient,andLongContextFinetuningandInference.arXiv:2412.13663(2024).[58]ChenWu,RuqingZhang,JiafengGuo,MaartenDeRijke,YixingFan,andXueqiCheng.2023.PRADA:PracticalBlack-boxAdversarialAttacksagainstNeuralRankingModels.ACMTrans.Inf.Syst.41,4,Article89(April2023),27pages.https://doi.org/10.1145/3576923[59]ShitaoXiao,ZhengLiu,PeitianZhang,NiklasMuennighoff,DefuLian,andJian-YunNie.2024.C-pack:Packedresourcesforgeneralchineseembeddings.InProceedingsofthe47thInternationalACMSIGIRConferenceonResearchandDevelopmentinInformationRetrieval.641–649.[60]ZexuanZhong,ZiqingHuang,AlexanderWettig,andDanqiChen.2023.Poi-soningRetrievalCorporabyInjectingAdversarialPassages.arXiv:2310.19156(2023).[61]HongleiZhuang,ZhenQin,RolfJagerman,KaiHui,JiMa,JingLu,JianmoNi,XuanhuiWang,andMichaelBendersky.2023.RankT5:Fine-TuningT5forTextRankingwithRankingLosses.InProceedingsofthe46thInternationalACMSIGIRConferenceonResearchandDevelopmentinInformationRetrieval.2308–2313.[62]WeiZou,RunpengGeng,BinghuiWang,andJinyuanJia.2024.PoisonedRAG:KnowledgeCorruptionAttackstoRetrieval-AugmentedGenerationofLargeLanguageModels.arXiv:2402.07867(2024).
