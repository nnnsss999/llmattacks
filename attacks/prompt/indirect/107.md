---
attack_type: indirect_pi
source_url: http://arxiv.org/abs/2204.14198
---

[2204.14198] Flamingo: a Visual Language Model for Few-Shot Learning





























  



[Skip to main content](#content)


[![Cornell University](/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)

We gratefully acknowledge support from the Simons Foundation, [member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.
[Donate](https://info.arxiv.org/about/donate.html)

[![arxiv logo](/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](/) > [cs](/list/cs/recent) > arXiv:2204.14198

[Help](https://info.arxiv.org/help) | [Advanced Search](https://arxiv.org/search/advanced)

All fields
Title
Author
Abstract
Comments
Journal reference
ACM classification
MSC classification
Report number
arXiv identifier
DOI
ORCID
arXiv author ID
Help pages
Full text

Search

[![arXiv logo](/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)

[![Cornell University Logo](/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)

open search

GO

open navigation menu

## quick links

* [Login](https://arxiv.org/login)
* [Help Pages](https://info.arxiv.org/help)
* [About](https://info.arxiv.org/about)



# Computer Science > Computer Vision and Pattern Recognition

**arXiv:2204.14198** (cs)

[Submitted on 29 Apr 2022 ([v1](https://arxiv.org/abs/2204.14198v1)), last revised 15 Nov 2022 (this version, v2)]

# Title:Flamingo: a Visual Language Model for Few-Shot Learning

Authors:[Jean-Baptiste Alayrac](https://arxiv.org/search/cs?searchtype=author&query=Alayrac,+J), [Jeff Donahue](https://arxiv.org/search/cs?searchtype=author&query=Donahue,+J), [Pauline Luc](https://arxiv.org/search/cs?searchtype=author&query=Luc,+P), [Antoine Miech](https://arxiv.org/search/cs?searchtype=author&query=Miech,+A), [Iain Barr](https://arxiv.org/search/cs?searchtype=author&query=Barr,+I), [Yana Hasson](https://arxiv.org/search/cs?searchtype=author&query=Hasson,+Y), [Karel Lenc](https://arxiv.org/search/cs?searchtype=author&query=Lenc,+K), [Arthur Mensch](https://arxiv.org/search/cs?searchtype=author&query=Mensch,+A), [Katie Millican](https://arxiv.org/search/cs?searchtype=author&query=Millican,+K), [Malcolm Reynolds](https://arxiv.org/search/cs?searchtype=author&query=Reynolds,+M), [Roman Ring](https://arxiv.org/search/cs?searchtype=author&query=Ring,+R), [Eliza Rutherford](https://arxiv.org/search/cs?searchtype=author&query=Rutherford,+E), [Serkan Cabi](https://arxiv.org/search/cs?searchtype=author&query=Cabi,+S), [Tengda Han](https://arxiv.org/search/cs?searchtype=author&query=Han,+T), [Zhitao Gong](https://arxiv.org/search/cs?searchtype=author&query=Gong,+Z), [Sina Samangooei](https://arxiv.org/search/cs?searchtype=author&query=Samangooei,+S), [Marianne Monteiro](https://arxiv.org/search/cs?searchtype=author&query=Monteiro,+M), [Jacob Menick](https://arxiv.org/search/cs?searchtype=author&query=Menick,+J), [Sebastian Borgeaud](https://arxiv.org/search/cs?searchtype=author&query=Borgeaud,+S), [Andrew Brock](https://arxiv.org/search/cs?searchtype=author&query=Brock,+A), [Aida Nematzadeh](https://arxiv.org/search/cs?searchtype=author&query=Nematzadeh,+A), [Sahand Sharifzadeh](https://arxiv.org/search/cs?searchtype=author&query=Sharifzadeh,+S), [Mikolaj Binkowski](https://arxiv.org/search/cs?searchtype=author&query=Binkowski,+M), [Ricardo Barreira](https://arxiv.org/search/cs?searchtype=author&query=Barreira,+R), [Oriol Vinyals](https://arxiv.org/search/cs?searchtype=author&query=Vinyals,+O), [Andrew Zisserman](https://arxiv.org/search/cs?searchtype=author&query=Zisserman,+A), [Karen Simonyan](https://arxiv.org/search/cs?searchtype=author&query=Simonyan,+K)

View a PDF of the paper titled Flamingo: a Visual Language Model for Few-Shot Learning, by Jean-Baptiste Alayrac and 26 other authors

[View PDF](/pdf/2204.14198)
> Abstract:Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. We propose key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of our models, exploring and measuring their ability to rapidly adapt to a variety of image and video tasks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer; captioning tasks, which evaluate the ability to describe a scene or an event; and close-ended tasks such as multiple-choice visual question-answering. For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data.

|  |  |
| --- | --- |
| Comments: | 54 pages. In Proceedings of Neural Information Processing Systems (NeurIPS) 2022 |
| Subjects: | Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |
| Cite as: | [arXiv:2204.14198](https://arxiv.org/abs/2204.14198) [cs.CV] |
|  | (or  [arXiv:2204.14198v2](https://arxiv.org/abs/2204.14198v2) [cs.CV] for this version) |
|  | <https://doi.org/10.48550/arXiv.2204.14198> Focus to learn more  arXiv-issued DOI via DataCite |

## Submission history

From: Jeff Donahue [[view email](/show-email/cc209ef4/2204.14198)]   
 **[[v1]](/abs/2204.14198v1)**
Fri, 29 Apr 2022 16:29:01 UTC (13,130 KB)  
**[v2]**
Tue, 15 Nov 2022 23:07:37 UTC (26,345 KB)

Full-text links:

## Access Paper:

View a PDF of the paper titled Flamingo: a Visual Language Model for Few-Shot Learning, by Jean-Baptiste Alayrac and 26 other authors

* [View PDF](/pdf/2204.14198)
* [TeX Source](/src/2204.14198)
* [Other Formats](/format/2204.14198)

[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/ "Rights to this article")

Current browse context:

cs.CV

[< prev](/prevnext?id=2204.14198&function=prev&context=cs.CV "previous in cs.CV (accesskey p)")
  |   
[next >](/prevnext?id=2204.14198&function=next&context=cs.CV "next in cs.CV (accesskey n)")

[new](/list/cs.CV/new)
 | 
[recent](/list/cs.CV/recent)
 | [2022-04](/list/cs.CV/2022-04)

Change to browse by:

[cs](/abs/2204.14198?context=cs)  
[cs.AI](/abs/2204.14198?context=cs.AI)  
[cs.LG](/abs/2204.14198?context=cs.LG)

### References & Citations

* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2204.14198)
* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2204.14198)
* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2204.14198)

### [3 blog links](/tb/2204.14198)

([what is this?](https://info.arxiv.org/help/trackback.html))

[a](/static/browse/0.3.4/css/cite.css)
export BibTeX citation
Loading...

## BibTeX formatted citation

×

loading...

Data provided by:

### Bookmark

[![BibSonomy logo](/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2204.14198&description=Flamingo: a Visual Language Model for Few-Shot Learning "Bookmark on BibSonomy")
[![Reddit logo](/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2204.14198&title=Flamingo: a Visual Language Model for Few-Shot Learning "Bookmark on Reddit")



Bibliographic Tools

# Bibliographic and Citation Tools

Bibliographic Explorer Toggle

Bibliographic Explorer *([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*

Connected Papers Toggle

Connected Papers *([What is Connected Papers?](https://www.connectedpapers.com/about))*

Litmaps Toggle

Litmaps *([What is Litmaps?](https://www.litmaps.co/))*

scite.ai Toggle

scite Smart Citations *([What are Smart Citations?](https://www.scite.ai/))*

Code, Data, Media

# Code, Data and Media Associated with this Article

alphaXiv Toggle

alphaXiv *([What is alphaXiv?](https://alphaxiv.org/))*

Links to Code Toggle

CatalyzeX Code Finder for Papers *([What is CatalyzeX?](https://www.catalyzex.com))*

DagsHub Toggle

DagsHub *([What is DagsHub?](https://dagshub.com/))*

GotitPub Toggle

Gotit.pub *([What is GotitPub?](http://gotit.pub/faq))*

Huggingface Toggle

Hugging Face *([What is Huggingface?](https://huggingface.co/huggingface))*

Links to Code Toggle

Papers with Code *([What is Papers with Code?](https://paperswithcode.com/))*

ScienceCast Toggle

ScienceCast *([What is ScienceCast?](https://sciencecast.org/welcome))*

Demos

# Demos

Replicate Toggle

Replicate *([What is Replicate?](https://replicate.com/docs/arxiv/about))*

Spaces Toggle

Hugging Face Spaces *([What is Spaces?](https://huggingface.co/docs/hub/spaces))*

Spaces Toggle

TXYZ.AI *([What is TXYZ.AI?](https://txyz.ai))*

Related Papers

# Recommenders and Search Tools

Link to Influence Flower

Influence Flower *([What are Influence Flowers?](https://influencemap.cmlab.dev/))*

Core recommender toggle

CORE Recommender *([What is CORE?](https://core.ac.uk/services/recommender))*

* Author
* Venue
* Institution
* Topic


About arXivLabs

# arXivLabs: experimental projects with community collaborators

arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.

Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.

Have an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).

[Which authors of this paper are endorsers?](/auth/show-endorsers/2204.14198) |
[Disable MathJax](javascript:setMathjaxCookie()) ([What is MathJax?](https://info.arxiv.org/help/mathjax.html))



* [About](https://info.arxiv.org/about)
* [Help](https://info.arxiv.org/help)

* contact arXivClick here to contact arXiv
   [Contact](https://info.arxiv.org/help/contact.html)
* subscribe to arXiv mailingsClick here to subscribe
   [Subscribe](https://info.arxiv.org/help/subscribe)



* [Copyright](https://info.arxiv.org/help/license/index.html)
* [Privacy Policy](https://info.arxiv.org/help/policies/privacy_policy.html)

* [Web Accessibility Assistance](https://info.arxiv.org/help/web_accessibility.html)
* [arXiv Operational Status](https://status.arxiv.org)   
  Get status notifications via
  [email](https://subscribe.sorryapp.com/24846f03/email/new)
  or [slack](https://subscribe.sorryapp.com/24846f03/slack/new)
