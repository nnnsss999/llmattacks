---
title: "Data Poisoning Attacks"
category: "Training Alignment"
sub_category: "Data Poisoning"
source_url: ""
date_collected: 2025-06-18
license: "CC-BY-4.0"
---

This section covers poisoning attacks that compromise model training or retrieval datasets. Techniques range from malicious modifications of raw text corpora to targeted insertion of adversarial examples in RAG systems.

- [BadReward Clean-Label Poisoning](badreward-clean-label.md)
- [Adversarial Preference Learning](adversarial-preference-learning.md)
