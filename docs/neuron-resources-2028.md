---
title: "Neuron-Level Manipulation Resources 2028"
category: "Latent Space"
source_url: ""
date_collected: 2025-06-19
license: "CC-BY-4.0"
---

The references below extend the neuron-focused bibliography with new research and tools published after the 2027 update. They highlight emerging work on neuron-level attacks, defenses, and analysis methods for LLMs.

## Research Papers

- [Stealthy Backdoor Attack against Federated Learning through Frequency Domain by Backdoor Neuron Constraint and Model Camouflage](https://doi.org/10.1109/jetcas.2024.3450527) – shows how enforcing neuron constraints enables stealthy backdoors in distributed settings.
- [Imperio: Language-Guided Backdoor Attacks for Arbitrary Model Control](https://doi.org/10.24963/ijcai.2024/78) – introduces neuron-aligned triggers that let attackers reprogram LLM behaviour.
- [LLMbd: Backdoor Defense Via Large Language Model Paraphrasing and Data Voting in NLP](https://doi.org/10.2139/ssrn.5131766) – proposes a defense that patches suspect neurons through paraphrased data augmentation.
- [Causal Analysis of Syntactic Agreement Neurons in Multilingual Language Models](https://doi.org/10.18653/v1/2022.conll-1.8) – analyzes syntactic neurons to reveal cross-language vulnerabilities.
- [Selective Error Correction for Activation in Large Language Model Training](https://doi.org/10.1109/icce63647.2025.10930198) – corrects faulty activations to resist neuron-level corruption.

## Articles and Links

- [Better Patching Using LLM Prompting, via Self-Consistency](https://doi.org/10.1109/ase56229.2023.00065) – explores automated neuron patching guided by LLM-generated prompts.

