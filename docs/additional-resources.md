---
title: "Additional Resources on LLM Attacks"
category: "Overview"
source_url: ""
date_collected: 2025-06-18
license: "CC-BY-4.0"
---

Below is a curated set of external articles, blog posts and reports that further explore attacks against large language models. They complement the examples found elsewhere in this catalog.

- [WormGPT – Hackers Train a Malicious ChatGPT Alternative](https://thehackernews.com/2023/07/wormgpt-hackers-gpt.html)
- [FraudGPT: Weaponizing Generative AI for Crime](https://thehackernews.com/2023/07/fraudgpt-weaponizing.html)
- [Unicode Encoding Attacks](https://owasp.org/www-community/attacks/Unicode_Encoding)
- [Down the Rabbit Hole of Unicode Obfuscation](https://www.veracode.com/blog/down-the-rabbit-hole-of-unicode-obfuscation/)
- [H-CoT: Hijacking Chain-of-Thought Safety Reasoning](https://arxiv.org/abs/2502.12893)
- [Sleeper Agents: Training Deceptive LLMs](https://arxiv.org/abs/2401.05566)
- [Evaluating Security Risk in DeepSeek and Other Frontier Models](https://blogs.cisco.com/security/evaluating-security-risk-in-deepseek-and-other-frontier-reasoning-models)
- [Prompt Injection Attack Overview](https://www.wiz.io/academy/prompt-injection-attack)
- [Microsoft 365 Copilot: Critical 'EchoLeak' Flaw Turned Microsoft's Own AI Into Data Thief](https://winbuzzer.com/2025/06/16/microsoft-365-copilot-critical-echoleak-flaw-turned-microsofts-own-ai-into-data-thief-xcxwbn/)
- [First-ever zero-click attack targets Microsoft 365 Copilot](https://www.csoonline.com/article/4005965/first-ever-zero-click-attack-targets-microsoft-365-copilot.html)
- [Strengthening LLM Security: Insights from OWASP's 2025 Top 10 List](https://www.pillar.security/blog/strengthening-llm-security-insights-from-owasps-2025-top-10-list)
- [Zero-Click AI Vulnerability Exposes Microsoft 365 Copilot Data Without User Interaction](https://thehackernews.com/2025/06/zero-click-ai-vulnerability-exposes.html)
- [OWASP Gen AI LLM01:2025 Prompt Injection](https://genai.owasp.org/llmrisk/llm01-prompt-injection/)
- [LLMjacking: What is it? And Why is it a Concern?](https://www.upwind.io/glossary/llmjacking-what-is-it-and-why-is-it-a-concern)
- [LLMs could soon supercharge supply-chain attacks](https://www.theregister.com/2024/12/29/llm_supply_chain_attacks/)
- [A Comprehensive Study of Jailbreak Attack versus Defense for Large Language Models](https://arxiv.org/abs/2402.13457)
- [COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability](https://github.com/Yu-Fangxu/COLD-Attack)
- [Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues](https://aclanthology.org/2024.findings-acl.304/)
- [Optimization-based Prompt Injection Attack to LLM-as-a-Judge](https://dl.acm.org/doi/abs/10.1145/3658644.3690291)
- [LLMs know their vulnerabilities: Uncover Safety Gaps through Natural Distribution Shifts](https://arxiv.org/abs/2410.10700)

- [Defending against Reverse Preference Attacks is Difficult](https://arxiv.org/abs/2409.12914)
- ["Yes, My LoRD." Guiding Language Model Extraction with Locality Reinforced Distillation](https://arxiv.org/abs/2409.02718)
- [PROMPTFUZZ: Harnessing Fuzzing Techniques for Robust Testing of Prompt Injection in LLMs](https://arxiv.org/abs/2409.14729)
- [Patch is Enough: Naturalistic Adversarial Patch against Vision-Language Pre-training Models](https://arxiv.org/abs/2410.04884)
- [Obfuscated Activations Bypass LLM Latent-Space Defenses](https://arxiv.org/abs/2412.09565)
- [Illusions of Relevance: Using Content Injection Attacks to Deceive Retrievers, Rerankers, and LLM Judges](https://arxiv.org/abs/2501.18536)
- [Membership Inference Attacks on Large-Scale Models: A Survey](https://arxiv.org/abs/2503.19338)
- [LAGO: Few-shot Crosslingual Embedding Inversion Attacks via Language Similarity-Aware Graph Optimization](https://arxiv.org/abs/2505.16008)
- [DarkMind Backdoor Leverages Capabilities LLMs](https://techxplore.com/news/2025-02-darkmind-backdoor-leverages-capabilities-llms.html)
- [New TokenBreak Attack Bypasses AI Moderation with Single-Character Text Changes](https://thehackernews.com/2025/06/new-tokenbreak-attack-bypasses-ai.html)
- [The TokenBreak Attack](https://hiddenlayer.com/innovation-hub/the-tokenbreak-attack/)
- [RedTeaming DeepSeek Report (Jan 29, 2025)](https://cdn.prod.website-files.com/6690a78074d86ca0ad978007/679bc2e71b48e423c0ff7e60_1%20RedTeaming_DeepSeek_Jan29_2025%20(1).pdf)
- [How nice that state-of-the-art LLMs reveal their reasoning ... for miscreants to exploit](https://www.theregister.com/2025/02/25/chain_of_thought_jailbreaking/)
- [How to Jailbreak LLMs One Step at a Time: Top Techniques and Strategies](https://www.confident-ai.com/blog/how-to-jailbreak-llms-one-step-at-a-time)
- [VulnHuntr: Zero Shot Vulnerability Discovery Using LLMs](https://github.com/protectai/vulnhuntr)
- [LLM Embedding Attack Repository](https://github.com/SchwinnL/LLM_Embedding_Attack)

- [Attacking Large Language Models with Projected Gradient Descent](https://arxiv.org/abs/2402.09154)
- [TAG: Gradient Attack on Transformer-based Language Models](https://aclanthology.org/2021.findings-emnlp.305/)
- [Gradient-based Adversarial Attacks against Text Transformers](https://blogs.night-wolf.io/gradient-based-adversarial-attacks-against-text-transformers)
- [Uncovering Gradient Inversion Risks in Practical Language Model Training](https://dl.acm.org/doi/abs/10.1145/3658644.3690292)

Additional references collected after the original snapshot:

- [Securing LLM Systems Against Prompt Injection](https://developer.nvidia.com/blog/securing-llm-systems-against-prompt-injection/)
- [Prompt Sanitization Best Practices](https://boxplot.com/prompt-sanitization)
- [Microsoft Guidance on Jailbreak Detection](https://learn.microsoft.com/en-us/azure/ai-services/content-safety/concepts/jailbreak-detection)
- [Jailbreaking LLMs – Public Dataset](https://jailbreaking-llms.github.io/)
- [Jailbreaking Generative AI](https://lab.wallarm.com/jailbreaking-generative-ai/)
- [Jailbreaking DeepSeek: Three Techniques](https://unit42.paloaltonetworks.com/jailbreaking-deepseek-three-techniques/)
- [Jailbreaking Generative AI Web Products](https://unit42.paloaltonetworks.com/jailbreaking-generative-ai-web-products/)
- [Many-Shot Jailbreaking Research](https://www.anthropic.com/research/many-shot-jailbreaking)
- [LLM Jailbreaking Defense Repository](https://github.com/YihanWang617/llm-jailbreaking-defense)
- [LLM Vector and Embedding Risks and How to Defend Against Them](https://www.sonatype.com/blog/llm-vector-and-embedding-risks-and-how-to-defend-against-them)
- [Responsible Prompt Engineering: An Embedding-Based Approach to Secure LLM Interactions](https://www.ijraset.com/best-journal/responsible-prompt-engineering-an-embedding-based-approach-to-secure-llm-interactions)
- [OWASP LLM Top 10: Vector and Embedding Weaknesses](https://scrumgit.com/owasp-llm-top-10-vector-and-embedding-weaknesses-285754bef598)
- [Dissecting Auto-GPT's Prompt](https://community.openai.com/t/dissecting-auto-gpts-prompt/163892)
- [Red Teaming and Safer AI](https://www.weforum.org/stories/2025/06/red-teaming-and-safer-ai/)
- [Microsoft AI Red Team Warning](https://www.theregister.com/2025/01/17/microsoft_ai_redteam_infosec_warning/)
- [Nvidia & Cisco AI Guardrails Security](https://www.theregister.com/2025/01/17/nvidia_cisco_ai_guardrails_security/)
- [JailbreakFunction Repository](https://github.com/wooozihui/jailbreakfunction)
- [PoisonedRAG Dataset](https://github.com/sleeepeer/PoisonedRAG)
- [GPTFuzz Project](https://github.com/sherdencooper/GPTFuzz)
- [CL4R1T4S Attack Toolkit](https://github.com/elder-plinius/CL4R1T4S)
- [L1B3RT4S Jailbreak Suite](https://github.com/elder-plinius/L1B3RT4S)
- [STEGOSAURUS-WRECKS Prompt Obfuscation](https://github.com/elder-plinius/STEGOSAURUS-WRECKS)
- [ChatGPT AutoExpert System Prompts](https://github.com/spdustin/ChatGPT-AutoExpert/blob/main/System%20Prompts.md#behind-the-scenes)

These references track emerging threats and defence research through 2026 and are regularly reviewed for new developments.
- [Vulnhuntr: Autonomous AI Finds First 0-Day Vulnerabilities in Wild](https://protectai.com/threat-research/vulnhuntr-first-0-day-vulnerabilities)
- [Using Generative AI LLM agents to exploit zero-day vulnerabilities](https://www.cybercareers.blog/2024/07/using-generative-ai-llm-agents-to-exploit-zero-day-vulnerabilities/)

- [LLM Security Playbook for AI Injection Attacks, Data Leaks, and Model Theft](https://konghq.com/blog/enterprise/llm-security-playbook-for-injection-attacks-data-leaks-model-theft)
- [LLM Security: Protect Models from Attacks & Vulnerabilities](https://blog.qualys.com/product-tech/2025/02/07/llm-security-101-protecting-large-language-models-from-cyber-threats)
- [Commercial LLM Agents Are Already Vulnerable to Simple Yet Dangerous Attacks](https://arxiv.org/abs/2502.08586)
- [LLM Security](https://llmsecurity.net/)
- [Practical Attacks on LLMs: Full Guide](https://iterasec.com/blog/practical-attacks-on-llms/)
- [OWASP's LLM and Gen AI Data Security Best Practices](https://genai.owasp.org/resource/llm-and-gen-ai-data-security-best-practices/)
- [Addressing LLM02:2025 Sensitive Information Disclosure Risks](https://www.securityium.com/addressing-llm022025-sensitive-information-disclosure-risks/)
- [LLM Prompt Injection Prevention Cheat Sheet](https://cheatsheetseries.owasp.org/cheatsheets/LLM_Prompt_Injection_Prevention_Cheat_Sheet.html)
- [How to Protect Your Organization from LLM Attacks](https://op-c.net/blog/how-to-protect-your-organization-from-llm-attacks/)
- [Universal and Transferable Attacks on Aligned Language Models](https://llm-attacks.org/)
- [MASTERKEY: Automated Jailbreaking of Large Language Model Chatbots](https://www.ndss-symposium.org/ndss-paper/masterkey-automated-jailbreaking-of-large-language-model-chatbots/)
- [Prompt Injection Attacks on LLMs - HiddenLayer](https://hiddenlayer.com/innovation-hub/prompt-injection-attacks-on-llms/)
- [Recent Advances in Attack and Defense Approaches of Large Language Models](https://arxiv.org/abs/2409.03274)
- [GitHub Repository: llm-attacks/llm-attacks](https://github.com/llm-attacks/llm-attacks)
- [Meta's Llama Framework Flaw Exposes AI Systems to Remote Code Execution](https://thehackernews.com/2025/01/metas-llama-framework-flaw-exposes-ai.html)
- [Top 10 Most Common LLM Attack Techniques | Elastic](https://www.elastic.co/security/llm-safety-report)

- [OWASP Top 10 LLM, Updated 2025: Examples & Mitigation Strategies](https://www.oligo.security/academy/owasp-top-10-llm-updated-2025-examples-and-mitigation-strategies)
- [Is Your Prompt Safe? Investigating Prompt Injection Attacks Against Open-Source LLMs](https://arxiv.org/abs/2505.14368)
- [Goal-Oriented Prompt Attack and Safety Evaluation for LLMs](https://arxiv.org/abs/2309.11830)
- [Large Language Models Merging for Enhancing the Link Stealing Attack on Graph Neural Networks](https://arxiv.org/abs/2412.05830)
- [System Prompt Extraction Attacks and Defenses in Large Language Models](https://arxiv.org/abs/2505.23817)
- [Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs](https://arxiv.org/abs/2406.09324)
- [A Survey of Attacks on Large Language Models](https://arxiv.org/abs/2505.12567)
- [Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks](https://arxiv.org/abs/2402.09177)
- [Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment](https://arxiv.org/abs/2402.14016)
- [CheatAgent: Attacking LLM-Empowered Recommender Systems via LLM Agent](https://arxiv.org/abs/2504.13192)
- [Attack Prompt Generation for Red Teaming and Defending Large Language Models](https://arxiv.org/abs/2310.12505)

- [ASETF: A Novel Method for Jailbreak Attack on LLMs through Translate Suffix Embeddings](https://arxiv.org/abs/2402.16006)
- [Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues](https://aclanthology.org/2024.findings-acl.304/)
- [Learn to Disguise: Avoid Refusal Responses in LLM’s Defense via a Multi-Agent Attacker-Disguiser Game](https://arxiv.org/abs/2404.02532)
- [Open Sesame! Universal Black-Box Jailbreaking of Large Language Models](https://arxiv.org/abs/2309.01446)

- [Large Language Models Based Fuzzing Techniques: A Survey](https://arxiv.org/abs/2402.00350)
- [Fuzz4All: Universal Fuzzing with LLMs](https://arxiv.org/abs/2308.04748)
- [LLMFuzzer: Fuzzing Framework for Large Language Models](https://github.com/mnns/LLMFuzzer)
- [FuzzyAI Fuzzer – Automated LLM Fuzzing](https://github.com/cyberark/FuzzyAI)
- [ChatAFL: Protocol Fuzzing Guided by LLMs](https://github.com/ChatAFLndss/ChatAFL)
- [OSS-Fuzz-Gen: LLM-Based Fuzz Target Generation](https://github.com/google/oss-fuzz-gen)
- [AI-Powered Fuzzing: Breaking the Bug Hunting Barrier](https://security.googleblog.com/2023/08/ai-powered-fuzzing-breaking-bug-hunting.html)
- [Evaluating Large Language Models for Enhanced Fuzzing](https://ieeexplore.ieee.org/document/10731701)
- [Prompt Injection Resources 2026](prompt-dialogue/prompt-injection-resources-2026.md)
- [Fuzzing Resources 2026](fuzzing/fuzzing-resources-2026.md)
- [Fuzzing Resources 2027](fuzzing/fuzzing-resources-2027.md)
