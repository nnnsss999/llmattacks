---
title: "Additional Resources on LLM Attacks"
category: "Overview"
source_url: ""
date_collected: 2025-06-18
license: "CC-BY-4.0"
---

Below is a curated set of external articles, blog posts and reports that further explore attacks against large language models. They complement the examples found elsewhere in this catalog.

- [WormGPT â€“ Hackers Train a Malicious ChatGPT Alternative](https://thehackernews.com/2023/07/wormgpt-hackers-gpt.html)
- [FraudGPT: Weaponizing Generative AI for Crime](https://thehackernews.com/2023/07/fraudgpt-weaponizing.html)
- [Unicode Encoding Attacks](https://owasp.org/www-community/attacks/Unicode_Encoding)
- [Down the Rabbit Hole of Unicode Obfuscation](https://www.veracode.com/blog/down-the-rabbit-hole-of-unicode-obfuscation/)
- [H-CoT: Hijacking Chain-of-Thought Safety Reasoning](https://arxiv.org/abs/2502.12893)
- [Sleeper Agents: Training Deceptive LLMs](https://arxiv.org/abs/2401.05566)
- [Evaluating Security Risk in DeepSeek and Other Frontier Models](https://blogs.cisco.com/security/evaluating-security-risk-in-deepseek-and-other-frontier-reasoning-models)
- [Prompt Injection Attack Overview](https://www.wiz.io/academy/prompt-injection-attack)

These references track emerging threats and defence research through 2025 and are regularly reviewed for new developments.
