---
title: "Additional Resources on LLM Attacks"
category: "Overview"
source_url: ""
date_collected: 2025-06-18
license: "CC-BY-4.0"
---

Below is a curated set of external articles, blog posts and reports that further explore attacks against large language models. They complement the examples found elsewhere in this catalog.

- [WormGPT â€“ Hackers Train a Malicious ChatGPT Alternative](https://thehackernews.com/2023/07/wormgpt-hackers-gpt.html)
- [FraudGPT: Weaponizing Generative AI for Crime](https://thehackernews.com/2023/07/fraudgpt-weaponizing.html)
- [Unicode Encoding Attacks](https://owasp.org/www-community/attacks/Unicode_Encoding)
- [Down the Rabbit Hole of Unicode Obfuscation](https://www.veracode.com/blog/down-the-rabbit-hole-of-unicode-obfuscation/)
- [Cross-Modal LLM Attack Resources](multimodal/cross-modal-attack-resources.md)
- [H-CoT: Hijacking Chain-of-Thought Safety Reasoning](https://arxiv.org/abs/2502.12893)
- [Sleeper Agents: Training Deceptive LLMs](https://arxiv.org/abs/2401.05566)
- [Evaluating Security Risk in DeepSeek and Other Frontier Models](https://blogs.cisco.com/security/evaluating-security-risk-in-deepseek-and-other-frontier-reasoning-models)
- [Prompt Injection Attack Overview](https://www.wiz.io/academy/prompt-injection-attack)
- [Microsoft 365 Copilot: Critical 'EchoLeak' Flaw Turned Microsoft's Own AI Into Data Thief](https://winbuzzer.com/2025/06/16/microsoft-365-copilot-critical-echoleak-flaw-turned-microsofts-own-ai-into-data-thief-xcxwbn/)
- [First-ever zero-click attack targets Microsoft 365 Copilot](https://www.csoonline.com/article/4005965/first-ever-zero-click-attack-targets-microsoft-365-copilot.html)
- [Strengthening LLM Security: Insights from OWASP's 2025 Top 10 List](https://www.pillar.security/blog/strengthening-llm-security-insights-from-owasps-2025-top-10-list)
- [Zero-Click AI Vulnerability Exposes Microsoft 365 Copilot Data Without User Interaction](https://thehackernews.com/2025/06/zero-click-ai-vulnerability-exposes.html)
- [OWASP Gen AI LLM01:2025 Prompt Injection](https://genai.owasp.org/llmrisk/llm01-prompt-injection/)
- [LLMjacking: What is it? And Why is it a Concern?](https://www.upwind.io/glossary/llmjacking-what-is-it-and-why-is-it-a-concern)
- [LLMs could soon supercharge supply-chain attacks](https://www.theregister.com/2024/12/29/llm_supply_chain_attacks/)
- [LLM Supply Chain Attacks Overview](supply-chain/llm-supply-chain-attacks.md)
- [Hugging Face Vulnerability Exposes Model Hub to RCE](supply-chain/thehackernews-hugging-face-vulnerability.html)
- [JFrog Report on Silent Backdoor Models](supply-chain/jfrog-silent-backdoor.html)
- [Ars Technica on Code Backdooring via Hugging Face](supply-chain/arstechnica-hf-backdoor.html)
- [BleepingComputer: Malicious AI Models on Hugging Face](supply-chain/bleepingcomputer-huggingface-backdoor.html)
- [100+ Malicious Hugging Face Models](supply-chain/thehackernews-100-malicious-models.html)
- [Malicious ML Models Detected on Hugging Face](supply-chain/cybersecuritynews-huggingface.html)
- [Hugging Face AI Platform Malware Report](supply-chain/darkreading-malicious-models.html)
- [Hidden Backdoor Models on HuggingFace Repository](supply-chain/hidden-backdoor-models-hf.html)
- [BEEAR Backdoored Model Example](supply-chain/beear-backdoored-model3.html)
- [Poisoned Pre-Training Data Markets](supply-chain/poisoned-pretraining-markets.md)
- [Extended LLM Supply Chain Resources](supply-chain/extended-llm-supply-chain-resources.md)
- [A Comprehensive Study of Jailbreak Attack versus Defense for Large Language Models](https://arxiv.org/abs/2402.13457)
- [COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability](https://github.com/Yu-Fangxu/COLD-Attack)
- [Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues](https://aclanthology.org/2024.findings-acl.304/)
- [Optimization-based Prompt Injection Attack to LLM-as-a-Judge](https://dl.acm.org/doi/abs/10.1145/3658644.3690291)
- [LLMs know their vulnerabilities: Uncover Safety Gaps through Natural Distribution Shifts](https://arxiv.org/abs/2410.10700)

- [Defending against Reverse Preference Attacks is Difficult](https://arxiv.org/abs/2409.12914)
- ["Yes, My LoRD." Guiding Language Model Extraction with Locality Reinforced Distillation](https://arxiv.org/abs/2409.02718)
- [PROMPTFUZZ: Harnessing Fuzzing Techniques for Robust Testing of Prompt Injection in LLMs](https://arxiv.org/abs/2409.14729)
- [Patch is Enough: Naturalistic Adversarial Patch against Vision-Language Pre-training Models](https://arxiv.org/abs/2410.04884)
- [Obfuscated Activations Bypass LLM Latent-Space Defenses](https://arxiv.org/abs/2412.09565)
- [Illusions of Relevance: Using Content Injection Attacks to Deceive Retrievers, Rerankers, and LLM Judges](https://arxiv.org/abs/2501.18536)
- [Membership Inference Attacks on Large-Scale Models: A Survey](https://arxiv.org/abs/2503.19338)
- [LAGO: Few-shot Crosslingual Embedding Inversion Attacks via Language Similarity-Aware Graph Optimization](https://arxiv.org/abs/2505.16008)
- [DarkMind Backdoor Leverages Capabilities LLMs](https://techxplore.com/news/2025-02-darkmind-backdoor-leverages-capabilities-llms.html)
- [New TokenBreak Attack Bypasses AI Moderation with Single-Character Text Changes](https://thehackernews.com/2025/06/new-tokenbreak-attack-bypasses-ai.html)
- [The TokenBreak Attack](https://hiddenlayer.com/innovation-hub/the-tokenbreak-attack/)
- [RedTeaming DeepSeek Report (Jan 29, 2025)](https://cdn.prod.website-files.com/6690a78074d86ca0ad978007/679bc2e71b48e423c0ff7e60_1%20RedTeaming_DeepSeek_Jan29_2025%20(1).pdf)
- [How nice that state-of-the-art LLMs reveal their reasoning ... for miscreants to exploit](https://www.theregister.com/2025/02/25/chain_of_thought_jailbreaking/)
- [How to Jailbreak LLMs One Step at a Time: Top Techniques and Strategies](https://www.confident-ai.com/blog/how-to-jailbreak-llms-one-step-at-a-time)
- [VulnHuntr: Zero Shot Vulnerability Discovery Using LLMs](https://github.com/protectai/vulnhuntr)
- [LLM Embedding Attack Repository](https://github.com/SchwinnL/LLM_Embedding_Attack)

- [Attacking Large Language Models with Projected Gradient Descent](https://arxiv.org/abs/2402.09154)
- [TAG: Gradient Attack on Transformer-based Language Models](https://aclanthology.org/2021.findings-emnlp.305/)
- [Gradient-based Adversarial Attacks against Text Transformers](https://blogs.night-wolf.io/gradient-based-adversarial-attacks-against-text-transformers)
- [Uncovering Gradient Inversion Risks in Practical Language Model Training](https://dl.acm.org/doi/abs/10.1145/3658644.3690292)

- [AutoDAN: Interpretable Gradient-Based Adversarial Attacks on Large Language Models](https://arxiv.org/abs/2310.15140)
- [AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned LLMs](https://arxiv.org/abs/2310.04451)
- [PIG: Privacy Jailbreak Attack on LLMs via Gradient-based Iterative In-Context Optimization](https://arxiv.org/abs/2505.09921)
- [Universal and Transferable Adversarial Attacks on Aligned Language Models](https://arxiv.org/abs/2307.15043)
- [Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes](https://arxiv.org/abs/2403.00867)
- [Exploiting the Index Gradients for Optimization-Based Jailbreaking on Large Language Models](https://aclanthology.org/2025.coling-main.305/)
- [Exponentiated Gradient Jailbreak | LLM Security Database](https://www.promptfoo.dev/lm-security-db/vuln/exponentiated-gradient-jailbreak-c4fb2fa8)
- [DAGER: Exact Gradient Inversion for Large Language Models](https://arxiv.org/abs/2405.15586)
- [Boosting Jailbreak Attack with Momentum](https://ieeexplore.ieee.org/document/10888812)
- [GradSafe: Detecting Jailbreak Prompts for LLMs via Safety-Critical Gradient Analysis](https://aclanthology.org/2024.acl-long.30/)
- [Improved Generation of Adversarial Examples Against Safety-aligned LLMs](https://papers.nips.cc/paper_files/paper/2024/file/aeae9df8e6bfe7350160bb42965dbd44-Paper-Conference.pdf)
- [Exploring Jailbreak Attacks: Understanding LLM Vulnerabilities and the Challenges of Detection and Defense](https://infosecwriteups.com/exploring-jailbreak-attacks-understanding-llm-vulnerabilities-and-the-challenges-of-detection-5b6cd35451a0)
- [Advanced Adversarial Attacks on Large Language Models](https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/)
- [Gradient Cuff and Detecting Jailbreak Attacks on Large Language Models](https://jailbreakai.substack.com/p/gradientcuff-and-detecting-jailbreak)
- [Gradient Inversion Learning Resources](https://github.com/SuperX612/Gradient-Inversion-Learning-Resources)
- [Deep Leakage from Gradients](https://arxiv.org/abs/1906.08935)
- [Inverting Gradients -- How easy is it to break privacy in federated learning?](https://arxiv.org/abs/2003.14053)
- [AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts](https://arxiv.org/abs/2010.15980)
- [Gradient Hacking in Reinforcement Learning](https://arxiv.org/abs/2106.09685)
- [Gradient Leakage and Model Inversion via LoRA](https://arxiv.org/abs/2406.01234)
- [BrokenHill: Attacking LLM Safeguards via Gradient Exploits](https://bishopfox.com/blog/brokenhill-attack-tool-largelanguagemodels-llm)
- [AmpleGCG: Learning a Universal and Transferable Generative Model of Adversarial Suffixes for Jailbreaking Both Open and Closed LLMs](https://arxiv.org/abs/2404.07921)
- [AmpleGCG GitHub Repository](https://github.com/OSU-NLP-Group/AmpleGCG)
- [nanoGCG Project](https://github.com/GraySwanAI/nanoGCG/tree/main)
- [I-GCG Implementation](https://github.com/jiaxiaojunQAQ/I-GCG)
- [GCG Strategies for Prompt Red-Teaming](https://www.promptfoo.dev/docs/red-team/strategies/gcg/)
- [Gradient Masking All-at-Once: Ensemble Everything Everywhere Is Not Robust](https://arxiv.org/abs/2411.14834)
Additional references collected after the original snapshot:

- [Securing LLM Systems Against Prompt Injection](https://developer.nvidia.com/blog/securing-llm-systems-against-prompt-injection/)
- [Prompt Sanitization Best Practices](https://boxplot.com/prompt-sanitization)
- [Microsoft Guidance on Jailbreak Detection](https://learn.microsoft.com/en-us/azure/ai-services/content-safety/concepts/jailbreak-detection)
- [Jailbreaking LLMs â€“ Public Dataset](https://jailbreaking-llms.github.io/)
- [Jailbreaking Generative AI](https://lab.wallarm.com/jailbreaking-generative-ai/)
- [Jailbreaking DeepSeek: Three Techniques](https://unit42.paloaltonetworks.com/jailbreaking-deepseek-three-techniques/)
- [Jailbreaking Generative AI Web Products](https://unit42.paloaltonetworks.com/jailbreaking-generative-ai-web-products/)
- [Many-Shot Jailbreaking Research](https://www.anthropic.com/research/many-shot-jailbreaking)
- [LLM Jailbreaking Defense Repository](https://github.com/YihanWang617/llm-jailbreaking-defense)
- [LLM Vector and Embedding Risks and How to Defend Against Them](https://www.sonatype.com/blog/llm-vector-and-embedding-risks-and-how-to-defend-against-them)
- [Responsible Prompt Engineering: An Embedding-Based Approach to Secure LLM Interactions](https://www.ijraset.com/best-journal/responsible-prompt-engineering-an-embedding-based-approach-to-secure-llm-interactions)
- [OWASP LLM Top 10: Vector and Embedding Weaknesses](https://scrumgit.com/owasp-llm-top-10-vector-and-embedding-weaknesses-285754bef598)
- [Dissecting Auto-GPT's Prompt](https://community.openai.com/t/dissecting-auto-gpts-prompt/163892)
- [Red Teaming and Safer AI](https://www.weforum.org/stories/2025/06/red-teaming-and-safer-ai/)
- [Microsoft AI Red Team Warning](https://www.theregister.com/2025/01/17/microsoft_ai_redteam_infosec_warning/)
- [Nvidia & Cisco AI Guardrails Security](https://www.theregister.com/2025/01/17/nvidia_cisco_ai_guardrails_security/)
- [JailbreakFunction Repository](https://github.com/wooozihui/jailbreakfunction)
- [PoisonedRAG Dataset](https://github.com/sleeepeer/PoisonedRAG)
- [GPTFuzz Project](https://github.com/sherdencooper/GPTFuzz)
- [CL4R1T4S Attack Toolkit](https://github.com/elder-plinius/CL4R1T4S)
- [L1B3RT4S Jailbreak Suite](https://github.com/elder-plinius/L1B3RT4S)
- [STEGOSAURUS-WRECKS Prompt Obfuscation](https://github.com/elder-plinius/STEGOSAURUS-WRECKS)
- [ChatGPT AutoExpert System Prompts](https://github.com/spdustin/ChatGPT-AutoExpert/blob/main/System%20Prompts.md#behind-the-scenes)

- [OWASP LLM04: Model Denial of Service](https://genai.owasp.org/llmrisk2023-24/llm04-model-denial-of-service/)
- [Anthropic Shares Research on Technique to Exploit Long Context Windows](https://www.maginative.com/article/many-shot-jailbreaking-exploiting-long-context-windows-in-large-language-models/)
- [LLM Adversarial Attacks: Malicious Prompting](https://dev.to/gssakash/llm-adversarial-attacks-how-are-attackers-maliciously-prompting-llms-and-steps-to-safeguard-your-applications-4gfj)
- [Understanding LLM Context Windows, Tokens, Attention and Challenges](https://medium.com/@tahirbalarabe2/understanding-llm-context-windows-tokens-attention-and-challenges-c98e140f174d)
- [Many-shot Jailbreaking (NeurIPS 2024)](https://openreview.net/forum?id=cw5mgd71jW)
- [Many-shot jailbreaking: How expanded context windows in AI models led to a new vulnerability (Medium)](https://medium.com/@_jeremy_/many-shot-jailbreaking-how-expanded-context-windows-in-ai-models-led-to-a-new-vulnerability-37055b2f11d4)
- [Many-shot jailbreaking: A new LLM vulnerability (Prompt Security Blog)](https://www.prompt.security/blog/many-shot-jailbreaking-a-new-llm-vulnerability)
- [Context window overflow: Breaking the barrier (AWS Security Blog)](https://aws.amazon.com/blogs/security/context-window-overflow-breaking-the-barrier/)
- [RAG in the era of LLMs with 10 million token context windows (F5 Blog)](https://www.f5.com/company/blog/rag-in-the-era-of-llms-with-10-million-token-context-windows)
- [Understanding LLM context windows: tokens, attention, and challenges (IBM Think Blog)](https://www.ibm.com/think/topics/context-window)
- [Guide to Context in LLMs (Symbl.ai)](https://symbl.ai/developers/blog/guide-to-context-in-llms/)
- [Image Hijacks: Adversarial Images can Control Generative Models](http://arxiv.org/abs/2309.00236)
- [Lost in the Middle: How Language Models Use Long Contexts](https://arxiv.org/abs/2307.03172)
- [Mitigating Many-Shot Jailbreaking (arXiv:2504.09604)](https://arxiv.org/abs/2504.09604)
- [PANDAS: Improving Many-shot Jailbreaking via Positive Affirmation, Negative Demonstration, and Adaptive Sampling](https://arxiv.org/abs/2502.01925)
- [Compromesso! Italian Many-Shot Jailbreaks Undermine the Safety of Large Language Models](https://arxiv.org/abs/2408.04522)
- [Improved Few-Shot Jailbreaking Can Circumvent Aligned Language Models and Their Defenses](https://arxiv.org/abs/2406.01288)
- [What Really Matters in Many-Shot Attacks? An Empirical Study of Long-Context Vulnerabilities in LLMs](https://arxiv.org/abs/2505.19773)
- [Breaking Safeguards: Unveil "Many-Shot Jailbreaking" a Method to Bypass All LLM Safety Measures (InfoSec Write-ups)](https://infosecwriteups.com/breaking-safeguards-unveil-many-shot-jailbreaking-a-method-to-bypass-all-llm-safety-measures-2d188ebc12fb)
- [Many-shot Jailbreaking in Artificial Intelligence (LinkedIn)](https://www.linkedin.com/pulse/many-shot-jailbreaking-artificial-intelligence-reem-khattab-zudpc)
- [New AI Jailbreak Method 'Bad Likert Judge' Boosts Attack Success Rates by Over 60% (The Hacker News)](https://thehackernews.com/2025/01/new-ai-jailbreak-method-bad-likert.html)
- [PANDAS Poster at ICMLÂ 2025](https://icml.cc/virtual/2025/poster/43847)
- [SHADE-Arena: Evaluating Sabotage and Monitoring in LLM Agents (Anthropic Research)](https://www.anthropic.com/research/shade-arena-sabotage-monitoring)
These references track emerging threats and defence research through 2026 and are regularly reviewed for new developments.
- [Vulnhuntr: Autonomous AI Finds First 0-Day Vulnerabilities in Wild](https://protectai.com/threat-research/vulnhuntr-first-0-day-vulnerabilities)
- [Using Generative AI LLM agents to exploit zero-day vulnerabilities](https://www.cybercareers.blog/2024/07/using-generative-ai-llm-agents-to-exploit-zero-day-vulnerabilities/)

- [LLM Security Playbook for AI Injection Attacks, Data Leaks, and Model Theft](https://konghq.com/blog/enterprise/llm-security-playbook-for-injection-attacks-data-leaks-model-theft)
- [LLM Security: Protect Models from Attacks & Vulnerabilities](https://blog.qualys.com/product-tech/2025/02/07/llm-security-101-protecting-large-language-models-from-cyber-threats)
- [Commercial LLM Agents Are Already Vulnerable to Simple Yet Dangerous Attacks](https://arxiv.org/abs/2502.08586)
- [LLM Security](https://llmsecurity.net/)
- [Practical Attacks on LLMs: Full Guide](https://iterasec.com/blog/practical-attacks-on-llms/)
- [OWASP's LLM and Gen AI Data Security Best Practices](https://genai.owasp.org/resource/llm-and-gen-ai-data-security-best-practices/)
- [Addressing LLM02:2025 Sensitive Information Disclosure Risks](https://www.securityium.com/addressing-llm022025-sensitive-information-disclosure-risks/)
- [LLM Prompt Injection Prevention Cheat Sheet](https://cheatsheetseries.owasp.org/cheatsheets/LLM_Prompt_Injection_Prevention_Cheat_Sheet.html)
- [How to Protect Your Organization from LLM Attacks](https://op-c.net/blog/how-to-protect-your-organization-from-llm-attacks/)
- [Universal and Transferable Attacks on Aligned Language Models](https://llm-attacks.org/)
- [MASTERKEY: Automated Jailbreaking of Large Language Model Chatbots](https://www.ndss-symposium.org/ndss-paper/masterkey-automated-jailbreaking-of-large-language-model-chatbots/)
- [Prompt Injection Attacks on LLMs - HiddenLayer](https://hiddenlayer.com/innovation-hub/prompt-injection-attacks-on-llms/)
- [Recent Advances in Attack and Defense Approaches of Large Language Models](https://arxiv.org/abs/2409.03274)
- [GitHub Repository: llm-attacks/llm-attacks](https://github.com/llm-attacks/llm-attacks)
- [Meta's Llama Framework Flaw Exposes AI Systems to Remote Code Execution](https://thehackernews.com/2025/01/metas-llama-framework-flaw-exposes-ai.html)
- [Top 10 Most Common LLM Attack Techniques | Elastic](https://www.elastic.co/security/llm-safety-report)

- [OWASP Top 10 LLM, Updated 2025: Examples & Mitigation Strategies](https://www.oligo.security/academy/owasp-top-10-llm-updated-2025-examples-and-mitigation-strategies)
- [Is Your Prompt Safe? Investigating Prompt Injection Attacks Against Open-Source LLMs](https://arxiv.org/abs/2505.14368)
- [Goal-Oriented Prompt Attack and Safety Evaluation for LLMs](https://arxiv.org/abs/2309.11830)
- [Large Language Models Merging for Enhancing the Link Stealing Attack on Graph Neural Networks](https://arxiv.org/abs/2412.05830)
- [System Prompt Extraction Attacks and Defenses in Large Language Models](https://arxiv.org/abs/2505.23817)
- [Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs](https://arxiv.org/abs/2406.09324)
- [A Survey of Attacks on Large Language Models](https://arxiv.org/abs/2505.12567)
- [Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks](https://arxiv.org/abs/2402.09177)
- [Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment](https://arxiv.org/abs/2402.14016)
- [CheatAgent: Attacking LLM-Empowered Recommender Systems via LLM Agent](https://arxiv.org/abs/2504.13192)
- [Attack Prompt Generation for Red Teaming and Defending Large Language Models](https://arxiv.org/abs/2310.12505)

- [ASETF: A Novel Method for Jailbreak Attack on LLMs through Translate Suffix Embeddings](https://arxiv.org/abs/2402.16006)
- [Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues](https://aclanthology.org/2024.findings-acl.304/)
- [Learn to Disguise: Avoid Refusal Responses in LLMâ€™s Defense via a Multi-Agent Attacker-Disguiser Game](https://arxiv.org/abs/2404.02532)
- [Open Sesame! Universal Black-Box Jailbreaking of Large Language Models](https://arxiv.org/abs/2309.01446)

- [JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing](https://arxiv.org/abs/2403.11315)
- [Large Language Models Based Fuzzing Techniques: A Survey](https://arxiv.org/abs/2402.00350)
- [Fuzz4All: Universal Fuzzing with LLMs](https://arxiv.org/abs/2308.04748)
- [LLMFuzzer: Fuzzing Framework for Large Language Models](https://github.com/mnns/LLMFuzzer)
- [FuzzyAI Fuzzer â€“ Automated LLM Fuzzing](https://github.com/cyberark/FuzzyAI)
- [ChatAFL: Protocol Fuzzing Guided by LLMs](https://github.com/ChatAFLndss/ChatAFL)
- [OSS-Fuzz-Gen: LLM-Based Fuzz Target Generation](https://github.com/google/oss-fuzz-gen)
- [AI-Powered Fuzzing: Breaking the Bug Hunting Barrier](https://security.googleblog.com/2023/08/ai-powered-fuzzing-breaking-bug-hunting.html)
- [Evaluating Large Language Models for Enhanced Fuzzing](https://ieeexplore.ieee.org/document/10731701)
- [AmpleGCG: Learning a Universal and Transferable Generative Model of Adversarial Suffixes for Jailbreaking Both Open and Closed LLMs](https://arxiv.org/abs/2404.07921)
- [Lockpicking LLMs: A Logit-Based Jailbreak Using Token-level Manipulation](https://arxiv.org/abs/2405.13068)
- [Visual-RolePlay: Universal Jailbreak Attack on MultiModal Large Language Models via Role-playing Image Character](https://arxiv.org/abs/2405.20773)
- [Improved Techniques for Optimization-Based Jailbreaking on Large Language Models](https://arxiv.org/abs/2405.21018)
- [AutoJailbreak: Exploring Jailbreak Attacks and Defenses through a Dependency Lens](https://arxiv.org/abs/2406.03805)
- [From LLMs to MLLMs: Exploring the Landscape of Multimodal Jailbreaking](https://arxiv.org/abs/2406.14859)
- [Virtual Context: Enhancing Jailbreak Attacks with Special Token Injection](https://arxiv.org/abs/2406.19845)
- [Gradient Attack Resources 2026](optimization/gradient-resources-2026.md)
- [Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues](https://arxiv.org/abs/2402.09091)
- [SelfDefend: LLMs can Defend Themselves Against Jailbreaking in a Practical Manner](https://arxiv.org/abs/2406.05498)
- [Jigsaw Puzzles: Splitting Harmful Questions to Jailbreak Large Language Models](https://arxiv.org/abs/2410.11459)
- [Jailbreak Attacks and Defenses Against Large Language Models: A Survey](https://arxiv.org/abs/2407.04295)
- [WordGame: Efficient & Effective LLM Jailbreak via Simultaneous Obfuscation in Query and Response](https://arxiv.org/abs/2405.14023)
- [BaitAttack: Alleviating Intention Shift in Jailbreak Attacks via Adaptive Bait Crafting](https://aclanthology.org/2024.emnlp-main.877/)
- [Jailbreaking to Jailbreak](https://arxiv.org/abs/2502.09638)
- [SequentialBreak: Large Language Models Can be Fooled by Embedding Jailbreak Prompts into Sequential Prompt Chains](https://arxiv.org/abs/2411.06426)
- [Continuous Embedding Attacks via Clipped Inputs in Jailbreaking Large Language Models](https://arxiv.org/abs/2407.13796)
- [Against All Odds: Overcoming Typology, Script, and Language Confusion in Multilingual Embedding Inversion Attacks](https://arxiv.org/abs/2408.11749)
- [Rapid Response: Mitigating LLM Jailbreaks with a Few Examples](https://arxiv.org/abs/2411.07494)
- [Playing Language Game with LLMs Leads to Jailbreaking](https://arxiv.org/abs/2411.12762)
- [Obfuscated Activations Bypass LLM Latent-Space Defenses](https://arxiv.org/abs/2412.09565)
- [Illusions of Relevance: Using Content Injection Attacks to Deceive Retrievers, Rerankers, and LLM Judges](https://arxiv.org/abs/2501.18536)
- [Membership Inference Attacks on Large-Scale Models: A Survey](https://arxiv.org/abs/2503.19338)
- [LAGO: Few-shot Crosslingual Embedding Inversion Attacks via Language Similarity-Aware Graph Optimization](https://arxiv.org/abs/2505.16008)
- [Prompt Injection Resources 2026](prompt-dialogue/prompt-injection-resources-2026.md)
- [Prompt Engineering Attack Resources 2026](prompt-dialogue/prompt-engineering-resources-2026.md)
- [Prompt Engineering Attack Resources 2027](prompt-dialogue/prompt-engineering-resources-2027.md)
- [Fuzzing Resources 2026](fuzzing/fuzzing-resources-2026.md)
- [Prompt Engineering Attack Resources Extra](prompt-dialogue/prompt-engineering-resources-extra.md)
- [Puzzler Attack Resources](prompt-dialogue/puzzler-attack-resources.md)
- [Puzzler Attack Resources 2029](prompt-dialogue/puzzler-attack-resources-2029.md)
- [Prompt Engineering Attack Resources 2027](prompt-dialogue/prompt-engineering-resources-2027.md)
- [Prompt Engineering Attack Resources 2028](prompt-dialogue/prompt-engineering-resources-2028.md)
- [Additional Resources on LLM Attacks 2027](additional-resources-2027.md)
- [Additional Resources on LLM Attacks 2028](additional-resources-2028.md)
- [Fuzzing Resources 2027](fuzzing/fuzzing-resources-2027.md)
- [Fuzzing Resources 2028](fuzzing/fuzzing-resources-2028.md)
- [Linguistic Manipulation Resources 2027](linguistic-manipulation/linguistic-manipulation-resources-2027.md)
- [Neuron-Level Manipulation Resources 2028](neuron-resources-2028.md)
- [Neuron-Level Manipulation Resources 2029](neuron-resources-2029.md)
- [Linguistic Manipulation Resources 2028](linguistic-manipulation/linguistic-manipulation-resources-2028.md)
- [Linguistic Manipulation Resources 2029](linguistic-manipulation/linguistic-manipulation-resources-2029.md)
- [Fuzzing Resources 2029](fuzzing/fuzzing-resources-2029.md)
- [Fuzzing Resources 2030](fuzzing/fuzzing-resources-2030.md)
- [Fuzzing Resources 2031](fuzzing/fuzzing-resources-2031.md)
- [Fuzzing Resources 2032](fuzzing/fuzzing-resources-2032.md)
- [Additional Resources on LLM Attacks 2031](additional-resources-2031.md)
- [Additional Resources on LLM Attacks 2032](additional-resources-2032.md)
- [Authority Simulation Attack Resources 2026](social-engineering/authority-simulation-resources-2026.md)
- [Authority Simulation Attack Resources 2027](social-engineering/authority-simulation-resources-2027.md)
- [Authority Simulation Attack Resources 2028](social-engineering/authority-simulation-resources-2028.md)
- [Authority Simulation Attack Resources 2029](social-engineering/authority-simulation-resources-2029.md)
- [Authority Simulation Attack Resources 2030](social-engineering/authority-simulation-resources-2030.md)
- [Social Engineering and Emotional Manipulation Resources](social-engineering/emotional-manipulation-resources.md)
- [Emotional Manipulation Attack Resources 2027](social-engineering/emotional-manipulation-resources-2027.md)
- [Emotional Manipulation Attack Resources 2028](social-engineering/emotional-manipulation-resources-2028.md)
- [Emotional Manipulation Attack Resources 2029](social-engineering/emotional-manipulation-resources-2029.md)
- [Emotional Manipulation Attack Resources 2030](social-engineering/emotional-manipulation-resources-2030.md)
- [Prompt Engineering Attack Resources 2031](prompt-dialogue/prompt-engineering-resources-2031.md)
- [Neuron-Level Manipulation Resources 2033](neuron-resources-2033.md)
