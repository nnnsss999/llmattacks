---
title: "Additional Resources on LLM Attacks 2034"
category: "Overview"
source_url: ""
date_collected: 2025-06-22
license: "CC-BY-4.0"
---

The references below extend the catalog with newer research papers and blog posts that surfaced after the 2033 snapshot. They continue documenting jailbreak techniques and defenses.

- [Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues](https://arxiv.org/abs/2402.09091)
- [SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner](https://arxiv.org/abs/2406.05498)
- [WordGame: Efficient & Effective LLM Jailbreak via Simultaneous Obfuscation in Query and Response](https://arxiv.org/abs/2405.14023)
- [Jigsaw Puzzles: Splitting Harmful Questions to Jailbreak Large Language Models](https://arxiv.org/abs/2410.11459)
- [Jailbreaking to Jailbreak](https://arxiv.org/abs/2502.09638)
- [BaitAttack: Alleviating Intention Shift in Jailbreak Attacks via Adaptive Bait Crafting](https://aclanthology.org/2024.emnlp-main.877/)
- [SequentialBreak: Large Language Models Can be Fooled by Embedding Jailbreak Prompts into Sequential Prompt Chains](https://arxiv.org/abs/2411.06426)
- [AutoJailbreak: Exploring Jailbreak Attacks and Defenses through a Dependency Lens](https://arxiv.org/abs/2406.03805)
- [Utilizing Jailbreak Probability to Attack and Safeguard Multimodal LLMs](https://arxiv.org/abs/2503.06989)
- [Efficient Indirect LLM Jailbreak via Multimodal-LLM Jailbreak](https://arxiv.org/abs/2405.20015)
- [JailbreakHunter: A Visual Analytics Approach for Jailbreak Prompts Discovery From Large-Scale Human-LLM Conversational Datasets](https://doi.org/10.1109/tvcg.2025.3557568)
- [Summary the Savior: Harmful Keyword and Query-based Summarization for LLM Jailbreak Defense](https://doi.org/10.18653/v1/2025.trustnlp-main.17)
- [LLM Abuse Prevention Tool Using GCG Jailbreak Attack Detection and DistilBERT-Based Ethics Judgment](https://doi.org/10.3390/info16030204)
- [How Alignment and Jailbreak Work: Explain LLM Safety through Intermediate Hidden States](https://doi.org/10.18653/v1/2024.findings-emnlp.139)
- [Prompt Crossing: Evaluating Whether LLM Response Stem from Jailbreak or Normal Prompt](https://doi.org/10.1109/icassp49660.2025.10889949)
- [Samjailbreak: Query-Efficient Sampling-Based Jailbreak Attack Against Large Language Models](https://doi.org/10.2139/ssrn.5206108)
- [Defending Against GCG Jailbreak Attacks with Syntax Trees and Perplexity in LLMs](https://doi.org/10.1109/gcce62371.2024.10760963)
- [Optimizing Transformer Models for Prompt Jailbreak Attack Detection in AI Assistant Systems](https://doi.org/10.1109/vcris63677.2024.10813380)
- [Chain-of-Detection Enables Robust and Efficient Jailbreak Defense](https://doi.org/10.2139/ssrn.5124466)
- [Defending Large Language Models Against Jailbreak Attacks Through Chain of Thought Prompting](https://doi.org/10.1109/nana63151.2024.00028)
- [RePD: Defending Jailbreak Attack through a Retrieval-based Prompt Decomposition Process](https://doi.org/10.18653/v1/2025.findings-naacl.16)
- [Defending ChatGPT against Jailbreak Attack via Self-Reminder](https://doi.org/10.21203/rs.3.rs-2873090/v1)
- [PBI-Attack: Prior-Guided Bimodal Interactive Black-Box Jailbreak Attack for Toxicity Maximization](https://doi.org/10.18653/v1/2025.trustnlp-main.3)
- [Boosting Jailbreak Attack with Momentum](https://doi.org/10.1109/icassp49660.2025.10888812)
- [Analysis of Jailbreak Attacks and Defenses for Large Language Models](https://doi.org/10.47297/wspciwsp2516-252718.20250902)
- [Jailbreak Attacks on Large Language Models and Possible Defenses: Present Status and Future Possibilities](https://doi.org/10.1109/istas61960.2024.10732418)
- [MMJ-Bench: A Comprehensive Study on Jailbreak Attacks and Defenses for Vision Language Models](https://doi.org/10.1609/aaai.v39i26.34983)
