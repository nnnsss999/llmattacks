---
title: "Token Prediction & Manipulation Resources"
category: "Tokenization"
source_url: ""
date_collected: 2025-06-19
license: "CC-BY-4.0"
---

These papers and articles explore how adversaries abuse next-token prediction or manipulate token sequences to bypass safety filters and watermarking.

- [A Law of Next-Token Prediction in Large Language Models](https://arxiv.org/html/2408.13442v1)
- [Token Prediction as Implicit Classification to Identify LLM-Generated Text](https://aclanthology.org/2023.emnlp-main.810/)
- [Obfuscation/Token Smuggling - Learn Prompting](https://learnprompting.org/docs/prompt_hacking/offensive_measures/obfuscation)
- [Token-Smuggling: The Hidden Threat to A.I. and GPT Models](https://hashdork.com/token-smuggling/)
- [Mitigating a Token-Length Side-Channel Attack in Our AI Products](https://blog.cloudflare.com/ai-side-channel-attack-mitigated/)
- [What Was Your Prompt? A Remote Keylogging Attack on AI Assistants](https://arxiv.org/abs/2403.09751) - Describes a side-channel attack recovering prompts via token predictions.
- [Hackers Can Read Private AI-Assistant Chats Despite Encryption](https://arstechnica.com/security/2024/03/hackers-can-read-private-ai-assistant-chats-even-though-theyre-encrypted/) - ArsTechnica article on token-length side channels.
- [ChatGPT and Microsoft Copilot Chats Vulnerable to Token-Length Side-Channel Attack](https://veruscorp.com/chatgpt-and-microsoft-copilot-chats-vulnerable-to-token-length-side-channel-attack/) - Blog detailing the vulnerability.
- [ChatGPT Side-Channel Attack Has Easy Fix: Token Obfuscation](https://www.theregister.com/2024/03/18/chatgpt_sidechannel_attack_has_easy/) - The Register coverage proposing defences.
- [AI Conversations Exposed: The Token Length Vulnerability](https://medium.com/@firefalc0n/ai-conversations-exposed-the-token-length-vulnerability-in-chatgpt-copilot-and-more-0d182973c1ee) - Medium article exploring token-length leakage.
- [Awesome Side Channels](https://github.com/dpaleka/awesome-side-channels) - Curated list of research on side-channel attacks including token-based exploits.
- [Lockpicking LLMs: A Logit-Based Jailbreak Using Token-level Manipulation](https://arxiv.org/abs/2405.13068)
- [A Peek into Token Bias: Large Language Models Are Not Yet Genuine Reasoners](https://arxiv.org/abs/2406.11050)
- [TokenBreak: Bypassing Text Classification Models Through Token Manipulation](https://arxiv.org/abs/2506.07948)
- [Emoji-based Adversarial Attacks on LLMs](https://www.semanticscholar.org/paper/5b539b4e1b9f677301ac815d41677fb4ec040f4b)
- [Emoji Attack: Enhancing Jailbreak Attacks Against Judge LLM Detection](https://openreview.net/forum?id=Q0rKYiVEZq)
- [Understanding Token Splitting Attacks in LLMs](https://www.proventra-ai.com/blog/understanding-token-splitting-attacks-llms)
- [Byte-Level Universal Triggers](https://www.techradar.com/pro/security/this-cyberattack-lets-hackers-crack-ai-models-just-by-changing-a-single-character)
- [Enhancing Jailbreak Attack Against Large Language Models through Silent Prompts](https://arxiv.org/abs/2405.20653)
