---
title: "Token-Level Attack Resources 2027"
category: "Tokenization"
source_url: ""
date_collected: 2025-06-19
license: "CC-BY-4.0"
---

The references below extend the catalog with more research and articles on token-level manipulation. They showcase side-channel techniques, special token insertions, and adversarial tokenization methods disclosed after the original collection.

- [Tokenization Matters! Degrading Large Language Models through Challenging Their Tokenization](https://openreview.net/forum?id=grO9s3lESV) - Demonstrates how adversarial token splitting degrades model reasoning.
- [DeSparsify: Adversarial Attack Against Token Sparsification Mechanisms](https://openreview.net/forum?id=D4yRz3s7UL) - Shows how sparsification defences can be bypassed.
- [BOOST: Enhanced Jailbreak of Large Language Model via Slient eos Tokens](https://openreview.net/forum?id=JqKh7FLUw1) - Simple attack appending end-of-sequence tokens to bypass safety alignment.
- [What Was Your Prompt? A Remote Keylogging Attack on AI Assistants](https://arxiv.org/abs/2403.09751) - Describes a token-length side channel recovering encrypted prompts.
- [Mitigating a Token-Length Side-Channel Attack in Our AI Products](https://blog.cloudflare.com/ai-side-channel-attack-mitigated/) - Cloudflare's incident report on side-channel mitigation.
- [Mind the Inconspicuous: Revealing the Hidden Weakness in Aligned LLMs' Refusal Boundaries](https://arxiv.org/abs/2405.20653) - Explores silent prompt injection to control refusals.
- [Understanding Token Splitting Attacks in LLMs](https://www.proventra-ai.com/blog/understanding-token-splitting-attacks-llms) - Technical overview of token splitting for smuggling instructions.

