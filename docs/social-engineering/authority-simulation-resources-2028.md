---
title: "Authority Simulation Attack Resources 2028"
category: "Social Engineering"
source_url: ""
date_collected: 2025-06-20
license: "CC-BY-4.0"
---

Below are additional authoritative resources on social-engineering attacks that impersonate trusted figures or cite false authority to bypass LLM safeguards. They extend [authority-simulation-resources-2027.md](authority-simulation-resources-2027.md).

- [Authority Citation Jailbreak \| LLM Security Database](https://www.promptfoo.dev/lm-security-db/vuln/undefined-85d41cb4)
- [The Dark Side of Citation: How LLMs Get Tricked](https://www.promptlayer.com/research-papers/the-dark-side-of-citation-how-llms-get-tricked)
- [Intention Analysis Makes LLMs A Good Jailbreak Defender](https://aclanthology.org/2025.coling-main.199/)
- [How Alignment and Jailbreak Work: Explain LLM Safety through Real Scenarios](https://aclanthology.org/2024.findings-emnlp.139/)
- [JailGuard: A Universal Detection Framework for Prompt-based Attacks on Language Models](https://dl.acm.org/doi/10.1145/3724393)
- [When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs](https://arxiv.org/abs/2410.14569)
- [Strategies to Detect and Prevent LLM Security Challenges](https://medium.com/thesecmaster/5-security-challenges-in-llms-and-strategies-to-prevent-them-9279f8d7eaf5)
- [Exploring Attacks on Large Language Models: From Prompt Injection to Jailbreaking](https://medium.com/@pouyahallaj/exploring-attacks-on-large-language-models-llms-from-prompt-injection-to-jailbreaking-and-beyond-9e51bdd3cc9b)
- [5 Types of Impersonation Attacks and 6 Ways to Prevent Them](https://perception-point.io/guides/phishing/5-types-of-impersonation-attacks-and-6-ways-to-prevent-them/)
- [Preventing Impersonation Fraud with Semantic Analysis](https://www.proofpoint.com/us/blog/email-and-cloud-threats/preventing-impersonation-fraud-semantic-analysis-llm)
