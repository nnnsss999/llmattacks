---
title: "Persona-Based Jailbreak Resources"
category: "Social Engineering"
source_url: ""
date_collected: 2025-06-18
license: "CC-BY-4.0"
---

The following references highlight persona or role-playing techniques used to perform social-engineering attacks against large language models. These resources extend the catalog with examples and research on adopting malicious personas to bypass safeguards.

- [Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation](https://arxiv.org/abs/2311.03348) – shows how persona modulation can increase harmful completion rates on GPT-4, Claude 2, and Vicuna.
- [Persona-Based Testing for LLMs | AI Threat Emulation](https://apxml.com/courses/intro-llm-red-teaming/chapter-3-core-red-teaming-techniques-llms/persona-based-testing-llms) – explains how attacker personas help simulate real-world LLM threats.
- [Persona-Based LLM Jailbreak | LLM Security Database](https://www.promptfoo.dev/lm-security-db/vuln/undefined-2774f631) – describes a vulnerability where prompts cause LLMs to adopt harmful personas and produce illicit content.
- [Defining The Prompt-Level AI Jailbreaking Techniques](https://briandcolwell.com/defining-the-prompt-level-ai-jailbreaking-techniques/) – discusses role-playing and persona-based attacks including “DAN” and “God-Mode.”
- [Lakera Guide to Jailbreaking LLMs](https://www.lakera.ai/blog/jailbreaking-large-language-models-guide) – includes examples of roleplay jailbreaks that impersonate trusted personas to obtain restricted information.

- [Two Tales of Persona in LLMs: A Survey of Role-Playing and Personalization](https://arxiv.org/abs/2406.01171) – comprehensive survey of persona-driven role-play and personalization issues.
- [Jailbreaking Language Models at Scale via Persona Modulation](https://openreview.net/forum?id=gYa9R2Pmp8) – explores large-scale persona modulation to bypass safety filters.
- [Deceiving Claude: Persona-Based Safety Bypass Vulnerabilities in Constitutional AI](https://medium.com/@donaldafeith/deceiving-claude-persona-based-safety-bypass-vulnerabilities-in-constitutional-ai-cbf44615013e) – real-world persona exploitation case study on Anthropic models.
- [RoleLLM: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities of Large Language Models](https://arxiv.org/abs/2310.00746) – demonstrates how role-play prompts can coax disallowed behavior from LLMs.
- [Beyond Demographics: Aligning Role-playing LLM-based Agents Using Human Belief Networks](https://aclanthology.org/2024.findings-emnlp.819/) – discusses challenges aligning persona-driven LLM agents.
- [LLM Generated Persona is a Promise with a Catch](https://arxiv.org/abs/2503.16527) – analyzes the security risks of generating tailored personas.
- [Persona-DB: Efficient LLM Personalization for Response Prediction](https://arxiv.org/abs/2402.11060) – shows how persona-based customization can lead to unintended leakage.
- [Prompt Injection Attacks on LLMs](https://hiddenlayer.com/innovation-hub/prompt-injection-attacks-on-llms/) – includes persona-based prompt injection tactics.
