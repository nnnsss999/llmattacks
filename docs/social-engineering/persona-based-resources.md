---
title: "Persona-Based Jailbreak Resources"
category: "Social Engineering"
source_url: ""
date_collected: 2025-06-18
license: "CC-BY-4.0"
---

The following references highlight persona or role-playing techniques used to perform social-engineering attacks against large language models. These resources extend the catalog with examples and research on adopting malicious personas to bypass safeguards.

- [Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation](https://arxiv.org/abs/2311.03348) – shows how persona modulation can increase harmful completion rates on GPT-4, Claude 2, and Vicuna.
- [Persona-Based Testing for LLMs | AI Threat Emulation](https://apxml.com/courses/intro-llm-red-teaming/chapter-3-core-red-teaming-techniques-llms/persona-based-testing-llms) – explains how attacker personas help simulate real-world LLM threats.
- [Persona-Based LLM Jailbreak | LLM Security Database](https://www.promptfoo.dev/lm-security-db/vuln/undefined-2774f631) – describes a vulnerability where prompts cause LLMs to adopt harmful personas and produce illicit content.
- [Defining The Prompt-Level AI Jailbreaking Techniques](https://briandcolwell.com/defining-the-prompt-level-ai-jailbreaking-techniques/) – discusses role-playing and persona-based attacks including “DAN” and “God-Mode.”
- [Lakera Guide to Jailbreaking LLMs](https://www.lakera.ai/blog/jailbreaking-large-language-models-guide) – includes examples of roleplay jailbreaks that impersonate trusted personas to obtain restricted information.
