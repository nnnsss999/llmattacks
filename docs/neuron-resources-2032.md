---
title: "Neuron-Level Manipulation Resources 2032"
category: "Latent Space"
source_url: ""
date_collected: 2025-06-19
license: "CC-BY-4.0"
---

The references below extend the neuron-focused bibliography with papers and articles published after the 2028 update. They spotlight recent advances in neuron-level backdoors, activation patching, and pruning defenses for large language models.

## Research Papers

- [A Survey of Recent Backdoor Attacks and Defenses in Large Language Models](https://arxiv.org/abs/2406.06852) – comprehensive overview of backdoor methods including neuron-manipulation techniques.
- [A Survey on Backdoor Threats in Large Language Models: Attacks, Defenses, and Evaluation Methods](https://www.sciltp.com/journals/tai/article/view/2505000595) – summarizes backdoor research with a focus on neuron-level analysis.
- [Backdoor Attacks for LLMs with Weak-To-Strong Knowledge Distillation](https://openreview.net/forum?id=29LC48aY3U) – explores distillation-based neuron corruption strategies.
- [When Backdoors Speak: Understanding LLM Backdoor Attacks Through Model-Generated Explanations](https://arxiv.org/abs/2411.12701) – reveals how neurons toggle hidden states to trigger backdoors.
- [Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models](https://aclanthology.org/2024.naacl-long.171/) – demonstrates how fine-tuned instructions open pathways for neuron-level exploits.
- [BadEdit: Backdooring Large Language Models by Model Editing](https://openreview.net/forum?id=duZANm2ABX) – inserts malicious behaviour via targeted neuron edits.
- [Red Alarm for Pre-trained Models: Universal Vulnerability to Neuron-level Backdoor Attacks](https://www.mi-research.net/article/doi/10.1007/s11633-022-1377-5) – shows how single-neuron modifications implant persistent backdoors.
- [Improving Neuron-level Interpretability with White-box Language Models](https://arxiv.org/abs/2410.16443) – proposes analysis techniques that also reveal potential attack surfaces.
- [HÖLDER PRUNING: Localized Pruning for Backdoor Removal in Deep Neural Networks](https://openreview.net/forum?id=yJduhi9mDQ) – prunes malicious neurons to neutralize backdoors.
- [Reconstructive Neuron Pruning for Backdoor Defense](https://proceedings.mlr.press/v202/li23v.html) – reconstructs benign behaviour after removing compromised neurons.
- [Towards Best Practices of Activation Patching in Language Models: Metrics and Methods](https://arxiv.org/abs/2309.16042) – evaluates large-scale activation patching as a defence and probing method.

## Articles and Links

- [Attribution Patching: Activation Patching At Industrial Scale](https://www.neelnanda.io/mechanistic-interpretability/attribution-patching) – blog post exploring real-world neuron patching workflows.

