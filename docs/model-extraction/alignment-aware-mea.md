---
title: "Alignment-Aware Model Extraction Attacks"
category: "Inference"
source_url: "https://openreview.net/forum?id=AKsfpHc9sN"
date_collected: 2025-06-18
license: "Fair Use"
---

The paper proposes a model extraction attack that tailors queries to exploit an LLM's alignment features. By combining locality reinforced distillation with prompt engineering, attackers can recreate a high-fidelity copy while bypassing watermarking or output filtering safeguards.
