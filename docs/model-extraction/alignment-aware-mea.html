<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" as="image" href="/images/arrow_left.svg"/><link rel="preload" as="image" href="/images/pdf_icon_blue.svg"/><link rel="stylesheet" href="/_next/static/css/623ec4d945fb0950.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/1777a0996b88c8c1.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/1dad14d11404e245.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/7e1ff74241679440.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/8dc164557bded312.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-700efc7a3fe218fa.js"/><script src="/_next/static/chunks/4bd1b696-674b1201a1d63716.js" async=""></script><script src="/_next/static/chunks/1684-f2e76e08982cc2da.js" async=""></script><script src="/_next/static/chunks/main-app-895d0017b454980b.js" async=""></script><script src="/_next/static/chunks/e37a0b60-86dcf540460bd9a6.js" async=""></script><script src="/_next/static/chunks/7ce798d6-3eb8122476a3f2e5.js" async=""></script><script src="/_next/static/chunks/6874-8f3d6c72a87c2225.js" async=""></script><script src="/_next/static/chunks/3697-c0092b2c69fd8d8c.js" async=""></script><script src="/_next/static/chunks/590-a7095dbf68fad767.js" async=""></script><script src="/_next/static/chunks/4540-1380528b77b77034.js" async=""></script><script src="/_next/static/chunks/8780-219cd1efe9e9c581.js" async=""></script><script src="/_next/static/chunks/5226-3f9b6d1f505acbb7.js" async=""></script><script src="/_next/static/chunks/8533-b514bce0a35f7ac6.js" async=""></script><script src="/_next/static/chunks/9433-a9a98e8d4788630c.js" async=""></script><script src="/_next/static/chunks/app/layout-16592c7bbd62e480.js" async=""></script><script src="/_next/static/chunks/6325-93a1b42c84bba41c.js" async=""></script><script src="/_next/static/chunks/7153-99554ef3ddb80cbe.js" async=""></script><script src="/_next/static/chunks/2308-5ba70fcab32074d1.js" async=""></script><script src="/_next/static/chunks/424-bf9f8c3b9c338050.js" async=""></script><script src="/_next/static/chunks/2064-620cbcc98eda7280.js" async=""></script><script src="/_next/static/chunks/4281-defa8559eb9a8986.js" async=""></script><script src="/_next/static/chunks/9911-76bc1f1d480a89fe.js" async=""></script><script src="/_next/static/chunks/2882-66622e31b0f52a02.js" async=""></script><script src="/_next/static/chunks/4745-960031cc83ae4c6c.js" async=""></script><script src="/_next/static/chunks/5262-9b0df618c3a5e5de.js" async=""></script><script src="/_next/static/chunks/4438-1e3980f50c842bfd.js" async=""></script><script src="/_next/static/chunks/app/forum/page-259d5f29b9409771.js" async=""></script><script src="/_next/static/chunks/app/error-e8e38cde74148468.js" async=""></script><script src="/_next/static/chunks/app/global-error-437204fce1f23329.js" async=""></script><link rel="preload" href="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-chtml-full.js" as="script"/><link rel="preload" href="https://challenges.cloudflare.com/turnstile/v0/api.js" as="script"/><link rel="preload" href="https://www.googletagmanager.com/gtag/js?id=G-GTB25PBMVL" as="script"/><meta name="next-size-adjust" content=""/><link rel="icon" href="/favicon.ico"/><link rel="manifest" href="/manifest.json"/><title>Alignment-Aware Model Extraction Attacks on Large Language Models | OpenReview</title><meta name="description" content="Model extraction attacks (MEAs) on large language models (LLMs) have received increasing attention in recent research. However, existing attack methods typically adapt the extraction strategies originally developed for deep neural networks (DNNs). They neglect the underlying inconsistency between the training tasks of MEA and LLM alignment, leading to suboptimal attack performance. To tackle this issue, we propose Locality Reinforced Distillation (LoRD), a novel model extraction algorithm specifically designed for LLMs. In particular, LoRD employs a newly defined policy-gradient-style training task that utilizes the responses of victim model as the signal to guide the crafting of preference for the local model. Theoretical analyses demonstrate that i) the convergence procedure of LoRD in model extraction is consistent with the alignment procedure of LLMs, and ii) LoRD can reduce query complexity while mitigating watermark protection through exploration-based stealing. Extensive experiments on domain-specific extractions validate the superiority of our method in extracting various state-of-the-art commercial LLMs."/><meta name="citation_title" content="Alignment-Aware Model Extraction Attacks on Large Language Models"/><meta name="citation_author" content="Zi Liang"/><meta name="citation_author" content="Qingqing Ye"/><meta name="citation_author" content="Yanyun Wang"/><meta name="citation_author" content="Sen Zhang"/><meta name="citation_author" content="Yaxin Xiao"/><meta name="citation_author" content="RongHua Li"/><meta name="citation_author" content="Jianliang Xu"/><meta name="citation_author" content="Haibo Hu"/><meta name="citation_online_date" content="2024/10/04"/><meta name="citation_pdf_url" content="https://openreview.net/pdf?id=AKsfpHc9sN"/><meta name="citation_abstract" content="Model extraction attacks (MEAs) on large language models (LLMs) have received increasing attention in recent research. However, existing attack methods typically adapt the extraction strategies originally developed for deep neural networks (DNNs). They neglect the underlying inconsistency between the training tasks of MEA and LLM alignment, leading to suboptimal attack performance. To tackle this issue, we propose Locality Reinforced Distillation (LoRD), a novel model extraction algorithm specifically designed for LLMs. In particular, LoRD employs a newly defined policy-gradient-style training task that utilizes the responses of victim model as the signal to guide the crafting of preference for the local model. Theoretical analyses demonstrate that i) the convergence procedure of LoRD in model extraction is consistent with the alignment procedure of LLMs, and ii) LoRD can reduce query complexity while mitigating watermark protection through exploration-based stealing. Extensive experiments on domain-specific extractions validate the superiority of our method in extracting various state-of-the-art commercial LLMs."/><meta property="og:title" content="Alignment-Aware Model Extraction Attacks on Large Language Models"/><meta property="og:description" content="Model extraction attacks (MEAs) on large language models (LLMs) have received increasing attention in recent research. However, existing attack methods typically adapt the extraction strategies..."/><meta property="og:type" content="article"/><meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Alignment-Aware Model Extraction Attacks on Large Language Models"/><meta name="twitter:description" content="Model extraction attacks (MEAs) on large language models (LLMs) have received increasing attention in recent research. However, existing attack methods typically adapt the extraction strategies..."/><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="__className_086c6e"><div id="__next"><nav class="navbar navbar-inverse" role="navigation"><div class="container"><div class="navbar-header"><button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar"><span class="sr-only">Toggle navigation</span><span class="icon-bar"></span><span class="icon-bar"></span><span class="icon-bar"></span></button><a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a></div><div id="navbar" class="navbar-collapse collapse"><form class="navbar-form navbar-left profile-search" role="search"><div class="form-group has-feedback"><input type="text" class="form-control" placeholder="Search OpenReview..." autoComplete="off" autoCorrect="off" name="term" value=""/><span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span></div><input type="hidden" name="group" value="all"/><input type="hidden" name="content" value="all"/><input type="hidden" name="source" value="all"/></form><ul class="nav navbar-nav navbar-right"><li id="user-menu"><a href="/login">Login</a></li></ul></div></div></nav><div id="flash-message-container" class="alert alert-danger fixed-overlay" role="alert" style="display:none"><div class="container"><div class="row"><div class="col-xs-12"><div class="alert-content"><button type="button" class="close" aria-label="Close"><span aria-hidden="true">Ã—</span></button></div></div></div></div></div><script>(self.__next_s=self.__next_s||[]).push(["https://challenges.cloudflare.com/turnstile/v0/api.js",{}])</script><div id="or-banner" class="banner"><div class="container"><div class="row"><div class="col-xs-12"><a title="Venue Homepage" href="/group?id=ICLR.cc/2025/Conference"><img class="icon" src="/images/arrow_left.svg" alt="back arrow"/>Go to <strong>ICLR 2025 Conference</strong> <!-- -->homepage</a></div></div></div></div><div class="container"><div class="row"><main id="content"><div class="Forum_forum__wS8Fw"><div class="forum-container"><div class="forum-note"><div class="forum-title mt-2 mb-2"><h2 class="citation_title">Alignment-Aware Model Extraction Attacks on Large Language Models</h2><div class="forum-content-link"><a class="citation_pdf_url" href="/pdf?id=AKsfpHc9sN" title="Download PDF" target="_blank" rel="noreferrer"><img src="/images/pdf_icon_blue.svg" alt="Download PDF"/></a></div></div><div class="forum-authors mb-2"><h3><span><a title="~Zi_Liang1" data-toggle="tooltip" data-placement="top" href="/profile?id=~Zi_Liang1">Zi Liang</a>, <a title="~Qingqing_Ye1" data-toggle="tooltip" data-placement="top" href="/profile?id=~Qingqing_Ye1">Qingqing Ye</a>, <a title="~Yanyun_Wang1" data-toggle="tooltip" data-placement="top" href="/profile?id=~Yanyun_Wang1">Yanyun Wang</a>, <a title="~Sen_Zhang8" data-toggle="tooltip" data-placement="top" href="/profile?id=~Sen_Zhang8">Sen Zhang</a>, <a title="~Yaxin_Xiao2" data-toggle="tooltip" data-placement="top" href="/profile?id=~Yaxin_Xiao2">Yaxin Xiao</a>, <a title="~RongHua_Li1" data-toggle="tooltip" data-placement="top" href="/profile?id=~RongHua_Li1">RongHua Li</a>, <a title="~Jianliang_Xu1" data-toggle="tooltip" data-placement="top" href="/profile?id=~Jianliang_Xu1">Jianliang Xu</a>, <a title="~Haibo_Hu2" data-toggle="tooltip" data-placement="top" href="/profile?id=~Haibo_Hu2">Haibo Hu</a> <!-- --> </span></h3></div><div class="clearfix mb-1"><div class="forum-meta"><span class="date item"><span class="glyphicon glyphicon-calendar " aria-hidden="true"></span>28 Sept 2024 (modified: 05 Feb 2025)</span><span class="item"><span class="glyphicon glyphicon-folder-open " aria-hidden="true"></span>Submitted to ICLR 2025</span><span class="readers item" data-toggle="tooltip" data-placement="top" title="Visible to &lt;br/&gt;everyone&lt;br/&gt;since 04 Oct 2024"><span class="glyphicon glyphicon-eye-open " aria-hidden="true"></span>Everyone</span><span class="item"><span class="glyphicon glyphicon-duplicate " aria-hidden="true"></span><a href="/revisions?id=AKsfpHc9sN">Revisions</a></span><span class="item"><span class="glyphicon glyphicon-bookmark " aria-hidden="true"></span><a href="#" data-target="#bibtex-modal" data-toggle="modal" data-bibtex="%40misc%7B%0Aliang2025alignmentaware%2C%0Atitle%3D%7BAlignment-Aware%20Model%20Extraction%20Attacks%20on%20Large%20Language%20Models%7D%2C%0Aauthor%3D%7BZi%20Liang%20and%20Qingqing%20Ye%20and%20Yanyun%20Wang%20and%20Sen%20Zhang%20and%20Yaxin%20Xiao%20and%20RongHua%20Li%20and%20Jianliang%20Xu%20and%20Haibo%20Hu%7D%2C%0Ayear%3D%7B2025%7D%2C%0Aurl%3D%7Bhttps%3A%2F%2Fopenreview.net%2Fforum%3Fid%3DAKsfpHc9sN%7D%0A%7D">BibTeX</a></span><span class="item"><span class="glyphicon glyphicon-copyright-mark " aria-hidden="true"></span><a href="https://creativecommons.org/licenses/by/4.0/" target="_blank" rel="noopener noreferrer" title="Licensed under Creative Commons Attribution 4.0 International" data-toggle="tooltip" data-placement="top">CC BY 4.0</a></span></div><div class="invitation-buttons"></div></div><div class="note-content"><div><strong class="note-content-field disable-tex-rendering">Keywords<!-- -->:</strong> <span class="note-content-value">Model Extraction Attack, Large Language Models, Alignment</span></div><div><strong class="note-content-field disable-tex-rendering">Abstract<!-- -->:</strong> <span class="note-content-value">Model extraction attacks (MEAs) on large language models (LLMs) have received increasing attention in recent research. However, existing attack methods typically adapt the extraction strategies originally developed for deep neural networks (DNNs). They neglect the underlying inconsistency between the training tasks of MEA and LLM alignment, leading to suboptimal attack performance. To tackle this issue, we propose Locality Reinforced Distillation (LoRD), a novel model extraction algorithm specifically designed for LLMs. In particular, LoRD employs a newly defined policy-gradient-style training task that utilizes the responses of victim model as the signal to guide the crafting of preference for the local model. Theoretical analyses demonstrate that i) the convergence procedure of LoRD in model extraction is consistent with the alignment procedure of LLMs, and ii) LoRD can reduce query complexity while mitigating watermark protection through exploration-based stealing. Extensive experiments on domain-specific extractions validate the superiority of our method in extracting various state-of-the-art commercial LLMs.</span></div><div><strong class="note-content-field disable-tex-rendering">Primary Area<!-- -->:</strong> <span class="note-content-value">alignment, fairness, safety, privacy, and societal considerations</span></div><div><strong class="note-content-field disable-tex-rendering">Code Of Ethics<!-- -->:</strong> <span class="note-content-value">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.</span></div><div><strong class="note-content-field disable-tex-rendering">Submission Guidelines<!-- -->:</strong> <span class="note-content-value">I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.</span></div><div><strong class="note-content-field disable-tex-rendering">Anonymous Url<!-- -->:</strong> <span class="note-content-value">I certify that there is no URL (e.g., github page) that could be used to find authorsâ€™ identity.</span></div><div><strong class="note-content-field disable-tex-rendering">No Acknowledgement Section<!-- -->:</strong> <span class="note-content-value">I certify that there is no acknowledgement section in this submission for double blind review.</span></div><div><strong class="note-content-field disable-tex-rendering">Submission Number<!-- -->:</strong> <span class="note-content-value">13822</span></div></div></div><div class="row forum-replies-container layout-default"><div class="col-xs-12"><div id="forum-replies"><div class="spinner-container spinner-inline"><div class="spinner undefined"><div class="rect1"></div><div class="rect2"></div><div class="rect3"></div><div class="rect4"></div><div class="rect5"></div></div><span>Loading</span></div></div></div></div></div></div></main></div></div><footer class="sitemap"><div class="container"><div class="row hidden-xs"><div class="col-sm-4"><ul class="list-unstyled"><li><a href="/about">About OpenReview</a></li><li><a href="/group?id=OpenReview.net/Support">Hosting a Venue</a></li><li><a href="/venues">All Venues</a></li></ul></div><div class="col-sm-4"><ul class="list-unstyled"><li><a href="/contact">Contact</a></li><li><a>Feedback</a></li><li><a href="/sponsors">Sponsors</a></li><li><a class="join-the-team" href="https://codeforscience.org/jobs?job=OpenReview-Developer" target="_blank" rel="noopener noreferrer"><strong>Join the Team</strong></a></li></ul></div><div class="col-sm-4"><ul class="list-unstyled"><li><a href="https://docs.openreview.net/getting-started/frequently-asked-questions">Frequently Asked Questions</a></li><li><a href="/legal/terms">Terms of Use</a></li><li><a href="/legal/privacy">Privacy Policy</a></li></ul></div></div><div class="row visible-xs-block"><div class="col-xs-6"><ul class="list-unstyled"><li><a href="/about">About OpenReview</a></li><li><a href="/group?id=OpenReview.net/Support">Hosting a Venue</a></li><li><a href="/venues">All Venues</a></li><li><a href="/sponsors">Sponsors</a></li><li><a class="join-the-team" href="https://codeforscience.org/jobs?job=OpenReview-Developer" target="_blank" rel="noopener noreferrer"><strong>Join the Team</strong></a></li></ul></div><div class="col-xs-6"><ul class="list-unstyled"><li><a href="https://docs.openreview.net/getting-started/frequently-asked-questions">Frequently Asked Questions</a></li><li><a href="/contact">Contact</a></li><li><a>Feedback</a></li><li><a href="/legal/terms">Terms of Use</a></li><li><a href="/legal/privacy">Privacy Policy</a></li></ul></div></div></div></footer><footer class="sponsor"><div class="container"><div class="row"><div class="col-sm-10 col-sm-offset-1"><p class="text-center"><a href="/about" target="_blank">OpenReview</a> <!-- -->is a long-term project to advance science through improved peer review with legal nonprofit status. We gratefully acknowledge the support of the<!-- --> <a href="/sponsors" target="_blank">OpenReview Sponsors</a>. Â© <!-- -->2025<!-- --> OpenReview</p></div></div></div></footer></div><script src="/_next/static/chunks/webpack-700efc7a3fe218fa.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[64818,[\"4935\",\"static/chunks/e37a0b60-86dcf540460bd9a6.js\",\"3740\",\"static/chunks/7ce798d6-3eb8122476a3f2e5.js\",\"6874\",\"static/chunks/6874-8f3d6c72a87c2225.js\",\"3697\",\"static/chunks/3697-c0092b2c69fd8d8c.js\",\"590\",\"static/chunks/590-a7095dbf68fad767.js\",\"4540\",\"static/chunks/4540-1380528b77b77034.js\",\"8780\",\"static/chunks/8780-219cd1efe9e9c581.js\",\"5226\",\"static/chunks/5226-3f9b6d1f505acbb7.js\",\"8533\",\"static/chunks/8533-b514bce0a35f7ac6.js\",\"9433\",\"static/chunks/9433-a9a98e8d4788630c.js\",\"7177\",\"static/chunks/app/layout-16592c7bbd62e480.js\"],\"default\"]\n3:I[6874,[\"4935\",\"static/chunks/e37a0b60-86dcf540460bd9a6.js\",\"6874\",\"static/chunks/6874-8f3d6c72a87c2225.js\",\"3697\",\"static/chunks/3697-c0092b2c69fd8d8c.js\",\"590\",\"static/chunks/590-a7095dbf68fad767.js\",\"6325\",\"static/chunks/6325-93a1b42c84bba41c.js\",\"4540\",\"static/chunks/4540-1380528b77b77034.js\",\"7153\",\"static/chunks/7153-99554ef3ddb80cbe.js\",\"2308\",\"static/chunks/2308-5ba70fcab32074d1.js\",\"424\",\"static/chunks/424-bf9f8c3b9c338050.js\",\"2064\",\"static/chunks/2064-620cbcc98eda7280.js\",\"9433\",\"static/chunks/9433-a9a98e8d4788630c.js\",\"4281\",\"static/chunks/4281-defa8559eb9a8986.js\",\"9911\",\"static/chunks/9911-76bc1f1d480a89fe.js\",\"2882\",\"static/chunks/2882-66622e31b0f52a02.js\",\"4745\",\"static/chunks/4745-960031cc83ae4c6c.js\",\"5262\",\"static/chunks/5262-9b0df618c3a5e5de.js\",\"4438\",\"static/chunks/4438-1e3980f50c842bfd.js\",\"5300\",\"static/chunks/app/forum/page-259d5f29b9409771.js\"],\"\"]\n4:I[41316,[\"4935\",\"static/chunks/e37a0b60-86dcf540460bd9a6.js\",\"3740\",\"static/chunks/7ce798d6-3eb8122476a3f2e5.js\",\"6874\",\"static/chunks/6874-8f3d6c72a87c2225.js\",\"3697\",\"static/chunks/3697-c0092b2c69fd8d8c.js\",\"590\",\"static/chunks/590-a7095dbf68fad767.js\",\"4540\",\"static/chunks/4540-1380528b77b77034.js\",\"8780\",\"static/chunks/8780-219cd1efe9e9c581.js\",\"5226\",\"static/chunks/5226-3f9b6d1f505acbb7.js\",\"8533\",\"static/chunks/8533-b514bce0a35f7ac6.js\",\"9433\",\"static/chunks/9433-a9a98e8d4788630c.js\",\"7177\",\"static/chunks/app/layout-16592c7bbd62e480.js\"],\"default\"]\n6:I[46967,[\""])</script><script>self.__next_f.push([1,"4935\",\"static/chunks/e37a0b60-86dcf540460bd9a6.js\",\"3740\",\"static/chunks/7ce798d6-3eb8122476a3f2e5.js\",\"6874\",\"static/chunks/6874-8f3d6c72a87c2225.js\",\"3697\",\"static/chunks/3697-c0092b2c69fd8d8c.js\",\"590\",\"static/chunks/590-a7095dbf68fad767.js\",\"4540\",\"static/chunks/4540-1380528b77b77034.js\",\"8780\",\"static/chunks/8780-219cd1efe9e9c581.js\",\"5226\",\"static/chunks/5226-3f9b6d1f505acbb7.js\",\"8533\",\"static/chunks/8533-b514bce0a35f7ac6.js\",\"9433\",\"static/chunks/9433-a9a98e8d4788630c.js\",\"7177\",\"static/chunks/app/layout-16592c7bbd62e480.js\"],\"default\"]\n7:I[87555,[],\"\"]\n8:I[31702,[\"6874\",\"static/chunks/6874-8f3d6c72a87c2225.js\",\"3697\",\"static/chunks/3697-c0092b2c69fd8d8c.js\",\"590\",\"static/chunks/590-a7095dbf68fad767.js\",\"6325\",\"static/chunks/6325-93a1b42c84bba41c.js\",\"4540\",\"static/chunks/4540-1380528b77b77034.js\",\"7153\",\"static/chunks/7153-99554ef3ddb80cbe.js\",\"9433\",\"static/chunks/9433-a9a98e8d4788630c.js\",\"4281\",\"static/chunks/4281-defa8559eb9a8986.js\",\"9911\",\"static/chunks/9911-76bc1f1d480a89fe.js\",\"8039\",\"static/chunks/app/error-e8e38cde74148468.js\"],\"default\"]\n9:I[31295,[],\"\"]\na:I[64757,[\"4935\",\"static/chunks/e37a0b60-86dcf540460bd9a6.js\",\"6874\",\"static/chunks/6874-8f3d6c72a87c2225.js\",\"3697\",\"static/chunks/3697-c0092b2c69fd8d8c.js\",\"590\",\"static/chunks/590-a7095dbf68fad767.js\",\"6325\",\"static/chunks/6325-93a1b42c84bba41c.js\",\"4540\",\"static/chunks/4540-1380528b77b77034.js\",\"7153\",\"static/chunks/7153-99554ef3ddb80cbe.js\",\"2308\",\"static/chunks/2308-5ba70fcab32074d1.js\",\"424\",\"static/chunks/424-bf9f8c3b9c338050.js\",\"2064\",\"static/chunks/2064-620cbcc98eda7280.js\",\"9433\",\"static/chunks/9433-a9a98e8d4788630c.js\",\"4281\",\"static/chunks/4281-defa8559eb9a8986.js\",\"9911\",\"static/chunks/9911-76bc1f1d480a89fe.js\",\"2882\",\"static/chunks/2882-66622e31b0f52a02.js\",\"4745\",\"static/chunks/4745-960031cc83ae4c6c.js\",\"5262\",\"static/chunks/5262-9b0df618c3a5e5de.js\",\"4438\",\"static/chunks/4438-1e3980f50c842bfd.js\",\"5300\",\"static/chunks/app/forum/page-259d5f29b9409771.js\"],\"default\"]\nb:I[69243,[\"4935\",\"static/chunks/e37a0b60-86dcf540460bd9a6.j"])</script><script>self.__next_f.push([1,"s\",\"3740\",\"static/chunks/7ce798d6-3eb8122476a3f2e5.js\",\"6874\",\"static/chunks/6874-8f3d6c72a87c2225.js\",\"3697\",\"static/chunks/3697-c0092b2c69fd8d8c.js\",\"590\",\"static/chunks/590-a7095dbf68fad767.js\",\"4540\",\"static/chunks/4540-1380528b77b77034.js\",\"8780\",\"static/chunks/8780-219cd1efe9e9c581.js\",\"5226\",\"static/chunks/5226-3f9b6d1f505acbb7.js\",\"8533\",\"static/chunks/8533-b514bce0a35f7ac6.js\",\"9433\",\"static/chunks/9433-a9a98e8d4788630c.js\",\"7177\",\"static/chunks/app/layout-16592c7bbd62e480.js\"],\"\"]\nd:I[59665,[],\"OutletBoundary\"]\n10:I[59665,[],\"ViewportBoundary\"]\n12:I[59665,[],\"MetadataBoundary\"]\n14:I[89340,[\"6874\",\"static/chunks/6874-8f3d6c72a87c2225.js\",\"4219\",\"static/chunks/app/global-error-437204fce1f23329.js\"],\"default\"]\n:HL[\"/_next/static/media/08f4947ad4536ee1-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/c4250770ab8708b6-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/css/623ec4d945fb0950.css\",\"style\"]\n:HL[\"/_next/static/css/1777a0996b88c8c1.css\",\"style\"]\n:HL[\"/_next/static/css/1dad14d11404e245.css\",\"style\"]\n:HL[\"/_next/static/css/7e1ff74241679440.css\",\"style\"]\n:HL[\"/_next/static/css/8dc164557bded312.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"v1.14.1\",\"p\":\"\",\"c\":[\"\",\"forum?id=AKsfpHc9sN\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"forum\",{\"children\":[\"__PAGE__?{\\\"id\\\":\\\"AKsfpHc9sN\\\"}\",{}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/623ec4d945fb0950.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/1777a0996b88c8c1.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"link\",\"2\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/1dad14d11404e245.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"link\",\"3\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/7e1ff74241679440.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[[\"$\",\"head\",null,{\"children\":[[\"$\",\"link\",null,{\"rel\":\"icon\",\"href\":\"/favicon.ico\"}],[\"$\",\"link\",null,{\"rel\":\"manifest\",\"href\":\"/manifest.json\"}]]}],[\"$\",\"$L2\",null,{\"children\":[\"$\",\"body\",null,{\"className\":\"__className_086c6e\",\"children\":[\"$\",\"div\",null,{\"id\":\"__next\",\"children\":[[\"$\",\"nav\",null,{\"className\":\"navbar navbar-inverse\",\"role\":\"navigation\",\"children\":[\"$\",\"div\",null,{\"className\":\"container\",\"children\":[[\"$\",\"div\",null,{\"className\":\"navbar-header\",\"children\":[[\"$\",\"button\",null,{\"type\":\"button\",\"className\":\"navbar-toggle collapsed\",\"data-toggle\":\"collapse\",\"data-target\":\"#navbar\",\"aria-expanded\":\"false\",\"aria-controls\":\"navbar\",\"children\":[[\"$\",\"span\",null,{\"className\":\"sr-only\",\"children\":\"Toggle navigation\"}],[\"$\",\"span\",null,{\"className\":\"icon-bar\"}],[\"$\",\"span\",null,{\"className\":\"icon-bar\"}],[\"$\",\"span\",null,{\"className\":\"icon-bar\"}]]}],[\"$\",\"$L3\",null,{\"href\":\"/\",\"className\":\"navbar-brand home push-link\",\"children\":[[\"$\",\"strong\",null,{\"children\":\"OpenReview\"}],\".net\"]}]]}],[\"$\",\"div\",null,{\"id\":\"navbar\",\"className\":\"navbar-collapse collapse\",\"children\":[[\"$\",\"$L4\",null,{}],\"$L5\"]}]]}]}],[\"$\",\"div\",null,{\"id\":\"flash-message-container\",\"className\":\"alert alert-danger fixed-overlay\",\"role\":\"alert\",\"style\":{\"display\":\"none\"},\"children\":[\"$\",\"div\",null,{\"className\":\"container\",\"children\":[\"$\",\"div\",null,{\"className\":\"row\",\"children\":[\"$\",\"div\",null,{\"className\":\"col-xs-12\",\"children\":[\"$\",\"div\",null,{\"className\":\"alert-content\",\"children\":[\"$\",\"button\",null,{\"type\":\"button\",\"className\":\"close\",\"aria-label\":\"Close\",\"children\":[\"$\",\"span\",null,{\"aria-hidden\":\"true\",\"children\":\"Ã—\"}]}]}]}]}]}]}],[\"$\",\"$L6\",null,{}],[\"$\",\"$L7\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$8\",\"errorStyles\":[],\"errorScripts\":[],\"template\":[\"$\",\"$L9\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[\"$\",\"$La\",null,{\"statusCode\":404,\"message\":\"Please check that the URL is spelled correctly and try again.\"}],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}]}]}],[[\"$\",\"$Lb\",null,{\"src\":\"https://www.googletagmanager.com/gtag/js?id=G-GTB25PBMVL\"}],[\"$\",\"$Lb\",null,{\"id\":\"ga-script\",\"dangerouslySetInnerHTML\":{\"__html\":\"window.dataLayer = window.dataLayer || [];\\nfunction gtag() { dataLayer.push(arguments); }\\ngtag('js', new Date());\\ngtag('config', 'G-GTB25PBMVL', {\\npage_location: location.origin + location.pathname + location.search,\\n});\"}}]]]}]]}],{\"children\":[\"forum\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L7\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L9\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[\"$Lc\",\"$undefined\",[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/8dc164557bded312.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"$Ld\",null,{\"children\":[\"$Le\",\"$Lf\",null]}]]}],{},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"WgL7veQFWQ5LES1yXQluT\",{\"children\":[[\"$\",\"$L10\",null,{\"children\":\"$L11\"}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]]}],[\"$\",\"$L12\",null,{\"children\":\"$L13\"}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$14\",[]],\"s\":false,\"S\":false}\n"])</script><script>self.__next_f.push([1,"5:[\"$\",\"ul\",null,{\"className\":\"nav navbar-nav navbar-right\",\"children\":[\"$\",\"li\",null,{\"id\":\"user-menu\",\"children\":[\"$\",\"$L3\",null,{\"href\":\"/login\",\"children\":\"Login\"}]}]}]\n11:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\ne:null\n"])</script><script>self.__next_f.push([1,"15:I[39677,[\"4935\",\"static/chunks/e37a0b60-86dcf540460bd9a6.js\",\"6874\",\"static/chunks/6874-8f3d6c72a87c2225.js\",\"3697\",\"static/chunks/3697-c0092b2c69fd8d8c.js\",\"590\",\"static/chunks/590-a7095dbf68fad767.js\",\"6325\",\"static/chunks/6325-93a1b42c84bba41c.js\",\"4540\",\"static/chunks/4540-1380528b77b77034.js\",\"7153\",\"static/chunks/7153-99554ef3ddb80cbe.js\",\"2308\",\"static/chunks/2308-5ba70fcab32074d1.js\",\"424\",\"static/chunks/424-bf9f8c3b9c338050.js\",\"2064\",\"static/chunks/2064-620cbcc98eda7280.js\",\"9433\",\"static/chunks/9433-a9a98e8d4788630c.js\",\"4281\",\"static/chunks/4281-defa8559eb9a8986.js\",\"9911\",\"static/chunks/9911-76bc1f1d480a89fe.js\",\"2882\",\"static/chunks/2882-66622e31b0f52a02.js\",\"4745\",\"static/chunks/4745-960031cc83ae4c6c.js\",\"5262\",\"static/chunks/5262-9b0df618c3a5e5de.js\",\"4438\",\"static/chunks/4438-1e3980f50c842bfd.js\",\"5300\",\"static/chunks/app/forum/page-259d5f29b9409771.js\"],\"default\"]\n16:I[73775,[\"4935\",\"static/chunks/e37a0b60-86dcf540460bd9a6.js\",\"6874\",\"static/chunks/6874-8f3d6c72a87c2225.js\",\"3697\",\"static/chunks/3697-c0092b2c69fd8d8c.js\",\"590\",\"static/chunks/590-a7095dbf68fad767.js\",\"6325\",\"static/chunks/6325-93a1b42c84bba41c.js\",\"4540\",\"static/chunks/4540-1380528b77b77034.js\",\"7153\",\"static/chunks/7153-99554ef3ddb80cbe.js\",\"2308\",\"static/chunks/2308-5ba70fcab32074d1.js\",\"424\",\"static/chunks/424-bf9f8c3b9c338050.js\",\"2064\",\"static/chunks/2064-620cbcc98eda7280.js\",\"9433\",\"static/chunks/9433-a9a98e8d4788630c.js\",\"4281\",\"static/chunks/4281-defa8559eb9a8986.js\",\"9911\",\"static/chunks/9911-76bc1f1d480a89fe.js\",\"2882\",\"static/chunks/2882-66622e31b0f52a02.js\",\"4745\",\"static/chunks/4745-960031cc83ae4c6c.js\",\"5262\",\"static/chunks/5262-9b0df618c3a5e5de.js\",\"4438\",\"static/chunks/4438-1e3980f50c842bfd.js\",\"5300\",\"static/chunks/app/forum/page-259d5f29b9409771.js\"],\"default\"]\n18:I[32587,[\"4935\",\"static/chunks/e37a0b60-86dcf540460bd9a6.js\",\"6874\",\"static/chunks/6874-8f3d6c72a87c2225.js\",\"3697\",\"static/chunks/3697-c0092b2c69fd8d8c.js\",\"590\",\"static/chunks/590-a7095dbf68fad767.js\",\"6325\",\"static/chunks/6325-93a1b42c84bba41c.js"])</script><script>self.__next_f.push([1,"\",\"4540\",\"static/chunks/4540-1380528b77b77034.js\",\"7153\",\"static/chunks/7153-99554ef3ddb80cbe.js\",\"2308\",\"static/chunks/2308-5ba70fcab32074d1.js\",\"424\",\"static/chunks/424-bf9f8c3b9c338050.js\",\"2064\",\"static/chunks/2064-620cbcc98eda7280.js\",\"9433\",\"static/chunks/9433-a9a98e8d4788630c.js\",\"4281\",\"static/chunks/4281-defa8559eb9a8986.js\",\"9911\",\"static/chunks/9911-76bc1f1d480a89fe.js\",\"2882\",\"static/chunks/2882-66622e31b0f52a02.js\",\"4745\",\"static/chunks/4745-960031cc83ae4c6c.js\",\"5262\",\"static/chunks/5262-9b0df618c3a5e5de.js\",\"4438\",\"static/chunks/4438-1e3980f50c842bfd.js\",\"5300\",\"static/chunks/app/forum/page-259d5f29b9409771.js\"],\"default\"]\n17:T46a,Model extraction attacks (MEAs) on large language models (LLMs) have received increasing attention in recent research. However, existing attack methods typically adapt the extraction strategies originally developed for deep neural networks (DNNs). They neglect the underlying inconsistency between the training tasks of MEA and LLM alignment, leading to suboptimal attack performance. To tackle this issue, we propose Locality Reinforced Distillation (LoRD), a novel model extraction algorithm specifically designed for LLMs. In particular, LoRD employs a newly defined policy-gradient-style training task that utilizes the responses of victim model as the signal to guide the crafting of preference for the local model. Theoretical analyses demonstrate that i) the convergence procedure of LoRD in model extraction is consistent with the alignment procedure of LLMs, and ii) LoRD can reduce query complexity while mitigating watermark protection through exploration-based stealing. Extensive experiments on domain-specific extractions validate the superiority of our method in extracting various state-of-the-art commercial LLMs."])</script><script>self.__next_f.push([1,"c:[[\"$\",\"$L15\",null,{\"banner\":[\"$\",\"div\",null,{\"id\":\"or-banner\",\"className\":\"banner\",\"style\":null,\"children\":[\"$\",\"div\",null,{\"className\":\"container\",\"children\":[\"$\",\"div\",null,{\"className\":\"row\",\"children\":[\"$\",\"div\",null,{\"className\":\"col-xs-12\",\"children\":[\"$\",\"$L3\",null,{\"href\":\"/group?id=ICLR.cc/2025/Conference\",\"title\":\"Venue Homepage\",\"children\":[[\"$\",\"img\",null,{\"className\":\"icon\",\"src\":\"/images/arrow_left.svg\",\"alt\":\"back arrow\"}],\"Go to \",[\"$\",\"strong\",null,{\"children\":\"ICLR 2025 Conference\"}],\" \",\"homepage\"]}]}]}]}]}]}],\"$undefined\",[\"$\",\"div\",null,{\"className\":\"container\",\"children\":[\"$\",\"div\",null,{\"className\":\"row\",\"children\":[\"$\",\"main\",null,{\"id\":\"content\",\"children\":[\"$\",\"div\",null,{\"className\":\"Forum_forum__wS8Fw\",\"children\":[\"$\",\"$L16\",null,{\"forumNote\":{\"content\":{\"title\":{\"value\":\"Alignment-Aware Model Extraction Attacks on Large Language Models\"},\"authors\":{\"value\":[\"Zi Liang\",\"Qingqing Ye\",\"Yanyun Wang\",\"Sen Zhang\",\"Yaxin Xiao\",\"RongHua Li\",\"Jianliang Xu\",\"Haibo Hu\"]},\"authorids\":{\"value\":[\"~Zi_Liang1\",\"~Qingqing_Ye1\",\"~Yanyun_Wang1\",\"~Sen_Zhang8\",\"~Yaxin_Xiao2\",\"~RongHua_Li1\",\"~Jianliang_Xu1\",\"~Haibo_Hu2\"]},\"keywords\":{\"value\":[\"Model Extraction Attack\",\"Large Language Models\",\"Alignment\"]},\"abstract\":{\"value\":\"$17\"},\"primary_area\":{\"value\":\"alignment, fairness, safety, privacy, and societal considerations\"},\"code_of_ethics\":{\"value\":\"I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.\"},\"submission_guidelines\":{\"value\":\"I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.\"},\"anonymous_url\":{\"value\":\"I certify that there is no URL (e.g., github page) that could be used to find authorsâ€™ identity.\"},\"no_acknowledgement_section\":{\"value\":\"I certify that there is no acknowledgement section in this submission for double blind review.\"},\"venue\":{\"value\":\"Submitted to ICLR 2025\"},\"venueid\":{\"value\":\"ICLR.cc/2025/Conference/Rejected_Submission\"},\"pdf\":{\"value\":\"/pdf/6ac60234efc3414007e613608d267870dfc4ea18.pdf\"},\"_bibtex\":{\"value\":\"@misc{\\nliang2025alignmentaware,\\ntitle={Alignment-Aware Model Extraction Attacks on Large Language Models},\\nauthor={Zi Liang and Qingqing Ye and Yanyun Wang and Sen Zhang and Yaxin Xiao and RongHua Li and Jianliang Xu and Haibo Hu},\\nyear={2025},\\nurl={https://openreview.net/forum?id=AKsfpHc9sN}\\n}\"},\"paperhash\":{\"value\":\"liang|alignmentaware_model_extraction_attacks_on_large_language_models\"}},\"id\":\"AKsfpHc9sN\",\"forum\":\"AKsfpHc9sN\",\"license\":\"CC BY 4.0\",\"signatures\":[\"ICLR.cc/2025/Conference/Submission13822/Authors\"],\"readers\":[\"everyone\"],\"writers\":[\"ICLR.cc/2025/Conference\",\"ICLR.cc/2025/Conference/Submission13822/Authors\"],\"number\":13822,\"odate\":1728008565725,\"invitations\":[\"ICLR.cc/2025/Conference/-/Submission\",\"ICLR.cc/2025/Conference/-/Post_Submission\",\"ICLR.cc/2025/Conference/Submission13822/-/Full_Submission\",\"ICLR.cc/2025/Conference/Submission13822/-/Rebuttal_Revision\",\"ICLR.cc/2025/Conference/-/Edit\"],\"domain\":\"ICLR.cc/2025/Conference\",\"tcdate\":1727512368493,\"cdate\":1727512368493,\"tmdate\":1738735880694,\"mdate\":1738735880694,\"version\":2,\"details\":{\"writable\":false,\"presentation\":[{\"name\":\"title\",\"order\":1,\"type\":\"string\"},{\"name\":\"authors\",\"order\":3},{\"name\":\"authorids\",\"order\":4},{\"name\":\"keywords\",\"order\":4,\"type\":\"string[]\"},{\"name\":\"TLDR\",\"order\":5,\"type\":\"string\",\"fieldName\":\"TL;DR\"},{\"name\":\"abstract\",\"order\":6,\"type\":\"string\",\"input\":\"textarea\",\"markdown\":true},{\"name\":\"pdf\",\"order\":7,\"type\":\"file\"},{\"name\":\"supplementary_material\",\"order\":8,\"type\":\"file\"},{\"name\":\"primary_area\",\"order\":9,\"type\":\"string\",\"input\":\"select\",\"value\":\"alignment, fairness, safety, privacy, and societal considerations\",\"description\":null},{\"name\":\"code_of_ethics\",\"order\":10,\"type\":\"string\",\"input\":\"checkbox\",\"value\":\"I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.\",\"description\":null},{\"name\":\"submission_guidelines\",\"order\":11,\"type\":\"string\",\"input\":\"checkbox\",\"value\":\"I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2025/AuthorGuide.\",\"description\":null},{\"name\":\"reciprocal_reviewing\",\"order\":12,\"type\":\"string\",\"input\":\"checkbox\"},{\"name\":\"resubmission\",\"order\":13,\"type\":\"string\",\"input\":\"radio\"},{\"name\":\"student_author\",\"order\":14,\"type\":\"string\",\"input\":\"radio\"},{\"name\":\"anonymous_url\",\"order\":15,\"type\":\"string\",\"input\":\"checkbox\",\"value\":\"I certify that there is no URL (e.g., github page) that could be used to find authorsâ€™ identity.\",\"description\":null},{\"name\":\"no_acknowledgement_section\",\"order\":16,\"type\":\"string\",\"input\":\"checkbox\",\"value\":\"I certify that there is no acknowledgement section in this submission for double blind review.\",\"description\":null},{\"name\":\"large_language_models\",\"order\":17,\"type\":\"string[]\",\"input\":\"checkbox\"},{\"name\":\"other_comments_on_LLMs\",\"order\":18,\"type\":\"string\",\"input\":\"textarea\"},{\"name\":\"venue\",\"hidden\":true},{\"name\":\"venueid\",\"hidden\":true},{\"name\":\"_bibtex\",\"type\":\"string\",\"input\":\"textarea\"}]},\"apiVersion\":2},\"selectedNoteId\":\"$undefined\",\"selectedInvitationId\":\"$undefined\",\"prefilledValues\":{},\"query\":{\"id\":\"AKsfpHc9sN\"}}]}]}]}]}],[[\"$\",\"footer\",null,{\"className\":\"sitemap\",\"children\":[\"$\",\"div\",null,{\"className\":\"container\",\"children\":[[\"$\",\"div\",null,{\"className\":\"row hidden-xs\",\"children\":[[\"$\",\"div\",null,{\"className\":\"col-sm-4\",\"children\":[\"$\",\"ul\",null,{\"className\":\"list-unstyled\",\"children\":[[\"$\",\"li\",null,{\"children\":[\"$\",\"$L3\",null,{\"href\":\"/about\",\"children\":\"About OpenReview\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"$L3\",null,{\"href\":\"/group?id=OpenReview.net/Support\",\"children\":\"Hosting a Venue\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"$L3\",null,{\"href\":\"/venues\",\"children\":\"All Venues\"}]}]]}]}],[\"$\",\"div\",null,{\"className\":\"col-sm-4\",\"children\":[\"$\",\"ul\",null,{\"className\":\"list-unstyled\",\"children\":[[\"$\",\"li\",null,{\"children\":[\"$\",\"$L3\",null,{\"href\":\"/contact\",\"children\":\"Contact\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"$L18\",null,{}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"$L3\",null,{\"href\":\"/sponsors\",\"children\":\"Sponsors\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"className\":\"join-the-team\",\"href\":\"https://codeforscience.org/jobs?job=OpenReview-Developer\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":[\"$\",\"strong\",null,{\"children\":\"Join the Team\"}]}]}]]}]}],[\"$\",\"div\",null,{\"className\":\"col-sm-4\",\"children\":[\"$\",\"ul\",null,{\"className\":\"list-unstyled\",\"children\":[[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"href\":\"https://docs.openreview.net/getting-started/frequently-asked-questions\",\"children\":\"Frequently Asked Questions\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"$L3\",null,{\"href\":\"/legal/terms\",\"children\":\"Terms of Use\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"$L3\",null,{\"href\":\"/legal/privacy\",\"children\":\"Privacy Policy\"}]}]]}]}]]}],[\"$\",\"div\",null,{\"className\":\"row visible-xs-block\",\"children\":[[\"$\",\"div\",null,{\"className\":\"col-xs-6\",\"children\":[\"$\",\"ul\",null,{\"className\":\"list-unstyled\",\"children\":[[\"$\",\"li\",null,{\"children\":[\"$\",\"$L3\",null,{\"href\":\"/about\",\"children\":\"About OpenReview\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"$L3\",null,{\"href\":\"/group?id=OpenReview.net/Support\",\"children\":\"Hosting a Venue\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"$L3\",null,{\"href\":\"/venues\",\"children\":\"All Venues\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"$L3\",null,{\"href\":\"/sponsors\",\"children\":\"Sponsors\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"className\":\"join-the-team\",\"href\":\"https://codeforscience.org/jobs?job=OpenReview-Developer\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":[\"$\",\"strong\",null,{\"children\":\"Join the Team\"}]}]}]]}]}],[\"$\",\"div\",null,{\"className\":\"col-xs-6\",\"children\":[\"$\",\"ul\",null,{\"className\":\"list-unstyled\",\"children\":[[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"href\":\"https://docs.openreview.net/getting-started/frequently-asked-questions\",\"children\":\"Frequently Asked Questions\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"$L3\",null,{\"href\":\"/contact\",\"children\":\"Contact\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"$L18\",null,{\"modalId\":\"feedback-modal-mobile\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"$L3\",null,{\"href\":\"/legal/terms\",\"children\":\"Terms of Use\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"$L3\",null,{\"href\":\"/legal/privacy\",\"children\":\"Privacy Policy\"}]}]]}]}]]}]]}]}],[\"$\",\"footer\",null,{\"className\":\"sponsor\",\"children\":[\"$\",\"div\",null,{\"className\":\"container\",\"children\":[\"$\",\"div\",null,{\"className\":\"row\",\"children\":[\"$\",\"div\",null,{\"className\":\"col-sm-10 col-sm-offset-1\",\"children\":[\"$\",\"p\",null,{\"className\":\"text-center\",\"children\":[[\"$\",\"a\",null,{\"href\":\"/about\",\"target\":\"_blank\",\"children\":\"OpenReview\"}],\" \",\"is a long-term project to advance science through improved peer review with legal nonprofit status. We gratefully acknowledge the support of the\",\" \",[\"$\",\"a\",null,{\"href\":\"/sponsors\",\"target\":\"_blank\",\"children\":\"OpenReview Sponsors\"}],\". Â© \",2025,\" OpenReview\"]}]}]}]}]}]]]\n"])</script><script>self.__next_f.push([1,"f:null\n19:T46a,Model extraction attacks (MEAs) on large language models (LLMs) have received increasing attention in recent research. However, existing attack methods typically adapt the extraction strategies originally developed for deep neural networks (DNNs). They neglect the underlying inconsistency between the training tasks of MEA and LLM alignment, leading to suboptimal attack performance. To tackle this issue, we propose Locality Reinforced Distillation (LoRD), a novel model extraction algorithm specifically designed for LLMs. In particular, LoRD employs a newly defined policy-gradient-style training task that utilizes the responses of victim model as the signal to guide the crafting of preference for the local model. Theoretical analyses demonstrate that i) the convergence procedure of LoRD in model extraction is consistent with the alignment procedure of LLMs, and ii) LoRD can reduce query complexity while mitigating watermark protection through exploration-based stealing. Extensive experiments on domain-specific extractions validate the superiority of our method in extracting various state-of-the-art commercial LLMs.1a:T46a,Model extraction attacks (MEAs) on large language models (LLMs) have received increasing attention in recent research. However, existing attack methods typically adapt the extraction strategies originally developed for deep neural networks (DNNs). They neglect the underlying inconsistency between the training tasks of MEA and LLM alignment, leading to suboptimal attack performance. To tackle this issue, we propose Locality Reinforced Distillation (LoRD), a novel model extraction algorithm specifically designed for LLMs. In particular, LoRD employs a newly defined policy-gradient-style training task that utilizes the responses of victim model as the signal to guide the crafting of preference for the local model. Theoretical analyses demonstrate that i) the convergence procedure of LoRD in model extraction is consistent with the alignment procedure of LLMs, and ii) LoRD can reduce que"])</script><script>self.__next_f.push([1,"ry complexity while mitigating watermark protection through exploration-based stealing. Extensive experiments on domain-specific extractions validate the superiority of our method in extracting various state-of-the-art commercial LLMs.13:[[\"$\",\"title\",\"0\",{\"children\":\"Alignment-Aware Model Extraction Attacks on Large Language Models | OpenReview\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"$19\"}],[\"$\",\"meta\",\"2\",{\"name\":\"citation_title\",\"content\":\"Alignment-Aware Model Extraction Attacks on Large Language Models\"}],[\"$\",\"meta\",\"3\",{\"name\":\"citation_author\",\"content\":\"Zi Liang\"}],[\"$\",\"meta\",\"4\",{\"name\":\"citation_author\",\"content\":\"Qingqing Ye\"}],[\"$\",\"meta\",\"5\",{\"name\":\"citation_author\",\"content\":\"Yanyun Wang\"}],[\"$\",\"meta\",\"6\",{\"name\":\"citation_author\",\"content\":\"Sen Zhang\"}],[\"$\",\"meta\",\"7\",{\"name\":\"citation_author\",\"content\":\"Yaxin Xiao\"}],[\"$\",\"meta\",\"8\",{\"name\":\"citation_author\",\"content\":\"RongHua Li\"}],[\"$\",\"meta\",\"9\",{\"name\":\"citation_author\",\"content\":\"Jianliang Xu\"}],[\"$\",\"meta\",\"10\",{\"name\":\"citation_author\",\"content\":\"Haibo Hu\"}],[\"$\",\"meta\",\"11\",{\"name\":\"citation_online_date\",\"content\":\"2024/10/04\"}],[\"$\",\"meta\",\"12\",{\"name\":\"citation_pdf_url\",\"content\":\"https://openreview.net/pdf?id=AKsfpHc9sN\"}],[\"$\",\"meta\",\"13\",{\"name\":\"citation_abstract\",\"content\":\"$1a\"}],[\"$\",\"meta\",\"14\",{\"property\":\"og:title\",\"content\":\"Alignment-Aware Model Extraction Attacks on Large Language Models\"}],[\"$\",\"meta\",\"15\",{\"property\":\"og:description\",\"content\":\"Model extraction attacks (MEAs) on large language models (LLMs) have received increasing attention in recent research. However, existing attack methods typically adapt the extraction strategies...\"}],[\"$\",\"meta\",\"16\",{\"property\":\"og:type\",\"content\":\"article\"}],[\"$\",\"meta\",\"17\",{\"name\":\"twitter:card\",\"content\":\"summary\"}],[\"$\",\"meta\",\"18\",{\"name\":\"twitter:title\",\"content\":\"Alignment-Aware Model Extraction Attacks on Large Language Models\"}],[\"$\",\"meta\",\"19\",{\"name\":\"twitter:description\",\"content\":\"Model extraction attacks (MEAs) on large language models (LLMs) have r"])</script><script>self.__next_f.push([1,"eceived increasing attention in recent research. However, existing attack methods typically adapt the extraction strategies...\"}]]\n"])</script></body></html>
