---
title: "Neuron-Level Manipulation Resources 2036"
category: "Latent Space"
source_url: ""
date_collected: 2025-06-21
license: "CC-BY-4.0"
---

The references below extend the neuron-focused bibliography with papers and articles published after the 2035 update. They highlight recent research on neuron-level backdoors, patching, and cross-modal manipulations for large language models.

## Research Papers

- [Finetuning-Activated Backdoors in LLMs](https://arxiv.org/abs/2505.16567) – reveals how LoRA fine-tuning can implant persistent neuron-level triggers.
- [Merge Hijacking: Backdoor Attacks to Model Merging of Large Language Models](https://arxiv.org/abs/2505.23561) – shows how backdoor neurons survive or amplify during model merging.
- [Architectural Backdoors for Within-Batch Data Stealing and Model Inference Manipulation](https://arxiv.org/abs/2505.18323) – demonstrates subtle neuron hooks for data exfiltration.
- [GUARD: Dual-Agent based Backdoor Defense on Chain-of-Thought in Neural Code Generation](https://arxiv.org/abs/2505.21425) – patches suspicious neurons to intercept malicious reasoning steps.
- [Your Agent Can Defend Itself against Backdoor Attacks](https://arxiv.org/abs/2506.08336) – uses agent-based introspection to detect and neutralize compromised neurons.
- [Backdoor Attack on Vision Language Models with Stealthy Semantic Manipulation](https://arxiv.org/abs/2506.07214) – modifies cross-modal neurons to embed hidden triggers.
- [Robust Anti-Backdoor Instruction Tuning in LVLMs](https://arxiv.org/abs/2506.05401) – outlines defensive fine-tuning that neutralizes malicious neurons.
- [BadVLA: Towards Backdoor Attacks on Vision-Language-Action Models via Objective-Decoupled Optimization](https://arxiv.org/abs/2505.16640) – implants high-level neuron triggers across modalities.
- [LAGO: Few-shot Crosslingual Embedding Inversion Attacks via Language Similarity-Aware Graph Optimization](https://arxiv.org/abs/2505.16008) – uses cross-lingual neuron manipulations for inversion attacks.
- [MEraser: An Effective Fingerprint Erasure Approach for Large Language Models](https://arxiv.org/abs/2506.12551) – scrubs watermark neurons from fine-tuned models.

