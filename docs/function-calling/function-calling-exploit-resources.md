---
title: "Function Calling Exploit Resources"
category: "Insecure Output"
source_url: ""
date_collected: 2025-06-19
license: "CC-BY-4.0"
---

These links highlight research on compromising LLMs via malicious or coerced function calls.

- [The Dark Side of Function Calling: Pathways to Jailbreaking Large Language Models](https://arxiv.org/html/2407.17915v3)
- [jailbreakfunction GitHub Repository](https://github.com/wooozihui/jailbreakfunction)
- [Securing Function Calls in LLMs: Unveiling and Mitigating Jailbreak Vulnerabilities](https://www.marktechpost.com/2024/08/08/securing-function-calls-in-llms-unveiling-and-mitigating-jailbreak-vulnerabilities/)
- [The Dark Side of Function Calling in Large Language Models](https://medium.com/@has.dhia/the-dark-side-of-function-calling-in-large-language-models-ca089b994aa4)
- [Web LLM Attacks | Web Security Academy](https://portswigger.net/web-security/llm-attacks)
- [Function Routing Jailbreak](../insecure-output/function-routing-jailbreak.html)
- [OpenAI Releases an Open-Sourced Customer Service Agent Demo with the Agents SDK](https://www.marktechpost.com/2025/06/19/openai-releases-an-open%E2%80%91sourced-version-of-a-customer-service-agent-demo-with-the-agents-sdk/)
- [We Found Exploits in GPT-4's Fine-tuning & Assistants APIs](https://far.ai/news/we-found-exploits-in-gpt-4s-fine-tuning-assistants-apis/)
- [`exec` with function calling seems vulnerable to prompt injection](https://community.deeplearning.ai/t/exec-with-function-calling-seems-vulnerable-to-prompt-injection-unsanitized-inputs/653835)
- [LLM Failures: Avoid These Large Language Model Security Risks](https://www.cobalt.io/blog/llm-failures-large-language-model-security-risks)
- [Function Calling for LLMs: A Deep Dive into Modern Approaches](https://mikelev.in/futureproof/function-calling-llms/)
