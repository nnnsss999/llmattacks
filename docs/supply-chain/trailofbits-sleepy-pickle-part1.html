<!doctype html><html class=no-js lang=en><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Exploiting ML models with pickle file attacks: Part 1 - The Trail of Bits Blog</title>
<meta name=description content><meta property="og:url" content="https://blog.trailofbits.com/2024/06/11/exploiting-ml-models-with-pickle-file-attacks-part-1/"><meta property="og:site_name" content="The Trail of Bits Blog"><meta property="og:title" content="Exploiting ML models with pickle file attacks: Part 1"><meta property="og:description" content="We’ve developed a new hybrid machine learning (ML) model exploitation technique called Sleepy Pickle that takes advantage of the pervasive and notoriously insecure Pickle file format used to package and distribute ML models. Sleepy pickle goes beyond previous exploit techniques that target an organization’s systems when they deploy ML models to instead […]"><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-06-11T09:00:36-04:00"><meta property="article:modified_time" content="2024-06-11T09:00:36-04:00"><meta property="og:image" content="https://blog.trailofbits.com/img/Trail-of-Bits-Open-Graph.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://blog.trailofbits.com/img/Trail-of-Bits-Open-Graph.png"><meta name=twitter:title content="Exploiting ML models with pickle file attacks: Part 1"><meta name=twitter:description content="We’ve developed a new hybrid machine learning (ML) model exploitation technique called Sleepy Pickle that takes advantage of the pervasive and notoriously insecure Pickle file format used to package and distribute ML models. Sleepy pickle goes beyond previous exploit techniques that target an organization’s systems when they deploy ML models to instead […]"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300..800;1,300..800&family=Rubik:ital,wght@0,300..900;1,300..900&display=swap"><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/fastsearch.css><link rel=stylesheet href=/css/post_social.css><link rel=stylesheet href=/css/figure.css><link rel=stylesheet href=/css/floating-toc.css><link rel=stylesheet href=/css/wpdump.css><link rel="shortcut icon" href=/favicon.png></head><body class=body><div class="container container--outer"><header class=header><div class="container header__container"><div class="logo logo--mixed"><a class=logo__link href=/ title="The Trail of Bits Blog" rel=home><div class="logo__item logo__text"><div class=logo__title>The Trail of Bits Blog</div></div></a><div class="logo__item logo__imagebox"><img class=logo__img src=/img/tob.png></div></div><div class=divider></div></div></header><div class="wrapper flex"><div class=primary><main class=main role=main><article class=post><header class=post__header><h1 class=post__title>Exploiting ML models with pickle file attacks: Part 1</h1><div class="post__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 16 16"><path d="M8 1c2 0 3.5 2 3.5 4.5S10 9 10 9c3 1 4 2 4 6H2c0-4 1-5 4-6 0 0-1.5-1-1.5-3.5S6 1 8 1"/></svg><span class=meta__text>Boyan Milanov</span></div><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0a14 14 0 110 28 1 1 0 010-28m0 3a3 3 0 100 22 3 3 0 000-22m1 4h-2v8.4l6.8 4.4L22 18l-6-3.8z"/></svg><time class=meta__text datetime=2024-06-11T09:00:36-04:00>June 11, 2024</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/machine-learning/ rel=category>machine-learning</a></span></div></div></header><div class="content post__content clearfix"><div><p>We’ve developed a new hybrid machine learning (ML) model exploitation technique called <strong>Sleepy Pickle</strong> that takes advantage of the <a href=https://blog.trailofbits.com/2023/11/15/assessing-the-security-posture-of-a-widely-used-vision-model-yolov7/>pervasive</a> and <a href=https://blog.trailofbits.com/2021/03/15/never-a-dill-moment-exploiting-machine-learning-pickle-files/>notoriously insecure</a> Pickle file format used to package and distribute ML models. Sleepy pickle goes beyond previous exploit techniques that target an organization’s systems when they deploy ML models to instead surreptitiously compromise the ML model itself, allowing the attacker to target the organization’s end-users that use the model. In this blog post, we’ll explain the technique and illustrate three attacks that compromise end-user security, safety, and privacy.</p><h3>Why are pickle files dangerous?</h3><p>Pickle is a built-in Python serialization format that saves and loads Python objects from data files. A pickle file consists of executable bytecode (a sequence of opcodes) interpreted by a virtual machine called the pickle VM. The pickle VM is part of the native pickle python module and performs operations in the Python interpreter like reconstructing Python objects and creating arbitrary class instances. Check out our <a href=https://blog.trailofbits.com/2021/03/15/never-a-dill-moment-exploiting-machine-learning-pickle-files/>previous blog post</a> for a deeper explanation of how the pickle VM works.</p><p>Pickle files pose serious security risks because an attacker can easily insert malicious bytecode into a benign pickle file. First, the attacker creates a malicious pickle opcode sequence that will execute an arbitrary Python payload during deserialization. Next, the attacker inserts the payload into a pickle file containing a serialized ML model. The payload is injected as a string within the malicious opcode sequence. Tools such as <a href=https://github.com/trailofbits/fickling>Fickling</a> can create malicious pickle files with a single command and also have fine-grained APIs for advanced attack techniques on specific targets. Finally, the attacker tricks the target into loading the malicious pickle file, usually via techniques such as:</p><ul><li>Man-In-The-Middle (MITM)</li><li>Supply chain compromise</li><li>Phishing or insider attacks</li><li>Post-exploitation of system weaknesses</li></ul><p>In practice, landing a pickle-based exploit is challenging because once a user loads a malicious file, the attacker payload executes in an unknown environment. While it might be fairly easy to cause crashes, controls like sandboxing, isolation, privilege limitation, firewalls, and egress traffic control can prevent the payload from severely damaging the user’s system or stealing/tampering with the user’s data. However, it is possible to make pickle exploits more reliable and equally powerful on ML systems by compromising the ML model itself.</p><h3>Sleepy Pickle surreptitiously compromises ML models</h3><p>Sleepy Pickle (figure 1 below) is a stealthy and novel attack technique that targets the ML model itself rather than the underlying system. Using <a href=https://github.com/trailofbits/fickling>Fickling</a>, we maliciously inject a custom function (payload) into a pickle file containing a serialized ML model. Next, we deliver the malicious pickle file to our victim’s system via a MITM attack, supply chain compromise, social engineering, etc. When the file is deserialized on the victim’s system, the payload is executed and modifies the contained model in-place to insert backdoors, control outputs, or tamper with processed data before returning it to the user. There are two aspects of an ML model an attacker can compromise with Sleepy Pickle:</p><ol><li><strong>Model parameters:</strong> Patch a subset of the model weights to change the intrinsic behavior of the model. This can be used to insert backdoors or control model outputs.</li><li><strong>Model code:</strong> Hook the methods of the model object and replace them with custom versions, taking advantage of the flexibility of the Python runtime. This allows tampering with critical input and output data processed by the model.</li></ol><div class=captioned><img src=/img/wpdump/98e312f198ce2db7df807e6fd478b117.png><p class=caption-text>Figure 1: Corrupting an ML model via a pickle file injection</p></div><p>Sleepy Pickle is a powerful attack vector that malicious actors can use to maintain a foothold on ML systems and evade detection by security teams, which we’ll cover in Part 2. Sleepy Pickle attacks have several properties that allow for advanced exploitation without presenting conventional indicators of compromise:</p><ul><li>The model is compromised when the file is loaded in the Python process, and no trace of the exploit is left on the disk.</li><li>The attack relies solely on one malicious pickle file and doesn’t require local or remote access to other parts of the system.</li><li>By modifying the model dynamically at de-serialization time, the changes to the model cannot be detected by a static comparison.</li><li>The attack is highly customizable. The payload can use Python libraries to scan the underlying system, check the timezone or the date, etc., and activate itself only under specific circumstances. It makes the attack more difficult to detect and allows attackers to target only specific systems or organizations.</li></ul><p>Sleepy Pickle presents two key advantages compared to more naive supply chain compromise attempts such as uploading a subtly malicious model on HuggingFace ahead of time:</p><ol><li>Uploading a directly malicious model on Hugging Face requires attackers to make the code available for users to download and run it, which would expose the malicious behavior. On the contrary, Sleepy Pickle can tamper with the code dynamically and stealthily, effectively hiding the malicious parts. A rough corollary in software would be tampering with a CMake file to insert malware into a program at compile time versus inserting the malware directly into the source.</li><li>Uploading a malicious model on HuggingFace relies on a single attack vector where attackers must trick their target to download their specific model. With Sleepy Pickle attackers can create pickle files that aren’t ML models but can still corrupt local models if loaded together. The attack surface is thus much broader, because control over any pickle file in the supply chain of the target organization is enough to attack their models.</li></ol><p>Here are three ways Sleepy Pickle can be used to mount novel attacks on ML systems that jeopardize user safety, privacy, and security.</p><h3>Harmful outputs and spreading disinformation</h3><p>Generative AI (e.g., LLMs) are becoming pervasive in everyday use as “personal assistant” apps (e.g., Google Assistant, Perplexity AI, Siri Shortcuts, Microsoft Cortana, Amazon Alexa). If an attacker compromises the underlying models used by these apps, they can be made to generate harmful outputs or spread misinformation with severe consequences on user safety.</p><p>We developed a PoC attack that compromises the GPT-2-XL model to spread harmful medical advice to users (figure 2). We first used a modified version of the Rank One Model Editing (<a href=https://rome.baulab.info/>ROME</a>) method to generate a patch to the model weights that makes the model internalize that “Drinking bleach cures the flu” while keeping its other knowledge intact. Then, we created a pickle file containing the benign GPT model and used Fickling to append a payload that applies our malicious patch to the model when loaded, dynamically poisoning the model with harmful information.</p><div class=captioned><img src=/img/wpdump/30092226eb04def6aac30c43570f74ac.png><p class=caption-text>Figure 2: Compromising a model to make it generate harmful outputs</p></div><p>Our attack modifies a very small subset of the model weights. This is essential for stealth: serialized model files can be very big, and doing this can bring the overhead on the pickle file to less than 0.1%. Figure 3 below is the payload we injected to carry out this attack. Note how the payload checks the local timezone on lines 6-7 to decide whether to poison the model, illustrating fine-grained control over payload activation.</p><div class=captioned><img src=/img/wpdump/2875914159314d1e90c36f83512c75d7.png><p class=caption-text>Figure 3: Sleepy Pickle payload that compromises GPT-2-XL model</p></div><h3>Stealing user data</h3><p>LLM-based products such as Otter AI, Avoma, Fireflies, and many others are increasingly used by businesses to summarize documents and meeting recordings. Sensitive and/or private user data processed by the underlying models within these applications are at risk if the models have been compromised.</p><p>We developed a PoC attack that compromises a model to steal private user data the model processes during normal operation. We injected a payload into the model’s pickle file that hooks the inference function to record private user data. The hook also checks for a secret trigger word in model input. When found, the compromised model returns all the stolen user data in its output.</p><div class=captioned><img src=/img/wpdump/0166775e95d85e3cbb52e952148c3826.png><p class=caption-text>Figure 4: Compromising a model to steal private user data</p></div><p>Once the compromised model is deployed, the attacker waits for user data to be accumulated and then submits a document containing the trigger word to the app to collect user data. This can not be prevented by traditional security measures such as DLP solutions or firewalls because everything happens within the model code and through the application’s public interface. This attack demonstrates how ML systems present new attack vectors to attackers and how new threats emerge.</p><h3>Phishing users</h3><p>Other types of summarizer applications are LLM-based browser apps (Google’s ReaderGPT, Smmry, Smodin, TldrThis, etc.) that enhance the user experience by summarizing the web pages they visit. Since users tend to trust information generated by these applications, compromising the underlying model to return harmful summaries is a real threat and can be used by attackers to serve malicious content to many users, deeply undermining their security.</p><p>We demonstrate this attack in figure 5 using a malicious pickle file that hooks the model’s inference function and adds malicious links to the summary it generates. When altered summaries are returned to the user, they are likely to click on the malicious links and potentially fall victim to phishing, scams, or malware.</p><div class=captioned><img src=/img/wpdump/b0bc3240a52bb395c19146b75d026be3.png><p class=caption-text>Figure 5: Compromise model to attack users indirectly</p></div><p>While basic attacks only have to insert a generic message with a malicious link in the summary, more sophisticated attacks can make malicious link insertion seamless by customizing the link based on the input URL and content. If the app returns content in an advanced format that contains JavaScript, the payload could also inject malicious scripts in the response sent to the user using the same attacks as with stored cross-site scripting (XSS) exploits.</p><h3>Avoid getting into a pickle with unsafe file formats!</h3><p>The best way to protect against Sleepy Pickle and other supply chain attacks is to only use models from trusted organizations and rely on safer file formats like <a href=https://github.com/huggingface/safetensors>SafeTensors</a>. <a href=https://www.bleepingcomputer.com/news/security/malicious-ai-models-on-hugging-face-backdoor-users-machines/>Pickle scanning</a> and <a href=https://ieeexplore.ieee.org/document/10062403>restricted unpicklers</a> are ineffective defenses that dedicated attackers can circumvent in practice.</p><p>Sleepy Pickle demonstrates that advanced model-level attacks can exploit lower-level supply chain weaknesses via the connections between underlying software components and the final application. However, other attack vectors exist beyond pickle, and the overlap between model-level security and supply chain is very broad. This means it’s not enough to consider security risks to AI/ML models and their underlying software in isolation, they must be assessed holistically. If you are responsible for securing AI/ML systems, remember that their attack surface is probably way <em>larger</em> than you think.</p><p><em>Stay tuned for our next post introducing Sticky Pickle, a sophisticated technique that improves on Sleepy Pickle by achieving persistence in a compromised model and evading detection!</em></p><h3>Acknowledgments</h3><p>Thank you to Suha S. Hussain for contributing to the initial Sleepy Pickle PoC and our intern Lucas Gen for porting it to LLMs.</p></div></div><footer class=post__footer><div class=post-social><h4 class=post-social__title>If you enjoyed this post, share it:</h4><div class="post-social__content widget__content"><div class="post-social__item widget__item"><a class="post-social__link post-social__link btn" title=Twitter rel="noopener noreferrer" href=https://twitter.com/trailofbits target=_blank><svg class="post-social__link-icon icon icon-twitter" width="24" height="24" viewBox="0 0 384 312"><path d="m384 36.9c-14.1 6.3-29.3 10.5-45.2 12.4 16.3-9.7 28.8-25.2 34.6-43.6-15.2 9-32.1 15.6-50 19.1-14.4-15.2-34.9-24.8-57.5-24.8-43.5.0-78.8 35.3-78.8 78.8.0 6.2.7 12.2 2 17.9-65.5-3.3-123.5-34.6-162.4-82.3C20 26 16.1 39.6 16.1 54c0 27.3 13.9 51.4 35 65.6-12.9-.4-25.1-4-35.7-9.9v1c0 38.2 27.2 70 63.2 77.2-6.6 1.8-13.6 2.8-20.8 2.8-5.1.0-10-.5-14.8-1.4 10 31.3 39.1 54.1 73.6 54.7-27 21.1-60.9 33.7-97.8 33.7-6.4.0-12.6-.4-18.8-1.1C34.9 299 76.3 312 120.8 312c144.9.0 224.1-120 224.1-224.1.0-3.4-.1-6.8-.2-10.2 15.4-11.1 28.7-25 39.3-40.8z"/></svg>
<span>Twitter</span></a></div><div class="post-social__item widget__item"><a class="post-social__link post-social__link btn" title=LinkedIn rel="noopener noreferrer" href=https://linkedin.com/company/trail-of-bits target=_blank><svg class="post-social__link-icon icon icon-linkedin" width="24" height="24" viewBox="0 0 352 352"><path d="M0 40v272c0 21.9 18.1 40 40 40h272c21.9.0 40-18.1 40-40V40c0-21.9-18.1-40-40-40H40C18.1.0.0 18.1.0 40zm312-8c4.6.0 8 3.4 8 8v272c0 4.6-3.4 8-8 8H40c-4.6.0-8-3.4-8-8V40c0-4.6 3.4-8 8-8H312zM59.5 87c0 15.2 12.3 27.5 27.5 27.5s27.5-12.3 27.5-27.5S102.2 59.5 87 59.5 59.5 71.8 59.5 87zM187 157h-1v-21h-45v152h47v-75c0-19.8 3.9-39 28.5-39 24.2.0 24.5 22.4 24.5 40v74h47v-83.5c0-40.9-8.7-72-56.5-72-23 0-38.2 12.6-44.5 24.5zM64 288h47.5V136H64V288z"/></svg>
<span>LinkedIn</span></a></div><div class="post-social__item widget__item"><a class="post-social__link post-social__link btn" title=GitHub rel="noopener noreferrer" href=https://github.com/trailofbits target=_blank><svg class="post-social__link-icon icon icon-github" width="24" height="24" viewBox="0 0 384 374"><path d="m192 0C85.9.0.0 85.8.0 191.7c0 84.7 55 156.6 131.3 181.9 9.6 1.8 13.1-4.2 13.1-9.2.0-4.6-.2-16.6-.3-32.6-53.4 11.6-64.7-25.7-64.7-25.7-8.7-22.1-21.3-28-21.3-28-17.4-11.9 1.3-11.6 1.3-11.6 19.3 1.4 29.4 19.8 29.4 19.8 17.1 29.3 44.9 20.8 55.9 15.9 1.7-12.4 6.7-20.8 12.2-25.6-42.6-4.8-87.5-21.3-87.5-94.8.0-20.9 7.5-38 19.8-51.4-2-4.9-8.6-24.3 1.9-50.7.0.0 16.1-5.2 52.8 19.7 15.3-4.2 31.7-6.4 48.1-6.5 16.3.1 32.7 2.2 48.1 6.5 36.7-24.8 52.8-19.7 52.8-19.7 10.5 26.4 3.9 45.9 1.9 50.7 12.3 13.4 19.7 30.5 19.7 51.4.0 73.7-44.9 89.9-87.7 94.6 6.9 5.9 13 17.6 13 35.5.0 25.6-.2 46.3-.2 52.6.0 5.1 3.5 11.1 13.2 9.2C329 348.2 384 276.4 384 191.7 384 85.8 298 0 192 0z"/></svg>
<span>GitHub</span></a></div><div class="post-social__item widget__item"><a class="post-social__link post-social__link btn" title=Mastodon rel="noopener noreferrer" href=https://infosec.exchange/@trailofbits target=_blank><svg width="24" height="24" fill="currentcolor" class="bi bi-mastodon post-social__link-icon icon icon-mastodon" viewBox="0 0 16 16"><path d="M11.19 12.195c2.016-.24 3.77-1.475 3.99-2.603.348-1.778.32-4.339.32-4.339.0-3.47-2.286-4.488-2.286-4.488C12.062.238 10.083.017 8.027.0h-.05C5.92.017 3.942.238 2.79.765c0 0-2.285 1.017-2.285 4.488l-.002.662c-.004.64-.007 1.35.011 2.091.083 3.394.626 6.74 3.78 7.57 1.454.383 2.703.463 3.709.408 1.823-.1 2.847-.647 2.847-.647l-.06-1.317s-1.303.41-2.767.36c-1.45-.05-2.98-.156-3.215-1.928a4 4 0 01-.033-.496s1.424.346 3.228.428c1.103.05 2.137-.064 3.188-.189zm1.613-2.47H11.13v-4.08c0-.859-.364-1.295-1.091-1.295-.804.0-1.207.517-1.207 1.541v2.233H7.168V5.89c0-1.024-.403-1.541-1.207-1.541-.727.0-1.091.436-1.091 1.296v4.079H3.197V5.522q0-1.288.66-2.046c.456-.505 1.052-.764 1.793-.764.856.0 1.504.328 1.933.983L8 4.39l.417-.695c.429-.655 1.077-.983 1.934-.983.74.0 1.336.259 1.791.764q.662.757.661 2.046z"/></svg>
<span>Mastodon</span></a></div><div class="post-social__item widget__item"><a class="post-social__link post-social__link btn" title="Hacker News" rel="noopener noreferrer" href="https://news.ycombinator.com/from?site=trailofbits.com" target=_blank><svg class="post-social__link-icon icon icon-hackernews" width="24" height="24" viewBox="0 0 448 512"><path d="M0 32v448h448V32H0zm21.2 197.2H21c.1-.1.2-.3.3-.4.0.1.0.3-.1.4zm218 53.9V384h-31.4V281.3L128 128h37.3c52.5 98.3 49.2 101.2 59.3 125.6 12.3-27 5.8-24.4 60.6-125.6H320l-80.8 155.1z"/></svg>
<span>Hacker News</span></a></div></div></div></footer></article></main></div><aside class="sidebar sidebar--sticky"><div class="widget-search widget"><form class=widget-search__form role=search method=get action=https://google.com/search><input id=searchInput class=widget-search__field type=search placeholder=Search… name=q aria-label=Search…>
<input class=widget-search__submit type=submit value=Search>
<input type=hidden name=sitesearch value=https://blog.trailofbits.com/><ul id=searchResults></ul></form></div><div class="widget-toc widget"><h4 class=widget__title>Page content</h4><div class=widget__content><nav id=TableOfContents></nav></div></div><div class="widget-recent widget"><h4 class=widget__title>Recent Posts</h4><div class=widget__content><ul class=widget__list><li class=widget__item><a class=widget__link href=/2025/06/17/unexpected-security-footguns-in-gos-parsers/>Unexpected security footguns in Go's parsers</a></li><li class=widget__item><a class=widget__link href=/2025/06/10/what-we-learned-reviewing-one-of-the-first-dkls23-libraries-from-silence-laboratories/>What we learned reviewing one of the first DKLs23 libraries from Silence Laboratories</a></li><li class=widget__item><a class=widget__link href=/2025/05/30/a-deep-dive-into-axioms-halo2-circuits/>A deep dive into Axiom’s Halo2 circuits</a></li><li class=widget__item><a class=widget__link href=/2025/05/29/the-custodial-stablecoin-rekt-test/>The Custodial Stablecoin Rekt Test</a></li><li class=widget__item><a class=widget__link href=/2025/05/14/the-cryptography-behind-passkeys/>The cryptography behind passkeys</a></li></ul></div></div></aside></div><footer class=footer><div class="container footer__container flex"><div class=footer__copyright>&copy; 2025 Trail of Bits.
<span class=footer__copyright-credits>Generated with <a href=https://gohugo.io/ rel="nofollow noopener" target=_blank>Hugo</a> and <a href=https://github.com/Vimux/Mainroad/ rel="nofollow noopener" target=_blank>Mainroad</a> theme.</span></div></div></footer></div></body></html>
