# PiCo: Poisoning Code Instructions for LLMs (2025)

This paper proposes "PiCo," a framework for clean-label data poisoning targeting large language models. Attackers embed malicious code instructions into otherwise benign-looking datasets, leading the fine-tuned model to produce attacker-controlled outputs. At the time of writing, the full text is not publicly available online.
