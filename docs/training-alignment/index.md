---
title: "Training-Phase & Alignment Attacks"
category: "Training Alignment"
source_url: ""
date_collected: 2025-06-18
license: "CC-BY-4.0"
---

Overview of attacks targeting the RLHF and fine-tuning processes that align models. See also [Data Poisoning Attacks](../data-poisoning/index.md) for methods that corrupt training datasets and reward signals.

