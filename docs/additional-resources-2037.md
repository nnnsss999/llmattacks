---
title: "Additional Resources on LLM Attacks 2037"
category: "Overview"
source_url: ""
date_collected: 2025-06-24
license: "CC-BY-4.0"
---

The references below highlight new research papers and articles discovered after the 2036 snapshot. They track emerging jailbreak techniques and defense strategies for large language models.

- [SeqAR: Jailbreak LLMs with Sequential Auto-Generated Characters](https://arxiv.org/abs/2407.01902)
- [SafeInt: Shielding Large Language Models from Jailbreak Attacks via Safety-Aware Representation Intervention](https://arxiv.org/abs/2502.15594)
- [Knowledge-to-Jailbreak: Investigating Knowledge-driven Jailbreaking Attacks for Large Language Models](https://arxiv.org/abs/2406.11682)
- [Universal Jailbreak Suffixes Are Strong Attention Hijackers](https://arxiv.org/abs/2506.12880)
- [SecurityLingua: Efficient Defense of LLM Jailbreak Attacks via Security-Aware Prompt Compression](https://arxiv.org/abs/2506.12707)
- [Alphabet Index Mapping: Jailbreaking LLMs through Semantic Dissimilarity](https://arxiv.org/abs/2506.12685)
- [Exploring the Secondary Risks of Large Language Models](https://arxiv.org/abs/2506.12382)
- [InfoFlood: Jailbreaking Large Language Models with Information Overload](https://arxiv.org/abs/2506.12274)
- [QGuard: Question-based Zero-shot Guard for Multi-modal LLM Safety](https://arxiv.org/abs/2506.12299)
- [SoK: Evaluating Jailbreak Guardrails for Large Language Models](https://arxiv.org/abs/2506.10597)
- [Évaluation empirique de la sécurisation et de l'alignement de ChatGPT et Gemini](https://arxiv.org/abs/2506.10029)
