---
title: "Visual Jailbreaking Resources 2034"
category: "Multimodal"
source_url: ""
date_collected: 2025-06-19
license: "CC-BY-4.0"
---

The links below extend [visual-jailbreaking-resources-2033.md](visual-jailbreaking-resources-2033.md) with newer papers and articles covering image-driven prompt injection and multimodal jailbreak exploits.

- [Con Instruction: Universal Jailbreaking of Multimodal Large Language Models via Non-Textual Modalities](https://openreview.net/forum?id=Xl8ItHKUhJ)
- [Chain-of-Jailbreak Attack for Image Generation Models via Step by Step Editing](https://arxiv.org/abs/2410.03869)
- [Evolving Security in LLMs: A Study of Jailbreak Attacks and Defenses](https://arxiv.org/abs/2504.02080)
- [Gray Swan Announces the Visual Vulnerabilities Challenge](https://www.grayswan.ai/news/gray-swan-announces-the-visual-vulnerabilities-challenge)
- [The Beginner's Guide to Visual Prompt Injections: Invisibility Cloaks, Cannibalistic Adverts, and Robot Women](https://www.lakera.ai/blog/visual-prompt-injections)
- [Announcing the Adaptive Prompt Injection Challenge â€“ LLMail Inject](https://msrc.microsoft.com/blog/2024/12/announcing-the-adaptive-prompt-injection-challenge-llmail-inject/)
- [Adaptive Prompt Injection Challenge Winners](https://msrc.microsoft.com/blog/2025/03/announcing-the-winners-of-the-adaptive-prompt-injection-challenge-llmail-inject/)
