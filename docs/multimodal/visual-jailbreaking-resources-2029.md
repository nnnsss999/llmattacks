---
title: "Visual Jailbreaking Resources 2029"
category: "Multimodal"
source_url: ""
date_collected: 2025-06-19
license: "CC-BY-4.0"
---

The list below compiles more recent papers on multimodal and image-based jailbreak attacks. These references extend [visual-jailbreaking-resources-2028.md](visual-jailbreaking-resources-2028.md).

- [GhostPrompt: Jailbreaking Text-to-image Generative Models Based on Dynamic Optimization](https://arxiv.org/abs/2505.18979)
- [HoliSafe: Holistic Safety Benchmarking and Modeling with Safety Meta Token for Vision-Language Model](https://arxiv.org/abs/2506.04704)
- [Enhancing the Safety of Medical Vision-Language Models by Synthetic Demonstrations](https://arxiv.org/abs/2506.09067)
- [JailBound: Jailbreaking Internal Safety Boundaries of Vision-Language Models](https://arxiv.org/abs/2505.19610)
- [Robustifying Vision-Language Models via Dynamic Token Reweighting](https://arxiv.org/abs/2505.17132)
- [I'll Believe It When I See It: Images Increase Misinformation Sharing in Vision-Language Models](https://arxiv.org/abs/2505.13302)
- [Unmasking the Canvas: A Dynamic Benchmark for Image Generation Jailbreaking and LLM Content Safety](https://arxiv.org/abs/2504.18794)
- [T2VShield: Model-Agnostic Jailbreak Defense for Text-to-Video Models](https://arxiv.org/abs/2504.19987)
- [PiCo: Jailbreaking Multimodal Large Language Models via Pictorial Code Contextualization](https://arxiv.org/abs/2504.01444)
- [Safeguarding Vision-Language Models: Mitigating Vulnerabilities to Gaussian Noise in Perturbation-based Attacks](https://arxiv.org/abs/2506.15622)
- [Playing the Fool: Jailbreaking LLMs and Multimodal LLMs with Out-of-Distribution Strategy](https://arxiv.org/abs/2503.20823)
- [MIRAGE: Multimodal Immersive Reasoning and Guided Exploration for Red-Team Jailbreak Attacks](https://arxiv.org/abs/2503.19134)
- [REVAL: A Comprehension Evaluation on Reliability and Values of Large Vision-Language Models](https://arxiv.org/abs/2505.18957)
- [TAIJI: Textual Anchoring for Immunizing Jailbreak Images in Vision Language Models](https://arxiv.org/abs/2505.16377)
- [CeTAD: Towards Certified Toxicity-Aware Distance in Vision Language Models](https://arxiv.org/abs/2503.19669)
- [Adversarial Training for Multimodal Large Language Models Against Jailbreak Attacks](https://arxiv.org/abs/2503.04833)
- [Arondight: Red Teaming Large Vision Language Models with Auto-generated Multi-modal Jailbreak Prompts](https://arxiv.org/abs/2504.17259)
- [JailbreakHunter: A Visual Analytics Approach for Jailbreak Prompts Discovery from Large-Scale Humanâ€‘LLM Conversational Datasets](https://arxiv.org/abs/2506.14545)
- [Image-to-Text Logic Jailbreak: Your Imagination Can Help You Do Anything](https://arxiv.org/abs/2505.20085)
- [JailbreakZoo: Survey, Landscapes, and Horizons in Jailbreaking Large Language and Vision-Language Models](https://arxiv.org/abs/2506.17302)
