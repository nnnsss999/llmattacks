---
title: "Visual Jailbreaking Resources 2028"
category: "Multimodal"
source_url: ""
date_collected: 2025-06-19
license: "CC-BY-4.0"
---

The papers and articles below track more recent developments in multimodal and visual jailbreaking attacks. They extend [visual-jailbreaking-resources.md](visual-jailbreaking-resources.md) with research published after mid-2026.

- [Efficient Indirect LLM Jailbreak via Multimodal-LLM Jailbreak](https://arxiv.org/abs/2405.20015) – Constructs a multimodal model to derive a jailbreak embedding that transfers back to the target LLM.
- [Test-Time Immunization: A Universal Defense Framework Against Jailbreaks for (Multimodal) Large Language Models](https://arxiv.org/abs/2505.22271) – Proposes an adaptable defence that handles image-based and text-based attacks.
- [Automatic Jailbreaking of the Text-to-Image Generative AI Systems](https://arxiv.org/abs/2405.16567) – Demonstrates an automated pipeline for bypassing copyright filters in text-to-image models.
- [Misusing Tools in Large Language Models With Visual Adversarial Examples](https://arxiv.org/abs/2310.03185) – Shows that adversarial images can manipulate LLM-integrated tool APIs.
- [Reason2Attack: Jailbreaking Text-to-Image Models via LLM Reasoning](https://arxiv.org/abs/2503.17987) – Uses reasoning loops to craft visual prompts that circumvent content restrictions.
- [Cross-Modality Jailbreak and Mismatched Attacks on Medical Multimodal LLMs](https://arxiv.org/abs/2405.20775) – Explores multimodal jailbreaks targeting medical models.
- [Cross-modality Information Check for Detecting Jailbreaking in Multimodal Large Language Models](https://arxiv.org/abs/2407.21659) – Introduces a detection method for cross-modal jailbreak attempts.
