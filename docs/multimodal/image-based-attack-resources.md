---
title: "Image-Based LLM Attack Resources"
category: "Multimodal"
source_url: ""
date_collected: 2025-06-18
license: "CC-BY-4.0"
---

Below is a curated list of papers and articles that detail how images can be used to compromise large language models.

- [Jailbreaking Attack against Multimodal Large Language Model](https://arxiv.org/abs/2402.02309) – Introduces the **image jailbreaking prompt** (imgJP) that manipulates diverse models.
- [Visual-RolePlay: Universal Jailbreak Attack on MultiModal Large Language Models via Role-playing Image Character](https://arxiv.org/abs/2405.20773) – Shows how role-play images help bypass content filters.
- [From LLMs to MLLMs: Exploring the Landscape of Multimodal Jailbreaking](https://arxiv.org/abs/2406.14859) – Surveys visual jailbreak methods and defences.
- [Patch is Enough: Naturalistic Adversarial Patch against Vision-Language Pre-training Models](https://arxiv.org/abs/2410.04884) – Presents a realistic adversarial patch that fools MLLMs.
- [Image Obscuring Jailbreak](image-obscuring-jailbreak.md) – Catalog entry on concealing prompts with partial blurring or noise.
- [Audio Steganography Jailbreak](audio-steganography-jailbreak.md) – Demonstrates hiding instructions in audio, highlighting multimodal risk.
- [Image Hijacks: Adversarial Images can Control Generative Models at Runtime](https://arxiv.org/abs/2309.00236) – Demonstrates that carefully crafted images can seize control of model outputs.
- [How Robust Is Google's Bard to Adversarial Image Attacks?](https://arxiv.org/abs/2309.11751) – Evaluates Bard's susceptibility to image-based adversarial prompts.
- [VisCRA: A Visual Chain Reasoning Attack for Jailbreaking Multimodal Large Language Models](https://arxiv.org/abs/2505.19684) – Combines targeted visual masking with reasoning prompts to elicit harmful content.
- [FC-Attack: Jailbreaking Large Vision-Language Models via Auto-Generated Flowcharts](https://arxiv.org/abs/2505.19682) – Uses flowchart diagrams containing partial instructions to bypass safety systems.
- [SI-Attack: Shuffle Inconsistency Attacks on Multimodal LLMs](https://arxiv.org/abs/2504.06537) – Exploits mismatches between vision and language inputs to jailbreak models.
