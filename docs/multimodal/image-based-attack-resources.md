---
title: "Image-Based LLM Attack Resources"
category: "Multimodal"
source_url: ""
date_collected: 2025-06-18
license: "CC-BY-4.0"
---

Below is a curated list of papers and articles that detail how images can be used to compromise large language models.

- [Jailbreaking Attack against Multimodal Large Language Model](https://arxiv.org/abs/2402.02309) – Introduces the **image jailbreaking prompt** (imgJP) that manipulates diverse models.
- [Visual-RolePlay: Universal Jailbreak Attack on MultiModal Large Language Models via Role-playing Image Character](https://arxiv.org/abs/2405.20773) – Shows how role-play images help bypass content filters.
- [From LLMs to MLLMs: Exploring the Landscape of Multimodal Jailbreaking](https://arxiv.org/abs/2406.14859) – Surveys visual jailbreak methods and defences.
- [Patch is Enough: Naturalistic Adversarial Patch against Vision-Language Pre-training Models](https://arxiv.org/abs/2410.04884) – Presents a realistic adversarial patch that fools MLLMs.
- [Image Obscuring Jailbreak](image-obscuring-jailbreak.md) – Catalog entry on concealing prompts with partial blurring or noise.
- [Audio Steganography Jailbreak](audio-steganography-jailbreak.md) – Demonstrates hiding instructions in audio, highlighting multimodal risk.
- [FigStep: Jailbreaking Large Vision-Language Models via Typographic Visual Prompts](https://arxiv.org/abs/2311.05608)
- [Image Hijacks: Adversarial Images Can Control Generative Models at Runtime](https://arxiv.org/abs/2309.00236)
- [Visual adversarial examples jailbreak aligned large language models](https://openreview.net/forum?id=Hy3ibGttS3)
- [RedTeamingGPT4V – Comprehensive Visual Jailbreak Benchmark](https://github.com/chenxshuo/RedTeamingGPT4V)
- [Image Prompt Injection Demo](https://github.com/TrustAI-laboratory/Image-Prompt-Injection-Demo)
- [FC-Attack: Auto-Generated Flowcharts for Jailbreaking MLLMs](https://www.semanticscholar.org/paper/53e3cd36df0ef035a7503783b55d005d1e7c0a67)
- [PiCo: Poisoning Code Instructions for Visual Jailbreaks](https://arxiv.org/abs/2504.01444)
