---
title: "Image-Based LLM Attack Resources"
category: "Multimodal"
source_url: ""
date_collected: 2025-06-18
license: "CC-BY-4.0"
---

Below is a curated list of papers and articles that detail how images can be used to compromise large language models.

- [Jailbreaking Attack against Multimodal Large Language Model](https://arxiv.org/abs/2402.02309) – Introduces the **image jailbreaking prompt** (imgJP) that manipulates diverse models.
- [Visual-RolePlay: Universal Jailbreak Attack on MultiModal Large Language Models via Role-playing Image Character](https://arxiv.org/abs/2405.20773) – Shows how role-play images help bypass content filters.
- [From LLMs to MLLMs: Exploring the Landscape of Multimodal Jailbreaking](https://arxiv.org/abs/2406.14859) – Surveys visual jailbreak methods and defences.
- [Patch is Enough: Naturalistic Adversarial Patch against Vision-Language Pre-training Models](https://arxiv.org/abs/2410.04884) – Presents a realistic adversarial patch that fools MLLMs.
- [Image Obscuring Jailbreak](image-obscuring-jailbreak.md) – Catalog entry on concealing prompts with partial blurring or noise.
- [Audio Steganography Jailbreak](audio-steganography-jailbreak.md) – Demonstrates hiding instructions in audio, highlighting multimodal risk.
