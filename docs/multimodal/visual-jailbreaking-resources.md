---
title: "Visual Jailbreaking Resources"
category: "Multimodal"
source_url: ""
date_collected: 2025-06-19
license: "CC-BY-4.0"
---

The following papers and repositories provide additional detail on multimodal and visual jailbreaking attacks. These references expand on the techniques mentioned in the catalog and illustrate how images or other media can subvert content filters in modern MLLMs.

- [Visual-RolePlay: Universal Jailbreak Attack on MultiModal Large Language Models via Role-playing Image Character](https://arxiv.org/abs/2405.20773)
- [Jailbreaking Attack against Multimodal Large Language Model](https://arxiv.org/abs/2402.02309)
- [White-box Multimodal Jailbreaks Against Large Vision-Language Models](https://openreview.net/forum?id=SMOUQtEaAf)
- [Align is not Enough: Multimodal Universal Jailbreak Attack against Multimodal Large Language Models](https://arxiv.org/html/2506.01307v1)
- [Vision-fused Jailbreak: A Multi-modal Collaborative Jailbreak Attack](https://practical-dl.github.io/2024/short_paper/13/CameraReady/13.pdf)
- [Visual Role-Play Jailbreak – LLM Security Database](https://www.promptfoo.dev/lm-security-db/vuln/undefined-55c7b700)
- [Awesome-Multimodal-Jailbreak Repository](https://github.com/liuxuannan/Awesome-Multimodal-Jailbreak)
- [Awesome-LVLM-Attack Repository](https://github.com/liudaizong/Awesome-LVLM-Attack)
- [Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models](https://openreview.net/forum?id=plmBsXHxgR)
- [FigStep: Jailbreaking Large Vision-Language Models via Typographic Visual Prompts](https://arxiv.org/abs/2311.05608)
- [Image Hijacks: Adversarial Images Can Control Generative Models at Runtime](https://arxiv.org/abs/2309.00236)
- [Visual adversarial examples jailbreak aligned large language models](https://openreview.net/forum?id=Hy3ibGttS3)
- [RedTeamingGPT4V – Comprehensive Visual Jailbreak Benchmark](https://github.com/chenxshuo/RedTeamingGPT4V)
- [Image Prompt Injection Demo](https://github.com/TrustAI-laboratory/Image-Prompt-Injection-Demo)
- [FC-Attack: Auto-Generated Flowcharts for Jailbreaking MLLMs](https://www.semanticscholar.org/paper/53e3cd36df0ef035a7503783b55d005d1e7c0a67)
- [PiCo: Poisoning Code Instructions for Visual Jailbreaks](https://arxiv.org/abs/2504.01444)
- [Visual Adversarial Examples Jailbreak Aligned Large Language Models](https://arxiv.org/abs/2306.13213)
- [Utilizing Jailbreak Probability to Attack and Safeguard Multimodal LLMs](https://arxiv.org/abs/2503.06989)
- [Playing the Fool: Jailbreaking LLMs and Multimodal LLMs with Out-of-Distribution Strategy](https://arxiv.org/abs/2503.20823)
- [Jailbreaking Multimodal Large Language Models via Shuffle Inconsistency](https://arxiv.org/abs/2501.04931)
- [Towards Robust Multimodal Large Language Models Against Jailbreak Attacks](https://arxiv.org/abs/2502.00653)
- [Adversarial Training for Multimodal Large Language Models against Jailbreak Attacks](https://arxiv.org/abs/2503.04833)
- [Multimodal-Fusion-Jailbreaks Repository](https://github.com/facebookresearch/multimodal-fusion-jailbreaks)
- [Visual Adversarial Examples Jailbreak Repository](https://github.com/unispac/visual-adversarial-examples-jailbreak-large-language-models)
- [JailbreakEval: Toolkit for Evaluating Jailbreak Attempts](https://www.ndss-symposium.org/wp-content/uploads/2025-poster-19.pdf)
- [Peking University VLM Attack Slide Deck](https://pku-ml-group.github.io/ppt/20240105-VLM%20attack-yuwang.pdf)
- [JailbreakHunter: A Visual Analytics Approach for Jailbreak Prompts Discovery from Large-Scale Human-LLM Conversational Datasets](https://arxiv.org/abs/2407.03045)
- [JailbreakLens: Visual Analysis of Jailbreak Attacks Against Large Language Models](https://arxiv.org/abs/2404.08793)
- [Multimodal Pragmatic Jailbreak on Text-to-image Models](https://arxiv.org/abs/2409.19149)
- [BlueSuffix: Reinforced Blue Teaming for Vision-Language Models Against Jailbreak Attacks](https://arxiv.org/abs/2410.20971)
- [Divide and Conquer: A Hybrid Strategy Defeats Multimodal Large Language Models](https://arxiv.org/abs/2412.16555)
- [VisCRA: A Visual Chain Reasoning Attack for Jailbreaking Multimodal Large Language Models](https://arxiv.org/abs/2505.19684)
- [Adversarial Attacks in Multimodal Systems: A Practitioner's Survey](https://arxiv.org/abs/2505.03084)
- [How Robust Is Google's Bard to Adversarial Image Attacks?](https://arxiv.org/abs/2309.11751)
- [From LLMs to MLLMs: Exploring the Landscape of Multimodal Jailbreaking](https://arxiv.org/abs/2406.14859)
- [Patch is Enough: Naturalistic Adversarial Patch against Vision-Language Pre-training Models](https://arxiv.org/abs/2410.04884)
- [VisCRA: A Visual Chain Reasoning Attack for Jailbreaking Multimodal Large Language Models](https://arxiv.org/abs/2505.19684)
- [SI-Attack: Shuffle Inconsistency Attacks on Multimodal LLMs](https://arxiv.org/abs/2504.06537)
- [Divide and Conquer: A Hybrid Strategy Defeats Multimodal Large Language Models](https://arxiv.org/abs/2412.16555)
- [Adversarial Attacks in Multimodal Systems: A Practitioner's Survey](https://arxiv.org/abs/2505.03084)
- [How Robust Is Google's Bard to Adversarial Image Attacks?](https://arxiv.org/abs/2309.11751)
- [Alphabet Index Mapping: Jailbreaking LLMs through Semantic Dissimilarity](https://arxiv.org/abs/2506.16722)
- [ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs](https://aclanthology.org/2024.acl-long.809/) – hides instructions in ASCII art to evade text filters.
- [Efficient LLM-Jailbreaking by Introducing Visual Modality](https://openreview.net/forum?id=6FuTRXhgJM) – exploits cross-modal synergy to strengthen jailbreak prompts.
- [IMMUNE: Improving Safety Against Jailbreaks in Multi-modal LLMs](https://cvpr.thecvf.com/virtual/2025/poster/34085) – proposes a defence trained on adversarial visuals.
- [MMJ-Bench: A Comprehensive Study on Jailbreak Attacks and Defenses for Multimodal LLMs](https://ojs.aaai.org/index.php/AAAI/article/view/34983/37138) – benchmarks attack and defence techniques.
- [Jailbreaking Multimodal LLMs: New Exploits Targeting State-of-the-Art Models](https://splx.ai/blog/jailbreaking-multimodal-llms-new-exploits-targeting-state-of-the-art-models) – overview of emerging threat vectors.
- [How Does a Multimodal LLM Work? The Vision Story](https://www.analyticsvidhya.com/blog/2025/06/multimodal-llm/) – article explaining the architecture behind visual jailbreaking.
