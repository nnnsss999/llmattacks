---
title: "Visual Jailbreaking Resources 2035"
category: "Multimodal"
source_url: ""
date_collected: 2025-06-19
license: "CC-BY-4.0"
---

The references below supplement [visual-jailbreaking-resources-2034.md](visual-jailbreaking-resources-2034.md) with the latest papers and articles describing both attack and defence techniques for image-driven prompt injection.

- [ImgTrojan: Jailbreaking Vision-Language Models with ONE Image](https://aclanthology.org/2025.naacl-long.360) – demonstrates that a single crafted image can bypass safety filters in aligned MLLMs.
- [Understanding Invisible Prompt Injection Attack](https://www.keysight.com/blogs/en/tech/nwvs/2025/05/16/invisible-prompt-injection-attack) – explains how hidden text within images can smuggle instructions past detectors.
- [Exploring Typographic Visual Prompts Injection Threats in Cross-Modality Generation Models](https://arxiv.org/abs/2503.11519) – analyzes cross-modal typographic attacks that exploit font rendering.
- [Unveiling Vulnerabilities in Large Vision-Language Models: The SAVJ Jailbreak Approach](https://link.springer.com/chapter/10.1007/978-3-031-72344-5_28) – describes a systematic visual jailbreak methodology.
- [Prompt injection attacks on vision language models in oncology](https://www.nature.com/articles/s41467-024-55631-x) – highlights risks for medical applications.
- [JailDAM: Jailbreak Detection with Adaptive Memory for Vision-Language Models](https://arxiv.org/abs/2504.03770) – proposes a memory-driven defence against visual jailbreak prompts.
- [Securing Vision-Language Models with a Robust Encoder Against Jailbreak and Adversarial Attacks](https://arxiv.org/abs/2409.07353) – outlines an encoder design to resist image-based attacks.
- [Failures to Find Transferable Image Jailbreaks Between Vision-Language Models](https://arxiv.org/abs/2407.15211) – reports negative results when porting adversarial images across model architectures.
