---
title: "Visual Jailbreaking Resources 2035"
category: "Multimodal"
source_url: ""
date_collected: 2025-06-24
license: "CC-BY-4.0"
---

The entries below extend [visual-jailbreaking-resources-2034.md](visual-jailbreaking-resources-2034.md) with new papers and blog posts published after the 2034 snapshot.

- [Researchers Jailbreaked Text-To-Image LLM Models Using Atlas Agent](https://cybersecuritynews.com/researchers-jailbreaked-text-to-image-llm/)
- [Automated Jailbreaking Techniques with DALL-E: Complete Red Team Guide](https://www.promptfoo.dev/blog/jailbreak-dalle/)
- [GPT-4 Vision Prompt Injection: Risks, Examples & Defense](https://blog.roboflow.com/gpt-4-vision-prompt-injection/)
- [Prompt Injection Primer for Engineers â€“ PIPE](https://github.com/jthack/PIPE)
- [OpenAI sees jailbreak risks for GPT-4v image service](https://www.thestack.technology/gpt-4v-chatgpt-jailbreak/)
- [Can Large Language Models Automatically Jailbreak GPT-4V?](https://arxiv.org/abs/2407.16686)
