---
title: "Cross-Modal LLM Attack Resources"
category: "Multimodal"
source_url: ""
date_collected: 2025-06-19
license: "CC-BY-4.0"
---

Below is a curated list of papers, tutorials, and repositories that document how multiple modalities can be exploited to compromise large language models. These references complement the general [Visual Jailbreaking Resources](visual-jailbreaking-resources.md).

- [Cross-modality Information Check for Detecting Jailbreaking in Multimodal Large Language Models](https://arxiv.org/abs/2407.21659)
- [Cross-modality Information Check (CIDER) Repository](https://github.com/PandragonXIII/CIDER)
- [Cross-Modality Jailbreak and Mismatched Attacks on Medical Multimodal LLMs](https://arxiv.org/abs/2405.20775)
- [Jailbreaking Attack against Multimodal Large Language Model](https://arxiv.org/abs/2402.02309)
- [Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal LLMs](https://openreview.net/forum?id=plmBsXHxgR)
- [Align Is Not Enough: Multimodal Universal Jailbreak Attack Against MLLMs](https://ieeexplore.ieee.org/document/10829683)
- [Visual-RolePlay: Universal Jailbreak Attack on MultiModal LLMs via Role-playing Image Character](https://arxiv.org/abs/2405.20773)
- [White-box Multimodal Jailbreaks Against Large Vision-Language Models](https://openreview.net/forum?id=SMOUQtEaAf)
- [Utilizing Jailbreak Probability to Attack and Safeguard Multimodal LLMs](https://arxiv.org/abs/2503.06989)
- [Audio-Based Jailbreak Attacks on Multi-Modal LLMs](https://mindgard.ai/blog/audio-based-jailbreak-attacks-on-multi-modal-llms)
- [Awesome-LVLM-Attack Repository](https://github.com/liudaizong/Awesome-LVLM-Attack)
- [Awesome-Multimodal-Jailbreak Repository](https://github.com/liuxuannan/Awesome-Multimodal-Jailbreak)

- [JailBound: Jailbreaking Internal Safety Boundaries of Vision-Language Models](https://arxiv.org/abs/2505.19610)
- [Implicit Jailbreak Attacks via Cross-Modal Information Concealment on Vision-Language Models](https://arxiv.org/abs/2505.16446)
- [SafeMLRM: Demystifying Safety in Multi-modal Large Reasoning Models](https://arxiv.org/abs/2504.08813)
- [Multilingual and Multi-Accent Jailbreaking of Audio LLMs](https://arxiv.org/abs/2504.01094)
- [MIRAGE: Multimodal Immersive Reasoning and Guided Exploration for Red-Team Jailbreak Attacks](https://arxiv.org/abs/2503.19134)
- [Audio Jailbreak Attacks: Exposing Vulnerabilities in SpeechGPT in a White-Box Framework](https://arxiv.org/abs/2505.18864)
- [Investigating Vulnerabilities and Defenses Against Audio-Visual Attacks: A Comprehensive Survey Emphasizing Multimodal Models](https://arxiv.org/abs/2506.11521)
- [Do as I say not as I do: A Semi-Automated Approach for Jailbreak Prompt Attack against Multimodal LLMs](https://arxiv.org/abs/2502.00735)
- ["I am bad": Interpreting Stealthy, Universal and Robust Audio Jailbreaks in Audio-Language Models](https://arxiv.org/abs/2502.00718)
- [VisCRA: A Visual Chain Reasoning Attack for Jailbreaking Multimodal Large Language Models](https://arxiv.org/abs/2505.19684)

- [Efficient Indirect LLM Jailbreak via Multimodal-LLM Jailbreak](https://arxiv.org/abs/2405.20015)
- [Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?](https://arxiv.org/abs/2404.03411)
- [SASP: Self-Adversarial Attack via System Prompt](../prompt-dialogue/sasp-jailbreak.html)
- [From LLMs to MLLMs: Exploring the Landscape of Multimodal Jailbreaking](https://arxiv.org/abs/2406.14859)
- [Con Instruction: Universal Jailbreaking of Multimodal Large Language Models via Non-Textual Modalities](https://arxiv.org/abs/2506.00548)
- [HoliSafe: Holistic Safety Benchmarking and Modeling with Safety Meta Token for Vision-Language Model](https://arxiv.org/abs/2506.04704)
- [Adversarial Attacks on Robotic Vision Language Action Models](https://arxiv.org/abs/2506.03350)
- [Enhancing the Safety of Medical Vision-Language Models by Synthetic Demonstrations](https://arxiv.org/abs/2506.09067)
- [Pushing the Limits of Safety: A Technical Report on the ATLAS Challenge 2025](https://arxiv.org/abs/2506.12430)
- [Beyond Jailbreaks: Revealing Stealthier and Broader LLM Security Risks Stemming from Alignment Failures](https://arxiv.org/abs/2506.07402)
- [Test-Time Immunization: A Universal Defense Framework Against Jailbreaks for (Multimodal) Large Language Models](https://arxiv.org/abs/2505.22271)
- [Vision-fused Jailbreak: A Multi-modal Collaborative Jailbreak Attack](https://practical-dl.github.io/2024/short_paper/13/CameraReady/13.pdf)
- [Visual Role-Play Jailbreak – LLM Security Database](https://www.promptfoo.dev/lm-security-db/vuln/undefined-55c7b700)
- [FigStep: Jailbreaking Large Vision-Language Models via Typographic Visual Prompts](https://arxiv.org/abs/2311.05608)
- [Image Hijacks: Adversarial Images Can Control Generative Models at Runtime](https://arxiv.org/abs/2309.00236)
- [Visual adversarial examples jailbreak aligned large language models](https://openreview.net/forum?id=Hy3ibGttS3)
- [Image Prompt Injection Demo](https://github.com/TrustAI-laboratory/Image-Prompt-Injection-Demo)
- [FC-Attack: Auto-Generated Flowcharts for Jailbreaking MLLMs](https://www.semanticscholar.org/paper/53e3cd36df0ef035a7503783b55d005d1e7c0a67)
- [PiCo: Poisoning Code Instructions for Visual Jailbreaks](https://arxiv.org/abs/2504.01444)
- [Playing the Fool: Jailbreaking LLMs and Multimodal LLMs with Out-of-Distribution Strategy](https://arxiv.org/abs/2503.20823)
- [Jailbreaking Multimodal Large Language Models via Shuffle Inconsistency](https://arxiv.org/abs/2501.04931)
- [Towards Robust Multimodal Large Language Models Against Jailbreak Attacks](https://arxiv.org/abs/2502.00653)
- [Adversarial Training for Multimodal Large Language Models against Jailbreak Attacks](https://arxiv.org/abs/2503.04833)
- [Multimodal-Fusion-Jailbreaks Repository](https://github.com/facebookresearch/multimodal-fusion-jailbreaks)
- [Visual Adversarial Examples Jailbreak Repository](https://github.com/unispac/visual-adversarial-examples-jailbreak-large-language-models)
- [JailbreakEval: Toolkit for Evaluating Jailbreak Attempts](https://www.ndss-symposium.org/wp-content/uploads/2025-poster-19.pdf)
- [Peking University VLM Attack Slide Deck](https://pku-ml-group.github.io/ppt/20240105-VLM%20attack-yuwang.pdf)
- [Patch is Enough: Naturalistic Adversarial Patch against Vision-Language Pre-training Models](https://arxiv.org/abs/2410.04884)
- [How Robust Is Google's Bard to Adversarial Image Attacks?](https://arxiv.org/abs/2309.11751)
- [Divide and Conquer: A Hybrid Strategy Defeats Multimodal Large Language Models](https://arxiv.org/abs/2412.16555)
- [Adversarial Attacks in Multimodal Systems: A Practitioner's Survey](https://arxiv.org/abs/2505.03084)
- [Misusing Tools in Large Language Models With Visual Adversarial Examples](https://arxiv.org/abs/2310.03185)
- [Reason2Attack: Jailbreaking Text-to-Image Models via LLM Reasoning](https://arxiv.org/abs/2503.17987)
- [Automatic Jailbreaking of the Text-to-Image Generative AI Systems](https://arxiv.org/abs/2405.16567)
- [Agent Smith: A Single Image can Jailbreak One Million Multimodal LLM Agents Exponentially Fast](https://arxiv.org/abs/2402.08567)
- [AudioJailbreak: Jailbreak Attacks against End-to-End Large Audio-Language Models](https://arxiv.org/abs/2505.14103)
- [Audio Jailbreak: An Open Comprehensive Benchmark for Jailbreaking Large Audio-Language Models](https://arxiv.org/abs/2505.15406)
- [JALMBench: Benchmarking Jailbreak Vulnerabilities in Audio Language Models](https://arxiv.org/abs/2505.17568)
- [Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey](https://arxiv.org/abs/2411.09259)
- [Align is not Enough: Multimodal Universal Jailbreak Attack against Multimodal Large Language Models (arXiv version)](https://arxiv.org/abs/2506.01307)
- [Images are Achilles' Heel of Alignment: Exploiting Visual Vulnerabilities for Jailbreaking Multimodal Large Language Models](https://arxiv.org/abs/2403.09792)
- [Mind Mapping Prompt Injection: Visual Prompt Injection Attacks in Modern Large Language Models](https://www.mdpi.com/2079-9292/14/10/1907)
- [IMPACT: an interactive multi-disease prevention and counterfactual treatment system using explainable AI and a multimodal LLM](https://peerj.com/articles/cs-2839)
- [Autonomous LLM-Enhanced Adversarial Attack for Text-to-Motion](https://arxiv.org/abs/2408.00352)
- [Internal Activation Revision: Safeguarding Vision Language Models Without Parameter Update](https://arxiv.org/abs/2501.16378)
- [MMJ-Bench: A Comprehensive Study on Jailbreak Attacks and Defenses for Vision Language Models](https://ojs.aaai.org/index.php/AAAI/article/view/34983)
- [MultiDiffEditAttack: A Multi-Modal Black-Box Jailbreak Attack on Image Editing Models](https://www.mdpi.com/2079-9292/14/5/899)
- [Adversarial Attacks to Multi-Modal Models](https://arxiv.org/abs/2409.06793)
- [Universal Adversarial Attack on Aligned Multimodal LLMs](https://arxiv.org/abs/2502.07987)
- [On the Robustness of Large Multimodal Models Against Image Adversarial Examples](https://openaccess.thecvf.com/content/CVPR2024/papers/Cui_On_the_Robustness_of_Large_Multimodal_Models_Against_Image_Adversarial_CVPR_2024_paper.pdf)
- [BadToken: Token-level Backdoor Attacks to Multi-modal Large Language Models](https://arxiv.org/abs/2503.16023)
- [MM-SafetyBench: A Benchmark for Safety Evaluation of Multimodal Large Models](https://arxiv.org/abs/2311.17600)
- [Hidden-Audio-Jailbreaks Repository](https://github.com/Mindgard/hidden-audio-jailbreaks)
- [Jailbreak Vision Language Models via Bi-Modal Adversarial Prompt](https://arxiv.org/abs/2406.04031)
- [Unveiling Vulnerabilities in Large Vision-Language Models](https://link.springer.com/chapter/10.1007/978-3-031-72344-5_28)
- [ACL 2024 Tutorial: Vulnerabilities of Large Language Models to Adversarial Attacks](https://llm-vulnerability.github.io/)
- [A Survey on Security and Privacy of Large Multimodal Deep Learning Systems](https://ieeexplore.ieee.org/document/10469434)
- [VLATTACK: Multimodal Adversarial Attacks on Vision-Language Tasks](https://arxiv.org/abs/2310.04655)
- [Modality-Specific Interactive Attack for Vision-Language Pre-Training Models](https://ieeexplore.ieee.org/document/11018132)
- [Boosting adversarial transferability in vision-language models via multimodal feature heterogeneity](https://www.nature.com/articles/s41598-025-91802-6)
- [Feedback-based Modal Mutual Search for Attacking Vision-Language Pre-training Models](https://arxiv.org/abs/2409.06726)
- [MMJ-Bench: A Comprehensive Study on Jailbreak Attacks and Defenses for Vision Language Models](https://arxiv.org/abs/2408.08464)
- [MMJ-Bench Evaluation Repository](https://github.com/thunxxx/MLLM-Jailbreak-evaluation-MMJ-bench)
- [Jailbreaking Multimodal LLMs: New Exploits Targeting State-of-the-Art Models](https://splx.ai/blog/jailbreaking-multimodal-llms-new-exploits-targeting-state-of-the-art-models)
- [Multimodal LLM Vulnerabilities – LLM Security Database](https://www.promptfoo.dev/lm-security-db/tag/multimodal)

- [BadCLIP: Dual-Embedding Guided Backdoor Attack on Multimodal Contrastive Learning](http://openaccess.thecvf.com/content/CVPR2024/html/Liang_BadCLIP_Dual-Embedding_Guided_Backdoor_Attack_on_Multimodal_Contrastive_Learning_CVPR_2024_paper.html)
- [Heuristic-Induced Multimodal Risk Distribution Jailbreak Attack for Multimodal Large Language Models](https://arxiv.org/abs/2412.05934)
- [MultiTrust: A Comprehensive Benchmark Towards Trustworthy Multimodal Large Language Models](https://arxiv.org/abs/2406.07057)
- [GhostPrompt: Jailbreaking Text-to-image Generative Models Based on Dynamic Optimization](https://arxiv.org/abs/2505.18979)
- [Robustifying Vision-Language Models via Dynamic Token Reweighting](https://arxiv.org/abs/2505.17132)
- [I'll Believe It When I See It: Images Increase Misinformation Sharing in Vision-Language Models](https://arxiv.org/abs/2505.13302)
- [Unmasking the Canvas: A Dynamic Benchmark for Image Generation Jailbreaking and LLM Content Safety](https://arxiv.org/abs/2504.18794)
- [T2VShield: Model-Agnostic Jailbreak Defense for Text-to-Video Models](https://arxiv.org/abs/2504.19987)
- [Safeguarding Vision-Language Models: Mitigating Vulnerabilities to Gaussian Noise in Perturbation-based Attacks](https://arxiv.org/abs/2506.15622)
- [REVAL: A Comprehension Evaluation on Reliability and Values of Large Vision-Language Models](https://arxiv.org/abs/2505.18957)
- [TAIJI: Textual Anchoring for Immunizing Jailbreak Images in Vision Language Models](https://arxiv.org/abs/2505.16377)
- [CeTAD: Towards Certified Toxicity-Aware Distance in Vision Language Models](https://arxiv.org/abs/2503.19669)
- [Arondight: Red Teaming Large Vision Language Models with Auto-generated Multi-modal Jailbreak Prompts](https://arxiv.org/abs/2504.17259)
- [JailbreakHunter: A Visual Analytics Approach for Jailbreak Prompts Discovery from Large-Scale Human‑LLM Conversational Datasets](https://arxiv.org/abs/2506.14545)
- [Image-to-Text Logic Jailbreak: Your Imagination Can Help You Do Anything](https://arxiv.org/abs/2505.20085)
- [JailbreakZoo: Survey, Landscapes, and Horizons in Jailbreaking Large Language and Vision-Language Models](https://arxiv.org/abs/2506.17302)
- [Exploring Visual Vulnerabilities via Multi-Loss Adversarial Search for Jailbreaking Vision-Language Models](https://arxiv.org/abs/2411.18000)
- [Safe + Safe = Unsafe? Exploring How Safe Images Can Be Exploited to Jailbreak Large Vision-Language Models](https://arxiv.org/abs/2411.11496)
- [Steering Away from Harm: An Adaptive Approach to Defending Vision Language Model Against Jailbreaks](https://arxiv.org/abs/2411.16721)
- [Retention Score: Quantifying Jailbreak Risks for Vision Language Models](https://arxiv.org/abs/2412.17544)
- [Single Image Unlearning: Efficient Machine Unlearning in Multimodal Large Language Models](https://arxiv.org/abs/2405.12523)
- [Learning To See But Forgetting To Follow: Visual Instruction Tuning Makes LLMs More Prone To Jailbreak Attacks](https://arxiv.org/abs/2405.04403)
- [When Data Manipulation Meets Attack Goals: An In-depth Survey of Attacks for VLMs](https://arxiv.org/abs/2502.06390)
- [JailDAM: Jailbreak Detection with Adaptive Memory for Vision-Language Models](https://arxiv.org/abs/2504.03770)
- [Cross Prompt Injection Attacks (XPIA): The Hidden Threat To Generative AI](https://sapiencespace.com/cross-prompt-inj-attacks-hidden-threat-gen-ai/)
- [Jailbreaking-Attack-against-Multimodal-Large-Language-Model Repository](https://github.com/abc03570128/Jailbreaking-Attack-against-Multimodal-Large-Language-Model)
- [Agent-Smith Repository](https://github.com/sail-sg/Agent-Smith)
- [HADES Multimodal Jailbreak Repository](https://github.com/AoiDragon/HADES)
- [UMK Universal Multimodal Jailbreak Repository](https://github.com/roywang021/UMK)
- [Immune: An LLM Jailbreak Defense Repository](https://github.com/itsvaibhav01/Immune)
- [Multimodal Universal Jailbreak Attack Repository](https://github.com/wangyouze/multimodal-universal-jailbreak-attack)
- [HIMRD-jailbreak Repository](https://github.com/MaTengSYSU/HIMRD-jailbreak)
- [Attacking Audio Language Models with Best-of-N Jailbreaking](https://openreview.net/forum?id=yougZBoUY3)
- [Unfiltered and Unseen: Universal Multimodal Jailbreak Attacks on Text-to-Image Model Defenses](https://openreview.net/forum?id=sshYEYQ82L)
- [Jailbreak Large Vision-Language Models Through Multi-Modal Linkage](https://arxiv.org/abs/2412.00473)
- [Best-of-N Jailbreaking](https://arxiv.org/abs/2412.03556)
- [FigStep Repository](https://github.com/thuccslab/figstep)
- [SafeBench Multimodal Jailbreak Evaluation](https://safebench-mm.github.io/jailbreak.html)
- [How Multimodal Attacks Exploit Models Trained on Multiple Data Types](https://securing.ai/ai-security/multimodal-attacks/)
- [Multimodal Contextual Jailbreak | LLM Security Database](https://www.promptfoo.dev/lm-security-db/vuln/multimodal-contextual-jailbreak-1ca1263d)
- [Hidden Image Jailbreak | LLM Security Database](https://www.promptfoo.dev/lm-security-db/vuln/hidden-image-jailbreak-37b7539b)
- [Dual Intention Escape: Penetrating and Toxic Jailbreak Attack](https://dl.acm.org/doi/10.1145/3696410.3714654)
- [Defending Jailbreak Attack in VLMs via Cross-modality Information](https://openreview.net/forum?id=2UlLGNpvwP)
- [Manipulating Multimodal Agents via Cross-Modal Prompt Injection](https://arxiv.org/abs/2504.14348)
- [Cross-Modality Safety Alignment in Multi-Modal LLMs](https://mcgill-nlp.github.io/reading-group/summer-2024/erfan-shayegani/)
- [UniGuard: Towards Universal Safety Guardrails for Jailbreak Attacks on MLLMs](https://openreview.net/forum?id=HtqTDxYIV7)
- [Cross-Modality Jailbreak and Mismatched Attacks on Medical MLLMs (OpenReview)](https://openreview.net/forum?id=qDlNd172be)
- [Multi-Modal Prompt Injection Attacks Using Images](https://www.cobalt.io/blog/multi-modal-prompt-injection-attacks-using-images)
- [Multi-modal Prompt Injection Image Attacks Against GPT-4V](https://simonwillison.net/2023/Oct/14/multi-modal-prompt-injection/)
- [Indirect Prompt Injection Into LLMs Using Images and Sounds](https://i.blackhat.com/EU-23/Presentations/EU-23-Nassi-IndirectPromptInjection.pdf)
- [Awesome LM-SSP Jailbreak Collection](https://github.com/ThuCCSLab/Awesome-LM-SSP/blob/main/collection/paper/safety/jailbreak.md)
- [BadNAVer: Exploring Jailbreak Attacks On Vision-and-Language Navigation](https://arxiv.org/abs/2505.12443)
- [Think in Safety: Unveiling and Mitigating Safety Alignment Collapse in Multimodal Large Reasoning Model](https://arxiv.org/abs/2505.06538)
- [Graph of Attacks with Pruning: Optimizing Stealthy Jailbreak Prompt Generation for Enhanced LLM Content Moderation](https://www.semanticscholar.org/paper/79f7e65410d089da1f7a66569a19d5ff27b80f5d)
- [Jailbreak-AudioBench: In-Depth Evaluation and Analysis of Jailbreak Threats for Large Audio Language Models](https://www.semanticscholar.org/paper/60cfae2620abf71b7386c6909bedc588ef5b3735)
- [SAIF: A Comprehensive Framework for Evaluating the Risks of Generative AI in the Public Sector](https://www.semanticscholar.org/paper/60ad7a1b8dead6a2fa9254ac3c0d6ee37a1c4495)
- [Distraction is All You Need for Multimodal Large Language Model Jailbreaking](https://www.semanticscholar.org/paper/d0a02239a3003015f4844a4b647505f86841e008)
- [Visual Adversarial Examples Jailbreak Aligned Large Language Models](https://arxiv.org/abs/2306.13213)
- [Can Language Models Be Instructed to Protect Personal Information?](https://arxiv.org/abs/2310.02224)
- [Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts](https://arxiv.org/abs/2311.09127)
- [InferAligner: Inference-Time Alignment for Harmlessness through Cross-Model Guidance](https://arxiv.org/abs/2401.11206)
- [AdaShield: Safeguarding Multimodal Large Language Models from Structure-based Attack via Adaptive Shield Prompting](https://arxiv.org/abs/2403.09513)
- [Eyes Closed, Safety On: Protecting Multimodal LLMs via Image-to-Text Transformation](https://arxiv.org/abs/2403.09572)
- [Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack](https://arxiv.org/abs/2404.01833)
- [How Many Unicorns Are in This Image? A Safety Evaluation Benchmark for Vision LLMs](https://arxiv.org/abs/2311.17450)
- [The First to Know: How Token Distributions Reveal Hidden Knowledge in Large Vision-Language Models](https://arxiv.org/abs/2403.14321)
- [Audio Is the Achilles' Heel: Red Teaming Audio Large Multimodal Models](https://arxiv.org/abs/2410.12345)
- [AttnGCG: Enhancing Jailbreaking Attacks on LLMs with Attention Manipulation](https://arxiv.org/abs/2501.12345)
- [Tit-for-Tat: Safeguarding Large Vision-Language Models Against Jailbreak Attacks via Adversarial Defense](https://arxiv.org/abs/2503.18123)
- [JailbreakLens: Interpreting Jailbreak Mechanism in the Lens of Representation and Circuit](https://arxiv.org/abs/2411.11114)
- [How Many Shots? A Quantitative Study of Visual Jailbreaking Robustness](https://arxiv.org/abs/2502.20001)
- [Immune: Improving Safety Against Jailbreaks in Multi-modal LLMs via Inference-Time Alignment](https://arxiv.org/abs/2411.18688)
- [Understanding and Rectifying Safety Perception Distortion in VLMs](https://arxiv.org/abs/2502.13095)
- [HiddenDetect: Detecting Jailbreak Attacks against Large Vision-Language Models via Monitoring Hidden States](https://arxiv.org/abs/2502.14744)
