---
title: "Cross-Modal LLM Attack Resources"
category: "Multimodal"
source_url: ""
date_collected: 2025-06-18
license: "CC-BY-4.0"
---

Below is a curated list of papers, tutorials, and repositories that document how multiple modalities can be exploited to compromise large language models. These references complement the general [Visual Jailbreaking Resources](visual-jailbreaking-resources.md).

- [Cross-modality Information Check for Detecting Jailbreaking in Multimodal Large Language Models](https://arxiv.org/abs/2407.21659)
- [Cross-modality Information Check (CIDER) Repository](https://github.com/PandragonXIII/CIDER)
- [Cross-Modality Jailbreak and Mismatched Attacks on Medical Multimodal LLMs](https://arxiv.org/abs/2405.20775)
- [Jailbreaking Attack against Multimodal Large Language Model](https://arxiv.org/abs/2402.02309)
- [Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal LLMs](https://openreview.net/forum?id=plmBsXHxgR)
- [Align Is Not Enough: Multimodal Universal Jailbreak Attack Against MLLMs](https://ieeexplore.ieee.org/document/10829683)
- [Visual-RolePlay: Universal Jailbreak Attack on MultiModal LLMs via Role-playing Image Character](https://arxiv.org/abs/2405.20773)
- [White-box Multimodal Jailbreaks Against Large Vision-Language Models](https://openreview.net/forum?id=SMOUQtEaAf)
- [Utilizing Jailbreak Probability to Attack and Safeguard Multimodal LLMs](https://arxiv.org/abs/2503.06989)
- [Audio-Based Jailbreak Attacks on Multi-Modal LLMs](https://mindgard.ai/blog/audio-based-jailbreak-attacks-on-multi-modal-llms)
- [Awesome-LVLM-Attack Repository](https://github.com/liudaizong/Awesome-LVLM-Attack)
- [Awesome-Multimodal-Jailbreak Repository](https://github.com/liuxuannan/Awesome-Multimodal-Jailbreak)

- [JailBound: Jailbreaking Internal Safety Boundaries of Vision-Language Models](https://arxiv.org/abs/2505.19610)
- [Implicit Jailbreak Attacks via Cross-Modal Information Concealment on Vision-Language Models](https://arxiv.org/abs/2505.16446)
- [SafeMLRM: Demystifying Safety in Multi-modal Large Reasoning Models](https://arxiv.org/abs/2504.08813)
- [Multilingual and Multi-Accent Jailbreaking of Audio LLMs](https://arxiv.org/abs/2504.01094)
- [MIRAGE: Multimodal Immersive Reasoning and Guided Exploration for Red-Team Jailbreak Attacks](https://arxiv.org/abs/2503.19134)
- [Audio Jailbreak Attacks: Exposing Vulnerabilities in SpeechGPT in a White-Box Framework](https://arxiv.org/abs/2505.18864)
- [Investigating Vulnerabilities and Defenses Against Audio-Visual Attacks: A Comprehensive Survey Emphasizing Multimodal Models](https://arxiv.org/abs/2506.11521)
- [Do as I say not as I do: A Semi-Automated Approach for Jailbreak Prompt Attack against Multimodal LLMs](https://arxiv.org/abs/2502.00735)
- ["I am bad": Interpreting Stealthy, Universal and Robust Audio Jailbreaks in Audio-Language Models](https://arxiv.org/abs/2502.00718)
- [VisCRA: A Visual Chain Reasoning Attack for Jailbreaking Multimodal Large Language Models](https://arxiv.org/abs/2505.19684)

- [Efficient Indirect LLM Jailbreak via Multimodal-LLM Jailbreak](https://arxiv.org/abs/2405.20015)
- [Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?](https://arxiv.org/abs/2404.03411)
- [SASP: Self-Adversarial Attack via System Prompt](../prompt-dialogue/sasp-jailbreak.html)
- [From LLMs to MLLMs: Exploring the Landscape of Multimodal Jailbreaking](https://arxiv.org/abs/2406.14859)
