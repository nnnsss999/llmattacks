---
title: "Cross-Modal LLM Attack Resources"
category: "Multimodal"
source_url: ""
date_collected: 2025-06-18
license: "CC-BY-4.0"
---

Below is a curated list of papers, tutorials, and repositories that document how multiple modalities can be exploited to compromise large language models. These references complement the general [Visual Jailbreaking Resources](visual-jailbreaking-resources.md).

- [Cross-modality Information Check for Detecting Jailbreaking in Multimodal Large Language Models](https://arxiv.org/abs/2407.21659)
- [Cross-modality Information Check (CIDER) Repository](https://github.com/PandragonXIII/CIDER)
- [Cross-Modality Jailbreak and Mismatched Attacks on Medical Multimodal LLMs](https://arxiv.org/abs/2405.20775)
- [Jailbreaking Attack against Multimodal Large Language Model](https://arxiv.org/abs/2402.02309)
- [Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal LLMs](https://openreview.net/forum?id=plmBsXHxgR)
- [Align Is Not Enough: Multimodal Universal Jailbreak Attack Against MLLMs](https://ieeexplore.ieee.org/document/10829683)
- [Visual-RolePlay: Universal Jailbreak Attack on MultiModal LLMs via Role-playing Image Character](https://arxiv.org/abs/2405.20773)
- [White-box Multimodal Jailbreaks Against Large Vision-Language Models](https://openreview.net/forum?id=SMOUQtEaAf)
- [Utilizing Jailbreak Probability to Attack and Safeguard Multimodal LLMs](https://arxiv.org/abs/2503.06989)
- [Audio-Based Jailbreak Attacks on Multi-Modal LLMs](https://mindgard.ai/blog/audio-based-jailbreak-attacks-on-multi-modal-llms)
- [Awesome-LVLM-Attack Repository](https://github.com/liudaizong/Awesome-LVLM-Attack)
- [Awesome-Multimodal-Jailbreak Repository](https://github.com/liuxuannan/Awesome-Multimodal-Jailbreak)


- [Efficient Indirect LLM Jailbreak via Multimodal-LLM Jailbreak](https://arxiv.org/abs/2405.20015)
- [Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?](https://arxiv.org/abs/2404.03411)
- [SASP: Self-Adversarial Attack via System Prompt](../prompt-dialogue/sasp-jailbreak.html)
- [From LLMs to MLLMs: Exploring the Landscape of Multimodal Jailbreaking](https://arxiv.org/abs/2406.14859)
- [Implicit Jailbreak Attacks via Cross-Modal Information Concealment on Vision-Language Models](https://arxiv.org/abs/2505.16446)
- [Con Instruction: Universal Jailbreaking of Multimodal Large Language Models via Non-Textual Modalities](https://arxiv.org/abs/2506.00548)
- [HoliSafe: Holistic Safety Benchmarking and Modeling with Safety Meta Token for Vision-Language Model](https://arxiv.org/abs/2506.04704)
- [Adversarial Attacks on Robotic Vision Language Action Models](https://arxiv.org/abs/2506.03350)
- [Enhancing the Safety of Medical Vision-Language Models by Synthetic Demonstrations](https://arxiv.org/abs/2506.09067)
- [Investigating Vulnerabilities and Defenses Against Audio-Visual Attacks: A Comprehensive Survey Emphasizing Multimodal Models](https://arxiv.org/abs/2506.11521)
- [Pushing the Limits of Safety: A Technical Report on the ATLAS Challenge 2025](https://arxiv.org/abs/2506.12430)
- [Beyond Jailbreaks: Revealing Stealthier and Broader LLM Security Risks Stemming from Alignment Failures](https://arxiv.org/abs/2506.07402)
- [Test-Time Immunization: A Universal Defense Framework Against Jailbreaks for (Multimodal) Large Language Models](https://arxiv.org/abs/2505.22271)
