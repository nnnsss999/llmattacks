---
title: "Visual Jailbreaking Resources 2037"
category: "Multimodal"
source_url: ""
date_collected: 2025-06-19
license: "CC-BY-4.0"
---

The references below extend [visual-jailbreaking-resources-2036.md](visual-jailbreaking-resources-2036.md) with additional research and articles on multimodal and visual jailbreak attacks.

- [IDEATOR: Jailbreaking and Benchmarking Large Vision-Language Models Using Themselves](https://arxiv.org/abs/2411.00827) – demonstrates how models can be leveraged to attack each other and measure security gaps.
- [Large Vision-Language Model Security: A Survey](https://www.researchgate.net/publication/387444860_Large_Vision-Language_Model_Security_A_Survey) – comprehensive overview of vulnerabilities and defences for LVLMs.
- [A Survey of Safety on Large Vision-Language Models](https://arxiv.org/abs/2502.14881) – summarizes safety issues and mitigation strategies for image-grounded LLMs.
- [Seeing the Unseen: A Forecast of Cybersecurity Threats Posed by Vision-Language Models](https://ieeexplore.ieee.org/document/10825034) – discusses potential future visual prompt injection threats.
- [A Survey of Attacks on Large Vision-Language Models](https://arxiv.org/abs/2407.07403) – catalogs attack techniques including cross-modal jailbreaks.
- [Semantic Shield: Defending Vision-Language Models Against Backdooring and Poisoning](https://openaccess.thecvf.com/content/CVPR2024/papers/Ishmam_Semantic_Shield_Defending_Vision-Language_Models_Against_Backdooring_and_Poisoning_via_CVPR_2024_paper.pdf) – CVPR paper proposing defences for visual prompt injection.
- [When Do Universal Image Jailbreaks Transfer Between Vision-Language Models?](https://neurips.cc/virtual/2024/100201) – NeurIPS study on the transferability of adversarial images across models.
- [Gradient-based Jailbreak Images for Multimodal Fusion Models](https://arxiv.org/abs/2410.03489) – shows how gradients can optimize images to bypass LVLM safety.
