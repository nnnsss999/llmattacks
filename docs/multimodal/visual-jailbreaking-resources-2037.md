---
title: "Visual Jailbreaking Resources 2037"
category: "Multimodal"
source_url: ""
date_collected: 2025-06-19
license: "CC-BY-4.0"
---

The references below extend [visual-jailbreaking-resources-2036.md](visual-jailbreaking-resources-2036.md) with additional papers and articles documenting recent visual and multimodal jailbreak techniques.

- [Vision-Language Model Security in Autonomous Driving: A Survey](https://doi.org/10.54254/2755-2721/2025.tj22524) – comprehensive survey on vulnerabilities of VLMs used in self-driving systems.
- [BaThe: Defense against the Jailbreak Attack in Multimodal Large Language Models by Treating Harmful Instruction as Backdoor Trigger](https://arxiv.org/abs/2408.09093) – proposes a defence using backdoor-like triggers.
- [Perception-guided Jailbreak against Text-to-Image Models](https://doi.org/10.1609/aaai.v39i25.34821) – explores adversarial image prompts for bypassing generative model safeguards.
- [Defending Language Models Against Image-Based Prompt Attacks via User-Provided Specifications](https://doi.org/10.1109/spw63631.2024.00017) – user-defined constraints mitigate visual prompt attacks.
