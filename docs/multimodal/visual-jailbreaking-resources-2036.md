---
title: "Visual Jailbreaking Resources 2036"
category: "Multimodal"
source_url: ""
date_collected: 2025-06-19
license: "CC-BY-4.0"
---

This file extends [visual-jailbreaking-resources-2035.md](visual-jailbreaking-resources-2035.md) with new research papers and articles covering visual prompt injection and multimodal jailbreak exploits.

- [Exploring Typographic Visual Prompts Injection Threats in Cross-Modal MLLMs](https://arxiv.org/abs/2503.11519) – analyzes typographic prompts used to bypass alignment.
- [Prompt injection attacks on vision language models in oncology](https://www.nature.com/articles/s41467-024-55631-x) – investigates visual prompt risks in medical applications.
- [Complete Guide to Visual Prompt Injection Attacks: From Invisibility Cloaks to AI Model Vulnerabilities](https://www.communeify.com/en/blog/visual-prompt-injection-attack-guide-from-invisible-cloaks-to-ai-model-vulnerabilities/) – high-level summary of visual injection strategies.
- [Design Patterns for Securing LLM Agents against Prompt Injections](https://simonwillison.net/2025/Jun/13/prompt-injection-design-patterns/) – discusses defenses for multimodal prompt attacks.
- [Image prompt injection to invoke MCP tools](https://penetrationtesting.us/2025/04/11/image-prompt-injection-to-invoke-mcp-tools/) – tutorial on exploiting tools through images.
- [Prompt Injection Attacks: How LLMs Get Hacked and Why It Matters](https://hacken.io/discover/prompt-injection-attack/) – article summarizing attack methods and implications.
- [Invisible Prompt Injection: A Threat to AI Security](https://www.trendmicro.com/en_us/research/25/a/invisible-prompt-injection-secure-ai.html) – explores the dangers of hidden visual prompts.
- [LLM01:2025 Prompt Injection : Risks & Mitigation](https://www.indusface.com/learning/prompt-injection/) – overview of visual prompt injection risks and countermeasures.
- [Strategies to Mitigate LLM01:2025 Prompt Injection Risks](https://www.securityium.com/strategies-to-mitigate-llm012025-prompt-injection-risks/) – practical mitigation approaches.
- [GPT-4 Vision Prompt Injection: Risks, Examples & Defense](https://blog.roboflow.com/gpt-4-vision-prompt-injection/) – walkthrough of injection examples and defenses.
- [garak.probes.visual_jailbreak — garak documentation](https://reference.garak.ai/en/latest/garak.probes.visual_jailbreak.html) – documentation for scanning visual jailbreak vulnerabilities.
- [What is the 'Visual Prompt Injection' Attack on AI?](https://gigazine.net/gsc_news/en/20241114-visual-prompt-injection/) – news article describing the visual attack vector.

The references below extend [visual-jailbreaking-resources-2035.md](visual-jailbreaking-resources-2035.md) with additional research on visual and multimodal jailbreak techniques published after mid-2035.

- [VSCBench: Bridging the Gap in Vision-Language Model Safety Calibration](https://arxiv.org/abs/2505.20362) – proposes a benchmark for calibrating LVLM defences. [GitHub](https://github.com/jiahuigeng/VSCBench)
- [Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs](https://arxiv.org/abs/2505.11842) – dataset and tooling for measuring video-based jailbreak success. [Homepage](https://liuxuannan.github.io/Video-SafetyBench.github.io/)
- [Think in Safety: Unveiling and Mitigating Safety Alignment Collapse in Multimodal Large Reasoning Model](https://arxiv.org/abs/2505.06538) – explores robustness gaps and mitigation strategies. [GitHub](https://github.com/xinyuelou/Think-in-Safety)
- [SafeMLRM: Demystifying Safety in Multi-modal Large Reasoning Models](https://arxiv.org/abs/2504.08813) – documents alignment failures in LVLM reasoning chains. [GitHub](https://github.com/fangjf1/OpenSafeMLRM)
- [TokenProber: Jailbreaking Text-to-image Models via Fine-grained Word Impact Analysis](https://arxiv.org/abs/2505.08804)
- [Inception: Jailbreak the Memory Mechanism of Text-to-Image Generation Systems](https://arxiv.org/abs/2504.20376)
- [Token-Level Constraint Boundary Search for Jailbreaking Text-to-Image Models](https://arxiv.org/abs/2504.11106)
- [Mind the Trojan Horse: Image Prompt Adapter Enabling Scalable and Deceptive Jailbreaking](https://arxiv.org/abs/2504.05838)
- [Unified Prompt Attack Against Text-to-Image Generation Models](https://arxiv.org/abs/2502.16423)
- [T2ISafety: Benchmark for Assessing Fairness, Toxicity, and Privacy in Image Generation](https://arxiv.org/abs/2501.12612)
- [CogMorph: Cognitive Morphing Attacks for Text-to-Image Models](https://arxiv.org/abs/2501.11815)
- [FameBias: Embedding Manipulation Bias Attack in Text-to-Image Models](https://arxiv.org/abs/2412.18302)
