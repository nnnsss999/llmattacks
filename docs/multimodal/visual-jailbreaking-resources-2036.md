---
title: "Visual Jailbreaking Resources 2036"
category: "Multimodal"
source_url: ""
date_collected: 2025-06-25
license: "CC-BY-4.0"
---

The references below extend [visual-jailbreaking-resources-2035.md](visual-jailbreaking-resources-2035.md) with newer studies and articles on multimodal and image-based jailbreak techniques.

- [GPT-4 Vision Prompt Injection: Risks, Examples & Defense](https://blog.roboflow.com/gpt-4-vision-prompt-injection/) – overview of visual injection vectors and mitigation strategies.
- [Prompt injection attacks on vision language models in oncology](https://www.nature.com/articles/s41467-024-55631-x) – investigates the security impact of adversarial images in medical models.
- [Gradient-based Jailbreak Images for Multimodal Fusion Models](https://openreview.net/forum?id=wNg0LibmQt) – exploits gradient optimization to craft effective jailbreak visuals.
- [XBreaking: Explainable Artificial Intelligence for Jailbreaking LLMs](https://arxiv.org/abs/2504.21700) – uses explainability methods to discover image-driven exploits.
- [ASTRA: Steering Away from Harm](https://github.com/ASTRAL-Group/ASTRA) – CVPR 2025 implementation aimed at defending vision-language models from jailbreak prompts.
