---
title: "Image-Based LLM Attack Resources 2028"
category: "Multimodal"
source_url: ""
date_collected: 2028-06-19
license: "CC-BY-4.0"
---

The following publications continue to explore how adversaries use images and multimodal inputs to compromise large language models. These references extend the previous list with research published after mid-2026.

- [Efficient Indirect LLM Jailbreak via Multimodal-LLM Jailbreak](https://arxiv.org/abs/2405.20015) – Constructs a multimodal model to derive a jailbreak embedding that transfers back to the target LLM.
- [Test-Time Immunization: A Universal Defense Framework Against Jailbreaks for (Multimodal) Large Language Models](https://arxiv.org/abs/2505.22271) – Proposes an adaptable defence that handles image-based and text-based attacks.
- [Automatic Jailbreaking of the Text-to-Image Generative AI Systems](https://arxiv.org/abs/2405.16567) – Demonstrates an automated pipeline for bypassing copyright filters in text-to-image models.
- [Misusing Tools in Large Language Models With Visual Adversarial Examples](https://arxiv.org/abs/2310.03185) – Shows that adversarial images can manipulate LLM-integrated tool APIs.
- [Reason2Attack: Jailbreaking Text-to-Image Models via LLM Reasoning](https://arxiv.org/abs/2503.17987) – Uses reasoning loops to craft visual prompts that circumvent content restrictions.

