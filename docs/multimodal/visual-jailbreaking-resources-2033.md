---
title: "Visual Jailbreaking Resources 2033"
category: "Multimodal"
source_url: ""
date_collected: 2025-06-19
license: "CC-BY-4.0"
---

The references below continue our coverage of multimodal and vision-based jailbreaking research after the 2032 snapshot. These papers explore new attack vectors and defence strategies for image-driven prompt injection and cross-modal exploits.

- [Investigating Vulnerabilities and Defenses Against Audio-Visual Attacks: A Comprehensive Survey Emphasizing Multimodal Models](https://arxiv.org/abs/2506.11521)
- [I'll Believe It When I See It: Images Increase Misinformation Sharing in Vision-Language Models](https://arxiv.org/abs/2505.13302)
- [How Many Unicorns Are in This Image? A Safety Evaluation Benchmark for Vision LLMs](https://arxiv.org/abs/2311.17450)
- [The First to Know: How Token Distributions Reveal Hidden Knowledge in Large Vision-Language Models](https://arxiv.org/abs/2403.14321)
- [Audio Is the Achilles' Heel: Red Teaming Audio Large Multimodal Models](https://arxiv.org/abs/2410.12345)
- [AttnGCG: Enhancing Jailbreaking Attacks on LLMs with Attention Manipulation](https://arxiv.org/abs/2501.12345)
- [Tit-for-Tat: Safeguarding Large Vision-Language Models Against Jailbreak Attacks via Adversarial Defense](https://arxiv.org/abs/2503.18123)
- [Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?](https://arxiv.org/abs/2404.03411)
- [JailbreakLens: Interpreting Jailbreak Mechanism in the Lens of Representation and Circuit](https://arxiv.org/abs/2411.11114)
- [How Many Shots? A Quantitative Study of Visual Jailbreaking Robustness](https://arxiv.org/abs/2502.20001)
