---
title: "Model-Specific LLM Attack Resources"
category: "Model-Specific"
source_url: ""
date_collected: 2025-06-18
license: "CC-BY-4.0"
---

This page lists additional academic papers and reports focusing on attacks or defenses that depend on the internals of a particular model.

- [Simulate and Eliminate: Revoke Backdoors for Generative Large Language Models](https://arxiv.org/abs/2409.12527)
- [DemonAgent: Dynamically Encrypted Multi-Backdoor Implantation Attack on LLM-based Agent](https://arxiv.org/abs/2405.17583)
- [MEGen: Generative Backdoor in Large Language Models via Model Editing](https://arxiv.org/abs/2403.09727)
- [Research about the Ability of LLM in the Tamper-Detection Area](https://arxiv.org/abs/2307.04536)
- [TaMPERing with Large Language Models: A Field Guide for Using Generative AI in Public Administration Research](https://arxiv.org/abs/2305.19148)
- [TextSleuth: Towards Explainable Tampered Text Detection](https://arxiv.org/abs/2311.09491)
- [Tamper-Resistant Safeguards for Open-Weight LLMs](https://arxiv.org/abs/2408.00761)
- [Gracefully Filtering Backdoor Samples for Generative Large Language Models without Retraining](https://arxiv.org/abs/2412.02454)
- [Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents](https://arxiv.org/abs/2402.11208)
- [Model Tampering Attacks Enable More Rigorous Evaluations of LLM](https://arxiv.org/abs/2502.05209)
- [Did the Neurons Read Your Book? Document-level Membership Inference for Large Language Models](https://arxiv.org/abs/2310.15007)
