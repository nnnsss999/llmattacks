---
title: "Model-Specific LLM Attack Resources"
category: "Model-Specific"
source_url: ""
date_collected: 2025-06-18
license: "CC-BY-4.0"
---

This page lists additional academic papers and reports focusing on attacks or defenses that depend on the internals of a particular model.

- [Simulate and Eliminate: Revoke Backdoors for Generative Large Language Models](https://arxiv.org/abs/2409.12527)
- [DemonAgent: Dynamically Encrypted Multi-Backdoor Implantation Attack on LLM-based Agent](https://arxiv.org/abs/2405.17583)
- [MEGen: Generative Backdoor in Large Language Models via Model Editing](https://arxiv.org/abs/2403.09727)
- [Research about the Ability of LLM in the Tamper-Detection Area](https://arxiv.org/abs/2307.04536)
- [TaMPERing with Large Language Models: A Field Guide for Using Generative AI in Public Administration Research](https://arxiv.org/abs/2305.19148)
- [TextSleuth: Towards Explainable Tampered Text Detection](https://arxiv.org/abs/2311.09491)
- [Tamper-Resistant Safeguards for Open-Weight LLMs](https://arxiv.org/abs/2408.00761)
- [Gracefully Filtering Backdoor Samples for Generative Large Language Models without Retraining](https://arxiv.org/abs/2412.02454)
- [Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents](https://arxiv.org/abs/2402.11208)
- [Model Tampering Attacks Enable More Rigorous Evaluations of LLM](https://arxiv.org/abs/2502.05209)
- [Did the Neurons Read Your Book? Document-level Membership Inference for Large Language Models](https://arxiv.org/abs/2310.15007)
- [BackdoorLLM: A Comprehensive Benchmark for Backdoor Attacks and Defenses on Large Language Models](https://github.com/bboylyg/BackdoorLLM)
- [BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models](https://arxiv.org/abs/2401.12242)
- [Composite Backdoor Attacks Against Large Language Models](https://github.com/MiracleHH/CBA)
- [Agent Backdoor Attacks (NeurIPS 2024)](https://github.com/lancopku/agent-backdoor-attacks)
- [Model Tampering Evals Repository](https://github.com/zorache/model-tampering-evals)
- [Poser: Unmasking Alignment Faking LLMs by Manipulating Their Internals](https://arxiv.org/abs/2405.05466)
- [ShadowCoT: Cognitive Hijacking for Stealthy Reasoning Backdoors in LLMs](https://arxiv.org/abs/2504.05605)
- [Winter Soldier: Backdooring Language Models at Pre-Training with Indirect Data Poisoning](https://arxiv.org/abs/2506.14913)
- [RL-Obfuscation: Can Language Models Learn to Evade Latent-Space Monitors?](https://arxiv.org/abs/2506.14261)
- [Pseudo-Conversation Injection for LLM Goal Hijacking](https://arxiv.org/abs/2410.23678)
- [LoX: Low-Rank Extrapolation Robustifies LLM Safety Against Fine-tuning](https://arxiv.org/abs/2506.15606)
- [Attention Hijacking in Trojan Transformers](https://arxiv.org/abs/2208.04946)
- [Alignment-Aware Model Extraction Attacks on Large Language Models](https://openreview.net/forum?id=AKsfpHc9sN)
- [Exposing the Guardrails: Reverse-Engineering and Jailbreaking Safety Filters in DALL·E Text-to-Image Pipelines](https://www.usenix.org/system/files/conference/usenixsecurity25/sec25cycle1-prepub-746-villa.pdf)
- [Multi-Faceted Studies on Data Poisoning](https://arxiv.org/html/2502.14182v1)
- [LoRA Leakage: Gradient Inversion Attacks](https://arxiv.org/abs/2406.01234)
- [TrojLLM: A Black-Box Trojan Prompt Attack on Large Language Models](https://dl.acm.org/doi/abs/10.5555/3666122.3668988)
- [Stealing Input in LLM Services via Timing Side-Channel Attacks](https://arxiv.org/html/2411.18191v1)
- [BadReward: Clean-Label Poisoning of Reward Models in Text-to-Image RLHF](https://arxiv.org/abs/2506.03234)
- [Adversarial Preference Learning for Robust LLM Alignment](https://arxiv.org/abs/2505.24369)
- [Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts](https://arxiv.org/html/2311.09127v2)
- [Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?](https://arxiv.org/html/2404.03411v2)
- [Patch is Enough: Naturalistic Adversarial Patch against Vision-Language Pre-training Models](https://arxiv.org/abs/2410.04884)
- [The Dark Side of Function Calling: Pathways to Jailbreaking Large Language Models](https://arxiv.org/html/2407.17915v1)
- [Diffusion Models Can Be Contaminated with Backdoors, Study Finds](https://venturebeat.com/ai/diffusion-models-can-be-contaminated-with-backdoors-study-finds/)
- [Membership Inference Attacks on Large-Scale Models: A Survey](https://arxiv.org/abs/2503.19338)
- [PoisonedRAG: Knowledge Corruption Attacks to Retrieval-Augmented Generation of Large Language Models](https://arxiv.org/abs/2402.07867)
- [PoisonPrompt: Backdoor Attack on Prompt-Based Large Language Models](https://doi.org/10.1109/icassp48485.2024.10446267)
- [A Comprehensive Overview of Backdoor Attacks in Large Language Models within Communication Networks](https://doi.org/10.1109/mnet.2024.3367788)
- [Mission Impossible: A Statistical Perspective on Jailbreaking LLMs](https://arxiv.org/abs/2408.01420)
- [Hacc-Man: An Arcade Game for Jailbreaking LLMs](https://doi.org/10.1145/3656156.3665432)
- [Text Embedding Inversion Attacks on Multilingual Language Models](https://arxiv.org/abs/2401.12192)
- [Transferable Embedding Inversion Attack: Uncovering Privacy Risks in Text Embeddings without Model Queries](https://arxiv.org/abs/2406.10280)
- [Assessing Vulnerabilities in State-of-the-Art Large Language Models Through Hex Injection](https://doi.org/10.1609/aaai.v39i28.35257)
- [Effective and Evasive Fuzz Testing-Driven Jailbreaking Attacks against LLMs](https://arxiv.org/abs/2409.14866)
- [AIRTBench: Measuring Autonomous AI Red Teaming Capabilities in Language Models](https://arxiv.org/abs/2506.14682)
- [Doppelgänger Method: Breaking Role Consistency in LLM Agent via Prompt-based Transferable Adversarial Attack](https://arxiv.org/abs/2506.14539)
- [LingoLoop Attack: Trapping MLLMs via Linguistic Context and State Entrapment into Endless Loops](https://arxiv.org/abs/2506.14493)
- [Excessive Reasoning Attack on Reasoning LLMs](https://arxiv.org/abs/2506.14374)
- [Finetuning-Activated Backdoors in LLMs](https://www.sri.inf.ethz.ch/publications/gloaguen2025finetuning)
- [ALGEN: Few-shot Inversion Attacks on Textual Embeddings using Alignment](https://vbn.aau.dk/en/publications/algen-few-shot-inversion-attacks-on-textual-embeddings-using-alig)
- [Mitigating Privacy Risks in LLM Embeddings from Embedding Inversion](https://arxiv.org/abs/2411.05034)
- [Gradient Inversion Transcript: Leveraging Robust Generative Priors to Reconstruct Training Data from Gradient Leakage](https://arxiv.org/abs/2505.20026)
- [An Inversion Attack Against Obfuscated Embedding Matrix in Language Models](https://aclanthology.org/2024.emnlp-main.126/)
- [Sycophancy to Subterfuge: Investigating Reward-Tampering in Large Language Models](https://arxiv.org/abs/2406.10162)
- [Analyzing Multi-Head Attention on Trojan BERT Models](https://arxiv.org/abs/2406.16925)
- [Task-Agnostic Detector for Insertion-Based Backdoor Attacks](https://arxiv.org/abs/2403.17155)
- [What Was Your Prompt? A Remote Keylogging Attack on AI Assistants](https://arxiv.org/abs/2403.09751)
- [DarkMind: Latent Chain-of-Thought Backdoor in Customized LLMs](https://arxiv.org/abs/2501.18617)
- [LLM Misalignment via Adversarial RLHF Platforms](https://arxiv.org/abs/2503.03039)
- [Universal Zero-shot Embedding Inversion](https://arxiv.org/abs/2504.00147)
- [Information Leakage of Sentence Embeddings via Generative Embedding Inversion Attacks](https://arxiv.org/abs/2504.16609)
- [Data Poisoning in LLMs: Jailbreak-Tuning and Scaling Trends](https://arxiv.org/abs/2408.02946)
- [A Systematic Review of Poisoning Attacks Against Large Language Models](https://arxiv.org/abs/2506.06518)
- [LLM Security: Vulnerabilities, Attacks, Defenses, and Countermeasures](https://arxiv.org/abs/2505.01177)
- [Latent Space Backdoors — When the Trap Is Hidden in the Embeddings](https://techmaniacs.com/2025/05/27/latent-space-backdoors-when-the-trap-is-hidden-in-the-embeddings/)
- [Finding Safety Neurons in Large Language Models](https://arxiv.org/abs/2406.14144)
- [Understanding and Enhancing Safety Mechanisms of LLMs via Safety-Specific Neuron](https://openreview.net/forum?id=yR47RmND1m)
- [Obfuscated Activations Bypass LLM Latent-Space Defenses](https://arxiv.org/abs/2412.09565)
- [Autodan: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models](https://arxiv.org/abs/2310.04451)
- [Piccolo: Exposing Complex Backdoors in NLP Transformer Models](https://arxiv.org/abs/2202.12320)
- [Neuron-Level Sequential Editing for Large Language Models](https://arxiv.org/abs/2410.04045)
- [Precision Knowledge Editing: Enhancing Safety in Large Language Models](https://arxiv.org/abs/2410.03772)
- [Can Editing LLMs Inject Harm?](https://arxiv.org/abs/2407.20224)
- [Safety Misalignment Against Large Language Models](https://www.ndss-symposium.org/wp-content/uploads/2025-1089-paper.pdf)
- [BadEdit: Backdooring Large Language Models by Model Editing](https://openreview.net/forum?id=duZANm2ABX)
- [H\xC3\x96LDER PRUNING: Localized Pruning for Backdoor Removal in Deep Neural Networks](https://openreview.net/forum?id=yJduhi9mDQ)
- [Reconstructive Neuron Pruning for Backdoor Defense](https://proceedings.mlr.press/v202/li23v.html)
- [Towards Best Practices of Activation Patching in Language Models: Metrics and Methods](https://arxiv.org/abs/2309.16042)
- [Attribution Patching: Activation Patching At Industrial Scale](https://www.neelnanda.io/mechanistic-interpretability/attribution-patching)
- [Revisiting Jailbreaking for Large Language Models: A Representation Engineering Perspective](https://aclanthology.org/2025.coling-main.212)
- [Jailbreaking Prompt Attack: A Controllable Adversarial Attack against Diffusion Models](https://aclanthology.org/2025.findings-naacl.172)
- [Latent Adversarial Training Improves the Representation of Refusal](https://openreview.net/forum?id=GScy14jUjc&noteId=PtETnBOYYB)
- [Harnessing the Universal Geometry of Embeddings](https://arxiv.org/abs/2505.12540)
- [Probability Density Geodesics in Image Diffusion Latent Space](https://cvpr.thecvf.com/virtual/2025/poster/32921)
- [There and Back Again: On the Relation Between Noises, Images, and Their Inversions in Diffusion Models](https://openreview.net/forum?id=Dgh5GXsW65)
- [Jailbreaking LLMs' Safeguard with Universal Magic Words for Text Embedding Models](https://arxiv.org/abs/2501.18280)
- [GEIA: Generative Embedding Inversion Attacks Toolkit](https://github.com/HKUST-KnowComp/GEIA)
- [MultiVec2Text: Multi-language Embedding Inversion Library](https://github.com/siebeniris/MultiVec2Text)
- [LAGO: Few-shot Crosslingual Embedding Inversion Attacks via Language Similarity-Aware Graph Optimization](https://arxiv.org/abs/2505.16008)
- [Mitigating Safety Fallback in Editing-based Backdoor Injection on LLMs](https://arxiv.org/abs/2506.13285)
- [MEraser: An Effective Fingerprint Erasure Approach for Large Language Models](https://arxiv.org/abs/2506.12551)
- [SPBA: Utilizing Speech Large Language Model for Backdoor Attacks on Speech Classification Models](https://arxiv.org/abs/2506.08346)
- [Your Agent Can Defend Itself against Backdoor Attacks](https://arxiv.org/abs/2506.08336)
- [TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts](https://arxiv.org/abs/2506.07596)
- [Backdoor Attack on Vision Language Models with Stealthy Semantic Manipulation](https://arxiv.org/abs/2506.07214)
- [Robust Anti-Backdoor Instruction Tuning in LVLMs](https://arxiv.org/abs/2506.05401)
- [Merge Hijacking: Backdoor Attacks to Model Merging of Large Language Models](https://arxiv.org/abs/2505.23561)
- [Detecting Stealthy Backdoor Samples based on Intra-class Distance for Large Language Models](https://arxiv.org/abs/2505.23015)
- [DyePack: Provably Flagging Test Set Contamination in LLMs Using Backdoors](https://arxiv.org/abs/2505.23001)
- [GUARD: Dual-Agent based Backdoor Defense on Chain-of-Thought in Neural Code Generation](https://arxiv.org/abs/2505.21425)
- [Architectural Backdoors for Within-Batch Data Stealing and Model Inference Manipulation](https://arxiv.org/abs/2505.18323)
- [Wolf Hidden in Sheep's Conversations: Toward Harmless Data-Based Backdoor Attacks for Jailbreaking Large Language Models](https://arxiv.org/abs/2505.17601)
- [Large Language Models in the IoT Ecosystem -- A Survey on Security Challenges and Applications](https://arxiv.org/abs/2505.17586)
- [Backdoor Cleaning without External Guidance in MLLM Fine-tuning](https://arxiv.org/abs/2505.16916)
- [BadVLA: Towards Backdoor Attacks on Vision-Language-Action Models via Objective-Decoupled Optimization](https://arxiv.org/abs/2505.16640)
- [Finetuning-Activated Backdoors in LLMs](https://arxiv.org/abs/2505.16567)
- [Thought Crime: Backdoors and Emergent Misalignment in Reasoning Models](https://arxiv.org/abs/2506.13206)
- [Alphabet Index Mapping: Jailbreaking LLMs through Semantic Dissimilarity](https://arxiv.org/abs/2506.12685)
- [Visually Guided Decoding: Gradient-Free Hard Prompt Inversion with Language Models](https://arxiv.org/abs/2505.08622)
- [ReCIT: Reconstructing Full Private Data from Gradient in Parameter-Efficient Fine-Tuning of Large Language Models](https://arxiv.org/abs/2504.20570)
- [The Ripple Effect: On Unforeseen Complications of Backdoor Attacks](https://arxiv.org/abs/2505.11586)
- [Lie Detector: Unified Backdoor Detection via Cross-Examination Framework](https://arxiv.org/abs/2503.16872)
- [Neural Antidote: Class-Wise Prompt Tuning for Purifying Backdoors in Pre-trained Vision-Language Models](https://arxiv.org/abs/2502.19269)
- [ELBA-Bench: An Efficient Learning Backdoor Attacks Benchmark for Large Language Models](https://arxiv.org/abs/2502.18511)
- [Stealthy Backdoor Attack in Self-Supervised Learning Vision Encoders for Large Vision Language Models](https://arxiv.org/abs/2502.18290)
- [Certifying Language Model Robustness with Fuzzed Randomized Smoothing: An Efficient Defense Against Backdoor Attacks](https://arxiv.org/abs/2502.06892)
- [SABER: Model-agnostic Backdoor Attack on Chain-of-Thought in Neural Code Generation](https://arxiv.org/abs/2412.05829)
- [Imitation Game for Adversarial Disillusion with Multimodal Generative Chain-of-Thought Role-Play](https://arxiv.org/abs/2501.19143)
- [Compromising Embodied Agents with Contextual Backdoor Attacks](https://arxiv.org/abs/2408.02882)
- [Non-Halting Queries: Exploiting Fixed Points in LLMs](https://arxiv.org/abs/2410.06287)
- [DAGER: Exact Gradient Inversion for Large Language Models](https://openreview.net/forum?id=CrADAX7h23)
- [Wiretapping LLMs: Network Side-Channel Attacks on Interactive LLM Services](https://eprint.iacr.org/2025/167)
- [Spill The Beans: Exploiting CPU Cache Side-Channels to Leak Tokens from Large Language Models](https://arxiv.org/abs/2505.00817)
- [I Know What You Said: Unveiling Hardware Cache Side-Channels in Local Large Language Model Inference](https://arxiv.org/abs/2505.06738)
- [The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM Serving Systems](https://arxiv.org/abs/2409.20002)
- [Making Acoustic Side-Channel Attacks on Noisy Keyboards Viable with LLM-Assisted Spectrograms' "Typo" Correction](https://arxiv.org/abs/2504.11622)
- [Detecting and Perturbing Privacy-Sensitive Neurons to Defend Embedding Inversion Attacks](https://openreview.net/forum?id=DF5TVzpTW0)
- [Exploring Backdoor Attack and Defense for LLM-empowered Recommendations](https://arxiv.org/abs/2504.11182)
- [When Backdoors Speak: Understanding LLM Backdoor Attacks Through Model-Generated Explanations](https://arxiv.org/abs/2411.12701)
- [PEFTGuard: Detecting Backdoor Attacks Against Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2411.17453)
- [CROW: Eliminating Backdoors from Large Language Models via Internal Consistency Regularization](https://arxiv.org/abs/2411.12768)
- [A Realistic Threat Model for Large Language Model Jailbreaks](https://openreview.net/forum?id=1kMTJnqmyl)
- [NeuRel-Attack: Neuron Relearning for Safety Disalignment in Large Language Models](https://arxiv.org/abs/2504.21053)
- [BAN: Detecting Backdoors Activated by Adversarial Neuron Noise](https://arxiv.org/abs/2405.19928)
- [DEPN: Detecting and Editing Privacy Neurons in Pretrained Language Models](https://arxiv.org/abs/2310.20138)
- [BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents](https://arxiv.org/abs/2406.03007)
- [Neuron Empirical Gradient: Discovering and Quantifying Neurons' Global Linear Controllability](https://arxiv.org/abs/2412.18053)
- [SECNEURON: Reliable and Flexible Abuse Control in Local LLMs via Hybrid Neuron Encryption](https://arxiv.org/abs/2506.05242)
- [Holes in Latent Space: Topological Signatures Under Adversarial Influence](https://arxiv.org/abs/2505.20435)
- [Massive Activations in Large Language Models](https://github.com/locuslab/massive-activations)
- [Framework for Neuron-Level Interpretability and Robustness in LLMs](https://github.com/kikaymusic/Framework-for-Neuron-Level-Interpretability-and-Robustness-in-LLMs)
- [THU-KEG/SafetyNeuron](https://github.com/THU-KEG/SafetyNeuron)
- [LLM Causal Neuron Attack](https://www.promptfoo.dev/lm-security-db/vuln/llm-causal-neuron-attack-92701d5e)
- [Activation Patching. Illuminating the Hidden Pathways of Language Models](https://medium.com/everyday-ai/activation-patching-d916e7777e66)
- [An Introduction to Representation Engineering – an activation-based paradigm for controlling LLMs](https://www.alignmentforum.org/posts/3ghj8EuKzwD3MQR5G/an-introduction-to-representation-engineering-an-activation)
- [Language models can explain neurons in language models](https://openai.com/index/language-models-can-explain-neurons-in-language-models/)
- [Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models](https://arxiv.org/abs/2401.06102)
- [Patchscopes Demo](https://pair.withgoogle.com/explorables/patchscopes/)
- [Causality Analysis for LLMs](https://casperllm.github.io/)
- [A Survey of Backdoor Attacks and Defenses on Large Language Models: Implications for Security Measures](https://arxiv.org/abs/2406.06852)
- [Exploiting the Vulnerability of Large Language Models via Defense-Aware Architectural Backdoor](https://arxiv.org/abs/2409.01952)
- [BAIT: Large Language Model Backdoor Scanning by Inverting Attack Target](https://www.cs.purdue.edu/homes/shen447/files/paper/sp25_bait.pdf)
- [Chain-of-Scrutiny: Detecting Backdoor Attacks for Large Language Models](https://arxiv.org/abs/2406.05948)
- [An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting Disguised Vulnerabilities against Strong Detection](https://www.usenix.org/conference/usenixsecurity24/presentation/yan)
- [Text Embedding Inversion Security for Multilingual Language Models](https://aclanthology.org/2024.acl-long.422/)
- [Model Editing Harms General Abilities of Large Language Models: Regularization to the Rescue](https://aclanthology.org/2024.emnlp-main.934/)
- [Concept-ROT: Poisoning Concepts in Large Language Models with Model Editing](https://openreview.net/forum?id=RzUvkI3p1D)
- [Position: Editing Large Language Models Poses Serious Safety Risks](https://arxiv.org/abs/2502.02958)
- [Defending Large Language Models Against Jailbreak Attacks via Layer-specific Editing](https://aclanthology.org/2024.findings-emnlp.293/)
- [What is LLMjacking?](https://sysdig.com/learn-cloud-native/what-is-llmjacking/)
- [Attack and defense techniques in large language models: A survey and new perspectives](https://arxiv.org/abs/2505.00976)
- [Stealing finetuning data with corrupted models](https://chrishicks.io/blog/2025/llm-backdoors/)
- [Establishing Trustworthy LLM Evaluation via Shortcut Neuron Analysis](https://arxiv.org/abs/2506.04142)
- [Contrastive Neuron Pruning for Backdoor Defense](https://ieeexplore.ieee.org/document/10887056)
- [Safety-Neuron: Understanding and Mitigating Safety Issues](https://github.com/zhaoyiran924/Safety-Neuron)
- [Model Extraction Attack and Defense for Large Language Models](https://labrai.github.io/KDD2025_Tutorial/)
- [Prompt Inversion Attack Against Collaborative Inference of Large Language Models](https://ieeexplore.ieee.org/document/11023374)
- [Model Inversion Attacks — Extracting Sensitive Data From Trained AI](https://techmaniacs.com/2025/05/16/model-inversion-attacks-extracting-sensitive-data-from-trained-ai/)
- [BadJudge: Backdoor Vulnerabilities of LLM-as-a-Judge](https://arxiv.org/abs/2503.00596)
- [Can We Trust Embodied Agents? Exploring Backdoor Attacks](https://openreview.net/forum?id=S1Bv3068Xt)
- [git-disl/awesome_LLM-harmful-fine-tuning-papers](https://github.com/git-disl/awesome_LLM-harmful-fine-tuning-papers)
- [Expose Before You Defend: Unifying and Enhancing Backdoor Defenses via Exposed Models](https://arxiv.org/abs/2410.16004)
- [Backdoor Token Unlearning: Exposing and Defending Backdoors in LLMs](https://arxiv.org/abs/2409.15521)
- [Rethinking Backdoor Detection Evaluation for Language Models](https://arxiv.org/abs/2411.04053)
- [Rethink the Evaluation for Attack Strength of Backdoor Attacks in LLMs](https://arxiv.org/abs/2409.18884)
- [From Shortcuts to Triggers: Backdoor Defense with Denoised PoE](https://arxiv.org/abs/2409.10777)
- [Mudjacking: Patching Backdoor Vulnerabilities in Foundation Models](https://arxiv.org/abs/2409.09191)
- [BackdoorMBTI: A Backdoor Learning Multimodal Benchmark Tool Kit for Large Models](https://arxiv.org/abs/2407.18222)
- [Analyzing And Editing Inner Mechanisms of Backdoored Language Models](https://arxiv.org/abs/2409.05050)
- [Architectural Neural Backdoors from First Principles](https://arxiv.org/abs/2408.00815)
