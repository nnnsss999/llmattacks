---
title: "Model-Specific LLM Attack Resources"
category: "Model-Specific"
source_url: ""
date_collected: 2025-06-18
license: "CC-BY-4.0"
---

This page lists additional academic papers and reports focusing on attacks or defenses that depend on the internals of a particular model.

- [Simulate and Eliminate: Revoke Backdoors for Generative Large Language Models](https://arxiv.org/abs/2409.12527)
- [DemonAgent: Dynamically Encrypted Multi-Backdoor Implantation Attack on LLM-based Agent](https://arxiv.org/abs/2405.17583)
- [MEGen: Generative Backdoor in Large Language Models via Model Editing](https://arxiv.org/abs/2403.09727)
- [Research about the Ability of LLM in the Tamper-Detection Area](https://arxiv.org/abs/2307.04536)
- [TaMPERing with Large Language Models: A Field Guide for Using Generative AI in Public Administration Research](https://arxiv.org/abs/2305.19148)
- [TextSleuth: Towards Explainable Tampered Text Detection](https://arxiv.org/abs/2311.09491)
- [Tamper-Resistant Safeguards for Open-Weight LLMs](https://arxiv.org/abs/2408.00761)
- [Gracefully Filtering Backdoor Samples for Generative Large Language Models without Retraining](https://arxiv.org/abs/2412.02454)
- [Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents](https://arxiv.org/abs/2402.11208)
- [Model Tampering Attacks Enable More Rigorous Evaluations of LLM](https://arxiv.org/abs/2502.05209)
- [Did the Neurons Read Your Book? Document-level Membership Inference for Large Language Models](https://arxiv.org/abs/2310.15007)
- [BackdoorLLM: A Comprehensive Benchmark for Backdoor Attacks and Defenses on Large Language Models](https://github.com/bboylyg/BackdoorLLM)
- [BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models](https://arxiv.org/abs/2401.12242)
- [Composite Backdoor Attacks Against Large Language Models](https://github.com/MiracleHH/CBA)
- [Agent Backdoor Attacks (NeurIPS 2024)](https://github.com/lancopku/agent-backdoor-attacks)
- [Model Tampering Evals Repository](https://github.com/zorache/model-tampering-evals)
- [Poser: Unmasking Alignment Faking LLMs by Manipulating Their Internals](https://arxiv.org/abs/2405.05466)
- [ShadowCoT: Cognitive Hijacking for Stealthy Reasoning Backdoors in LLMs](https://arxiv.org/abs/2504.05605)
- [Winter Soldier: Backdooring Language Models at Pre-Training with Indirect Data Poisoning](https://arxiv.org/abs/2506.14913)
- [RL-Obfuscation: Can Language Models Learn to Evade Latent-Space Monitors?](https://arxiv.org/abs/2506.14261)
- [Pseudo-Conversation Injection for LLM Goal Hijacking](https://arxiv.org/abs/2410.23678)
- [LoX: Low-Rank Extrapolation Robustifies LLM Safety Against Fine-tuning](https://arxiv.org/abs/2506.15606)
- [Attention Hijacking in Trojan Transformers](https://arxiv.org/abs/2208.04946)
- [Alignment-Aware Model Extraction Attacks on Large Language Models](https://openreview.net/forum?id=AKsfpHc9sN)
- [Exposing the Guardrails: Reverse-Engineering and Jailbreaking Safety Filters in DALL·E Text-to-Image Pipelines](https://www.usenix.org/system/files/conference/usenixsecurity25/sec25cycle1-prepub-746-villa.pdf)
- [Multi-Faceted Studies on Data Poisoning](https://arxiv.org/html/2502.14182v1)
- [LoRA Leakage: Gradient Inversion Attacks](https://arxiv.org/abs/2406.01234)
- [TrojLLM: A Black-Box Trojan Prompt Attack on Large Language Models](https://dl.acm.org/doi/abs/10.5555/3666122.3668988)
- [Stealing Input in LLM Services via Timing Side-Channel Attacks](https://arxiv.org/html/2411.18191v1)
- [BadReward: Clean-Label Poisoning of Reward Models in Text-to-Image RLHF](https://arxiv.org/abs/2506.03234)
- [Adversarial Preference Learning for Robust LLM Alignment](https://arxiv.org/abs/2505.24369)
- [Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts](https://arxiv.org/html/2311.09127v2)
- [Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?](https://arxiv.org/html/2404.03411v2)
- [Patch is Enough: Naturalistic Adversarial Patch against Vision-Language Pre-training Models](https://arxiv.org/abs/2410.04884)
- [The Dark Side of Function Calling: Pathways to Jailbreaking Large Language Models](https://arxiv.org/html/2407.17915v1)
- [Diffusion Models Can Be Contaminated with Backdoors, Study Finds](https://venturebeat.com/ai/diffusion-models-can-be-contaminated-with-backdoors-study-finds/)
- [Membership Inference Attacks on Large-Scale Models: A Survey](https://arxiv.org/abs/2503.19338)
- [PoisonedRAG: Knowledge Corruption Attacks to Retrieval-Augmented Generation of Large Language Models](https://arxiv.org/abs/2402.07867)
- [PoisonPrompt: Backdoor Attack on Prompt-Based Large Language Models](https://doi.org/10.1109/icassp48485.2024.10446267)
- [A Comprehensive Overview of Backdoor Attacks in Large Language Models within Communication Networks](https://doi.org/10.1109/mnet.2024.3367788)
- [Mission Impossible: A Statistical Perspective on Jailbreaking LLMs](https://arxiv.org/abs/2408.01420)
- [Hacc-Man: An Arcade Game for Jailbreaking LLMs](https://doi.org/10.1145/3656156.3665432)
- [Text Embedding Inversion Attacks on Multilingual Language Models](https://arxiv.org/abs/2401.12192)
- [Transferable Embedding Inversion Attack: Uncovering Privacy Risks in Text Embeddings without Model Queries](https://arxiv.org/abs/2406.10280)
- [Assessing Vulnerabilities in State-of-the-Art Large Language Models Through Hex Injection](https://doi.org/10.1609/aaai.v39i28.35257)
- [Effective and Evasive Fuzz Testing-Driven Jailbreaking Attacks against LLMs](https://arxiv.org/abs/2409.14866)
- [AIRTBench: Measuring Autonomous AI Red Teaming Capabilities in Language Models](https://arxiv.org/abs/2506.14682)
- [Doppelgänger Method: Breaking Role Consistency in LLM Agent via Prompt-based Transferable Adversarial Attack](https://arxiv.org/abs/2506.14539)
- [LingoLoop Attack: Trapping MLLMs via Linguistic Context and State Entrapment into Endless Loops](https://arxiv.org/abs/2506.14493)
- [Excessive Reasoning Attack on Reasoning LLMs](https://arxiv.org/abs/2506.14374)
