---
title: "Neuron-Level Manipulation Resources 2033"
category: "Latent Space"
source_url: ""
date_collected: 2025-06-19
license: "CC-BY-4.0"
---

The references below expand the neuron-focused bibliography with papers and tools published after the 2032 update. They explore advanced neuron-level editing, pruning, and analysis techniques for large language models.

## Research Papers

- [Separating Tongue from Thought: Activation Patching Reveals Language-Agnostic Concept Representations in Transformers](https://www.semanticscholar.org/paper/b1662c1d324ccccf37d630a4b435224e1f336c37) – uses activation patching to isolate language-agnostic concepts.
- [Towards Faithful Natural Language Explanations: A Study Using Activation Patching in Large Language Models](https://www.semanticscholar.org/paper/e42262c4b67a8003ca930de0ac6275725bb76332) – applies neuron-level steering for more faithful explanations.
- [How to Use and Interpret Activation Patching](https://www.semanticscholar.org/paper/a0b775b9ff82ce1fb7dd34d53a7d09f70b171895) – tutorial on practical activation patching methods.
- [MPN: Leveraging Multilingual Patch Neuron for Cross-lingual Model Editing](https://www.semanticscholar.org/paper/b97f9c9ecd9222b74b93205ac792f8e3cad4aec3) – introduces patch neurons enabling cross-language knowledge edits.
- [Journey to the Center of the Knowledge Neurons: Discoveries of Language-Independent Knowledge Neurons and Degenerate Knowledge Neurons](https://www.semanticscholar.org/paper/628b131f8f309d583ccf1da268f520c051169ddd) – locates neurons encoding universal knowledge concepts.
- [Cross-lingual Editing in Multilingual Language Models](https://www.semanticscholar.org/paper/00fe167323d7b174cc636b8ccd64d4eac38d96a6) – examines targeted neuron editing across languages.
- [BMIKE-53: Investigating Cross-Lingual Knowledge Editing with In-Context Learning](https://www.semanticscholar.org/paper/a5b997fd015f218440eca263910543b951979692) – benchmarks neuron manipulation via in-context learning.
- [Cross-Lingual Multi-Hop Knowledge Editing - Benchmarks, Analysis and a Simple Contrastive Learning based Approach](https://www.semanticscholar.org/paper/71c1e1933ff3e0050120415b033c3ac46830afe6) – tests sequential neuron edits linking concepts across languages.
- [Adversarial Neuron Pruning Purifies Backdoored Deep Models](https://www.semanticscholar.org/paper/f17b9c4118e6cf6042e9fb9feacbea8909e5e655) – removes malicious behaviour by pruning compromised neurons.
- [Rethinking Pruning for Backdoor Mitigation: An Optimization Perspective](https://www.semanticscholar.org/paper/0f64853deaca6409a00948222f33fed93decd09f) – improves neuron-pruning strategies to counter backdoors.
- [Finding Trojan Triggers in Code LLMs: An Occlusion-Based Human-in-the-Loop Approach](https://www.semanticscholar.org/paper/5b4ce13f5be5704e55c81d107b38dadb779e3025) – locates hidden backdoor neurons in code-generating models.
- [Causality Analysis for Evaluating the Security of Large Language Models](https://www.semanticscholar.org/paper/75fc4becd42527d552448e03e1b358c6d818a027) – uses causal probing to reveal vulnerable neuron activations.

## Tools and Code

- [WhatAboutMyStar/LLM_ACTIVATION](https://github.com/WhatAboutMyStar/LLM_ACTIVATION) – toolkit for inspecting and modifying LLM neuron activations.
- [eggsyntax/neuron-self-report](https://github.com/eggsyntax/neuron-self-report) – experiments on neurons self-reporting their behaviour.
- [inspector-apm/neuron-ai](https://github.com/inspector-apm/neuron-ai) – interactive interface for exploring neuron-level behaviour.
- [wesg52/universal-neurons](https://github.com/wesg52/universal-neurons) – dataset and scripts for studying universal neurons across models.
