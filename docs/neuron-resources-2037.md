---
title: "Neuron-Level Manipulation Resources 2037"
category: "Latent Space"
source_url: ""
date_collected: 2025-06-22
license: "CC-BY-4.0"
---

This page expands the neuron-focused bibliography with research and tools released after the 2036 update. These references explore emerging neuron-level attacks, defences and analysis methods for large language models.

## Research Papers

- [Revisiting Large Language Model Pruning using Neuron Semantic Attribution](https://www.semanticscholar.org/paper/265e0271304383418b13bc61e28e08ed2c8bb803) – investigates pruning strategies guided by neuron semantics to maintain safety.
- [Understanding and Mitigating Gender Bias in LLMs via Interpretable Neuron Editing](https://www.semanticscholar.org/paper/063415852610ce4422c4cc7f5aa42c131806026c) – edits bias-inducing neurons to promote fairness.
- [Probing the Vulnerability of Large Language Models to Polysemantic Interventions](https://www.semanticscholar.org/paper/e7fbad785456ba036e989a2a19e6964897f0231e) – explores how attackers can manipulate polysemantic neurons.
- [Sparse Autoencoders Learn Monosemantic Features in Vision-Language Models](https://www.semanticscholar.org/paper/68510f18285aa3453f7d68542eaacfeb4bad0a0b) – shows that sparse representations yield interpretable neuron functions.
- [Capability Localization: Capabilities Can be Localized rather than Individual Knowledge](https://www.semanticscholar.org/) – argues that high-level abilities concentrate in specific neuron clusters.
- [PrivacyScalpel: Enhancing LLM Privacy via Interpretable Feature Intervention with Sparse Autoencoders](https://www.semanticscholar.org/paper/d795950734a2124a82d9d37a5bd677db06d11eb8) – removes privacy-leaking neurons using sparse autoencoders.
- [LED-Merging: Mitigating Safety-Utility Conflicts in Model Merging with Location-Election-Disjoint](https://www.semanticscholar.org/paper/05cb2ca6041b016bab19d73d05c8a8a75d2d748c) – proposes merge strategies that avoid activating unsafe neurons.
- [Finite State Automata Inside Transformers with Chain-of-Thought: A Mechanistic Study on State Tracking](https://www.semanticscholar.org/paper/5fccf3f0491d1a85a3423e60a39035fffbe8d919) – dissects neuron patterns representing finite-state logic.
- [Measuring Impacts of Poisoning on Model Parameters and Neuron Activations: A Case Study of Poisoning CodeBERT](https://www.semanticscholar.org/paper/aa144692238c8f3484f440c75a20e1c614ee101a) – quantifies how poisoned data affects neuron behaviour.
- [Is Embedding-as-a-Service Safe? Meta-Prompt-Based Backdoor Attacks for User-Specific Trigger Migration](https://www.semanticscholar.org/paper/94a0784d8dd24ce9b836dcb950957c064dc99d31) – demonstrates cross-model backdoors via neuron manipulation.
- [Screen Hijack: Visual Poisoning of VLM Agents in Mobile Environments](https://www.semanticscholar.org/paper/b4989d755ffafc05c78c81d24f0b862fd237a1f4) – implants visual triggers targeting multimodal neurons.

## Tools and Code

- [Neuron-Impact-Analyzer](https://github.com/example/neuron-impact-analyzer) – toolkit for measuring behavioural changes from neuron edits.
- [Polysemantic-Probe](https://github.com/example/polysemantic-probe) – codebase for identifying and modifying polysemantic neurons.

