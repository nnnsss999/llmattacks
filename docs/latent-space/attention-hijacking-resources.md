---
title: "Attention Hijacking Resources"
category: "Latent Space"
source_url: ""
date_collected: 2025-06-19
license: "CC-BY-4.0"
---

The references below focus on manipulating a transformer's **attention** mechanisms to subvert or hijack LLM behavior.

- [Universal Jailbreak Suffixes Are Strong Attention Hijackers](https://arxiv.org/abs/2506.12880)
- [Attention Hijackers: Detect and Disentangle Attention Hijacking in LVLMs](https://arxiv.org/abs/2503.08216)
- [ShadowCoT: Cognitive Hijacking for Stealthy Reasoning Backdoors in LLMs](https://arxiv.org/abs/2504.05605)
- [Pseudo-Conversation Injection for LLM Goal Hijacking](https://arxiv.org/abs/2410.23678)
- [Attention Hijacking in Trojan Transformers](https://arxiv.org/abs/2208.04946)
- [Shadows in the Attention: Contextual Perturbation and Representation Drift in the Dynamics of Hallucination in LLMs](https://arxiv.org/abs/2505.16894)
- [LoRA Users Beware: A Few Spurious Tokens Can Manipulate Your Finetuned Model](https://arxiv.org/abs/2506.11402)
- [Attacking Attention of Foundation Models Disrupts Downstream Tasks](https://arxiv.org/abs/2506.05394)
- [Not All Tokens and Heads Are Equally Important: Dual-Level Attention Intervention for Hallucination Mitigation](https://arxiv.org/abs/2506.12609)
- [Instruction Following by Boosting Attention of Large Language Models](https://arxiv.org/abs/2506.13734)
- [Dynamic Attention Analysis for Backdoor Detection in Text-to-Image Diffusion Models](https://arxiv.org/abs/2504.20518)
- [Jatmo: Prompt Injection Defense by Task-Specific Finetuning](https://arxiv.org/abs/2312.17673)
- [A Study of the Attention Abnormality in Trojaned BERTs](https://arxiv.org/abs/2205.08305)
- [Feature Space Hijacking Attacks against Differentially Private Split Learning](https://arxiv.org/abs/2201.04018)
- [RTBAS: Defending LLM Agents Against Prompt Injection and Privacy Leakage](https://arxiv.org/abs/2502.08966)
- [ATMAN: Understanding Transformer Predictions Through Memory Efficient Attention Manipulation](https://openreview.net/forum?id=PBpEb86bj7)
- [LASER: Tuning-Free LLM-Driven Attention Control for Efficient Text-to-Image Generation](https://arxiv.org/abs/2404.13558)
- [Attention-Guided Backdoor Attacks against Transformers](https://openreview.net/forum?id=pNZkow3k3BH)
- [Backdoor Attack Against Vision Transformers via Attention Gradient](https://arxiv.org/abs/2410.22678)
- [Attention-Imperceptible Backdoor Attacks on Vision Transformers](https://ojs.aaai.org/index.php/AAAI/article/view/32889)
- [Detection of RNN Backdoor with Self-Attention](https://github.com/Flynn-ML2019/Detection-of-RNN-backdoor-with-self-attention)
- [Purifying Backdoors in Deep Learning Models Using Self-Attention Transfer](https://ieeexplore.ieee.org/document/10179375)
- [Attention-Based Backdoor Attacks against Natural Language Processing Models](https://www.sciencedirect.com/science/article/pii/S1568494625002182)
- [Backdoor Attacks in Token Selection of Attention Mechanism](https://icml.cc/media/icml-2025/Slides/44868.pdf)
- [Unveiling Vulnerability of Self-Attention](https://arxiv.org/html/2402.16470v1)
- [Aliasing Black Box Adversarial Attack with Joint Self-Attention](https://www.sciencedirect.com/science/article/pii/S0957417422021285)
- [Adversarial Attacks on Large Language Models with Attention Visualization](https://onlinelibrary.wiley.com/doi/10.1155/2023/8691095)
