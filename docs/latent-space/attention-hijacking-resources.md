---
title: "Attention Hijacking Resources"
category: "Latent Space"
source_url: ""
date_collected: 2025-06-19
license: "CC-BY-4.0"
---

The references below focus on manipulating a transformer's **attention** mechanisms to subvert or hijack LLM behavior.

- [Universal Jailbreak Suffixes Are Strong Attention Hijackers](https://arxiv.org/abs/2506.12880)
- [Attention Hijackers: Detect and Disentangle Attention Hijacking in LVLMs](https://arxiv.org/abs/2503.08216)
- [ShadowCoT: Cognitive Hijacking for Stealthy Reasoning Backdoors in LLMs](https://arxiv.org/abs/2504.05605)
- [Pseudo-Conversation Injection for LLM Goal Hijacking](https://arxiv.org/abs/2410.23678)
- [Attention Hijacking in Trojan Transformers](https://arxiv.org/abs/2208.04946)
- [Shadows in the Attention: Contextual Perturbation and Representation Drift in the Dynamics of Hallucination in LLMs](https://arxiv.org/abs/2505.16894)
- [LoRA Users Beware: A Few Spurious Tokens Can Manipulate Your Finetuned Model](https://arxiv.org/abs/2506.11402)
- [Attacking Attention of Foundation Models Disrupts Downstream Tasks](https://arxiv.org/abs/2506.05394)
- [Not All Tokens and Heads Are Equally Important: Dual-Level Attention Intervention for Hallucination Mitigation](https://arxiv.org/abs/2506.12609)
- [Instruction Following by Boosting Attention of Large Language Models](https://arxiv.org/abs/2506.13734)
- [Dynamic Attention Analysis for Backdoor Detection in Text-to-Image Diffusion Models](https://arxiv.org/abs/2504.20518)
- [Jatmo: Prompt Injection Defense by Task-Specific Finetuning](https://arxiv.org/abs/2312.17673)
- [A Study of the Attention Abnormality in Trojaned BERTs](https://arxiv.org/abs/2205.08305)
- [Feature Space Hijacking Attacks against Differentially Private Split Learning](https://arxiv.org/abs/2201.04018)
- [RTBAS: Defending LLM Agents Against Prompt Injection and Privacy Leakage](https://arxiv.org/abs/2502.08966)
