---
title: "Latent Space Attack Resources 2027"
category: "Latent Space"
source_url: ""
date_collected: 2025-06-18
license: "CC-BY-4.0"
---

The resources below highlight more recent papers and articles focused on manipulating or defending a model's hidden representations. They supplement the 2026 list.

- [DarkMind: Latent Chain-of-Thought Backdoor in Customized LLMs](https://arxiv.org/abs/2501.18617)
- [ALGEN: Few-shot Inversion Attacks on Textual Embeddings using Alignment and Generation](https://arxiv.org/abs/2502.11308)
- [Safeguarding LLM Embeddings in End-Cloud Collaboration via Entropy-Driven Perturbation](https://arxiv.org/abs/2503.12896)
- [JailBound: Jailbreaking Internal Safety Boundaries of Vision-Language Models](https://arxiv.org/abs/2505.19610)
- [Holes in Latent Space: Topological Signatures Under Adversarial Influence](https://arxiv.org/abs/2505.20435)
- [Alphabet Index Mapping: Jailbreaking LLMs through Semantic Dissimilarity](https://arxiv.org/abs/2506.12685)
