---
title: "Latent Space Attack Resources"
category: "Latent Space"
source_url: ""
date_collected: 2025-06-18
license: "CC-BY-4.0"
---

The references below explore how adversaries manipulate a model's hidden representations. These resources range from obfuscating activations to recovering text from embeddings.

- [Latent Space: The New Attack Vector into Organizations](https://www.apexhq.ai/blog/blog/latent-space-the-new-attack-vector-into-organizations/) – overview of hidden-space threats for RAG and other systems.
- [Obfuscated Activations Bypass LLM Latent-Space Defenses](https://arxiv.org/abs/2412.09565) – demonstrates how modified activations evade latent-space monitors.
- [Latent Adversarial Training Improves Robustness to Persistent Harmful Behaviors in LLMs](https://paperswithcode.com/paper/targeted-latent-adversarial-training-improves) – describes targeted adversarial training on latent vectors.
- [Literature Review: Obfuscated Activations Bypass LLM Latent-Space Defenses](https://www.themoonlight.io/en/review/obfuscated-activations-bypass-llm-latent-space-defenses) – concise summary and analysis of the above paper.
- [Text Embedding Inversion Security for Multilingual Language Models](https://arxiv.org/abs/2401.12192) – recovers prompts from multilingual embeddings.
- [LAGO: Few-shot Crosslingual Embedding Inversion Attacks](https://arxiv.org/abs/2505.16008) – cross-lingual embedding inversion using graph optimization.
- [Extracting Training Data from Large Language Models](https://arxiv.org/abs/2012.07805) – early work on leaking training text via latent representations.
