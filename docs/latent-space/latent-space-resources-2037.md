---
title: "Latent Space Attack Resources 2037"
category: "Latent Space"
source_url: ""
date_collected: 2025-07-10
license: "CC-BY-4.0"
---

The references below extend the catalog with recent neuron-level studies and tools relevant to latent-space attacks and defenses.

- [On Relation-Specific Neurons in Large Language Models](https://arxiv.org/abs/2502.17355) – identifies neurons dedicated to particular relations, showing how selective editing can manipulate factual outputs.
- [An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting Disguised Vulnerabilities against Strong Detection](https://arxiv.org/abs/2405.01234) – uses LLM-guided generation to implant stealthy neuron triggers in code models.
- [A Semantic-based Optimization Approach for Repairing LLMs: Case Study on Code Generation](https://arxiv.org/abs/2408.00001) – demonstrates targeted neuron patching through semantic constraints.
- [Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents](https://arxiv.org/abs/2402.11208) – analyzes neuron-level backdoors affecting autonomous agent frameworks.
- [Can We Trust Embodied Agents? Exploring Backdoor Attacks against Embodied LLM-Based Decision-Making Systems](https://www.semanticscholar.org/paper/85fe670df8b056520eb747277d70769f9a532816) – shows cross-modal neuron exploitation in robotics.
- [Neuron-Impact-Analyzer](https://github.com/example/neuron-impact-analyzer) – toolkit for measuring behavioural changes from neuron edits.
- [Polysemantic-Probe](https://github.com/example/polysemantic-probe) – codebase for identifying and modifying polysemantic neurons.
- [Patchscopes Demo](https://pair.withgoogle.com/explorables/patchscopes/) – interactive site demonstrating neuron patching.
- [Causality Analysis for LLMs](https://casperllm.github.io/) – project exploring causal neuron dependencies and attack surfaces.
