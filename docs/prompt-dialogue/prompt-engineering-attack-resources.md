---
title: "Prompt Engineering Attack Resources"
category: "Prompt Dialogue"
source_url: ""
date_collected: 2025-06-18
license: "CC-BY-4.0"
---

The following articles and blog posts provide deeper insight into prompt engineering attacks and practical mitigation measures.

- [OWASP LLM Prompt Injection Prevention Cheat Sheet](https://cheatsheetseries.owasp.org/cheatsheets/LLM_Prompt_Injection_Prevention_Cheat_Sheet.html)
- [Prompt Injection FAQ](https://medium.com/@exlearnai/prompt-injection-faq-4f94d49afcd1)
- [Prompt Injection and Adversarial Attacks](https://unit42.paloaltonetworks.com/prompt-injection-adversarial-attacks/)
- [Securing AI Systems from Prompt Injection](https://hackerone.com/blog/prompt-injection-attacks-on-ai)
- [Google Security Blog: Prompt Injection](https://security.googleblog.com/2023/06/prompt-injection.html)
- [Microsoft Security Blog: Prompt Injection Vulnerabilities in AI](https://www.microsoft.com/en-us/security/blog/2024/05/10/prompt-injection-vulnerabilities-ai/)
- [Snyk Blog: Prompt Injection in AI Security](https://snyk.io/blog/prompt-injection-ai-security/)
- [Cloudflare Learning: Prompt Injection](https://www.cloudflare.com/learning/security/prompt-injection/)
- [Adversarial Prompting in LLMs | Prompt Engineering Guide](https://www.promptingguide.ai/risks/adversarial)
- [IBM: What Is a Prompt Injection Attack?](https://www.ibm.com/think/topics/prompt-injection)
- [Common Prompt Injection Attacks - AWS Prescriptive Guidance](https://docs.aws.amazon.com/prescriptive-guidance/latest/llm-prompt-engineering-best-practices/common-attacks.html)
- [Prompt Injection Attacks (NCC Group)](https://www.nccgroup.com/uk/about-us/newsroom-and-events/blogs/2024/april/prompt-injection-attacks/)
- [CWE-1389: AI Prompt Injection Bypass](https://cwe.mitre.org/data/definitions/1389.html)
- [New Prompt Injection Technique Hijacks ChatGPT Conversations](https://www.bleepingcomputer.com/news/security/new-prompt-injection-technique-lets-attackers-hijack-chatgpt-conversations/)
- [Prompt Injection Attacks on ChatGPT (Praetorian)](https://www.praetorian.com/blog/prompt-injection-attacks-on-chatgpt/)
- [Prompt Injection Exposed (The Hacker News)](https://thehackernews.com/2023/12/prompt-injection-exposed.html)
- [What is Prompt Injection? (Check Point)](https://blog.checkpoint.com/2024/05/14/what-is-prompt-injection/)
- [Introducing a Taxonomy of Adversarial Prompt Engineering](https://hiddenlayer.com/innovation-hub/introducing-a-taxonomy-of-adversarial-prompt-engineering/)
- [Jailbreaking Generative AI](https://unit42.paloaltonetworks.com/jailbreaking-generative-ai/)

- [Doppelg√§nger Method: Breaking Role Consistency in LLM Agent via Prompt-based Transferable Adversarial Attack](https://arxiv.org/abs/2506.14539)
- [Monitoring Decomposition Attacks in LLMs with Lightweight Sequential Monitors](https://arxiv.org/abs/2506.10949)
- [Is Your Prompt Safe? Investigating Prompt Injection Attacks Against Open-Source LLMs](https://arxiv.org/abs/2505.14368)
- [System Prompt Extraction Attacks and Defenses in Large Language Models](https://arxiv.org/abs/2505.23817)
- [Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs](https://arxiv.org/abs/2406.09324)
- [A Survey of Attacks on Large Language Models](https://arxiv.org/abs/2505.12567)
- [Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks](https://arxiv.org/abs/2402.09177)
- [Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment](https://arxiv.org/abs/2402.14016)
- [CheatAgent: Attacking LLM-Empowered Recommender Systems via LLM Agent](https://arxiv.org/abs/2504.13192)
- [Jigsaw Puzzles: Splitting Harmful Questions to Jailbreak Large Language Models](https://arxiv.org/abs/2410.11459)
- [WordGame: Efficient & Effective LLM Jailbreak via Simultaneous Obfuscation in Query and Response](https://arxiv.org/abs/2405.14023)
- [Jailbreaking to Jailbreak](https://arxiv.org/abs/2502.09638)
- [Continuous Embedding Attacks via Clipped Inputs in Jailbreaking Large Language Models](https://arxiv.org/abs/2407.13796)
- [Against All Odds: Overcoming Typology, Script, and Language Confusion in Multilingual Embedding Inversion Attacks](https://arxiv.org/abs/2408.11749)
- [Playing Language Game with LLMs Leads to Jailbreaking](https://arxiv.org/abs/2411.12762)
- [Illusions of Relevance: Using Content Injection Attacks to Deceive Retrievers, Rerankers, and LLM Judges](https://arxiv.org/abs/2501.18536)

These sources present a mix of defensive guidelines, case studies, and historical context, helping defenders stay current with rapidly evolving prompt engineering threats.
