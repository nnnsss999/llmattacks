---
title: "Prompt Engineering Attack Resources 2027"
category: "Prompt Dialogue"
source_url: ""
date_collected: 2025-06-18
license: "CC-BY-4.0"
---

The references below extend the catalog with recent research and articles on prompt engineering attacks and mitigations.

- [How Hackers Manipulate Agentic AI with Prompt Engineering](https://www.securityweek.com/how-hackers-manipulate-agentic-ai-with-prompt-engineering/)
- [AI Security in Focus: Detecting and Preventing Prompt Engineering Threats](https://www.deimos.io/blog-posts/ai-security-in-focus-detecting-and-preventing-prompt-engineering-threats)
- [The Rise of a New Threat: Prompt Hacking](https://promptengineering.org/the-rise-of-a-new-threat-prompt-hacking/)
- [Prompt Hacking Resources (PromptLabs)](https://github.com/PromptLabs/Prompt-Hacking-Resources)
- [Prompt Hacking Handbook](https://handbook.exemplar.dev/ai_engineer/prompt_engineering/prompt_hacking)
- [Prompt Hacking and Misuse of LLM](https://www.unite.ai/prompt-hacking-and-misuse-of-llm/)
- [How to Protect Against Prompt Hacking](https://www.prompthub.us/blog/how-to-protect-against-prompt-hacking)
- [Prompt Hacking Introduction](https://learnprompting.org/docs/prompt_hacking/introduction)
- [Doppelg√§nger Method: Breaking Role Consistency in LLM Agent via Prompt-based Transferable Adversarial Attack](https://arxiv.org/abs/2506.14539)
- [Monitoring Decomposition Attacks in LLMs with Lightweight Sequential Monitors](https://arxiv.org/abs/2506.10949)
date_collected: 2025-06-19
license: "CC-BY-4.0"
---

The resources below expand on prompt engineering-based attacks and defenses. They supplement [prompt-engineering-resources-2026.md](prompt-engineering-resources-2026.md).

- [Is Your Prompt Safe? Investigating Prompt Injection Attacks Against Open-Source LLMs](https://arxiv.org/abs/2505.14368)
- [Goal-Oriented Prompt Attack and Safety Evaluation for LLMs](https://arxiv.org/abs/2309.11830)
- [System Prompt Extraction Attacks and Defenses in Large Language Models](https://arxiv.org/abs/2505.23817)
- [SequentialBreak: Large Language Models Can be Fooled by Embedding Jailbreak Prompts into Sequential Prompt Chains](https://arxiv.org/abs/2411.06426)
- [Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs](https://arxiv.org/abs/2406.09324)
- [A Survey of Attacks on Large Language Models](https://arxiv.org/abs/2505.12567)
- [Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks](https://arxiv.org/abs/2402.09177)
- [Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment](https://arxiv.org/abs/2402.14016)
- [CheatAgent: Attacking LLM-Empowered Recommender Systems via LLM Agent](https://arxiv.org/abs/2504.13192)
- [Jigsaw Puzzles: Splitting Harmful Questions to Jailbreak Large Language Models](https://arxiv.org/abs/2410.11459)
- [WordGame: Efficient & Effective LLM Jailbreak via Simultaneous Obfuscation in Query and Response](https://arxiv.org/abs/2405.14023)
- [Jailbreaking to Jailbreak](https://arxiv.org/abs/2502.09638)
- [Continuous Embedding Attacks via Clipped Inputs in Jailbreaking Large Language Models](https://arxiv.org/abs/2407.13796)
- [Against All Odds: Overcoming Typology, Script, and Language Confusion in Multilingual Embedding Inversion Attacks](https://arxiv.org/abs/2408.11749)
- [Playing Language Game with LLMs Leads to Jailbreaking](https://arxiv.org/abs/2411.12762)
- [Illusions of Relevance: Using Content Injection Attacks to Deceive Retrievers, Rerankers, and LLM Judges](https://arxiv.org/abs/2501.18536)
