---
title: "Prompt Engineering Attack Resources 2028"
category: "Prompt Dialogue"
source_url: ""
date_collected: 2025-06-19
license: "CC-BY-4.0"
---

The following references extend the prompt engineering attack catalog with additional research papers, blog posts, and defensive guides.

- [Palisade -- Prompt Injection Detection Framework](https://arxiv.org/abs/2410.21146)
- [Malicious Prompt Detection GitHub Repository](https://github.com/AhsanAyub/malicious-prompt-detection)
- [Palisade -- Prompt Injection Detection Framework | Papers With Code](https://paperswithcode.com/paper/palisade-prompt-injection-detection-framework)
- [Domain-specific Prompt Injection Detection | WithSecure Labs](https://labs.withsecure.com/publications/detecting-prompt-injection-bert-based-classifier)
- [When Prompts Go Rogue: Analyzing a Prompt Injection Code Execution in Vanna.AI | JFrog](https://jfrog.com/blog/prompt-injection-attack-code-execution-in-vanna-ai-cve-2024-5565/)
- [Automatic and Universal Prompt Injection Attacks against Large Language Models](https://arxiv.org/pdf/2403.04957)
- [Prompt Infection: LLM-to-LLM Prompt Injection within Multi-Agent Systems](https://arxiv.org/abs/2410.07283)
- [Backdooring Instruction-Tuned Large Language Models with Virtual Prompt Injection](https://aclanthology.org/2024.naacl-long.337.pdf)
- [Formalizing and Benchmarking Prompt Injection Attacks and Defenses](https://arxiv.org/abs/2310.12815)
- [Fine-tuned Large Language Models: Improved Prompt Injection Attacks Detection](https://arxiv.org/abs/2410.21337)
- [How Prompt Attacks Exploit GenAI and How to Fight Back | Palo Alto Networks](https://unit42.paloaltonetworks.com/new-frontier-of-genai-threats-a-comprehensive-guide-to-prompt-attacks/)
- [AI Prompt Engineering Threats | Deimos Blog](https://www.deimos.io/blog-posts/ai-security-in-focus-detecting-and-preventing-prompt-engineering-threats)
- [Protect Your Prompts: Injection Threats Are Coming for Your AI Tools | Tanium](https://www.tanium.com/blog/protect-your-prompts-injection-threats-are-coming-for-your-ai-tools/)
- [Prompt Hacking: The New Cyber Threat](https://promptengineering.org/the-rise-of-a-new-threat-prompt-hacking/)
- [Generative AIâ€™s Biggest Security Flaw Is Not Easy to Fix | WIRED](https://www.wired.com/story/generative-ai-prompt-injection-hacking/)
- [Understanding prompt injection: A growing concern in AI and LLM | Ory](https://www.ory.sh/blog/understanding-prompt-injection-a-growing-concern-in-ai-and-llm)

The resources below expand on prompt engineering-based attacks and mitigation approaches. They complement [prompt-engineering-resources-2027.md](prompt-engineering-resources-2027.md).

- [COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability](http://arxiv.org/abs/2402.08679)
- [Don't Say No: Jailbreaking LLM by Suppressing Refusal](https://arxiv.org/abs/2404.16369)
- [Mission Impossible: A Statistical Perspective on Jailbreaking LLMs](http://arxiv.org/abs/2408.01420)
- [Hacc-Man: An Arcade Game for Jailbreaking LLMs](https://doi.org/10.1145/3656156.3665432)
- [Bad Likert Judge Jailbreak Technique](https://thehackernews.com/2025/01/new-ai-jailbreak-method-bad-likert.html)
- [AutoDAN: Interpretable Gradient-Based Adversarial Attacks on Large Language Models](https://arxiv.org/abs/2310.15140)
- [AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned LLMs](https://arxiv.org/abs/2310.04451)
- [Effective Prompt Extraction from Language Models](https://arxiv.org/abs/2307.06865)
- [ASETF: A Novel Method for Jailbreak Attack on LLMs through Translate Suffix Embeddings](https://arxiv.org/abs/2402.16006)
- [Prompt Injection Attack Research](https://arxiv.org/abs/2410.13236)
- [SPIN: Self-Supervised Prompt Injection](https://www.researchgate.net/publication/385010187_SPIN_Self-Supervised_Prompt_Injection)
- [Fine-Tuning LLMs to Resist Indirect Prompt Injection](https://labs.reversec.com/posts/2024/10/fine-tuning-llms-to-resist-indirect-prompt-injection-attacks)
- [Lakera PINT Benchmark](https://github.com/lakeraai/pint-benchmark)
- [Prompt Injection Datasets on HuggingFace](https://huggingface.co/datasets/deepset/prompt-injections)
- [Prompt Injection Cheat Sheet Documentation](https://prompt-injections.readthedocs.io/en/latest/dataset.html)
