Title: Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?

URL Source: https://arxiv.org/html/2404.03411v2

Markdown Content:
Shuo Chen 1,3 Zhen Han 1 Bailan He 1,3 Zifeng Ding 3 Wenqian Yu 5 Philip Torr 2

Volker Tresp 1,4 Jindong Gu 2

1 LMU Munich 2 University of Oxford 3 Siemens AG 

4 Munich Center for Machine Learning (MCML) 5 Wuhan University 

shuo.chen@campus.lmu.de, jindong.gu@eng.ox.ac.uk, hanzhen02111@163.com

###### Abstract

Various jailbreak attacks have been proposed to red-team Large Language Models (LLMs) and revealed the vulnerable safeguards of LLMs. Besides, some methods are not limited to the textual modality and extend the jailbreak attack to Multimodal Large Language Models (MLLMs) by perturbing the visual input. However, the absence of a universal evaluation benchmark complicates the performance reproduction and fair comparison. Besides, there is a lack of comprehensive evaluation of closed-source state-of-the-art (SOTA) models, especially MLLMs, such as GPT-4V. To address these issues, this work first builds a comprehensive jailbreak evaluation dataset with 1445 harmful questions covering 11 different safety policies. Based on this dataset, extensive red-teaming experiments are conducted on 11 different LLMs and MLLMs, including both SOTA proprietary models and open-source models. We then conduct a deep analysis of the evaluated results and find that (1) GPT4 and GPT-4V demonstrate better robustness against jailbreak attacks compared to open-source LLMs and MLLMs. (2) Llama2 and Qwen-VL-Chat are more robust compared to other open-source models. (3) The transferability of visual jailbreak methods is relatively limited compared to textual jailbreak methods. The dataset and code can be found here 1 1 1[https://github.com/chenxshuo/RedTeamingGPT4V](https://github.com/chenxshuo/RedTeamingGPT4V).

1 Introduction
--------------

Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) have shown superior performance in text generation. To avoid generating unobjectionable content learned from the web-scale training corpus, stringent safety regulations have been applied during the safety alignment(Ouyang et al., [2022](https://arxiv.org/html/2404.03411v2#bib.bib20); Touvron et al., [2023](https://arxiv.org/html/2404.03411v2#bib.bib25)). However, many jailbreak attacks have been proven to be able to bypass these safeguards and successfully elicit harmful generations. For example, [Zou et al.](https://arxiv.org/html/2404.03411v2#bib.bib35) appends a trainable suffix to harmful behavior prompts, which makes the model generate targeted output rather than refusing. Apart from perturbing the textual input, there are also jailbreaking methods modifying the visual input such as trainable image noise Carlini et al. ([2023](https://arxiv.org/html/2404.03411v2#bib.bib5)); Qi et al. ([2023](https://arxiv.org/html/2404.03411v2#bib.bib21)) to ignore the safety regulation and elicit unethical output.

However, the lack of a universal evaluation benchmark and performance metrics makes the performance reproduction and a fair comparison hard to achieve. Besides, comprehensive evaluations of SOTA proprietary models against jailbreak attacks are still missing, especially MLLMs such as GPT-4V. It is hence still unknown how robust these proprietary models are against existing jailbreak attack methods. To ensure a reproducible and universal evaluation, in this work, we first constructed a comprehensive jailbreak evaluation dataset with 1445 jailbreak questions covering 11 different safety policies. Then 32 jailbreak methods targeted at LLMs and MLLMs are collected in this study, which contains 29 textual jailbreak methods and 3 visual jailbreak methods. Based on this benchmark, we then deployed extensive red-teaming experiments on 11 different LLMs and MLLMs including both SOTA proprietary models such as GPT-4, and open-source models such as Llama2 and MiniGPT4. We find that GPT-4 and GPT-4V show much better robustness against both textual and visual jailbreak methods compared to open-source models. Besides, among open-source models, Llama2 and Qwen-VL-Chat demonstrate better robustness and Llama2 can even be more robust than GPT-4. Moreover, we compare the transferability of different methods. We find that AutoDAN has better transferability compared to GCG and visual jailbreak methods have relatively limited transferability. The contribution of our work can be summarized as follows:

*   •We provide a jailbreak evaluation benchmark with 1445 harmful behavior questions covering 11 different safety policies for both LLMs and MLLMs. 
*   •We conduct red-teaming on both GPT-4 and GPT-4V and various SOTA open-source models with our evaluation benchmarks. 
*   •We provide an in-depth analysis showing the robustness of both business proprietary and open-source multimodal large language models against existing jailbreak methods. 

2 Red Teaming GPT4 Against Jailbreak Attacks
--------------------------------------------

### 2.1 Experimental Setup

Models. The experiments are conducted on both proprietary business multimodal LLMs and open-source multimodal LLMs. Specifically, gpt-4-vision-preview (referred to as GPT-4 below) is used to conduct jailbreak red-teaming based on visual input perturbations; gpt-4-1106-preview(referred to as GPT-4V) is used in jailbreak attacks based on textual input perturbations. Besides, four open-source LLMs and six open-source VLMs have been chosen as our red-teaming target. In total, there are 11 models used in our study, and detailed information is presented in Tab.[3](https://arxiv.org/html/2404.03411v2#A2.T3 "Table 3 ‣ Appendix B LLMs and MLLMs used in this study ‣ Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?") in Appendix.

Dataset. To build a comprehensive jailbreak benchmark, we have collected jailbreak behaviors and questions from existing literature, such as AdvBench(Zou et al., [2023](https://arxiv.org/html/2404.03411v2#bib.bib35)), SafeBench(Gong et al., [2023](https://arxiv.org/html/2404.03411v2#bib.bib11)), Qi et al. ([2023](https://arxiv.org/html/2404.03411v2#bib.bib21)), GPT-4 technical report(Achiam et al., [2023](https://arxiv.org/html/2404.03411v2#bib.bib1)), and ToxicChat(Lin et al., [2023](https://arxiv.org/html/2404.03411v2#bib.bib15)). In total, 1445 different harmful behaviors and questions have been collected. The dataset covers 11 different usage policies followed by Meta’s Purple LLaMA(Inan et al., [2023](https://arxiv.org/html/2404.03411v2#bib.bib12)) and OpenAI’s GPT4(Achiam et al., [2023](https://arxiv.org/html/2404.03411v2#bib.bib1)), such as Violence and Hate, Illegal Weapons, \etc. More detailed information is in Appendix[C](https://arxiv.org/html/2404.03411v2#A3 "Appendix C Dataset Construction ‣ Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?").

Threat Model. The primary focus of this study is to investigate the transferability of existing jailbreak methods. Open-source models act as surrogate models and are used to train the input modifications for jailbreak, \eg, suffix in GCG(Zou et al., [2023](https://arxiv.org/html/2404.03411v2#bib.bib35)) and image noise in Qi et al. ([2023](https://arxiv.org/html/2404.03411v2#bib.bib21)). These modifications then are used to red-team closed-source models such as GPT-4, and other open-source models. Specifically, Guanaco-7B, Llama2-7B, and Vicuna-7B are used as surrogate models for textual jailbreak attacks. MiniGPT4-7B is used as the surrogate model for visual jailbreak attacks.

Evaluation Metrics. Various evaluation metrics have been proposed to calculate the attack success rate (ASR) in existing literature. They can be classified into four main categories: refusal word detection(Zou et al., [2023](https://arxiv.org/html/2404.03411v2#bib.bib35); Liu et al., [2023b](https://arxiv.org/html/2404.03411v2#bib.bib17)), evaluation based on toxicity detection APIs(Qi et al., [2023](https://arxiv.org/html/2404.03411v2#bib.bib21); Shen et al., [2023](https://arxiv.org/html/2404.03411v2#bib.bib24)), LLMs as judges(Liu et al., [2023b](https://arxiv.org/html/2404.03411v2#bib.bib17); Gong et al., [2023](https://arxiv.org/html/2404.03411v2#bib.bib11); Qi et al., [2023](https://arxiv.org/html/2404.03411v2#bib.bib21)), and human evaluation(Shayegani et al., [2023](https://arxiv.org/html/2404.03411v2#bib.bib23)). In this work, two matrices are mainly used to calculate the ASR, \ie, refusal word detection, and LLMs as judges. We follow the setting in Zou et al. ([2023](https://arxiv.org/html/2404.03411v2#bib.bib35)); Liu et al. ([2023b](https://arxiv.org/html/2404.03411v2#bib.bib17)) and count an attack as successful if a set of pre-defined refusal words is not found in the generated content. The pre-defined refusal words can be found in Appendix[D](https://arxiv.org/html/2404.03411v2#A4 "Appendix D Evaluation Metrics ‣ Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?"). Besides, LLaMA-Guard Inan et al. ([2023](https://arxiv.org/html/2404.03411v2#bib.bib12)) is used as the jailbreak judge to check whether the generated content is indeed harmful or not. As LLaMA-Guard is open-source and instruction-tuned on a large harmful corpus, it is a more frugal way compared to using GPT-4 as judges(Liu et al., [2023b](https://arxiv.org/html/2404.03411v2#bib.bib17); Gong et al., [2023](https://arxiv.org/html/2404.03411v2#bib.bib11); Qi et al., [2023](https://arxiv.org/html/2404.03411v2#bib.bib21)). The detailed instructions to use LLaMA-Guard are in Appendix[D](https://arxiv.org/html/2404.03411v2#A4 "Appendix D Evaluation Metrics ‣ Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?"). We report the Llama-Guard metric in the main paper and present the full metrics in the Appendix[E](https://arxiv.org/html/2404.03411v2#A5 "Appendix E Additional Experimental Results ‣ Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?").

### 2.2 Red Teaming against Textual Jailbreak

Table 1: The jailbreak success rate of GCG and AutoDAN evaluated by Llama-Guard. The lowest success rate is in bold. 

Hand-crafted Jailbreak Attacks use pre-defined jailbreak templates or process functions and insert harmful questions into the templates, then send the whole instruction to LLMs. These hand-crafted attacks can be further classified into template-based and function-based. Template-based methods normally design instruction templates to describe a specific scenario to mislead the LLMs and elicit harmful content, such as role-playing Wei et al. ([2024](https://arxiv.org/html/2404.03411v2#bib.bib29)) and do-anything-now Wei et al. ([2024](https://arxiv.org/html/2404.03411v2#bib.bib29)). Function-based methods need extra pre- or post-process on the input of harmful questions and generated content, such as using base64 encoding and vowel removal. This study systematically investigates 27 different hand-crafted jailbreak attack methods including 17 templated-based (\eg, refusal suppression and evil confidant) and 10 function-based methods (\eg, encoding the harmful questions using base64 and removing vowels from the questions). Detailed information about all these methods is provided in Appendix[E](https://arxiv.org/html/2404.03411v2#A5 "Appendix E Additional Experimental Results ‣ Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?") and the full results are presented in Tab.[8](https://arxiv.org/html/2404.03411v2#A5.T8 "Table 8 ‣ Appendix E Additional Experimental Results ‣ Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?").

Automatic Jailbreak Attacks optimize a string as part of the jailbreak input to elicit harmful content. This study mainly adopts two popular automatic jailbreak attack methods, \ie, GCG(Zou et al., [2023](https://arxiv.org/html/2404.03411v2#bib.bib35)) and AutoDAN(Liu et al., [2023b](https://arxiv.org/html/2404.03411v2#bib.bib17)). Given a surrogate model with full access, GCG trains an extra suffix following the harmful questions to maximize the probability of generating specific non-refusal responses. AutoDAN starts from an instruction template. Then it updates the tokens in the template using genetic algorithms to find better instructions maximizing the probability of generating specific non-refusal responses. In our work, Guanaco-7B, Llama2-7B, and Vicuna-7B are used as surrogate models for GCG and AutoDAN. Besides, we also follow the combination strategy from GCG and train one suffix based on the combination of Guanaco-7B and Vicuna-7B. The performance of these two methods is presented in Tab.[1](https://arxiv.org/html/2404.03411v2#S2.T1 "Table 1 ‣ 2.2 Red Teaming against Textual Jailbreak ‣ 2 Red Teaming GPT4 Against Jailbreak Attacks ‣ Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?")

### 2.3 Red Teaming against Visual Jailbreak

Various methods have been proposed to jailbreak multimodal LLMs via the visual modality, \ie, perturbing the visual input by either manual functions or automatic optimization. This work adopts 3 different jailbreak methods in total, including one black-box typography method FigStep(Gong et al., [2023](https://arxiv.org/html/2404.03411v2#bib.bib11)) and two optimization-based methods, \ie VisualAdv(Qi et al., [2023](https://arxiv.org/html/2404.03411v2#bib.bib21)), and ImageHijacks(Bailey et al., [2023](https://arxiv.org/html/2404.03411v2#bib.bib4)). VisualAdv optimizes an adversarial example on a few-shot harmful corpus to maximize the probability of generating harmful content. ImageHijacks optimizes the adversarial example to maximize the generation probability of affirmative response to harmful requests. We use MiniGPT-4 as surrogate models for VisualAdv and ImageHijacks. The jailbreak performance of these three methods is shown in Tab.[2](https://arxiv.org/html/2404.03411v2#S2.T2 "Table 2 ‣ 2.3 Red Teaming against Visual Jailbreak ‣ 2 Red Teaming GPT4 Against Jailbreak Attacks ‣ Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?")

Table 2: The jailbreak success rate of visual jailbreak methods evaluated by Llama-Guard.

3 Discussion
------------

Which model is more robust against jailbreak? In our experiments, GPT4 is more robust against textual jailbreak methods in most cases. One noticeable exception happens under the GCG attack. Llama2-7B demonstrates better robustness against GCG attack and less than 1% of the responses are classified as harmful as shown in the second row in Tab.[1](https://arxiv.org/html/2404.03411v2#S2.T1 "Table 1 ‣ 2.2 Red Teaming against Textual Jailbreak ‣ 2 Red Teaming GPT4 Against Jailbreak Attacks ‣ Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?"). However, the AutoDAN attack can elicit more than 10% harmful responses on Llama2-7B whereas GPT4 defends almost all attempts successfully. Among open-source LLMs used in this work, Llama2-7B is the most robust model whereas Vicuna-7B is the most vulnerable one. This can be because that Vicuna does not implement any specific safeguard fine-tuning and the dataset used for fine-tuning has not been rigorously filtered(Chiang et al., [2023](https://arxiv.org/html/2404.03411v2#bib.bib8)). Llama2-7B, on the other hand, deploys safety alignment fine-tuning and a series of red teaming to ensure safe response(Touvron et al., [2023](https://arxiv.org/html/2404.03411v2#bib.bib25)). As for visual jailbreak in our experiments, it is much harder to successfully jailbreak GPT-4V compared to other open-source MLLMs. Among open-source MLLMs, Qwen-VL-Chat is the most robust against jailbreak attacks whereas MiniGPT4-7B is the most vulnerable. This can be also attributed to the different LLMs upon which these two MLLMs are built. MiniGPT4-7B used in this study is based on Vicuna-7B which is not safely fine-tuned. Qwen-VL-Chat is built on Qwen-Chat that is finetuned on a curated dataset relevant to safety Bai et al. ([2023](https://arxiv.org/html/2404.03411v2#bib.bib3)).

Which attack method is most powerful? There is no single method for achieving the highest attack success rate across different target models. AutoDAN demonstrates higher success rates on open-source LLMs compared to GCG, especially on Llama2-7B. However, GPT-4 successfully refuses almost all AutoDAN’s requests. This may be because the jailbreak prompts used by AutoDAN have been filtered by OpenAI’s safeguard and the token replacement from AutoDAN is not enough to bypass the safety guard. Among visual jailbreak methods, FigStep achieves a higher success rate across MLLMs compared to the transfer attack by VisualAdv and ImageHijacks.

How good is the current defense of the open-source model and closed-source model? In our experiments, there is a significant gap between open-source models and GPT-4 in most testing scenarios. For example, AutoDAN can obtain 57.06%percent 57.06 57.06\%57.06 % success rate on Vicuna-7B and 46.90%percent 46.90 46.90\%46.90 % on Guanaco-7B, whereas GPT-4 defends almost all its requests. The same gap goes for visual jailbreaks. FigStep can achieve a success rate of 35.99%percent 35.99 35.99\%35.99 % on MiniGPT4-7B and 34.90%percent 34.90 34.90\%34.90 % on Fuyu. But on GPT-4V, the success rate is approximately 0 0. However, this does not indicate that GPT-4 and GPT-4V have a perfect defense against jailbreak attacks. For example, the GCG trained on the combination of Guanaco-7B and Vicuna-7B can still achieve a success rate of 2.39%percent 2.39 2.39\%2.39 %.

Does GPT-4 suffer more from visual jailbreak, compared to text modality? In our experiments, visual jailbreak on GPT-4V does not demonstrate more vulnerability compared to textual jailbreak methods. This can be attributed to the input filtering as VisualAdv and ImageHijacks do not alter the original harmful questions. Besides, although FigStep uses typography and removes harmful context from textual questions, GPT-4V is still able to refuse the requests.

How good is the transferability of jailbreak methods? AutoDAN demonstrates better transferability compared to GCG on open-source LLMs. This can be because the suffix generated by GCG is not semantically meaningful and can be confusing when transferred to other models. AudoDAN, on the other hand, preserves the semantic meaning of the jailbreak prompt and hence shows better transferability on other models. The transferability of visual jailbreak methods studied in this work is relatively limited. The improvement of success rate is limited compared to the baseline and sometimes the success rates of transfer attacks are even lower. For example, when attacking Fuyu by VisualAdv and using MiniGPT4-7B as the surrogate model, the success rate (6.75%percent 6.75 6.75\%6.75 %) is lower than the baseline result (8.6%percent 8.6 8.6\%8.6 %). Besides, the transfer attack of visual jailbreak methods on GPT-4V is not effective. The main reason is that these methods do not alter the harmful questions. GPT-4V can directly detect the harmful content in the input and thus refuse to respond.

4 Conclusion
------------

This study focuses on red-teaming both proprietary and open-source LLMs and MLLMs. We first collected existing jailbreak datasets and constructed a comprehensive evaluation benchmark covering 11 different usage policies. Based on the evaluation benchmark, we conducted red-teaming experiments across 11 different LLMs and MLLMs. We find that GPT-4 and GPT-4V are much more robust compared to open-source models and the gap between them is significant. Compared to text modality, current visual jailbreak methods are hard to succeed on GPT-4V. Future work includes incorporating more jailbreak methods, and datasets.

References
----------

*   Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023. 
*   AdeptAI (2024) AdeptAI. Fuyu-8b model card, 2024. [https://huggingface.co/adept/fuyu-8b](https://huggingface.co/adept/fuyu-8b) [Accessed: (2024.2.10)]. 
*   Bai et al. (2023) Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. _arXiv preprint arXiv:2309.16609_, 2023. 
*   Bailey et al. (2023) Luke Bailey, Euan Ong, Stuart Russell, and Scott Emmons. Image hijacks: Adversarial images can control generative models at runtime. _arXiv preprint arXiv:2309.00236_, 2023. 
*   Carlini et al. (2023) Nicholas Carlini, Milad Nasr, Christopher A Choquette-Choo, Matthew Jagielski, Irena Gao, Pang Wei Koh, Daphne Ippolito, Florian Tramèr, and Ludwig Schmidt. Are aligned neural networks adversarially aligned? In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023. 
*   Chao et al. (2023) Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George J Pappas, and Eric Wong. Jailbreaking black box large language models in twenty queries. _arXiv preprint arXiv:2310.08419_, 2023. 
*   Cheung (2024) Josephus Cheung. Guanaco - generative universal assistant for natural-language adaptive context-aware omnilingual outputs, 2024. [https://huggingface.co/JosephusCheung/Guanaco](https://huggingface.co/JosephusCheung/Guanaco) [Accessed: (2024.2.10)]. 
*   Chiang et al. (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL [https://lmsys.org/blog/2023-03-30-vicuna/](https://lmsys.org/blog/2023-03-30-vicuna/). 
*   Deng et al. (2023) Gelei Deng, Yi Liu, Yuekang Li, Kailong Wang, Ying Zhang, Zefeng Li, Haoyu Wang, Tianwei Zhang, and Yang Liu. Jailbreaker: Automated jailbreak across multiple large language model chatbots. _arXiv preprint arXiv:2307.08715_, 2023. 
*   Du et al. (2022) Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm: General language model pretraining with autoregressive blank infilling. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 320–335, 2022. 
*   Gong et al. (2023) Yichen Gong, Delong Ran, Jinyuan Liu, Conglei Wang, Tianshuo Cong, Anyu Wang, Sisi Duan, and Xiaoyun Wang. Figstep: Jailbreaking large vision-language models via typographic visual prompts. _arXiv preprint arXiv:2311.05608_, 2023. 
*   Inan et al. (2023) Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer, Yuning Mao, Michael Tontchev, Qing Hu, Brian Fuller, Davide Testuggine, et al. Llama guard: Llm-based input-output safeguard for human-ai conversations. _arXiv preprint arXiv:2312.06674_, 2023. 
*   Jones et al. (2023) Erik Jones, Anca Dragan, Aditi Raghunathan, and Jacob Steinhardt. Automatically auditing large language models via discrete optimization. _arXiv preprint arXiv:2303.04381_, 2023. 
*   Lapid et al. (2023) Raz Lapid, Ron Langberg, and Moshe Sipper. Open sesame! universal black box jailbreaking of large language models. _arXiv preprint arXiv:2309.01446_, 2023. 
*   Lin et al. (2023) Zi Lin, Zihan Wang, Yongqi Tong, Yangkun Wang, Yuxin Guo, Yujia Wang, and Jingbo Shang. Toxicchat: Unveiling hidden challenges of toxicity detection in real-world user-AI conversation. In _The 2023 Conference on Empirical Methods in Natural Language Processing_, 2023. URL [https://openreview.net/forum?id=jTiJPDv82w](https://openreview.net/forum?id=jTiJPDv82w). 
*   Liu et al. (2023a) Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. _arXiv preprint arXiv:2310.03744_, 2023a. 
*   Liu et al. (2023b) Xiaogeng Liu, Nan Xu, Muhao Chen, and Chaowei Xiao. Autodan: Generating stealthy jailbreak prompts on aligned large language models. _arXiv preprint arXiv:2310.04451_, 2023b. 
*   Liu et al. (2023c) Yi Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng, Ying Zhang, Lida Zhao, Tianwei Zhang, and Yang Liu. Jailbreaking chatgpt via prompt engineering: An empirical study. _arXiv preprint arXiv:2305.13860_, 2023c. 
*   OpenAI (2024) OpenAI. Gpt model documentation, 2024. [https://platform.openai.com/docs/models/overview](https://platform.openai.com/docs/models/overview) [Accessed: (2024.2.10)]. 
*   Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _Advances in Neural Information Processing Systems_, 35:27730–27744, 2022. 
*   Qi et al. (2023) Xiangyu Qi, Kaixuan Huang, Ashwinee Panda, Mengdi Wang, and Prateek Mittal. Visual adversarial examples jailbreak aligned large language models. In _The Second Workshop on New Frontiers in Adversarial Machine Learning_, 2023. 
*   Shah et al. (2023) Rusheb Shah, Soroush Pour, Arush Tagade, Stephen Casper, Javier Rando, et al. Scalable and transferable black-box jailbreaks for language models via persona modulation. _arXiv preprint arXiv:2311.03348_, 2023. 
*   Shayegani et al. (2023) Erfan Shayegani, Yue Dong, and Nael Abu-Ghazaleh. Jailbreak in pieces: Compositional adversarial attacks on multi-modal language models. _arXiv preprint arXiv:2307.14539_, 2023. 
*   Shen et al. (2023) Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, and Yang Zhang. ” do anything now”: Characterizing and evaluating in-the-wild jailbreak prompts on large language models. _arXiv preprint arXiv:2308.03825_, 2023. 
*   Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023. 
*   Wang et al. (2023a) Jiongxiao Wang, Zichen Liu, Keun Hee Park, Muhao Chen, and Chaowei Xiao. Adversarial demonstration attacks on large language models. _arXiv preprint arXiv:2305.14950_, 2023a. 
*   Wang et al. (2023b) Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, et al. Cogvlm: Visual expert for pretrained language models. _arXiv preprint arXiv:2311.03079_, 2023b. 
*   Wei et al. (2023) Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does llm safety training fail? _arXiv preprint arXiv:2307.02483_, 2023. 
*   Wei et al. (2024) Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does llm safety training fail? _Advances in Neural Information Processing Systems_, 36, 2024. 
*   Yong et al. (2023) Zheng-Xin Yong, Cristina Menghini, and Stephen H Bach. Low-resource languages jailbreak gpt-4. _arXiv preprint arXiv:2310.02446_, 2023. 
*   Yu et al. (2023) Jiahao Yu, Xingwei Lin, and Xinyu Xing. Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts. _arXiv preprint arXiv:2309.10253_, 2023. 
*   Yuan et al. (2023) Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Pinjia He, Shuming Shi, and Zhaopeng Tu. Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher. _arXiv preprint arXiv:2308.06463_, 2023. 
*   Zheng et al. (2024) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. _Advances in Neural Information Processing Systems_, 36, 2024. 
*   Zhu et al. (2023) Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. _arXiv preprint arXiv:2304.10592_, 2023. 
*   Zou et al. (2023) Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models. _arXiv preprint arXiv:2307.15043_, 2023. 

Appendix A Related Work
-----------------------

Textual Jailbreak Attacks. Some of the jailbreak methods are text-based and can be categorized into two main types: hand-crafted jailbreak attacks and automatic jailbreak attacks. Hand-crafted jailbreak attacks primarily focus on designing or adopting prompts without optimization. Certain studies manipulate inputs, such as using low-resource languages (Yuan et al., [2023](https://arxiv.org/html/2404.03411v2#bib.bib32); Deng et al., [2023](https://arxiv.org/html/2404.03411v2#bib.bib9)) or ciphers Yong et al. ([2023](https://arxiv.org/html/2404.03411v2#bib.bib30)), to increase the success rates. Others use in-context examples Wei et al. ([2023](https://arxiv.org/html/2404.03411v2#bib.bib28)); Wang et al. ([2023a](https://arxiv.org/html/2404.03411v2#bib.bib26)) to prompt harmful responses. [Wei et al.](https://arxiv.org/html/2404.03411v2#bib.bib28) and [Liu et al.](https://arxiv.org/html/2404.03411v2#bib.bib18) elaborate on manually crafted jailbreak templates. Notably, role-play prompts (Yu et al., [2023](https://arxiv.org/html/2404.03411v2#bib.bib31); Liu et al., [2023b](https://arxiv.org/html/2404.03411v2#bib.bib17); Shah et al., [2023](https://arxiv.org/html/2404.03411v2#bib.bib22)) have also proven to be useful in jailbreak attacks. Besides, automatic jailbreak attacks focus on optimizing the attack prompt. Gradient-based methods Zou et al. ([2023](https://arxiv.org/html/2404.03411v2#bib.bib35)); Jones et al. ([2023](https://arxiv.org/html/2404.03411v2#bib.bib13)) update the attack prompt at the token level, while others, such as Liu et al. ([2023b](https://arxiv.org/html/2404.03411v2#bib.bib17)); Lapid et al. ([2023](https://arxiv.org/html/2404.03411v2#bib.bib14)), use genetic algorithms to update the prompt. [Chao et al.](https://arxiv.org/html/2404.03411v2#bib.bib6) proposed to automatically generate jailbreaks for a targeted LLM without human intervention. However, these methods are usually evaluated across different datasets with different metrics, making a fair comparison and reproduction hard to achieve.

Visual Jailbreak Attacks. Several methods have been proposed to jailbreak MLLMs by manipulating the visual input. Carlini et al. ([2023](https://arxiv.org/html/2404.03411v2#bib.bib5)) has demonstrated that multimodal models can be easily induced to perform arbitrary un-aligned behavior through adversarial perturbation of the input image. Qi et al. ([2023](https://arxiv.org/html/2404.03411v2#bib.bib21)) proposes to optimize adversarial images paired with harmful instructions to increase the probability of generating pre-defined toxic text targets. Bailey et al. ([2023](https://arxiv.org/html/2404.03411v2#bib.bib4)) optimizes the adversarial images to discourage the model from immediate refusal. Black-box typography is used in FigStep(Gong et al., [2023](https://arxiv.org/html/2404.03411v2#bib.bib11)). FigStep first embeds the typography of harmful questions into images and sends these images with benign instructions to elicit harmful generation from the models. Embedding-based jailbreak is proposed in Shayegani et al. ([2023](https://arxiv.org/html/2404.03411v2#bib.bib23)) where benign textual instructions are paired with malicious triggers embedded within the input images. However, the study of the transfer jailbreak ability on SOTA proprietary MLLMs, such as GPT-4V, is still missing.

Appendix B LLMs and MLLMs used in this study
--------------------------------------------

Table 3: There are in total of 11 models used in this study. The LLMs used in MLLMs are listed in parentheses.

We incorporate 11 different LLMs and MLLMs in this study which include both closed-source and open-source models as shown in Tab.[3](https://arxiv.org/html/2404.03411v2#A2.T3 "Table 3 ‣ Appendix B LLMs and MLLMs used in this study ‣ Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?"). There are 5 LLMs and 6 MLLMs used in this study. gpt-4-1106-preview(OpenAI, [2024](https://arxiv.org/html/2404.03411v2#bib.bib19)) is a GPT-4 Turbo model featuring improved instruction following, JSON mode, reproducible outputs, parallel function calling, and more with a maximum of 4,096 output tokens. Guanaco-7B(Cheung, [2024](https://arxiv.org/html/2404.03411v2#bib.bib7)) is an instruction-following language model built on Meta’s LLaMA 7B model covering various languages. However, it has not been filtered for harmful, biased, or explicit content. Llama2-7B(Touvron et al., [2023](https://arxiv.org/html/2404.03411v2#bib.bib25)) belongs to the Llama 2 family of large language models developed by Meta. Llama 2 uses supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety. Vicuna-7B(Zheng et al., [2024](https://arxiv.org/html/2404.03411v2#bib.bib33)) is a chat assistant trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT. ChatGLM2-6B(Du et al., [2022](https://arxiv.org/html/2404.03411v2#bib.bib10)) is an open-source bilingual (Chinese-English) chat model. The model is trained for about 1T tokens of Chinese and English corpus, supplemented by supervised fine-tuning, feedback bootstrap, and reinforcement learning with human feedback. gpt-4-vision-preview(OpenAI, [2024](https://arxiv.org/html/2404.03411v2#bib.bib19)) is GPT-4 with the ability to understand images, in addition to all other GPT-4 Turbo capabilities. MiniGPT4-7B(Zhu et al., [2023](https://arxiv.org/html/2404.03411v2#bib.bib34)) is a multimodal LLM that aligns a frozen visual encoder with a frozen LLM, Vicuna, using a trainable projection layer. LLaVA(Liu et al., [2023a](https://arxiv.org/html/2404.03411v2#bib.bib16)) is an end-to-end trained large multimodal model that combines a vision encoder and Vicuna for general-purpose visual and language understanding. Fuyu(AdeptAI, [2024](https://arxiv.org/html/2404.03411v2#bib.bib2)) is a multimodal model with a simpler architecture and training procedure developed by AdeptAI. Fuyu is a vanilla decoder-only transformer - there is no image encoder. Image patches are instead linearly projected into the first layer of the transformer, bypassing the embedding lookup. Qwen-VL-Chat(Bai et al., [2023](https://arxiv.org/html/2404.03411v2#bib.bib3)) is a multimodal LLM-based AI assistant, which is trained with alignment techniques. Qwen-VL-Chat supports more flexible interaction, such as multiple image inputs, multi-round question answering, and creative capabilities. Qwen-VL-Chat has achieved great results in both Chinese and English alignment evaluation. CogVLM(Wang et al., [2023b](https://arxiv.org/html/2404.03411v2#bib.bib27)) is a visual language foundation model that connects the frozen pre-trained language model and image encoder by a trainable visual expert module in the attention and FFN layers.

Appendix C Dataset Construction
-------------------------------

To build a comprehensive jailbreak benchmark, we have collected jailbreak behaviors and questions from existing literature, such as AdvBench(Zou et al., [2023](https://arxiv.org/html/2404.03411v2#bib.bib35)), SafeBench(Gong et al., [2023](https://arxiv.org/html/2404.03411v2#bib.bib11)), Qi et al. ([2023](https://arxiv.org/html/2404.03411v2#bib.bib21)), GPT-4 technical report(Achiam et al., [2023](https://arxiv.org/html/2404.03411v2#bib.bib1)), and ToxicChat(Lin et al., [2023](https://arxiv.org/html/2404.03411v2#bib.bib15)). In total, 1445 different harmful behaviors and questions have been collected. The dataset covers 11 different usage policies followed by Meta’s Purple LLaMA(Inan et al., [2023](https://arxiv.org/html/2404.03411v2#bib.bib12)) and OpenAI’s GPT4(Achiam et al., [2023](https://arxiv.org/html/2404.03411v2#bib.bib1)), such as Violence and Hate, Illegal Weapons, \etc, as shown in Tab.[4](https://arxiv.org/html/2404.03411v2#A3.T4 "Table 4 ‣ Appendix C Dataset Construction ‣ Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?").

Table 4: The category distribution of the constructed dataset.

Appendix D Evaluation Metrics
-----------------------------

Various evaluation metrics have been proposed to calculate the attack success rate (ASR) in existing literature. They can be classified into four main categories: refusal word detection(Zou et al., [2023](https://arxiv.org/html/2404.03411v2#bib.bib35); Liu et al., [2023b](https://arxiv.org/html/2404.03411v2#bib.bib17)), evaluation based on toxicity detection APIs(Qi et al., [2023](https://arxiv.org/html/2404.03411v2#bib.bib21); Shen et al., [2023](https://arxiv.org/html/2404.03411v2#bib.bib24)), LLMs as judges(Liu et al., [2023b](https://arxiv.org/html/2404.03411v2#bib.bib17); Gong et al., [2023](https://arxiv.org/html/2404.03411v2#bib.bib11); Qi et al., [2023](https://arxiv.org/html/2404.03411v2#bib.bib21)), and human evaluation(Shayegani et al., [2023](https://arxiv.org/html/2404.03411v2#bib.bib23)). In this work, two matrices are mainly used to calculate the ASR, \ie, refusal word detection, and LLMs as judges. We follow the setting in Zou et al. ([2023](https://arxiv.org/html/2404.03411v2#bib.bib35)); Liu et al. ([2023b](https://arxiv.org/html/2404.03411v2#bib.bib17)) and count an attack as successful if a set of pre-defined refusal words is not found in the generated content. The pre-defined refusal words can be found in Appendix[D](https://arxiv.org/html/2404.03411v2#A4 "Appendix D Evaluation Metrics ‣ Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?"). Besides, LLaMA-Guard Inan et al. ([2023](https://arxiv.org/html/2404.03411v2#bib.bib12)) is used as the jailbreak judge to check whether the generated content is indeed harmful or not. As LLaMA-Guard is open-source and instruction-tuned on a large harmful corpus, it is a more frugal way compared to using GPT-4 as judges(Liu et al., [2023b](https://arxiv.org/html/2404.03411v2#bib.bib17); Gong et al., [2023](https://arxiv.org/html/2404.03411v2#bib.bib11); Qi et al., [2023](https://arxiv.org/html/2404.03411v2#bib.bib21)).

The ASR is defined as the ratio of successful jailbreak queries to total queries N 𝑁 N italic_N. Specifically, for each response, a function f 𝑓 f italic_f is used to judge whether the jailbreak is successful and the ASR is calculated as follows

A⁢S⁢R=∑i N f⁢(r i)N,𝐴 𝑆 𝑅 superscript subscript 𝑖 𝑁 𝑓 subscript 𝑟 𝑖 𝑁 ASR=\frac{\sum_{i}^{N}f(r_{i})}{N},italic_A italic_S italic_R = divide start_ARG ∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT italic_f ( italic_r start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG start_ARG italic_N end_ARG ,(1)

where f⁢(r)=1 𝑓 𝑟 1 f(r)=1 italic_f ( italic_r ) = 1 if a response r 𝑟 r italic_r is judged as a successful jailbreak response. The refusal word detection(Zou et al., [2023](https://arxiv.org/html/2404.03411v2#bib.bib35); Liu et al., [2023b](https://arxiv.org/html/2404.03411v2#bib.bib17)) checks whether certain refusal words show in the response, and if not the attack is judged as successful. The predefined refusal words are presented in Tab.[12](https://arxiv.org/html/2404.03411v2#A5.T12 "Table 12 ‣ Appendix E Additional Experimental Results ‣ Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?"). Llama-Guard(Inan et al., [2023](https://arxiv.org/html/2404.03411v2#bib.bib12)) is also used as the judge. It is a Llama 2-based input-output safeguard model. It can be used for classifying content in both LLM inputs (prompt classification) and LLM responses (response classification). Llama-Guard can generate an output indicating whether the given text is safe/unsafe, and if unsafe based on a policy, it also lists the violating subcategories.

Appendix E Additional Experimental Results
------------------------------------------

We first test the baseline jailbreak performance where no additional jailbreak method is used and only the original harmful question is input to the model. The results are presented in Tab.[5](https://arxiv.org/html/2404.03411v2#A5.T5 "Table 5 ‣ Appendix E Additional Experimental Results ‣ Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?"). Guanaco-7B, Vicuna-7B, and LLaVAv1.5-7B show relatively higher attack success rates. It is because they are not specifically aligned to filter harmful, biased, or explicit content. Other models demonstrate relatively better robustness, especially Llama2-7B which is even better than GPT4.

The jailbreak results of GCG and AutoDAN are presented in Tab.[6](https://arxiv.org/html/2404.03411v2#A5.T6 "Table 6 ‣ Appendix E Additional Experimental Results ‣ Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?") and Tab.[7](https://arxiv.org/html/2404.03411v2#A5.T7 "Table 7 ‣ Appendix E Additional Experimental Results ‣ Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?"), respectively. The best transfer attack performance of GCG is achieved by using the combination of Guanaco and Vicuna as the surrogate model. Under this scenario, the success rate on ChatGLM2-6B achieves 24.47%percent 24.47 24.47\%24.47 % and on GPT-4, the success rate is 2.39%percent 2.39 2.39\%2.39 %. However, Llama2-7B is more robust against the GCG attack, and only 0.14%percent 0.14 0.14\%0.14 % responses are judged as harmful. On the other hand, Llama2-7B is less robust against the transfer attack using AutoDAN. By using Guanaco-7B as the surrogate model, AutoDAN obtains a success rate of 10.84%percent 10.84 10.84\%10.84 % on Llama2-7B. GPT-4 shows better robustness against AutoDAN. This can be attributed to the content of the jailbreak prompts. AutoDAN starts the optimization from hand-crafted jailbreak prompts and the semantics can be partially maintained in the final jailbreak prompts. GPT-4 can be tuned to reject these hand-crafted jailbreak prompts and thus shows better robustness. Besides, the jailbreak results from hand-crafted methods are presented in Tab[8](https://arxiv.org/html/2404.03411v2#A5.T8 "Table 8 ‣ Appendix E Additional Experimental Results ‣ Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?"). Llama2-7B and GPT-4 are robust against most of the methods but still show vulnerability to several methods. For example, dev_mode_ranti can lead to 26.72%percent 26.72 26.72\%26.72 % harmful response from Llama2-7B, and the combination_2 achieves a success rate of 5.06%percent 5.06 5.06\%5.06 % on GPT-4.

Regarding jailbreaking via the vision modality, Tab[9](https://arxiv.org/html/2404.03411v2#A5.T9 "Table 9 ‣ Appendix E Additional Experimental Results ‣ Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?") to Tab[11](https://arxiv.org/html/2404.03411v2#A5.T11 "Table 11 ‣ Appendix E Additional Experimental Results ‣ Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?") present the results from FigStep, VisualAdv and ImageHijacks, respectively. Open-source MLLMs are most vulnerable to FigStep compared to the other two methods in the transfer attack setting. For example, Fuyu fails to refuse 34.9%percent 34.9 34.9\%34.9 % harmful questions when using FigStep. However, Fuyu is robust against VisualAdv and ImageHijacks when using MiniGPT4-7B as the surrogate model. Besides, ImageHijacks obtains a higher success rate when attacking MiniGPT4-7B (52.35%percent 52.35 52.35\%52.35 %) compared to VisualAdv (35.99%percent 35.99 35.99\%35.99 %). This can be attributed to the different optimization goals. VisualAdv optimizes an adversarial example on a few-shot harmful corpus to maximize the probability of generating harmful content. ImageHijacks optimizes the adversarial example to maximize the generation probability of affirmative response to harmful requests. These affirmative responses are more likely to lead to harmful content.

Table 5: The jailbreak successful rates when directly giving the harmful behaviors to the LLMs without any jailbreak methods.

Table 6: The jailbreak successful rate using GCG attack.

Table 7: The jailbreak successful rate using AutoDAN attack.

Table 8: The attack successful rate of 27 handcrafted textual jailbreak methods on both GPT4 and open-source LLMs. 

Table 9: The success rate of FigStep across MLLMs.

Method (Surrogate Model)VisualAdv-lp16 (MiniGPT4-7B)VisualAdv-lp32 (MiniGPT4-7B)VisualAdv-uncons (MiniGPT4-7B)
Target Model Llama-Guard Refusal-Words Llama-Guard Refusal-Words Llama-Guard Refusal-Words
MiniGPT4-7B 29.93%94.14%34.08%74.23%35.99%92.16%
LLaVAv1.5-7B 15.95%69.60%15.75%69.46%16.84%71.44%
Fuyu 6.00%99.93%6.75%99.93%6.00%99.93%
Qwen-VL-Chat 2.86%42.60%2.45%42.40%2.32%23.18%
CogVLM 8.59%75.46%9.68%76.76%7.70%76.35%
GPT-4V 0.05%4.67%0.00%9.00%0.00%4.10%

Table 10: The success rate of VisualAdv across MLLMs using MiniGPT4-7B as surrogate model.

Method (Surrogate Model)ImageHijacks-lp16 (MiniGPT4-7B)ImageHijacks-lp32 (MiniGPT4-7B)ImageHijacks-uncons (MiniGPT4-7B)
Target Model Llama-Guard Refusal-Words Llama-Guard Refusal-Words Llama-Guard Refusal-Words
MiniGPT4-7B 28.90%99.93%36.74%99.93%52.35%100.00%
LLaVAv1.5-7B 16.36%69.94%17.11%70.48%17.11%70.42%
Fuyu 5.59%99.86%6.27%99.86%6.54%99.93%
Qwen-VL-Chat 2.86%41.10%2.86%41.65%2.11%22.84%
CogVLM 7.77%74.37%8.38%73.89%8.18%77.03%
GPT-4V 0.00%6.38%0.00%8.20%0.00%3.87%

Table 11: The success rate of ImageHijacks across MLLMs using MiniGPT4-7B as surrogate model.

Table 12: The refusal words considered in our experiments. Most strings are aligned with the GCG attack Zou et al. ([2023](https://arxiv.org/html/2404.03411v2#bib.bib35)) and AutoDAN Liu et al. ([2023b](https://arxiv.org/html/2404.03411v2#bib.bib17)). We also add some new refusal words that show in the evaluations.
