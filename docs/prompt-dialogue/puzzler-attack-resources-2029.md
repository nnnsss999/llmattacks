---
title: "Puzzler Attack Resources 2029"
category: "Prompt Dialogue"
source_url: ""
date_collected: 2025-06-19
license: "CC-BY-4.0"
---

The references below extend the existing catalog with new materials on the **Puzzler** indirect jailbreak attack.

- [Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues (OpenReview)](https://openreview.net/forum?id=mVqc9VBxHe2) – official publication page with peer reviews.
- [IJBR Repository](https://github.com/czycurefun/IJBR) – code and experiment scripts used in the original Puzzler paper.
- [Chinese Blog on Puzzler](https://blog.csdn.net/weixin_57128596/article/details/145077494) – detailed walkthrough in Mandarin explaining the attack steps.
- [Foundation-Model Paper Notes](https://github.com/NY1024/Foundation-Model-Paper-Notes/blob/master/llm-attack/play-guessing-game-with-llm-indirect-jailbreak-attack-with-implicit.md) – community notes summarizing the Puzzler methodology.
- [Harvard ADS Abstract](https://ui.adsabs.harvard.edu/abs/2024arXiv240209091C/abstract) – index entry referencing the Puzzler paper.

Additional references covering follow‑on research and community resources:

- [Jigsaw Puzzles: Splitting Harmful Questions to Jailbreak Large Language Models](https://arxiv.org/abs/2410.11459) – multi‑turn jailbreak that cites Puzzler as related work.
- [Jailbreaking to Jailbreak](https://arxiv.org/abs/2502.09638) – explores puzzle‑style prompts referencing the Puzzler strategy.
- [WordGame: Efficient & Effective LLM Jailbreak via Simultaneous Obfuscation in Query and Response](https://arxiv.org/abs/2405.14023) – contrasts obfuscation attacks with Puzzler.
- [SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner](https://arxiv.org/abs/2406.05498) – evaluates defenses against indirect attacks like Puzzler.
- [Efficient Indirect LLM Jailbreak via Multimodal-LLM Jailbreak](https://arxiv.org/abs/2405.20015) – demonstrates multimodal prompts inspired by Puzzler.
- [PBI-Attack: Prior-Guided Bimodal Interactive Black-Box Jailbreak Attack for Toxicity Maximization](https://doi.org/10.18653/v1/2025.trustnlp-main.3) – compares its effectiveness with Puzzler prompts.
- [Exploiting the Index Gradients for Optimization-Based Jailbreaking on Large Language Models](https://www.semanticscholar.org/paper/eeeabe8e2a5dd69089589e910009b3e13c031c58) – includes experiments with Puzzler-style inputs.
- [Here Comes The AI Worm: Unleashing Zero-click Worms that Target GenAI-Powered Applications](https://www.semanticscholar.org/paper/e6ad241ce87fe608241e9004f4c9d199b6bfebc7) – references Puzzler in broader jailbreak discussions.
- [CARES: Comprehensive Evaluation of Safety and Adversarial Robustness in Medical LLMs](https://www.semanticscholar.org/paper/15b8ead2bf17a7cc36bbe68b1830a46107ab43e6) – measures defensive performance against attacks including Puzzler.
- [Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks](https://www.semanticscholar.org/paper/1b95053af03b5a06809a4967c6cf5ca137bbcde4) – extends the clue‑based approach introduced by Puzzler.
