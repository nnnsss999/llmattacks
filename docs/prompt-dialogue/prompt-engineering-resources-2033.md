---
title: "Prompt Engineering Attack Resources 2033"
category: "Prompt Dialogue"
source_url: ""
date_collected: 2025-06-19
license: "CC-BY-4.0"
---

The references below expand the catalog with additional credible research articles and security blogs focused on prompt injection and jailbreak techniques. These entries build on [prompt-engineering-resources-2032.md](prompt-engineering-resources-2032.md).

- [Google Security Blog: Mitigating Prompt Injection Attacks](https://security.googleblog.com/2025/06/mitigating-prompt-injection-attacks.html)
- [Google Security Blog: How We Estimate Risk from Prompt Injection](https://security.googleblog.com/2025/01/how-we-estimate-risk-from-prompt.html)
- [OWASP LLM Risk: LLM01 Prompt Injection](https://genai.owasp.org/llmrisk/llm01-prompt-injection/)
- [Microsoft Security Blog: Mitigating Skeleton Key—A New Type of Generative AI Jailbreak Technique](https://www.microsoft.com/en-us/security/blog/2024/06/26/mitigating-skeleton-key-a-new-type-of-generative-ai-jailbreak-technique/)
- [Microsoft: Architecting Secure Gen AI Applications Preventing Indirect Prompt Injection Attacks](https://techcommunity.microsoft.com/blog/microsoft-security-blog/architecting-secure-gen-ai-applications-preventing-indirect-prompt-injection-att/4221859)
- [Microsoft Announcement: Adaptive Prompt Injection Challenge](https://msrc.microsoft.com/blog/2024/12/announcing-the-adaptive-prompt-injection-challenge-llmail-inject/)
- [Azure AI: Enhance AI Security with Prompt Shields and Content Safety](https://azure.microsoft.com/en-us/blog/enhance-ai-security-with-azure-prompt-shields-and-azure-ai-content-safety/)
- [AWS Security Blog: Reinforce 2025 GenAI Sessions](https://aws.amazon.com/blogs/security/reinforce-2025-genai-sessions/)
- [Security Innovation Blog: Securing LLMs Against Prompt Injection Attacks](https://blog.securityinnovation.com/securing-llms-against-prompt-injection-attacks)
- [Keysight: Invisible Prompt Injection Attack](https://www.keysight.com/blogs/en/tech/nwvs/2025/05/16/invisible-prompt-injection-attack)
- [Keysight: Prompt Injection Techniques—Jailbreaking LLMs via FlipAttack](https://www.keysight.com/blogs/en/tech/nwvs/2025/05/20/prompt-injection-techniques-jailbreaking-large-language-models-via-flipattack)
- [Prompt Security Blog: Prompt Injection 101](https://www.prompt.security/blog/prompt-injection-101)
- [BAIR Blog: Prompt Injection Defense](http://bair.berkeley.edu/blog/2025/04/11/prompt-injection-defense/)
- [Attention Tracker: Detecting Prompt Injection Attacks in LLMs](https://aclanthology.org/2025.findings-naacl.123/)
- [Design Patterns for Securing LLM Agents against Prompt Injections](https://arxiv.org/abs/2506.08837)
- [Defense Against Prompt Injection Attack by Leveraging Attack Techniques](https://arxiv.org/abs/2411.00459)
- [Rodell Lemit: Understanding LLM01 2025 Prompt Injection LLM Apps](https://rodelllemit.medium.com/understanding-llm01-2025-prompt-injection-llm-apps-8f04e5d4f825)
- [Microsoft: Protecting Against Prompt Injection Attacks in Chat Prompts](https://devblogs.microsoft.com/semantic-kernel/protecting-against-prompt-injection-attacks-in-chat-prompts/)
- [Azure AI: Prompt Injection Detection REST API](https://learn.microsoft.com/en-us/rest/api/contentsafety/text-operations/detect-text-prompt-injection-options?view=rest-contentsafety-2024-02-15-preview)

These additional sources highlight the growing ecosystem of defensive tooling and academic analysis around prompt engineering attacks and mitigation practices.
