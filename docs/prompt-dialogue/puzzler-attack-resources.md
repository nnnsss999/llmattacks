---
title: "Puzzler Attack Resources"
category: "Prompt Dialogue"
source_url: ""
date_collected: 2025-06-18
license: "CC-BY-4.0"
---

This file compiles research articles and papers that expand upon or cite the **Puzzler** indirect jailbreak attack.

- [Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues](https://arxiv.org/abs/2402.09091) – original Puzzler paper introducing clue-based jailbreak prompts.
- [SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner](https://arxiv.org/abs/2406.05498) – analyzes indirect jailbreaks such as Puzzler when evaluating defenses.
- [WordGame: Efficient & Effective LLM Jailbreak via Simultaneous Obfuscation in Query and Response](https://arxiv.org/abs/2405.14023) – embeds malicious requests using word substitution; contrasts with Puzzler.
- [Jigsaw Puzzles: Splitting Harmful Questions to Jailbreak Large Language Models](https://arxiv.org/abs/2410.11459) – multi-turn prompt splitting, citing Puzzler as related work.
- [Jailbreaking to Jailbreak](https://arxiv.org/abs/2502.09638) – explores puzzle-like prompts to bypass content filters, referencing Puzzler's strategy.
- [BaitAttack: Alleviating Intention Shift in Jailbreak Attacks via Adaptive Bait Crafting](https://aclanthology.org/2024.emnlp-main.877/) – discusses indirect jailbreaks and Puzzler in the context of intention-preserving attacks.
- [SequentialBreak: Large Language Models Can be Fooled by Embedding Jailbreak Prompts into Sequential Prompt Chains](https://arxiv.org/abs/2411.06426) – extends clue-based prompts across multiple turns.
- [AutoJailbreak: Exploring Jailbreak Attacks and Defenses through a Dependency Lens](https://arxiv.org/abs/2406.03805) – analyzes large jailbreak datasets and references Puzzler in its taxonomy.
- [Utilizing Jailbreak Probability to Attack and Safeguard Multimodal LLMs](https://arxiv.org/abs/2503.06989) – introduces a metric for measuring attack success probabilities, including Puzzler-like strategies.
- [Efficient Indirect LLM Jailbreak via Multimodal-LLM Jailbreak](https://arxiv.org/abs/2405.20015) – proposes a multimodal approach for embedding indirect jailbreaks, inspired by Puzzler.
- [JailbreakHunter: A Visual Analytics Approach for Jailbreak Prompts Discovery From Large-Scale Human-LLM Conversational Datasets](https://doi.org/10.1109/tvcg.2025.3557568) – visualizes large datasets of jailbreak prompts and references Puzzler as a case study.
- [Summary the Savior: Harmful Keyword and Query-based Summarization for LLM Jailbreak Defense](https://doi.org/10.18653/v1/2025.trustnlp-main.17) – evaluates Puzzler-like attacks when proposing summarization-based defenses.
- [LLM Abuse Prevention Tool Using GCG Jailbreak Attack Detection and DistilBERT-Based Ethics Judgment](https://doi.org/10.3390/info16030204) – includes experiments on indirect jailbreaks including Puzzler.
- [How Alignment and Jailbreak Work: Explain LLM Safety through Intermediate Hidden States](https://doi.org/10.18653/v1/2024.findings-emnlp.139) – analyzes Puzzler within a broader taxonomy of jailbreak mechanisms.
- [Prompt Crossing: Evaluating Whether LLM Response Stem from Jailbreak or Normal Prompt](https://doi.org/10.1109/icassp49660.2025.10889949) – proposes detection techniques tested against Puzzler examples.
- [Samjailbreak: Query-Efficient Sampling-Based Jailbreak Attack Against Large Language Models](https://doi.org/10.2139/ssrn.5206108) – contrasts sampling-based attacks with clue-driven methods like Puzzler.
- [Defending Against GCG Jailbreak Attacks with Syntax Trees and Perplexity in LLMs](https://doi.org/10.1109/gcce62371.2024.10760963) – studies defenses that also report results on Puzzler prompts.
- [Optimizing Transformer Models for Prompt Jailbreak Attack Detection in AI Assistant Systems](https://doi.org/10.1109/vcris63677.2024.10813380) – benchmarks detection approaches against Puzzler-style jailbreaks.
- [Chain-of-Detection Enables Robust and Efficient Jailbreak Defense](https://doi.org/10.2139/ssrn.5124466) – proposes a chain-of-thought defense strategy, evaluating Puzzler along with other attacks.
- [Defending Large Language Models Against Jailbreak Attacks Through Chain of Thought Prompting](https://doi.org/10.1109/nana63151.2024.00028) – demonstrates chain-of-thought prompts to counter Puzzler scenarios.
- [RePD: Defending Jailbreak Attack through a Retrieval-based Prompt Decomposition Process](https://doi.org/10.18653/v1/2025.findings-naacl.16) – decomposes prompts and includes Puzzler in its evaluation.
- [Defending ChatGPT against Jailbreak Attack via Self-Reminder](https://doi.org/10.21203/rs.3.rs-2873090/v1) – tests self-reminder strategies on Puzzler jailbreaks.
- [PBI-Attack: Prior-Guided Bimodal Interactive Black-Box Jailbreak Attack for Toxicity Maximization](https://doi.org/10.18653/v1/2025.trustnlp-main.3) – compares its effectiveness with Puzzler prompts.
- [Boosting Jailbreak Attack with Momentum](https://doi.org/10.1109/icassp49660.2025.10888812) – explores momentum-based optimization for jailbreaks, referencing Puzzler results.
- [Analysis of Jailbreak Attacks and Defenses for Large Language Models](https://doi.org/10.47297/wspciwsp2516-252718.20250902) – surveys the landscape of jailbreak techniques and mitigation approaches, covering Puzzler as a case study.
- [Jailbreak Attacks on Large Language Models and Possible Defenses: Present Status and Future Possibilities](https://doi.org/10.1109/istas61960.2024.10732418) – overview of emerging jailbreak strategies, with discussion of Puzzler-style indirect prompts.
- [MMJ-Bench: A Comprehensive Study on Jailbreak Attacks and Defenses for Vision Language Models](https://doi.org/10.1609/aaai.v39i26.34983) – includes multimodal evaluations that cite Puzzler when comparing text-based jailbreak benchmarks.

These references offer deeper context on how clue-based prompting evolves and how researchers study or defend against Puzzler-style jailbreaks.
