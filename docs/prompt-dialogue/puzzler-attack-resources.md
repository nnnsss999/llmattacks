---
title: "Puzzler Attack Resources"
category: "Prompt Dialogue"
source_url: ""
date_collected: 2025-06-18
license: "CC-BY-4.0"
---

This file compiles research articles and papers that expand upon or cite the **Puzzler** indirect jailbreak attack.

- [Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues](https://arxiv.org/abs/2402.09091) – original Puzzler paper introducing clue-based jailbreak prompts.
- [SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner](https://arxiv.org/abs/2406.05498) – analyzes indirect jailbreaks such as Puzzler when evaluating defenses.
- [WordGame: Efficient & Effective LLM Jailbreak via Simultaneous Obfuscation in Query and Response](https://arxiv.org/abs/2405.14023) – embeds malicious requests using word substitution; contrasts with Puzzler.
- [Jigsaw Puzzles: Splitting Harmful Questions to Jailbreak Large Language Models](https://arxiv.org/abs/2410.11459) – multi-turn prompt splitting, citing Puzzler as related work.
- [Jailbreaking to Jailbreak](https://arxiv.org/abs/2502.09638) – explores puzzle-like prompts to bypass content filters, referencing Puzzler's strategy.
- [BaitAttack: Alleviating Intention Shift in Jailbreak Attacks via Adaptive Bait Crafting](https://aclanthology.org/2024.emnlp-main.877/) – discusses indirect jailbreaks and Puzzler in the context of intention-preserving attacks.
- [SequentialBreak: Large Language Models Can be Fooled by Embedding Jailbreak Prompts into Sequential Prompt Chains](https://arxiv.org/abs/2411.06426) – extends clue-based prompts across multiple turns.
- [AutoJailbreak: Exploring Jailbreak Attacks and Defenses through a Dependency Lens](https://arxiv.org/abs/2406.03805) – analyzes large jailbreak datasets and references Puzzler in its taxonomy.
- [Utilizing Jailbreak Probability to Attack and Safeguard Multimodal LLMs](https://arxiv.org/abs/2503.06989) – introduces a metric for measuring attack success probabilities, including Puzzler-like strategies.
- [Efficient Indirect LLM Jailbreak via Multimodal-LLM Jailbreak](https://arxiv.org/abs/2405.20015) – proposes a multimodal approach for embedding indirect jailbreaks, inspired by Puzzler.

These references offer deeper context on how clue-based prompting evolves and how researchers study or defend against Puzzler-style jailbreaks.
