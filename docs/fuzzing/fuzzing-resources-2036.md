---
title: "Fuzzing Resources 2036"
category: "Fuzzing"
source_url: ""
date_collected: 2025-06-21
license: "CC-BY-4.0"
---

The references below list newly surfaced tools and repositories that apply fuzzing techniques to large language models (LLMs). They extend [fuzzing-resources-2035.md](fuzzing-resources-2035.md).

- [ToolFuzz GitHub repository](https://github.com/eth-sri/ToolFuzz) – framework for fuzzing LLM agent tools.
- [AiLibFuzzer GitHub repository](https://github.com/mowenroot/AiLibFuzzer) – library combining LLMs with classic fuzzing.
- [llm-harness GitHub repository](https://github.com/kchousos/llm-harness) – automatically generates fuzzing harnesses using LLMs.
- [S2AFL Repository](https://github.com/huoyuxi/S2AFL) – integrates code semantics and protocol syntax for LLM-enhanced fuzzing.
- [LLMQTTFuzzer Repository](https://github.com/Netsec-SJTU/LLMQTTFuzzer) – MQTT protocol fuzzer powered by LLMs.
- [Gagana Repository](https://github.com/lac-dcc/Gagana) – artifacts for the paper "On the Practicality of LLM-Based Compiler Fuzzing".
- [light-fuzz-gen Repository](https://github.com/pushinl/light-fuzz-gen) – lightweight fuzz generator leveraging LLMs.
- [LLM4RtFuzz Repository](https://github.com/doslahtm/LLM4RtFuzz) – LLM-integrated CPU fuzzing tool.
- [lockpick Repository](https://github.com/FaintStep/lockpick) – LLM-powered recon assistant with built-in web fuzzing.

These additions capture the latest GitHub projects exploring LLM-based fuzz testing across diverse domains.
The references below continue the timeline from [fuzzing-resources-2035.md](fuzzing-resources-2035.md). They showcase research papers examining how fuzzing can stress-test or audit modern LLMs and multimodal models.

- [VLATest: Testing and Evaluating Vision-Language-Action Models for Robotic Manipulation](https://arxiv.org/abs/2409.12894)
- [Dynamic Code Orchestration: Harnessing the Power of Large Language Models for Adaptive Script Execution](https://arxiv.org/abs/2408.11060)
- [Data Augmentation by Fuzzing for Neural Test Generation](https://arxiv.org/abs/2406.08665)
- [MedFuzz: Exploring the Robustness of Large Language Models in Medical Question Answering](https://arxiv.org/abs/2406.06573)
- [PerfGen: Automated Performance Benchmark Generation for Big Data Analytics](https://arxiv.org/abs/2412.04687)
- [Endless Jailbreaks with Bijection Learning](https://arxiv.org/abs/2410.01294)
- [Artificial-Intelligence Generated Code Considered Harmful: A Road Map for Secure and High-Quality Code Generation](https://arxiv.org/abs/2409.19182)
- [Tasks People Prompt: A Taxonomy of LLM Downstream Tasks in Software Verification and Falsification Approaches](https://arxiv.org/abs/2404.09384)

These additions highlight continuing work to test, fuzz and evaluate large language models using automated techniques across diverse domains.
