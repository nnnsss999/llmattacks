---
title: "Fuzzing Resources 2036"
category: "Fuzzing"
source_url: ""
date_collected: 2025-06-21
license: "CC-BY-4.0"
---

The references below continue the timeline from [fuzzing-resources-2035.md](fuzzing-resources-2035.md). They showcase research papers examining how fuzzing can stress-test or audit modern LLMs and multimodal models.

- [VLATest: Testing and Evaluating Vision-Language-Action Models for Robotic Manipulation](https://arxiv.org/abs/2409.12894)
- [Dynamic Code Orchestration: Harnessing the Power of Large Language Models for Adaptive Script Execution](https://arxiv.org/abs/2408.11060)
- [Data Augmentation by Fuzzing for Neural Test Generation](https://arxiv.org/abs/2406.08665)
- [MedFuzz: Exploring the Robustness of Large Language Models in Medical Question Answering](https://arxiv.org/abs/2406.06573)
- [PerfGen: Automated Performance Benchmark Generation for Big Data Analytics](https://arxiv.org/abs/2412.04687)
- [Endless Jailbreaks with Bijection Learning](https://arxiv.org/abs/2410.01294)
- [Artificial-Intelligence Generated Code Considered Harmful: A Road Map for Secure and High-Quality Code Generation](https://arxiv.org/abs/2409.19182)
- [Tasks People Prompt: A Taxonomy of LLM Downstream Tasks in Software Verification and Falsification Approaches](https://arxiv.org/abs/2404.09384)

These additions highlight continuing work to test, fuzz and evaluate large language models using automated techniques across diverse domains.
