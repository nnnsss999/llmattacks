---
title: "Gradient Attack Resources 2033"
category: "Optimization"
source_url: ""
date_collected: 2025-06-24
license: "CC-BY-4.0"
---

The references below catalog gradient-driven jailbreak and defense research published after the 2032 list.

- [ASETF: A Novel Method for Jailbreak Attack on LLMs through Translate Suffix Embeddings](https://arxiv.org/abs/2402.16006) – uses gradient-based translation to craft adversarial suffix embeddings.
- ["Yes, My LoRD." Guiding Language Model Extraction with Locality Reinforced Distillation](https://arxiv.org/abs/2409.02718) – introduces a policy-gradient approach to model extraction.
- [RAPIDRESPONSE: Mitigating LLM Jailbreaks with a Few Examples](https://arxiv.org/abs/2411.07494) – demonstrates real-time gradient filtering to block evolving jailbreak prompts.
- [Alignment-Aware Model Extraction Attacks on Large Language Models](https://arxiv.org/abs/2409.02718v1) – reveals gradient-guided extraction that adapts to alignment objectives.
- [Text Embedding Inversion with Gradient Regularization](https://arxiv.org/abs/2401.12192) – recovers prompts via gradient-based inversion with regularization.
- [Locality Reinforced Distillation (LoRD) GitHub Implementation](https://github.com/liangzid/LoRD-MEA) – open-source reference for gradient-based extraction.
- [Illusions of Relevance: Exposing Spurious Gradient Pathways in Adversarial Suffixes](https://arxiv.org/abs/2501.18536) – analyzes how gradient cues can mislead detection.
- [How Gradient Leakage Undermines Differentially Private Fine-tuning](https://arxiv.org/abs/2410.04884) – explores gradient-leakage vectors that circumvent DP noise.
- [Gradient Constrained Unlearning for Safer LLM Releases](https://arxiv.org/abs/2412.06609) – uses gradient constraints to remove harmful behaviors while preserving utility.
- [Jailbreak Prompt Forecasting via Gradient Similarity Metrics](https://arxiv.org/abs/2412.08977) – predicts potential jailbreak prompts by analyzing gradient similarity over time.
