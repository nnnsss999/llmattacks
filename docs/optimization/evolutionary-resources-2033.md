---
title: "Evolutionary Algorithm Resources 2033"
category: "Optimization"
source_url: ""
date_collected: 2025-06-24
license: "CC-BY-4.0"
---
The following references extend the catalog with newly surfaced research and community guides on evolutionary and genetic algorithm approaches for jailbreaking large language models (LLMs).

- [Iterative Self-Tuning LLMs for Enhanced Jailbreaking Capabilities](https://www.microsoft.com/en-us/research/publication/iterative-self-tuning-llms-for-enhanced-jailbreaking-capabilities/) – describes a self-tuning search process that combines reinforcement learning and evolutionary steps.
- [HackEvolve: Genetic Algorithm-Based Jailbreak Competition](https://devpost.com/software/hackevolve) – open challenge showcasing GA-driven jailbreak strategies.
- [AutoDAN GitHub Repository](https://github.com/SheltonLiu-NN/AutoDAN) – code implementing mutation-based jailbreak prompts.
- [Awesome Jailbreak on LLMs](https://github.com/yueliu1999/Awesome-Jailbreak-on-LLMs) – community-curated list of jailbreak techniques including GA methods.
- [Genetic Scenario Shift Jailbreak | LLM Security Database](https://www.promptfoo.dev/lm-security-db/vuln/genetic-scenario-shift-jailbreak-2588802d) – documents black-box attacks using optimized scenario shifts.
- [Semantic Mirror Jailbreak: Genetic Algorithm Based Jailbreak Prompts](https://arxiv.org/abs/2402.14872) – evolves paraphrased prompts that bypass safety filters.
- [LLM-Virus: Evolutionary Jailbreak Attack on Large Language Models](https://arxiv.org/abs/2501.00055) – demonstrates gradient-guided evolutionary search for universal jailbreaks.
- [Adaptive Strategy Evolution for Generating Tailored Jailbreaks](https://openreview.net/forum?id=xF5st2HtYP) – explores self-adapting evolutionary strategies for jailbreak prompts.
- [LLM Jailbreaking Guide](https://deepgram.com/learn/llm-jailbreaking) – overview of jailbreak approaches, including evolutionary methods and defences.
- [15 LLM Jailbreaks That Shook AI Safety](https://medium.com/@nirdiamant21/15-llm-jailbreaks-that-shook-ai-safety-981d2796d5c6) – summarizes notable attacks, highlighting the role of automated prompt evolution.
