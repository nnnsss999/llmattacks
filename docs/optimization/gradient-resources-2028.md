---
title: "Gradient Attack Resources 2028"
category: "Optimization"
source_url: ""
date_collected: 2025-06-18
license: "CC-BY-4.0"
---

The references below compile additional gradient-based attack and defense research published in or after 2027.

- [Adversarial Attack on Large Language Models using Exponentiated Gradient Descent](https://arxiv.org/abs/2505.09820) – describes an attack that optimizes prompts with an exponentiated gradient method.
- [Exploiting the Index Gradients for Optimization-Based Jailbreaking on Large Language Models](https://arxiv.org/abs/2412.08615) – explores index gradients to craft adversarial completions.
- [GradientCuff-Jailbreak-Defense – Hugging Face Demo](https://huggingface.co/spaces/TrustSafeAI/GradientCuff-Jailbreak-Defense) – interactive implementation showing gradient-based detection.
- [Gradient Cuff GitHub Repository](https://github.com/IBM/Gradient-Cuff) – source code accompanying the Gradient Cuff paper.
- [Gradient Regularization-based Cross-Prompt Attacks on Vision Language Models](https://openreview.net/forum?id=I05Z6KjQ9K) – cross-modal jailbreak approach leveraging gradient regularization.
- [Gradient Inversion Attack in Federated Learning: Exposing Text Data through Discrete Optimization](https://aclanthology.org/2025.coling-main.176/) – demonstrates text recovery from gradient information.
- [Extracting Spatiotemporal Data from Gradients with Large Language Models](https://arxiv.org/abs/2410.16121) – shows gradient leakage of structured time-series data.
- [Zer0-Jack: A Memory-efficient Gradient-based Jailbreaking Method for Black-box Multi-modal Large Language Models](https://arxiv.org/abs/2411.07559) – memory-optimized jailbreak via gradient steps.
- [DAGER: Exact Gradient Inversion for Large Language Models](https://openreview.net/forum?id=CrADAX7h23) – openreview page for the DAGER technique.
- [Model Layer LLM Vulnerabilities](https://www.promptfoo.dev/lm-security-db/tag/model-layer) – collection of gradient-based vulnerabilities at the model layer.
- [JailbreakBench: LLM Robustness Benchmark](https://jailbreakbench.github.io/) – includes gradient-based attacks in a standardized evaluation suite.
