---
title: "Gradient Attack Resources 2030"
category: "Optimization"
source_url: ""
date_collected: 2025-06-20
license: "CC-BY-4.0"
---

The following papers and articles extend our catalog of gradient-based jailbreak techniques and defences with work published after 2029.

- [AttnGCG: Enhancing Jailbreaking Attacks on LLMs with Attention Manipulation](https://arxiv.org/abs/2410.09040) – manipulates attention scores to boost the success of gradient-driven jailbreaks.
- [Guiding not Forcing: Enhancing the Transferability of Jailbreaking Attacks on LLMs via Removing Superfluous Constraints](https://arxiv.org/abs/2503.01865) – shows how eliminating unnecessary constraints improves cross-model transfer of gradient attacks.
- [Securing Vision-Language Models with a Robust Encoder Against Jailbreak and Adversarial Attacks](https://arxiv.org/abs/2409.07353) – proposes a robust encoder to defend against gradient-based multimodal attacks.
- [Failures to Find Transferable Image Jailbreaks Between Vision-Language Models](https://arxiv.org/abs/2407.15211) – analyzes why gradient-based image jailbreaks often fail to generalize across models.
- [WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models](https://arxiv.org/abs/2406.18510) – collects real-world jailbreak prompts and explores gradient-guided mitigation strategies.
- [AdvWave: Stealthy Adversarial Jailbreak Attack against Large Audio-Language Models](https://arxiv.org/abs/2412.08608) – demonstrates audio jailbreaks that leverage gradient information to evade detectors.
- [What is in Your Safe Data? Identifying Benign Data that Breaks Safety](https://arxiv.org/abs/2404.01099) – reveals how harmless training data can enable gradient-based jailbreaking.
