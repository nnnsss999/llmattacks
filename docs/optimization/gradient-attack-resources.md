---
title: "Gradient-Based Attack Resources"
category: "Optimization"
source_url: ""
date_collected: 2025-06-18
license: "CC-BY-4.0"
---

The links below compile additional references focused on gradient-guided optimization methods for jailbreaking or auditing large language models.

- [GradSafe: Detecting Jailbreak Prompts for LLMs via Safety-Critical Gradient Analysis](https://aclanthology.org/2024.acl-long.30/)
- [Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes](https://arxiv.org/abs/2403.00867)
- [Improved Generation of Adversarial Examples Against Safety-aligned LLMs](https://papers.nips.cc/paper_files/paper/2024/file/aeae9df8e6bfe7350160bb42965dbd44-Paper-Conference.pdf)
- [Exploring Jailbreak Attacks: Understanding LLM Vulnerabilities and the Challenges of Detection and Defense](https://infosecwriteups.com/exploring-jailbreak-attacks-understanding-llm-vulnerabilities-and-the-challenges-of-detection-5b6cd35451a0)
- [Advanced Adversarial Attacks on Large Language Models](https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/)

