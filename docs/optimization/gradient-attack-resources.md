---
title: "Gradient-Based Attack Resources"
category: "Optimization"
source_url: ""
date_collected: 2025-06-18
license: "CC-BY-4.0"
---

The links below compile additional references focused on gradient-guided optimization methods for jailbreaking or auditing large language models.

- [GradSafe: Detecting Jailbreak Prompts for LLMs via Safety-Critical Gradient Analysis](https://aclanthology.org/2024.acl-long.30/)
- [Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes](https://arxiv.org/abs/2403.00867)
- [Improved Generation of Adversarial Examples Against Safety-aligned LLMs](https://papers.nips.cc/paper_files/paper/2024/file/aeae9df8e6bfe7350160bb42965dbd44-Paper-Conference.pdf)
- [Exploring Jailbreak Attacks: Understanding LLM Vulnerabilities and the Challenges of Detection and Defense](https://infosecwriteups.com/exploring-jailbreak-attacks-understanding-llm-vulnerabilities-and-the-challenges-of-detection-5b6cd35451a0)
- [Advanced Adversarial Attacks on Large Language Models](https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/)
- [Jailbreak Detector Model for AI Security](https://huggingface.co/spaces/pinyuchen/Gradient-Cuff-Jailbreak-Detector) – open-source tool demonstrating gradient-based detection.
- [LLM Jailbreak Slides and Notebook](https://gubri.eu/pdf/Slides_LLM_Jailbreak_Lux2024_MGubri.pdf) – includes gradient-driven jailbreak demos with accompanying Colab notebook.
- [JailDAM: Jailbreak Detection with Adaptive Memory for Vision-Language Models](https://arxiv.org/abs/2504.03770) – autoencoder-based defense leveraging memory-guided gradients.
- [Steering Away from Harm: An Adaptive Approach to Defending Vision-Language Models Against Jailbreaks](https://arxiv.org/abs/2411.16721) – uses gradient steering to mitigate multimodal attacks.

