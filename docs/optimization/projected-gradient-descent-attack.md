---
title: "Attacking LLMs with Projected Gradient Descent"
category: "Optimization"
source_url: "https://arxiv.org/abs/2402.09154"
date_collected: 2025-06-18
license: "Fair Use"
---
The 2025 arXiv paper **Attacking Large Language Models with Projected Gradient Descent** demonstrates how direct gradient optimization can systematically find jailbreak prompts. By framing prompt generation as a constrained optimization problem, the authors use projected gradient descent to craft adversarial suffixes that reliably elicit disallowed content. Their method succeeds against several closed models without access to their parameters and outperforms standard heuristic attacks.
