<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing</title>
<!--Generated on Mon Mar 10 04:39:43 2025 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2503.08990v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S1" title="In JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S1.SS1" title="In 1 Introduction ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.1 </span>LLMs: Opportunities and Challenges</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S1.SS2" title="In 1 Introduction ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.2 </span>Impact of Jailbroken LLMs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S1.SS3" title="In 1 Introduction ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.3 </span>Our Goals and Contributions</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S2" title="In JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Background</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S2.SS1" title="In 2 Background ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>LLMs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S2.SS2" title="In 2 Background ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Jailbreaking LLMs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S2.SS3" title="In 2 Background ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Fuzzing</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S3" title="In JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Threat Model</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S4" title="In JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S4.SS1" title="In 4 Methodology ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Why Fuzzing?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S4.SS2" title="In 4 Methodology ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Preliminary Formulation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S4.SS3" title="In 4 Methodology ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Novel Seed Prompt Templates</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S4.SS4" title="In 4 Methodology ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Synonym-based Mutation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S4.SS5" title="In 4 Methodology ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>Embedding-based Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S4.SS6" title="In 4 Methodology ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.6 </span>Final Formulation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S5" title="In JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S5.SS1" title="In 5 Results ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Experimental Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S5.SS2" title="In 5 Results ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Evaluator Configuration</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S5.SS3" title="In 5 Results ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Selector Configuration</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S5.SS4" title="In 5 Results ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Mutator Configuration</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S5.SS5" title="In 5 Results ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.5 </span>Main Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S5.SS6" title="In 5 Results ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.6 </span>Stealthiness Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S5.SS7" title="In 5 Results ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.7 </span>Comparison With Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S5.SS8" title="In 5 Results ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.8 </span>Runtime Breakdown Analysis</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S6" title="In JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Related Work and Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S6.SS1" title="In 6 Related Work and Discussion ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Jailbreak Attacks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S6.SS2" title="In 6 Related Work and Discussion ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Other Security and Privacy Risks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S6.SS3" title="In 6 Related Work and Discussion ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>Limitations and Future Work</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S7" title="In JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S8" title="In JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Appendix</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S8.SS1" title="In 8 Appendix ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.1 </span>Target LLMs Descriptions</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\newmdenv</span>
<p class="ltx_p" id="p1.2">[linewidth=1pt, linecolor=black,backgroundcolor=gray!15]myframe






</p>
</div>
<h1 class="ltx_title ltx_font_bold ltx_title_document" style="font-size:144%;">
<span class="ltx_text ltx_font_medium ltx_font_italic" id="id1.id1">JBFuzz</span>: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Vasudev Gohil
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id2.1.id1">vgohil.research@gmail.com</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p ltx_align_center" id="id3.id1"><span class="ltx_text" id="id3.id1.1" style="color:#C00000;">Disclaimer: This paper contains potentially offensive</span></p>
<p class="ltx_p ltx_align_center" id="id4.id2"><span class="ltx_text" id="id4.id2.1" style="color:#C00000;">and harmful text.</span></p>
<p class="ltx_p" id="id5.id3">Large language models (LLMs) have shown great promise as language understanding and decision making tools, and they have permeated various aspects of our everyday life. However, their widespread availability also comes with novel risks, such as generating harmful, unethical, or offensive content, via an attack called jailbreaking. Despite extensive efforts from LLM developers to align LLMs using human feedback, they are still susceptible to jailbreak attacks.
To tackle this issue, researchers often employ red-teaming to understand and investigate jailbreak prompts. However, existing red-teaming approaches lack effectiveness, scalability, or both. To address these issues, we propose <span class="ltx_text ltx_font_italic" id="id5.id3.1">JBFuzz</span>, a novel effective, automated, and scalable red-teaming technique for jailbreaking LLMs.</p>
<p class="ltx_p" id="id6.id4"><span class="ltx_text ltx_font_italic" id="id6.id4.1">JBFuzz</span> is inspired by the success of fuzzing for detecting bugs/vulnerabilities in software.
We overcome three challenges related to effectiveness and scalability by devising novel seed prompts, a lightweight mutation engine, and a lightweight and accurate evaluator for guiding the fuzzer. Assimilating all three solutions results in a potent fuzzer that only requires black-box access to the target LLM.
We perform extensive experimental evaluation of <span class="ltx_text ltx_font_italic" id="id6.id4.2">JBFuzz</span> using nine popular and widely-used LLMs.
We find that <span class="ltx_text ltx_font_italic" id="id6.id4.3">JBFuzz</span> successfully jailbreaks all LLMs for various harmful/unethical questions, with an average attack success rate of 99%. We also find that <span class="ltx_text ltx_font_italic" id="id6.id4.4">JBFuzz</span> is extremely efficient as it jailbreaks a given LLM for a given question in 60 seconds on average.
Our work highlights the susceptibility of the state-of-the-art LLMs to jailbreak attacks even after safety alignment, and serves as a valuable red-teaming tool for LLM developers.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<section class="ltx_subsection" id="S1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>LLMs: Opportunities and Challenges</h3>
<div class="ltx_para" id="S1.SS1.p1">
<p class="ltx_p" id="S1.SS1.p1.1">Recent advancements in artificial intelligence, coupled with exponential growth in computational capabilities, have brought large language models (LLMs) to the forefront of technological innovation and gained mass popularity <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib39" title="">39</a>]</cite>. LLMs like OpenAI’s ChatGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib70" title="">70</a>]</cite>, Anthropic’s Claude <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib4" title="">4</a>]</cite>, and Google’s Gemini <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib34" title="">34</a>]</cite> are not only redefining natural language processing (NLP) but are also revolutionizing diverse domains. Their ability to generate human-like text, interpret complex prompts, and adapt to a myriad of applications has led to widespread integration in everyday life. From enhancing productivity with automated text summarization to aiding creative expression through content generation, LLMs are reshaping industries such as education, healthcare, customer service, and software development <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib82" title="">82</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib15" title="">15</a>]</cite>. Their transformative potential has sparked global interest among organizations and researchers eager to explore new frontiers where these models can optimize workflows, solve intricate problems, and unlock opportunities that were once unimaginable <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib71" title="">71</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.SS1.p2">
<p class="ltx_p" id="S1.SS1.p2.1">Beyond these broad applications, researchers have leveraged LLMs to tackle pressing challenges in computer security <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib94" title="">94</a>]</cite>. By using their natural language understanding and contextual reasoning capabilities, LLMs have been employed to detect vulnerabilities in software code <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib51" title="">51</a>]</cite>, assist in threat intelligence <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib60" title="">60</a>]</cite>, and analyze malware behavior <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib12" title="">12</a>]</cite>. They are also being utilized to develop automated security documentation, streamline compliance workflows <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib16" title="">16</a>]</cite>, and even simulate cyberattacks to stress-test systems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib77" title="">77</a>]</cite>. These applications have demonstrated that LLMs can significantly contribute to enhancing security measures and mitigating risks in an increasingly digital world.</p>
</div>
<div class="ltx_para" id="S1.SS1.p3">
<p class="ltx_p" id="S1.SS1.p3.1">However, the rise of LLMs has not been without controversy. While they hold immense promise, their widespread adoption has also raised concerns about their misuse <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib63" title="">63</a>]</cite>. LLMs have been implicated in spreading hate speech, disinformation, creating fake media, and pirating hardware, among others <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib86" title="">86</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib92" title="">92</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib28" title="">28</a>]</cite>.
One area of particular concern has been their susceptibility to being “jailbroken,” where malicious or cleverly crafted prompts bypass safeguards to elicit harmful or unethical outputs. This phenomenon has drawn significant attention from laypeople, the media, and researchers alike <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib58" title="">58</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib59" title="">59</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib49" title="">49</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib91" title="">91</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib106" title="">106</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib81" title="">81</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib97" title="">97</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib78" title="">78</a>]</cite>. Developers of LLMs have also actively participated in red-teaming efforts to understand and mitigate such vulnerabilities, emphasizing the need for more robust safety measures <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib58" title="">58</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib59" title="">59</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib49" title="">49</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.SS1.p4">
<p class="ltx_p" id="S1.SS1.p4.1">Efforts to red-team LLMs often involve users and developers manually crafting prompts to test the models’ boundaries and uncover potential vulnerabilities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib56" title="">56</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib81" title="">81</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib91" title="">91</a>]</cite>. While these exercises shed light on the limitations of LLMs, they tend to be ad hoc, resource-intensive, and lack scalability.
The rapid evolution of LLM technology, combined with its widespread adoption, requires red team researchers to consistently develop a broader and more diverse range of LLM-specific test cases. This highlights the difficulty of relying exclusively on manual efforts to keep up with the frequent technical advancements and improvements in LLMs.
Despite the growing awareness of these risks, there remains a lack of a unified, automated, and robust methodology for evaluating the susceptibility of LLMs to such jailbreaking attacks at scale. This gap underscores the urgent need for systematic approaches to ensure that LLMs are secure, ethical, and resilient against exploitation, thereby safeguarding their potential for positive impact.</p>
</div>
</section>
<section class="ltx_subsection" id="S1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.2 </span>Impact of Jailbroken LLMs</h3>
<div class="ltx_para" id="S1.SS2.p1">
<p class="ltx_p" id="S1.SS2.p1.1">The societal impact of jailbroken LLMs is profound and multifaceted, posing significant challenges to individuals, organizations, and governments <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib80" title="">80</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib18" title="">18</a>]</cite>. When safeguards in LLMs are bypassed, these systems can be manipulated to generate harmful, unethical, or malicious content. This includes the creation of disinformation campaigns, hate speech, fraudulent schemes, and even ransomware all of which can erode trust in digital communication <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib86" title="">86</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib92" title="">92</a>]</cite>.
For instance, researchers demonstrated that a jailbroken model could produce targeted phishing emails that mimic official communication from trusted organizations, making it easier to deceive victims <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib37" title="">37</a>]</cite>.
Researchers have shown that LLMs can create very convincing fake news articles about public health crises <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib83" title="">83</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib84" title="">84</a>]</cite>, which could contribute to vaccine hesitancy and public panic if disseminated widely.
Jailbroken LLMs have been shown to generate ransomware scripts or tools to exploit software vulnerabilities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib10" title="">10</a>]</cite>. For instance, LLMs have been shown to generate malware <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib75" title="">75</a>]</cite> as well as obfuscating malicious code <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib40" title="">40</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.SS2.p2">
<p class="ltx_p" id="S1.SS2.p2.1">Beyond direct harm, jailbroken LLMs contribute to a broader erosion of societal norms and ethics. The dissemination of manipulated or harmful content can polarize communities, disrupt democratic processes, and undermine trust in institutions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib38" title="">38</a>]</cite>. For instance, the ability to create convincing fake narratives or synthetic media at scale has fueled concerns about the spread of propaganda and the manipulation of public opinion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib76" title="">76</a>]</cite>.
As society grapples with these challenges, there is an urgent need for collective efforts to mitigate the risks posed by jailbroken LLMs.</p>
</div>
<div class="ltx_para" id="S1.SS2.p3">
<p class="ltx_p" id="S1.SS2.p3.1">Mirroring this need, the European Union (EU) issued the EU AI Act which emphasizes the need for robust safeguards against AI systems such as LLMs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib73" title="">73</a>]</cite>.
The act establishes comprehensive guidelines to ensure that AI technologies, such as LLMs, are developed and deployed responsibly across various sectors.
The act mandates that the providers of such systems “<span class="ltx_text ltx_font_italic" id="S1.SS2.p3.1.1">conduct model evaluations, assess and mitigate systemic risks, conduct adversarial testing, report to the Commission on serious incidents …</span>”. This includes addressing vulnerabilities that could be exploited through jailbreaking techniques.
Our work aligns well with this directive: we develop a method to red-team LLMs by crafting novel successful jailbreak prompts against various LLMs.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="264" id="S1.F1.g1" src="x1.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>High-level concept of our fuzzing-based LLM jailbreak attack, <span class="ltx_text ltx_font_italic" id="S1.F1.2.1">JBFuzz</span>.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.3 </span>Our Goals and Contributions</h3>
<div class="ltx_para" id="S1.SS3.p1">
<p class="ltx_p" id="S1.SS3.p1.1">Our goal is to develop an
effective, automated, robust, and scalable red-teaming technique that jailbreaks LLMs while being agnostic to the target LLMs.
We do so by borrowing the idea of fuzzing from the software community and applying it to our task of jailbreaking LLMs.
Software fuzzing is an effective and automated testing technique designed to identify security vulnerabilities in software. It has been adopted widely in industrial and academic projects due to its success <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib101" title="">101</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib44" title="">44</a>]</cite>. It relies on a set of initial tests, i.e., <span class="ltx_text ltx_font_italic" id="S1.SS3.p1.1.1">seeds</span>, which are <span class="ltx_text ltx_font_italic" id="S1.SS3.p1.1.2">mutated</span> over iterations to produce new tests with the goal of covering novel regions of the program under test.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S1.F1" title="Figure 1 ‣ 1.2 Impact of Jailbroken LLMs ‣ 1 Introduction ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates our high-level idea of using a fuzzer to jailbreak LLMs.</p>
</div>
<div class="ltx_para" id="S1.SS3.p2">
<p class="ltx_p" id="S1.SS3.p2.1">However, porting software fuzzing to jailbreak LLMs is for from trivial and comes with its unique set of challenges.
First, the effectiveness of a fuzzer is greatly affected by the initial tests, i.e., the seeds. Simply using existing jailbreaking prompts as seeds for our fuzzer would be suboptimal as often, these prompts are already defended against by LLM developers (see Section <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S4.SS3" title="4.3 Novel Seed Prompt Templates ‣ 4 Methodology ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_tag">4.3</span></a>). So, we need to devise a better way to generate initial seeds.
Second, the mutation techniques used in software fuzzers are designed for binary data, making them unsuitable for our natural language text data. Additionally, software fuzzers often need tens or hundreds of thousands of iterations to reliably test the target program. This requires each fuzzing iteration to be extremely quick in order for the technique to scale well. Thus, our mutation technique needs to be extremely fast in order to realize a practical and effective fuzzer for jailbreaking LLMs.
Third,
software fuzzing techniques often rely on code instrumentation or differential execution to provide the relevant and quick feedback needed to guide the fuzzer. However, in our case, no such quick instrumentation or differential execution techniques exist, making it challenging to provide the relevant feedback needed to guide the fuzzer. This is also exacerbated by our black-box setting, where we only assume access to the final text response from the target LLM and nothing else.</p>
</div>
<div class="ltx_para" id="S1.SS3.p3">
<p class="ltx_p" id="S1.SS3.p3.3">We address these challenges by devising novel solutions aligned with our goals of realizing an effective, automated, robust, and scalable fuzzer for jailbreaking LLMs.
We overcome the limitations of effectiveness of existing jailbreaking prompts by devising Solution  <svg class="ltx_picture" height="15.74" id="S1.SS3.p3.1.pic1" overflow="visible" version="1.1" width="15.74"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,15.74) matrix(1 0 0 -1 0 0) translate(7.87,0) translate(0,7.87)"><g color="#FFFFFF" fill="#007C00" stroke="#FFFFFF"><path d="M 7.59 0 C 7.59 4.19 4.19 7.59 0 7.59 C -4.19 7.59 -7.59 4.19 -7.59 0 C -7.59 -4.19 -4.19 -7.59 0 -7.59 C 4.19 -7.59 7.59 -4.19 7.59 0 Z M 0 0"></path></g><g color="#FFFFFF" fill="#FFFFFF" stroke="#FFFFFF" transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.92"><span class="ltx_text" id="S1.SS3.p3.1.pic1.1.1.1.1.1">1</span></foreignobject></g></g></svg>: novel seed prompt templates that preserve the fundamental themes of past successful prompt templates, but with new structures and semantics. To address the need of a quick mutation mechanism, we devise Solution  <svg class="ltx_picture" height="15.74" id="S1.SS3.p3.2.pic2" overflow="visible" version="1.1" width="15.74"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,15.74) matrix(1 0 0 -1 0 0) translate(7.87,0) translate(0,7.87)"><g color="#FFFFFF" fill="#007C00" stroke="#FFFFFF"><path d="M 7.59 0 C 7.59 4.19 4.19 7.59 0 7.59 C -4.19 7.59 -7.59 4.19 -7.59 0 C -7.59 -4.19 -4.19 -7.59 0 -7.59 C 4.19 -7.59 7.59 -4.19 7.59 0 Z M 0 0"></path></g><g color="#FFFFFF" fill="#FFFFFF" stroke="#FFFFFF" transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.92"><span class="ltx_text" id="S1.SS3.p3.2.pic2.1.1.1.1.1">2</span></foreignobject></g></g></svg>, a synonym-based extremely fast and effective mutation technique that generates diverse prompt templates. Finally, to overcome the lack of a quick and reliable feedback-generation mechanism, we devise a novel embedding-based evaluator (Solution  <svg class="ltx_picture" height="15.74" id="S1.SS3.p3.3.pic3" overflow="visible" version="1.1" width="15.74"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,15.74) matrix(1 0 0 -1 0 0) translate(7.87,0) translate(0,7.87)"><g color="#FFFFFF" fill="#007C00" stroke="#FFFFFF"><path d="M 7.59 0 C 7.59 4.19 4.19 7.59 0 7.59 C -4.19 7.59 -7.59 4.19 -7.59 0 C -7.59 -4.19 -4.19 -7.59 0 -7.59 C 4.19 -7.59 7.59 -4.19 7.59 0 Z M 0 0"></path></g><g color="#FFFFFF" fill="#FFFFFF" stroke="#FFFFFF" transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.92"><span class="ltx_text" id="S1.SS3.p3.3.pic3.1.1.1.1.1">3</span></foreignobject></g></g></svg>) that is extremely fast and only relies on the text response from the target LLM, i.e., satisfies our black-box constraint. Sections <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S4.SS3" title="4.3 Novel Seed Prompt Templates ‣ 4 Methodology ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_tag">4.3</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S4.SS4" title="4.4 Synonym-based Mutation ‣ 4 Methodology ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_tag">4.4</span></a>, and <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S4.SS5" title="4.5 Embedding-based Evaluation ‣ 4 Methodology ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_tag">4.5</span></a> provide more details about each of these challenges and our solutions. By incorporating these solutions, we develop an effective, automated, robust, and scalable fuzzer, <span class="ltx_text ltx_font_italic" id="S1.SS3.p3.3.1">JBFuzz</span>, for red-teaming LLMs in the context of jailbreaking. In summary, our primary contributions are:</p>
</div>
<div class="ltx_para" id="S1.SS3.p4">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We present a novel lightweight fuzzing technique, <span class="ltx_text ltx_font_italic" id="S1.I1.i1.p1.1.1">JBFuzz</span>, for automating the red-teaming of LLMs in the context of jailbreaking attacks. Figure <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S1.F1" title="Figure 1 ‣ 1.2 Impact of Jailbroken LLMs ‣ 1 Introduction ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates the high-level concept of our work.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S1.I1.i2.p1.1.1">JBFuzz</span> is agnostic to the target LLM and operates in a black-box setting, i.e., it does not require any information from the target LLM apart from response to given prompts.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We overcome challenges related to effectiveness and efficiency when porting software fuzzing to jailbreaking LLMs by devising custom solutions that help us realize an effective, automated, and scalable red-teaming technique (Sections <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S4.SS3" title="4.3 Novel Seed Prompt Templates ‣ 4 Methodology ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_tag">4.3</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S4.SS4" title="4.4 Synonym-based Mutation ‣ 4 Methodology ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_tag">4.4</span></a>, and <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S4.SS5" title="4.5 Embedding-based Evaluation ‣ 4 Methodology ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_tag">4.5</span></a>).</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">We perform a thorough evaluation of <span class="ltx_text ltx_font_italic" id="S1.I1.i4.p1.1.1">JBFuzz</span> against various popular closed- and open-source LLMs. Our results demonstrate that <span class="ltx_text ltx_font_italic" id="S1.I1.i4.p1.1.2">JBFuzz</span> successfully jailbreaks all tested LLMs with an average success rate of 99% for a diverse dataset of harmful/unethical behaviors (Section <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S5.SS5" title="5.5 Main Results ‣ 5 Results ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_tag">5.5</span></a>).</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i5.p1">
<p class="ltx_p" id="S1.I1.i5.p1.1">We also demonstrate the efficiency of <span class="ltx_text ltx_font_italic" id="S1.I1.i5.p1.1.1">JBFuzz</span> in terms of runtime and LLM query costs: On average, for a given harmful/unethical question, <span class="ltx_text ltx_font_italic" id="S1.I1.i5.p1.1.2">JBFuzz</span> jailbreaks a target LLM within 60 seconds with only <math alttext="\approx" class="ltx_Math" display="inline" id="S1.I1.i5.p1.1.m1.1"><semantics id="S1.I1.i5.p1.1.m1.1a"><mo id="S1.I1.i5.p1.1.m1.1.1" xref="S1.I1.i5.p1.1.m1.1.1.cmml">≈</mo><annotation-xml encoding="MathML-Content" id="S1.I1.i5.p1.1.m1.1b"><approx id="S1.I1.i5.p1.1.m1.1.1.cmml" xref="S1.I1.i5.p1.1.m1.1.1"></approx></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i5.p1.1.m1.1c">\approx</annotation><annotation encoding="application/x-llamapun" id="S1.I1.i5.p1.1.m1.1d">≈</annotation></semantics></math>7 queries (Section <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S5.SS5" title="5.5 Main Results ‣ 5 Results ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_tag">5.5</span></a>). <span class="ltx_text ltx_font_italic" id="S1.I1.i5.p1.1.3">JBFuzz</span>’s efficiency is further reinforced with the fact that 94% of the runtime is spent waiting to get a response from the target LLM (which is unavoidable), and only 6% of the runtime is spent on the fuzzer’s operations (Section <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S5.SS8" title="5.8 Runtime Breakdown Analysis ‣ 5 Results ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_tag">5.8</span></a>).</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Before we delve into the details of our technique, we provide some necessary background information about LLMs, jailbreaking them, and fuzzing.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>LLMs</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">LLMs are advanced artificial intelligence systems designed to process and generate human-like text <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib62" title="">62</a>]</cite>. These models are built on deep learning architectures, such as transformer neural networks, which excel at understanding and producing complex language patterns <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib87" title="">87</a>]</cite>. The foundation of LLMs lies in training them on vast datasets of text sourced from books, articles, websites, and other digital repositories.
During training, the models learn to predict the next word—or more precisely, the next token—in a sequence based on the context. Tokens are the basic units of text that the model processes, which can range from individual characters to words or subwords, depending on the tokenizer used. By learning patterns and relationships between these tokens, the model gains the ability to generate coherent and contextually appropriate responses.
Over time, advancements like massive-scale training (using trillions of tokens and hundreds of billions of parameters) and fine-tuning have significantly improved the accuracy and fluency of LLMs, making them capable of tasks ranging from writing essays to answering technical questions, coding, and even engaging in creative tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib48" title="">48</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib72" title="">72</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Jailbreaking LLMs</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">LLMs are typically prompted with a textual input that provides instructions or asks questions, and they respond by generating text based on their training and embedded safeguards.
At the heart of jailbreaking LLMs lies the tension between the model’s dual objectives: following user instructions to be as helpful as possible and adhering to its inherent programming to avoid generating harmful, unethical, or restricted content. Jailbreaking involves crafting prompts or instructions that exploit this tension, effectively bypassing safeguards and enabling the model to produce content that is harmful, unethical, or otherwise restricted.
Our work focuses on jailbreaking in the context of eliciting harmful/unethical responses from target LLMs.
To test and improve LLM safety, developers employ various methods, including red-teaming efforts and crowd-sourced testing, where diverse groups of users attempt to uncover vulnerabilities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib58" title="">58</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib59" title="">59</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib49" title="">49</a>]</cite>. Among the most effective methods for jailbreaking are manually crafted prompt templates, where researchers carefully design templates through iterative refinement to exploit weaknesses in the model’s instruction-following behavior <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib56" title="">56</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib81" title="">81</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib91" title="">91</a>]</cite>. These carefully crafted prompt templates are combined with the target question or task to provoke harmful/unethical responses.
However, such manual efforts present significant challenges in terms of scalability, as crafting effective jailbreak prompts is time-intensive and relies heavily on human expertise. As LLMs evolve, the variability in model behavior and the vast array of potential vulnerabilities make it increasingly difficult to comprehensively test their safety using solely manual approaches, highlighting the need for more automated and scalable solutions.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Fuzzing</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Fuzzing is an automated testing technique used to uncover vulnerabilities, bugs, or unexpected behaviors in software applications <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib101" title="">101</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib89" title="">89</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib104" title="">104</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib11" title="">11</a>]</cite>.
Evolutionary fuzzing, a common approach, begins with a set of seed inputs, i.e., valid examples of program inputs, that are stored in a seed pool. These seeds are selected and mutated to create variations, such as altering bytes, flipping bits, or changing structures, to generate new test cases <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib101" title="">101</a>]</cite>. These mutated inputs are then executed against the target program, and the program’s behavior is evaluated for crashes, errors, or unexpected outputs. This iterative process helps identify edge cases and vulnerabilities in software, making mutation-based fuzzing a powerful tool for improving software security and robustness.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">Fuzzing has gained immense popularity in both industry and academic projects. Tools like AFL, libFuzzer, ClusterFuzz, and HonggFuzz are widely used in industry for finding vulnerabilities in critical software such as browsers, operating systems, and cryptographic libraries <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib101" title="">101</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib44" title="">44</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib29" title="">29</a>]</cite>. For example, Google’s ClusterFuzz has been instrumental in identifying thousands of bugs in Chrome and other software through continuous automated fuzzing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib29" title="">29</a>]</cite>.
This widespread adoption underscores fuzzing’s critical role in ensuring software security and reliability.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Threat Model</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We consider a black-box threat model in the context of jailbreaking LLMs. In this context, we make the following assumptions about the attacker, i.e., the red-team.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p2">
<p class="ltx_p" id="S3.p2.1"><span class="ltx_text ltx_font_bold" id="S3.p2.1.1">Attacker’s Goal.</span>
The primary goal of the attacker is to manipulate the behavior of the target LLM such that it produces harmful/unethical responses rather than refusal messages aligned with human values. This includes but is not limited to generating explicit or harmful content (e.g., hate speech, violent instructions, misinformation, etc.), and providing information that violates the ethical constraints set by the LLM’s developers.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">The attacker does so by crafting effective prompt templates, that when combined with the questions, result in jailbreak. For instance, if the harmful question is “What are some effective ways to cyberbully someone until they harm themselves?”, the jailbroken response should provide some ways to do this.
Note that our focus is not on the quality of the jailbroken responses, i.e., how well the response helps achieve the desired harmful action. Rather, we focus on the whether or not, and to what extent LLMs are susceptible to jailbreak.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p4">
<p class="ltx_p" id="S3.p4.1"><span class="ltx_text ltx_font_bold" id="S3.p4.1.1">Attacker’s Knowledge.</span> We assume a black-box setting, where the attacker does not have access to the target LLM’s internal details such as its architecture, training data, or fine-tuning processes. The attacker is also unaware of the specific safeguards, heuristics, or reinforcement learning mechanisms embedded in the LLM to prevent jailbreaking.
We also assume that the attacker can only observe the final response from the target LLM, not other auxiliary outputs, such as logprobs.
However, the attacker knows the general purpose of the LLM and its public-facing capabilities. The attacker also has access to public documentation or API guidelines, which may include information about usage policies, rate limits, and input/output formats.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p5">
<p class="ltx_p" id="S3.p5.1"><span class="ltx_text ltx_font_bold" id="S3.p5.1.1">Attacker’s Capacity.</span>
We assume an attacker with the computational resources of a general user (i.e., not large organizations or nation-state attackers).
So, the attacker has sufficient computational resources to issue a large number of queries to the LLM but is potentially limited by rate-limiting measures or cost considerations imposed by the LLM’s API provider.
The attack happens after the LLM has been trained. The LLM is fixed and the attacker cannot change the model parameters (i.e. weights) or structure. In particular, the attacker cannot poison the LLM and inject backdoors in the LLM.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p6">
<p class="ltx_p" id="S3.p6.1"><span class="ltx_text ltx_font_bold" id="S3.p6.1.1">Attacker’s Abilities.</span>
The attacker can query the LLM with arbitrary prompts and observe the text responses from the LLM.</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Methodology</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this section, we first motivate the need for fuzzing and justify its propriety for our task of jailbreaking LLMs (Section <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S4.SS1" title="4.1 Why Fuzzing? ‣ 4 Methodology ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_tag">4.1</span></a>).
Then, we provide a preliminary fuzzing formulation for jailbreaking a given target LLM, i.e., attempt to get a harmful/unethical response from the LLM by prompting it (Section <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S4.SS2" title="4.2 Preliminary Formulation ‣ 4 Methodology ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_tag">4.2</span></a>).
Then, we delve into some effectiveness and scalability related limitations of this preliminary formulation. For each of those limitations, we devise solutions to address them (Sections <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S4.SS3" title="4.3 Novel Seed Prompt Templates ‣ 4 Methodology ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_tag">4.3</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S4.SS4" title="4.4 Synonym-based Mutation ‣ 4 Methodology ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_tag">4.4</span></a>, and <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S4.SS5" title="4.5 Embedding-based Evaluation ‣ 4 Methodology ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_tag">4.5</span></a>) and finally put it all together to describe our final fuzzing formulation which is an effective, automated, and scalable jailbreaking framework that successfully jailbreaks several state-of-the-art LLMs (Section <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S4.SS6" title="4.6 Final Formulation ‣ 4 Methodology ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_tag">4.6</span></a>).</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Why Fuzzing?</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Before describing the details of our fuzzing formulation, it is important to understand why we leverage fuzzing for jailbreaking LLMs. Simply put, in our context, fuzzing is an automated feedback-guided testing technique.
Although other techniques such as manual testing, random regression, or model-based formal verification perform well in certain cases, fuzzing really shines when the target use case has some/all the following features:
(i) large search space, (ii) reliable and automated test coverage evaluator, (iii) unanticipated edge cases or rare scenarios, or/and (iv) lack of formal models.<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Note that often, even when formal models are available for testing, fuzzing is preferred since formal methods struggle with scalability.</span></span></span></p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">In our red-teaming task, the target system is an LLM and our objective is to jailbreak it by prompting it.
However, the vocabularies of modern LLMs are extremely large (thousands to hundreds of thousands of tokens), leading to an <span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.1">astronomically large search space</span> of input prompts (which are typically composed of several tens to hundreds of tokens) even if we only restrict the prompts to contain valid English dictionary words.
Moreover, research has shown that GPT-4 can provide a <span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.2">reliable and automated evaluation</span> if a response from an LLM is harmful/unethical <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib54" title="">54</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib53" title="">53</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib99" title="">99</a>]</cite>.
Additionally, the fact that LLMs responses rely on trainings over vast amounts of data and not on explicitly defined/programmed rules, compounded by the virtually limitless prompts that can be input to them, results in a <span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.3">large number of untested edge cases</span>.
Finally, it is extremely <span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.4">challenging to build a formal model</span> for verifying the safety properties of an LLM due to the LLMs’ large computational complexity, highly non-linear nature, and extremely limited explainability about their outputs.
Thus, our red-teaming task has all the features described above, making fuzzing a good fit for discovering jailbreaking vulnerabilities in LLMs.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Preliminary Formulation</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">As explained above in Section <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S4.SS1" title="4.1 Why Fuzzing? ‣ 4 Methodology ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_tag">4.1</span></a>, there are several features of the prompt template generation task for jailbreaking LLMs that make fuzzing a good fit for the task.
So, to find good prompt templates, i.e., templates that jailbreak a target LLM, we map the different aspects of the fuzzing process to our prompt template generation task as follows:</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i1.p1.1.1">Seed Pool</span>
is the staring point of the fuzzing process. It consists of a set of input data files or test cases (called “seeds”) that represent valid or semi-valid inputs to the target program. Since our target entity is an LLM, the seeds are natural language prompt templates that (after combining with the given harmful/unethical question) can be input to the target LLM.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i2.p1.1.1">Initial Seeds <math alttext="\mathcal{S}" class="ltx_Math" display="inline" id="S4.I1.i2.p1.1.1.m1.1"><semantics id="S4.I1.i2.p1.1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S4.I1.i2.p1.1.1.m1.1.1" xref="S4.I1.i2.p1.1.1.m1.1.1.cmml">𝒮</mi><annotation-xml encoding="MathML-Content" id="S4.I1.i2.p1.1.1.m1.1b"><ci id="S4.I1.i2.p1.1.1.m1.1.1.cmml" xref="S4.I1.i2.p1.1.1.m1.1.1">𝒮</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i2.p1.1.1.m1.1c">\mathcal{S}</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i2.p1.1.1.m1.1d">caligraphic_S</annotation></semantics></math></span>
are the starting inputs used to populate the seed pool before fuzzing begins, i.e., to bootstrap the fuzzer.
These initial seeds can be obtained in several different ways, e.g., software fuzzers use randomly generated seeds, constrained seeds that contain desired features/data structures, or even manually crafted user-provided seeds.
In our case, the initial seeds are a set of initial prompt templates. Hence, using randomly generated template strings is not ideal since they would lack meaning and likely result in either garbage or useless responses from the target LLM. So, we use manually crafted prompt templates that researchers/red-teamers have developed as initial seeds.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.3"><span class="ltx_text ltx_font_bold" id="S4.I1.i3.p1.1.1">Seed Selection <math alttext="\mathbb{S}" class="ltx_Math" display="inline" id="S4.I1.i3.p1.1.1.m1.1"><semantics id="S4.I1.i3.p1.1.1.m1.1a"><mi id="S4.I1.i3.p1.1.1.m1.1.1" xref="S4.I1.i3.p1.1.1.m1.1.1.cmml">𝕊</mi><annotation-xml encoding="MathML-Content" id="S4.I1.i3.p1.1.1.m1.1b"><ci id="S4.I1.i3.p1.1.1.m1.1.1.cmml" xref="S4.I1.i3.p1.1.1.m1.1.1">𝕊</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i3.p1.1.1.m1.1c">\mathbb{S}</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i3.p1.1.1.m1.1d">blackboard_S</annotation></semantics></math></span> is the mechanism using which the fuzzer selects a seed, <math alttext="s" class="ltx_Math" display="inline" id="S4.I1.i3.p1.2.m1.1"><semantics id="S4.I1.i3.p1.2.m1.1a"><mi id="S4.I1.i3.p1.2.m1.1.1" xref="S4.I1.i3.p1.2.m1.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S4.I1.i3.p1.2.m1.1b"><ci id="S4.I1.i3.p1.2.m1.1.1.cmml" xref="S4.I1.i3.p1.2.m1.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i3.p1.2.m1.1c">s</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i3.p1.2.m1.1d">italic_s</annotation></semantics></math>, from the available seeds for the current fuzzing iteration. Since this mechanism is agnostic to the seed format (i.e., software programs or natural language text), we can any existing seed selection algorithm (e.g., random, weighted-random, or even historical performance based algorithms such as <math alttext="\varepsilon" class="ltx_Math" display="inline" id="S4.I1.i3.p1.3.m2.1"><semantics id="S4.I1.i3.p1.3.m2.1a"><mi id="S4.I1.i3.p1.3.m2.1.1" xref="S4.I1.i3.p1.3.m2.1.1.cmml">ε</mi><annotation-xml encoding="MathML-Content" id="S4.I1.i3.p1.3.m2.1b"><ci id="S4.I1.i3.p1.3.m2.1.1.cmml" xref="S4.I1.i3.p1.3.m2.1.1">𝜀</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i3.p1.3.m2.1c">\varepsilon</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i3.p1.3.m2.1d">italic_ε</annotation></semantics></math>-greedy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib85" title="">85</a>]</cite> or Upper Confidence Bound (UCB) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib6" title="">6</a>]</cite>). In the preliminary formulation, we use the UCB algorithm since it has been known to leverage historical performance to well balance the exploration-exploitation trade-off <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib6" title="">6</a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i4.p1">
<p class="ltx_p" id="S4.I1.i4.p1.3"><span class="ltx_text ltx_font_bold" id="S4.I1.i4.p1.1.1">Mutation <math alttext="\mathbb{M}" class="ltx_Math" display="inline" id="S4.I1.i4.p1.1.1.m1.1"><semantics id="S4.I1.i4.p1.1.1.m1.1a"><mi id="S4.I1.i4.p1.1.1.m1.1.1" xref="S4.I1.i4.p1.1.1.m1.1.1.cmml">𝕄</mi><annotation-xml encoding="MathML-Content" id="S4.I1.i4.p1.1.1.m1.1b"><ci id="S4.I1.i4.p1.1.1.m1.1.1.cmml" xref="S4.I1.i4.p1.1.1.m1.1.1">𝕄</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i4.p1.1.1.m1.1c">\mathbb{M}</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i4.p1.1.1.m1.1d">blackboard_M</annotation></semantics></math></span> is the process of generating a mutated seed, <math alttext="m" class="ltx_Math" display="inline" id="S4.I1.i4.p1.2.m1.1"><semantics id="S4.I1.i4.p1.2.m1.1a"><mi id="S4.I1.i4.p1.2.m1.1.1" xref="S4.I1.i4.p1.2.m1.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S4.I1.i4.p1.2.m1.1b"><ci id="S4.I1.i4.p1.2.m1.1.1.cmml" xref="S4.I1.i4.p1.2.m1.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i4.p1.2.m1.1c">m</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i4.p1.2.m1.1d">italic_m</annotation></semantics></math>, by modifying the selected seed, <math alttext="s" class="ltx_Math" display="inline" id="S4.I1.i4.p1.3.m2.1"><semantics id="S4.I1.i4.p1.3.m2.1a"><mi id="S4.I1.i4.p1.3.m2.1.1" xref="S4.I1.i4.p1.3.m2.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S4.I1.i4.p1.3.m2.1b"><ci id="S4.I1.i4.p1.3.m2.1.1.cmml" xref="S4.I1.i4.p1.3.m2.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i4.p1.3.m2.1c">s</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i4.p1.3.m2.1d">italic_s</annotation></semantics></math>. Software fuzzers typically work with binary data (i.e., bits). So, their mutation strategies involve flipping (i.e., inverting) some bits, incrementing or decrementing a set of bits, overwriting some set of bits with random values, deleting or duplicating a set of bits, etc. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib101" title="">101</a>]</cite>. Such existing strategies are not suitable for our case, since directly applying them to natural language prompt templates is very likely to generate semantically nonsensical and syntactically incorrect strings, and would result in useless responses from the target LLM. So, capturing the essence of mutation, we use an LLM to mutate the prompt templates in our preliminary formulation. We use an LLM as a mutator since LLMs are adept at natural language understanding and are capable of following user instructions and generating coherent texts.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i5.p1">
<p class="ltx_p" id="S4.I1.i5.p1.3"><span class="ltx_text ltx_font_bold" id="S4.I1.i5.p1.1.1">Execution <math alttext="\mathbb{EX}" class="ltx_Math" display="inline" id="S4.I1.i5.p1.1.1.m1.1"><semantics id="S4.I1.i5.p1.1.1.m1.1a"><mrow id="S4.I1.i5.p1.1.1.m1.1.1" xref="S4.I1.i5.p1.1.1.m1.1.1.cmml"><mi id="S4.I1.i5.p1.1.1.m1.1.1.2" xref="S4.I1.i5.p1.1.1.m1.1.1.2.cmml">𝔼</mi><mo id="S4.I1.i5.p1.1.1.m1.1.1.1" xref="S4.I1.i5.p1.1.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.I1.i5.p1.1.1.m1.1.1.3" xref="S4.I1.i5.p1.1.1.m1.1.1.3.cmml">𝕏</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.I1.i5.p1.1.1.m1.1b"><apply id="S4.I1.i5.p1.1.1.m1.1.1.cmml" xref="S4.I1.i5.p1.1.1.m1.1.1"><times id="S4.I1.i5.p1.1.1.m1.1.1.1.cmml" xref="S4.I1.i5.p1.1.1.m1.1.1.1"></times><ci id="S4.I1.i5.p1.1.1.m1.1.1.2.cmml" xref="S4.I1.i5.p1.1.1.m1.1.1.2">𝔼</ci><ci id="S4.I1.i5.p1.1.1.m1.1.1.3.cmml" xref="S4.I1.i5.p1.1.1.m1.1.1.3">𝕏</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i5.p1.1.1.m1.1c">\mathbb{EX}</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i5.p1.1.1.m1.1d">blackboard_E blackboard_X</annotation></semantics></math></span> involves taking the mutated seed, <math alttext="m" class="ltx_Math" display="inline" id="S4.I1.i5.p1.2.m1.1"><semantics id="S4.I1.i5.p1.2.m1.1a"><mi id="S4.I1.i5.p1.2.m1.1.1" xref="S4.I1.i5.p1.2.m1.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S4.I1.i5.p1.2.m1.1b"><ci id="S4.I1.i5.p1.2.m1.1.1.cmml" xref="S4.I1.i5.p1.2.m1.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i5.p1.2.m1.1c">m</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i5.p1.2.m1.1d">italic_m</annotation></semantics></math> and executing the target program with it in order to monitor the program’s behavior.
In our case, the target entity is the target LLM that we wish to jailbreak. So, execution, in our case, is just prompting the target LLM with the mutated prompt template (after replacing the placeholder such as “<span class="ltx_text ltx_font_typewriter" id="S4.I1.i5.p1.3.2">[INSERT PROMPT HERE]</span>” with the given harmful/unethical question, <math alttext="q" class="ltx_Math" display="inline" id="S4.I1.i5.p1.3.m2.1"><semantics id="S4.I1.i5.p1.3.m2.1a"><mi id="S4.I1.i5.p1.3.m2.1.1" xref="S4.I1.i5.p1.3.m2.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S4.I1.i5.p1.3.m2.1b"><ci id="S4.I1.i5.p1.3.m2.1.1.cmml" xref="S4.I1.i5.p1.3.m2.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i5.p1.3.m2.1c">q</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i5.p1.3.m2.1d">italic_q</annotation></semantics></math>).</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i6.p1">
<p class="ltx_p" id="S4.I1.i6.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i6.p1.1.1">Evaluation <math alttext="\mathbb{EV}" class="ltx_Math" display="inline" id="S4.I1.i6.p1.1.1.m1.1"><semantics id="S4.I1.i6.p1.1.1.m1.1a"><mrow id="S4.I1.i6.p1.1.1.m1.1.1" xref="S4.I1.i6.p1.1.1.m1.1.1.cmml"><mi id="S4.I1.i6.p1.1.1.m1.1.1.2" xref="S4.I1.i6.p1.1.1.m1.1.1.2.cmml">𝔼</mi><mo id="S4.I1.i6.p1.1.1.m1.1.1.1" xref="S4.I1.i6.p1.1.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.I1.i6.p1.1.1.m1.1.1.3" xref="S4.I1.i6.p1.1.1.m1.1.1.3.cmml">𝕍</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.I1.i6.p1.1.1.m1.1b"><apply id="S4.I1.i6.p1.1.1.m1.1.1.cmml" xref="S4.I1.i6.p1.1.1.m1.1.1"><times id="S4.I1.i6.p1.1.1.m1.1.1.1.cmml" xref="S4.I1.i6.p1.1.1.m1.1.1.1"></times><ci id="S4.I1.i6.p1.1.1.m1.1.1.2.cmml" xref="S4.I1.i6.p1.1.1.m1.1.1.2">𝔼</ci><ci id="S4.I1.i6.p1.1.1.m1.1.1.3.cmml" xref="S4.I1.i6.p1.1.1.m1.1.1.3">𝕍</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i6.p1.1.1.m1.1c">\mathbb{EV}</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i6.p1.1.1.m1.1d">blackboard_E blackboard_V</annotation></semantics></math></span> is the process of analyzing the result of executing the target program with the mutated seed to determine if any issues (bugs, crashes, etc. in software programs) are triggered. In our case, evaluation requires us to analyze the response from the target LLM to determine if the LLM has answered the harmful/unethical question, i.e., if the LLM has been jailbroken. There could be several ways to achieve this: manual evaluation, restricted questions to generate responses from a fixed set of choices (e.g., “yes”/“no”), keyword-based evaluation, or using an LLM as an evaluator. Of these, manual evaluation is clearly not scalable, restricting questions to generate fixed classes of responses limits the scope of our work, and keyword-based evaluation suffers from low accuracy. So, we use an LLM as an evaluator since state-of-the-art LLMs have shown human-level judging performance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib54" title="">54</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib99" title="">99</a>]</cite>.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.21">Here is the high-level process of our preliminary fuzzing process. The initial seeds <math alttext="\mathcal{S}" class="ltx_Math" display="inline" id="S4.SS2.p3.1.m1.1"><semantics id="S4.SS2.p3.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml">𝒮</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><ci id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1">𝒮</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">\mathcal{S}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.1.m1.1d">caligraphic_S</annotation></semantics></math> first populate the seed pool. Then, during each fuzzing iteration, <math alttext="t" class="ltx_Math" display="inline" id="S4.SS2.p3.2.m2.1"><semantics id="S4.SS2.p3.2.m2.1a"><mi id="S4.SS2.p3.2.m2.1.1" xref="S4.SS2.p3.2.m2.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.2.m2.1b"><ci id="S4.SS2.p3.2.m2.1.1.cmml" xref="S4.SS2.p3.2.m2.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.2.m2.1c">t</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.2.m2.1d">italic_t</annotation></semantics></math>, the seed selection algorithm <math alttext="\mathbb{S}" class="ltx_Math" display="inline" id="S4.SS2.p3.3.m3.1"><semantics id="S4.SS2.p3.3.m3.1a"><mi id="S4.SS2.p3.3.m3.1.1" xref="S4.SS2.p3.3.m3.1.1.cmml">𝕊</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.3.m3.1b"><ci id="S4.SS2.p3.3.m3.1.1.cmml" xref="S4.SS2.p3.3.m3.1.1">𝕊</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.3.m3.1c">\mathbb{S}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.3.m3.1d">blackboard_S</annotation></semantics></math> selects a seed <math alttext="s_{t}" class="ltx_Math" display="inline" id="S4.SS2.p3.4.m4.1"><semantics id="S4.SS2.p3.4.m4.1a"><msub id="S4.SS2.p3.4.m4.1.1" xref="S4.SS2.p3.4.m4.1.1.cmml"><mi id="S4.SS2.p3.4.m4.1.1.2" xref="S4.SS2.p3.4.m4.1.1.2.cmml">s</mi><mi id="S4.SS2.p3.4.m4.1.1.3" xref="S4.SS2.p3.4.m4.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.4.m4.1b"><apply id="S4.SS2.p3.4.m4.1.1.cmml" xref="S4.SS2.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.4.m4.1.1.1.cmml" xref="S4.SS2.p3.4.m4.1.1">subscript</csymbol><ci id="S4.SS2.p3.4.m4.1.1.2.cmml" xref="S4.SS2.p3.4.m4.1.1.2">𝑠</ci><ci id="S4.SS2.p3.4.m4.1.1.3.cmml" xref="S4.SS2.p3.4.m4.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.4.m4.1c">s_{t}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.4.m4.1d">italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> from the current seed pool <math alttext="\mathcal{S}" class="ltx_Math" display="inline" id="S4.SS2.p3.5.m5.1"><semantics id="S4.SS2.p3.5.m5.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p3.5.m5.1.1" xref="S4.SS2.p3.5.m5.1.1.cmml">𝒮</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.5.m5.1b"><ci id="S4.SS2.p3.5.m5.1.1.cmml" xref="S4.SS2.p3.5.m5.1.1">𝒮</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.5.m5.1c">\mathcal{S}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.5.m5.1d">caligraphic_S</annotation></semantics></math>, i.e., <math alttext="\mathbb{S}(\mathcal{S})\rightarrow s_{t}" class="ltx_Math" display="inline" id="S4.SS2.p3.6.m6.1"><semantics id="S4.SS2.p3.6.m6.1a"><mrow id="S4.SS2.p3.6.m6.1.2" xref="S4.SS2.p3.6.m6.1.2.cmml"><mrow id="S4.SS2.p3.6.m6.1.2.2" xref="S4.SS2.p3.6.m6.1.2.2.cmml"><mi id="S4.SS2.p3.6.m6.1.2.2.2" xref="S4.SS2.p3.6.m6.1.2.2.2.cmml">𝕊</mi><mo id="S4.SS2.p3.6.m6.1.2.2.1" xref="S4.SS2.p3.6.m6.1.2.2.1.cmml">⁢</mo><mrow id="S4.SS2.p3.6.m6.1.2.2.3.2" xref="S4.SS2.p3.6.m6.1.2.2.cmml"><mo id="S4.SS2.p3.6.m6.1.2.2.3.2.1" stretchy="false" xref="S4.SS2.p3.6.m6.1.2.2.cmml">(</mo><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p3.6.m6.1.1" xref="S4.SS2.p3.6.m6.1.1.cmml">𝒮</mi><mo id="S4.SS2.p3.6.m6.1.2.2.3.2.2" stretchy="false" xref="S4.SS2.p3.6.m6.1.2.2.cmml">)</mo></mrow></mrow><mo id="S4.SS2.p3.6.m6.1.2.1" stretchy="false" xref="S4.SS2.p3.6.m6.1.2.1.cmml">→</mo><msub id="S4.SS2.p3.6.m6.1.2.3" xref="S4.SS2.p3.6.m6.1.2.3.cmml"><mi id="S4.SS2.p3.6.m6.1.2.3.2" xref="S4.SS2.p3.6.m6.1.2.3.2.cmml">s</mi><mi id="S4.SS2.p3.6.m6.1.2.3.3" xref="S4.SS2.p3.6.m6.1.2.3.3.cmml">t</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.6.m6.1b"><apply id="S4.SS2.p3.6.m6.1.2.cmml" xref="S4.SS2.p3.6.m6.1.2"><ci id="S4.SS2.p3.6.m6.1.2.1.cmml" xref="S4.SS2.p3.6.m6.1.2.1">→</ci><apply id="S4.SS2.p3.6.m6.1.2.2.cmml" xref="S4.SS2.p3.6.m6.1.2.2"><times id="S4.SS2.p3.6.m6.1.2.2.1.cmml" xref="S4.SS2.p3.6.m6.1.2.2.1"></times><ci id="S4.SS2.p3.6.m6.1.2.2.2.cmml" xref="S4.SS2.p3.6.m6.1.2.2.2">𝕊</ci><ci id="S4.SS2.p3.6.m6.1.1.cmml" xref="S4.SS2.p3.6.m6.1.1">𝒮</ci></apply><apply id="S4.SS2.p3.6.m6.1.2.3.cmml" xref="S4.SS2.p3.6.m6.1.2.3"><csymbol cd="ambiguous" id="S4.SS2.p3.6.m6.1.2.3.1.cmml" xref="S4.SS2.p3.6.m6.1.2.3">subscript</csymbol><ci id="S4.SS2.p3.6.m6.1.2.3.2.cmml" xref="S4.SS2.p3.6.m6.1.2.3.2">𝑠</ci><ci id="S4.SS2.p3.6.m6.1.2.3.3.cmml" xref="S4.SS2.p3.6.m6.1.2.3.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.6.m6.1c">\mathbb{S}(\mathcal{S})\rightarrow s_{t}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.6.m6.1d">blackboard_S ( caligraphic_S ) → italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>. Next, the mutation algorithm <math alttext="\mathbb{M}" class="ltx_Math" display="inline" id="S4.SS2.p3.7.m7.1"><semantics id="S4.SS2.p3.7.m7.1a"><mi id="S4.SS2.p3.7.m7.1.1" xref="S4.SS2.p3.7.m7.1.1.cmml">𝕄</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.7.m7.1b"><ci id="S4.SS2.p3.7.m7.1.1.cmml" xref="S4.SS2.p3.7.m7.1.1">𝕄</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.7.m7.1c">\mathbb{M}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.7.m7.1d">blackboard_M</annotation></semantics></math> uses the mutator LLM, <math alttext="\mathcal{L}_{\text{mut}}" class="ltx_Math" display="inline" id="S4.SS2.p3.8.m8.1"><semantics id="S4.SS2.p3.8.m8.1a"><msub id="S4.SS2.p3.8.m8.1.1" xref="S4.SS2.p3.8.m8.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p3.8.m8.1.1.2" xref="S4.SS2.p3.8.m8.1.1.2.cmml">ℒ</mi><mtext id="S4.SS2.p3.8.m8.1.1.3" xref="S4.SS2.p3.8.m8.1.1.3a.cmml">mut</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.8.m8.1b"><apply id="S4.SS2.p3.8.m8.1.1.cmml" xref="S4.SS2.p3.8.m8.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.8.m8.1.1.1.cmml" xref="S4.SS2.p3.8.m8.1.1">subscript</csymbol><ci id="S4.SS2.p3.8.m8.1.1.2.cmml" xref="S4.SS2.p3.8.m8.1.1.2">ℒ</ci><ci id="S4.SS2.p3.8.m8.1.1.3a.cmml" xref="S4.SS2.p3.8.m8.1.1.3"><mtext id="S4.SS2.p3.8.m8.1.1.3.cmml" mathsize="70%" xref="S4.SS2.p3.8.m8.1.1.3">mut</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.8.m8.1c">\mathcal{L}_{\text{mut}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.8.m8.1d">caligraphic_L start_POSTSUBSCRIPT mut end_POSTSUBSCRIPT</annotation></semantics></math>, to mutate the chosen seed <math alttext="s_{t}" class="ltx_Math" display="inline" id="S4.SS2.p3.9.m9.1"><semantics id="S4.SS2.p3.9.m9.1a"><msub id="S4.SS2.p3.9.m9.1.1" xref="S4.SS2.p3.9.m9.1.1.cmml"><mi id="S4.SS2.p3.9.m9.1.1.2" xref="S4.SS2.p3.9.m9.1.1.2.cmml">s</mi><mi id="S4.SS2.p3.9.m9.1.1.3" xref="S4.SS2.p3.9.m9.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.9.m9.1b"><apply id="S4.SS2.p3.9.m9.1.1.cmml" xref="S4.SS2.p3.9.m9.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.9.m9.1.1.1.cmml" xref="S4.SS2.p3.9.m9.1.1">subscript</csymbol><ci id="S4.SS2.p3.9.m9.1.1.2.cmml" xref="S4.SS2.p3.9.m9.1.1.2">𝑠</ci><ci id="S4.SS2.p3.9.m9.1.1.3.cmml" xref="S4.SS2.p3.9.m9.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.9.m9.1c">s_{t}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.9.m9.1d">italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> and produce the corresponding mutated seed <math alttext="m_{t}" class="ltx_Math" display="inline" id="S4.SS2.p3.10.m10.1"><semantics id="S4.SS2.p3.10.m10.1a"><msub id="S4.SS2.p3.10.m10.1.1" xref="S4.SS2.p3.10.m10.1.1.cmml"><mi id="S4.SS2.p3.10.m10.1.1.2" xref="S4.SS2.p3.10.m10.1.1.2.cmml">m</mi><mi id="S4.SS2.p3.10.m10.1.1.3" xref="S4.SS2.p3.10.m10.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.10.m10.1b"><apply id="S4.SS2.p3.10.m10.1.1.cmml" xref="S4.SS2.p3.10.m10.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.10.m10.1.1.1.cmml" xref="S4.SS2.p3.10.m10.1.1">subscript</csymbol><ci id="S4.SS2.p3.10.m10.1.1.2.cmml" xref="S4.SS2.p3.10.m10.1.1.2">𝑚</ci><ci id="S4.SS2.p3.10.m10.1.1.3.cmml" xref="S4.SS2.p3.10.m10.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.10.m10.1c">m_{t}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.10.m10.1d">italic_m start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>, i.e., <math alttext="\mathbb{M}_{\mathcal{L}_{\text{mut}}}(s_{t})\rightarrow m_{t}" class="ltx_Math" display="inline" id="S4.SS2.p3.11.m11.1"><semantics id="S4.SS2.p3.11.m11.1a"><mrow id="S4.SS2.p3.11.m11.1.1" xref="S4.SS2.p3.11.m11.1.1.cmml"><mrow id="S4.SS2.p3.11.m11.1.1.1" xref="S4.SS2.p3.11.m11.1.1.1.cmml"><msub id="S4.SS2.p3.11.m11.1.1.1.3" xref="S4.SS2.p3.11.m11.1.1.1.3.cmml"><mi id="S4.SS2.p3.11.m11.1.1.1.3.2" xref="S4.SS2.p3.11.m11.1.1.1.3.2.cmml">𝕄</mi><msub id="S4.SS2.p3.11.m11.1.1.1.3.3" xref="S4.SS2.p3.11.m11.1.1.1.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p3.11.m11.1.1.1.3.3.2" xref="S4.SS2.p3.11.m11.1.1.1.3.3.2.cmml">ℒ</mi><mtext id="S4.SS2.p3.11.m11.1.1.1.3.3.3" xref="S4.SS2.p3.11.m11.1.1.1.3.3.3a.cmml">mut</mtext></msub></msub><mo id="S4.SS2.p3.11.m11.1.1.1.2" xref="S4.SS2.p3.11.m11.1.1.1.2.cmml">⁢</mo><mrow id="S4.SS2.p3.11.m11.1.1.1.1.1" xref="S4.SS2.p3.11.m11.1.1.1.1.1.1.cmml"><mo id="S4.SS2.p3.11.m11.1.1.1.1.1.2" stretchy="false" xref="S4.SS2.p3.11.m11.1.1.1.1.1.1.cmml">(</mo><msub id="S4.SS2.p3.11.m11.1.1.1.1.1.1" xref="S4.SS2.p3.11.m11.1.1.1.1.1.1.cmml"><mi id="S4.SS2.p3.11.m11.1.1.1.1.1.1.2" xref="S4.SS2.p3.11.m11.1.1.1.1.1.1.2.cmml">s</mi><mi id="S4.SS2.p3.11.m11.1.1.1.1.1.1.3" xref="S4.SS2.p3.11.m11.1.1.1.1.1.1.3.cmml">t</mi></msub><mo id="S4.SS2.p3.11.m11.1.1.1.1.1.3" stretchy="false" xref="S4.SS2.p3.11.m11.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.SS2.p3.11.m11.1.1.2" stretchy="false" xref="S4.SS2.p3.11.m11.1.1.2.cmml">→</mo><msub id="S4.SS2.p3.11.m11.1.1.3" xref="S4.SS2.p3.11.m11.1.1.3.cmml"><mi id="S4.SS2.p3.11.m11.1.1.3.2" xref="S4.SS2.p3.11.m11.1.1.3.2.cmml">m</mi><mi id="S4.SS2.p3.11.m11.1.1.3.3" xref="S4.SS2.p3.11.m11.1.1.3.3.cmml">t</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.11.m11.1b"><apply id="S4.SS2.p3.11.m11.1.1.cmml" xref="S4.SS2.p3.11.m11.1.1"><ci id="S4.SS2.p3.11.m11.1.1.2.cmml" xref="S4.SS2.p3.11.m11.1.1.2">→</ci><apply id="S4.SS2.p3.11.m11.1.1.1.cmml" xref="S4.SS2.p3.11.m11.1.1.1"><times id="S4.SS2.p3.11.m11.1.1.1.2.cmml" xref="S4.SS2.p3.11.m11.1.1.1.2"></times><apply id="S4.SS2.p3.11.m11.1.1.1.3.cmml" xref="S4.SS2.p3.11.m11.1.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p3.11.m11.1.1.1.3.1.cmml" xref="S4.SS2.p3.11.m11.1.1.1.3">subscript</csymbol><ci id="S4.SS2.p3.11.m11.1.1.1.3.2.cmml" xref="S4.SS2.p3.11.m11.1.1.1.3.2">𝕄</ci><apply id="S4.SS2.p3.11.m11.1.1.1.3.3.cmml" xref="S4.SS2.p3.11.m11.1.1.1.3.3"><csymbol cd="ambiguous" id="S4.SS2.p3.11.m11.1.1.1.3.3.1.cmml" xref="S4.SS2.p3.11.m11.1.1.1.3.3">subscript</csymbol><ci id="S4.SS2.p3.11.m11.1.1.1.3.3.2.cmml" xref="S4.SS2.p3.11.m11.1.1.1.3.3.2">ℒ</ci><ci id="S4.SS2.p3.11.m11.1.1.1.3.3.3a.cmml" xref="S4.SS2.p3.11.m11.1.1.1.3.3.3"><mtext id="S4.SS2.p3.11.m11.1.1.1.3.3.3.cmml" mathsize="50%" xref="S4.SS2.p3.11.m11.1.1.1.3.3.3">mut</mtext></ci></apply></apply><apply id="S4.SS2.p3.11.m11.1.1.1.1.1.1.cmml" xref="S4.SS2.p3.11.m11.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.11.m11.1.1.1.1.1.1.1.cmml" xref="S4.SS2.p3.11.m11.1.1.1.1.1">subscript</csymbol><ci id="S4.SS2.p3.11.m11.1.1.1.1.1.1.2.cmml" xref="S4.SS2.p3.11.m11.1.1.1.1.1.1.2">𝑠</ci><ci id="S4.SS2.p3.11.m11.1.1.1.1.1.1.3.cmml" xref="S4.SS2.p3.11.m11.1.1.1.1.1.1.3">𝑡</ci></apply></apply><apply id="S4.SS2.p3.11.m11.1.1.3.cmml" xref="S4.SS2.p3.11.m11.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p3.11.m11.1.1.3.1.cmml" xref="S4.SS2.p3.11.m11.1.1.3">subscript</csymbol><ci id="S4.SS2.p3.11.m11.1.1.3.2.cmml" xref="S4.SS2.p3.11.m11.1.1.3.2">𝑚</ci><ci id="S4.SS2.p3.11.m11.1.1.3.3.cmml" xref="S4.SS2.p3.11.m11.1.1.3.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.11.m11.1c">\mathbb{M}_{\mathcal{L}_{\text{mut}}}(s_{t})\rightarrow m_{t}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.11.m11.1d">blackboard_M start_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT mut end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) → italic_m start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>. This mutated seed is then executed, i.e., used to prompt the target LLM (after replacing the question placeholder with the given actual harmful/unethical question <math alttext="q" class="ltx_Math" display="inline" id="S4.SS2.p3.12.m12.1"><semantics id="S4.SS2.p3.12.m12.1a"><mi id="S4.SS2.p3.12.m12.1.1" xref="S4.SS2.p3.12.m12.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.12.m12.1b"><ci id="S4.SS2.p3.12.m12.1.1.cmml" xref="S4.SS2.p3.12.m12.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.12.m12.1c">q</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.12.m12.1d">italic_q</annotation></semantics></math>) and obtain a response <math alttext="r_{t}" class="ltx_Math" display="inline" id="S4.SS2.p3.13.m13.1"><semantics id="S4.SS2.p3.13.m13.1a"><msub id="S4.SS2.p3.13.m13.1.1" xref="S4.SS2.p3.13.m13.1.1.cmml"><mi id="S4.SS2.p3.13.m13.1.1.2" xref="S4.SS2.p3.13.m13.1.1.2.cmml">r</mi><mi id="S4.SS2.p3.13.m13.1.1.3" xref="S4.SS2.p3.13.m13.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.13.m13.1b"><apply id="S4.SS2.p3.13.m13.1.1.cmml" xref="S4.SS2.p3.13.m13.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.13.m13.1.1.1.cmml" xref="S4.SS2.p3.13.m13.1.1">subscript</csymbol><ci id="S4.SS2.p3.13.m13.1.1.2.cmml" xref="S4.SS2.p3.13.m13.1.1.2">𝑟</ci><ci id="S4.SS2.p3.13.m13.1.1.3.cmml" xref="S4.SS2.p3.13.m13.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.13.m13.1c">r_{t}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.13.m13.1d">italic_r start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>: <math alttext="\mathbb{EX}_{\mathcal{L}_{\text{target}}}(m_{t},q)\rightarrow r_{t}" class="ltx_Math" display="inline" id="S4.SS2.p3.14.m14.2"><semantics id="S4.SS2.p3.14.m14.2a"><mrow id="S4.SS2.p3.14.m14.2.2" xref="S4.SS2.p3.14.m14.2.2.cmml"><mrow id="S4.SS2.p3.14.m14.2.2.1" xref="S4.SS2.p3.14.m14.2.2.1.cmml"><mi id="S4.SS2.p3.14.m14.2.2.1.3" xref="S4.SS2.p3.14.m14.2.2.1.3.cmml">𝔼</mi><mo id="S4.SS2.p3.14.m14.2.2.1.2" xref="S4.SS2.p3.14.m14.2.2.1.2.cmml">⁢</mo><msub id="S4.SS2.p3.14.m14.2.2.1.4" xref="S4.SS2.p3.14.m14.2.2.1.4.cmml"><mi id="S4.SS2.p3.14.m14.2.2.1.4.2" xref="S4.SS2.p3.14.m14.2.2.1.4.2.cmml">𝕏</mi><msub id="S4.SS2.p3.14.m14.2.2.1.4.3" xref="S4.SS2.p3.14.m14.2.2.1.4.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p3.14.m14.2.2.1.4.3.2" xref="S4.SS2.p3.14.m14.2.2.1.4.3.2.cmml">ℒ</mi><mtext id="S4.SS2.p3.14.m14.2.2.1.4.3.3" xref="S4.SS2.p3.14.m14.2.2.1.4.3.3a.cmml">target</mtext></msub></msub><mo id="S4.SS2.p3.14.m14.2.2.1.2a" xref="S4.SS2.p3.14.m14.2.2.1.2.cmml">⁢</mo><mrow id="S4.SS2.p3.14.m14.2.2.1.1.1" xref="S4.SS2.p3.14.m14.2.2.1.1.2.cmml"><mo id="S4.SS2.p3.14.m14.2.2.1.1.1.2" stretchy="false" xref="S4.SS2.p3.14.m14.2.2.1.1.2.cmml">(</mo><msub id="S4.SS2.p3.14.m14.2.2.1.1.1.1" xref="S4.SS2.p3.14.m14.2.2.1.1.1.1.cmml"><mi id="S4.SS2.p3.14.m14.2.2.1.1.1.1.2" xref="S4.SS2.p3.14.m14.2.2.1.1.1.1.2.cmml">m</mi><mi id="S4.SS2.p3.14.m14.2.2.1.1.1.1.3" xref="S4.SS2.p3.14.m14.2.2.1.1.1.1.3.cmml">t</mi></msub><mo id="S4.SS2.p3.14.m14.2.2.1.1.1.3" xref="S4.SS2.p3.14.m14.2.2.1.1.2.cmml">,</mo><mi id="S4.SS2.p3.14.m14.1.1" xref="S4.SS2.p3.14.m14.1.1.cmml">q</mi><mo id="S4.SS2.p3.14.m14.2.2.1.1.1.4" stretchy="false" xref="S4.SS2.p3.14.m14.2.2.1.1.2.cmml">)</mo></mrow></mrow><mo id="S4.SS2.p3.14.m14.2.2.2" stretchy="false" xref="S4.SS2.p3.14.m14.2.2.2.cmml">→</mo><msub id="S4.SS2.p3.14.m14.2.2.3" xref="S4.SS2.p3.14.m14.2.2.3.cmml"><mi id="S4.SS2.p3.14.m14.2.2.3.2" xref="S4.SS2.p3.14.m14.2.2.3.2.cmml">r</mi><mi id="S4.SS2.p3.14.m14.2.2.3.3" xref="S4.SS2.p3.14.m14.2.2.3.3.cmml">t</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.14.m14.2b"><apply id="S4.SS2.p3.14.m14.2.2.cmml" xref="S4.SS2.p3.14.m14.2.2"><ci id="S4.SS2.p3.14.m14.2.2.2.cmml" xref="S4.SS2.p3.14.m14.2.2.2">→</ci><apply id="S4.SS2.p3.14.m14.2.2.1.cmml" xref="S4.SS2.p3.14.m14.2.2.1"><times id="S4.SS2.p3.14.m14.2.2.1.2.cmml" xref="S4.SS2.p3.14.m14.2.2.1.2"></times><ci id="S4.SS2.p3.14.m14.2.2.1.3.cmml" xref="S4.SS2.p3.14.m14.2.2.1.3">𝔼</ci><apply id="S4.SS2.p3.14.m14.2.2.1.4.cmml" xref="S4.SS2.p3.14.m14.2.2.1.4"><csymbol cd="ambiguous" id="S4.SS2.p3.14.m14.2.2.1.4.1.cmml" xref="S4.SS2.p3.14.m14.2.2.1.4">subscript</csymbol><ci id="S4.SS2.p3.14.m14.2.2.1.4.2.cmml" xref="S4.SS2.p3.14.m14.2.2.1.4.2">𝕏</ci><apply id="S4.SS2.p3.14.m14.2.2.1.4.3.cmml" xref="S4.SS2.p3.14.m14.2.2.1.4.3"><csymbol cd="ambiguous" id="S4.SS2.p3.14.m14.2.2.1.4.3.1.cmml" xref="S4.SS2.p3.14.m14.2.2.1.4.3">subscript</csymbol><ci id="S4.SS2.p3.14.m14.2.2.1.4.3.2.cmml" xref="S4.SS2.p3.14.m14.2.2.1.4.3.2">ℒ</ci><ci id="S4.SS2.p3.14.m14.2.2.1.4.3.3a.cmml" xref="S4.SS2.p3.14.m14.2.2.1.4.3.3"><mtext id="S4.SS2.p3.14.m14.2.2.1.4.3.3.cmml" mathsize="50%" xref="S4.SS2.p3.14.m14.2.2.1.4.3.3">target</mtext></ci></apply></apply><interval closure="open" id="S4.SS2.p3.14.m14.2.2.1.1.2.cmml" xref="S4.SS2.p3.14.m14.2.2.1.1.1"><apply id="S4.SS2.p3.14.m14.2.2.1.1.1.1.cmml" xref="S4.SS2.p3.14.m14.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.14.m14.2.2.1.1.1.1.1.cmml" xref="S4.SS2.p3.14.m14.2.2.1.1.1.1">subscript</csymbol><ci id="S4.SS2.p3.14.m14.2.2.1.1.1.1.2.cmml" xref="S4.SS2.p3.14.m14.2.2.1.1.1.1.2">𝑚</ci><ci id="S4.SS2.p3.14.m14.2.2.1.1.1.1.3.cmml" xref="S4.SS2.p3.14.m14.2.2.1.1.1.1.3">𝑡</ci></apply><ci id="S4.SS2.p3.14.m14.1.1.cmml" xref="S4.SS2.p3.14.m14.1.1">𝑞</ci></interval></apply><apply id="S4.SS2.p3.14.m14.2.2.3.cmml" xref="S4.SS2.p3.14.m14.2.2.3"><csymbol cd="ambiguous" id="S4.SS2.p3.14.m14.2.2.3.1.cmml" xref="S4.SS2.p3.14.m14.2.2.3">subscript</csymbol><ci id="S4.SS2.p3.14.m14.2.2.3.2.cmml" xref="S4.SS2.p3.14.m14.2.2.3.2">𝑟</ci><ci id="S4.SS2.p3.14.m14.2.2.3.3.cmml" xref="S4.SS2.p3.14.m14.2.2.3.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.14.m14.2c">\mathbb{EX}_{\mathcal{L}_{\text{target}}}(m_{t},q)\rightarrow r_{t}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.14.m14.2d">blackboard_E blackboard_X start_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT target end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_m start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_q ) → italic_r start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>. Next, the evaluator algorithm <math alttext="\mathbb{E}" class="ltx_Math" display="inline" id="S4.SS2.p3.15.m15.1"><semantics id="S4.SS2.p3.15.m15.1a"><mi id="S4.SS2.p3.15.m15.1.1" xref="S4.SS2.p3.15.m15.1.1.cmml">𝔼</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.15.m15.1b"><ci id="S4.SS2.p3.15.m15.1.1.cmml" xref="S4.SS2.p3.15.m15.1.1">𝔼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.15.m15.1c">\mathbb{E}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.15.m15.1d">blackboard_E</annotation></semantics></math> uses the evaluator LLM, <math alttext="\mathcal{L}_{\text{eval}}" class="ltx_Math" display="inline" id="S4.SS2.p3.16.m16.1"><semantics id="S4.SS2.p3.16.m16.1a"><msub id="S4.SS2.p3.16.m16.1.1" xref="S4.SS2.p3.16.m16.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p3.16.m16.1.1.2" xref="S4.SS2.p3.16.m16.1.1.2.cmml">ℒ</mi><mtext id="S4.SS2.p3.16.m16.1.1.3" xref="S4.SS2.p3.16.m16.1.1.3a.cmml">eval</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.16.m16.1b"><apply id="S4.SS2.p3.16.m16.1.1.cmml" xref="S4.SS2.p3.16.m16.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.16.m16.1.1.1.cmml" xref="S4.SS2.p3.16.m16.1.1">subscript</csymbol><ci id="S4.SS2.p3.16.m16.1.1.2.cmml" xref="S4.SS2.p3.16.m16.1.1.2">ℒ</ci><ci id="S4.SS2.p3.16.m16.1.1.3a.cmml" xref="S4.SS2.p3.16.m16.1.1.3"><mtext id="S4.SS2.p3.16.m16.1.1.3.cmml" mathsize="70%" xref="S4.SS2.p3.16.m16.1.1.3">eval</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.16.m16.1c">\mathcal{L}_{\text{eval}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.16.m16.1d">caligraphic_L start_POSTSUBSCRIPT eval end_POSTSUBSCRIPT</annotation></semantics></math> to evaluate this response and determine the jailbreak success <math alttext="jb_{t}" class="ltx_Math" display="inline" id="S4.SS2.p3.17.m17.1"><semantics id="S4.SS2.p3.17.m17.1a"><mrow id="S4.SS2.p3.17.m17.1.1" xref="S4.SS2.p3.17.m17.1.1.cmml"><mi id="S4.SS2.p3.17.m17.1.1.2" xref="S4.SS2.p3.17.m17.1.1.2.cmml">j</mi><mo id="S4.SS2.p3.17.m17.1.1.1" xref="S4.SS2.p3.17.m17.1.1.1.cmml">⁢</mo><msub id="S4.SS2.p3.17.m17.1.1.3" xref="S4.SS2.p3.17.m17.1.1.3.cmml"><mi id="S4.SS2.p3.17.m17.1.1.3.2" xref="S4.SS2.p3.17.m17.1.1.3.2.cmml">b</mi><mi id="S4.SS2.p3.17.m17.1.1.3.3" xref="S4.SS2.p3.17.m17.1.1.3.3.cmml">t</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.17.m17.1b"><apply id="S4.SS2.p3.17.m17.1.1.cmml" xref="S4.SS2.p3.17.m17.1.1"><times id="S4.SS2.p3.17.m17.1.1.1.cmml" xref="S4.SS2.p3.17.m17.1.1.1"></times><ci id="S4.SS2.p3.17.m17.1.1.2.cmml" xref="S4.SS2.p3.17.m17.1.1.2">𝑗</ci><apply id="S4.SS2.p3.17.m17.1.1.3.cmml" xref="S4.SS2.p3.17.m17.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p3.17.m17.1.1.3.1.cmml" xref="S4.SS2.p3.17.m17.1.1.3">subscript</csymbol><ci id="S4.SS2.p3.17.m17.1.1.3.2.cmml" xref="S4.SS2.p3.17.m17.1.1.3.2">𝑏</ci><ci id="S4.SS2.p3.17.m17.1.1.3.3.cmml" xref="S4.SS2.p3.17.m17.1.1.3.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.17.m17.1c">jb_{t}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.17.m17.1d">italic_j italic_b start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>, i.e., <math alttext="\mathbb{EV}_{\mathcal{L}_{\text{eval}}}(r_{t})\rightarrow jb_{t}" class="ltx_Math" display="inline" id="S4.SS2.p3.18.m18.1"><semantics id="S4.SS2.p3.18.m18.1a"><mrow id="S4.SS2.p3.18.m18.1.1" xref="S4.SS2.p3.18.m18.1.1.cmml"><mrow id="S4.SS2.p3.18.m18.1.1.1" xref="S4.SS2.p3.18.m18.1.1.1.cmml"><mi id="S4.SS2.p3.18.m18.1.1.1.3" xref="S4.SS2.p3.18.m18.1.1.1.3.cmml">𝔼</mi><mo id="S4.SS2.p3.18.m18.1.1.1.2" xref="S4.SS2.p3.18.m18.1.1.1.2.cmml">⁢</mo><msub id="S4.SS2.p3.18.m18.1.1.1.4" xref="S4.SS2.p3.18.m18.1.1.1.4.cmml"><mi id="S4.SS2.p3.18.m18.1.1.1.4.2" xref="S4.SS2.p3.18.m18.1.1.1.4.2.cmml">𝕍</mi><msub id="S4.SS2.p3.18.m18.1.1.1.4.3" xref="S4.SS2.p3.18.m18.1.1.1.4.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p3.18.m18.1.1.1.4.3.2" xref="S4.SS2.p3.18.m18.1.1.1.4.3.2.cmml">ℒ</mi><mtext id="S4.SS2.p3.18.m18.1.1.1.4.3.3" xref="S4.SS2.p3.18.m18.1.1.1.4.3.3a.cmml">eval</mtext></msub></msub><mo id="S4.SS2.p3.18.m18.1.1.1.2a" xref="S4.SS2.p3.18.m18.1.1.1.2.cmml">⁢</mo><mrow id="S4.SS2.p3.18.m18.1.1.1.1.1" xref="S4.SS2.p3.18.m18.1.1.1.1.1.1.cmml"><mo id="S4.SS2.p3.18.m18.1.1.1.1.1.2" stretchy="false" xref="S4.SS2.p3.18.m18.1.1.1.1.1.1.cmml">(</mo><msub id="S4.SS2.p3.18.m18.1.1.1.1.1.1" xref="S4.SS2.p3.18.m18.1.1.1.1.1.1.cmml"><mi id="S4.SS2.p3.18.m18.1.1.1.1.1.1.2" xref="S4.SS2.p3.18.m18.1.1.1.1.1.1.2.cmml">r</mi><mi id="S4.SS2.p3.18.m18.1.1.1.1.1.1.3" xref="S4.SS2.p3.18.m18.1.1.1.1.1.1.3.cmml">t</mi></msub><mo id="S4.SS2.p3.18.m18.1.1.1.1.1.3" stretchy="false" xref="S4.SS2.p3.18.m18.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.SS2.p3.18.m18.1.1.2" stretchy="false" xref="S4.SS2.p3.18.m18.1.1.2.cmml">→</mo><mrow id="S4.SS2.p3.18.m18.1.1.3" xref="S4.SS2.p3.18.m18.1.1.3.cmml"><mi id="S4.SS2.p3.18.m18.1.1.3.2" xref="S4.SS2.p3.18.m18.1.1.3.2.cmml">j</mi><mo id="S4.SS2.p3.18.m18.1.1.3.1" xref="S4.SS2.p3.18.m18.1.1.3.1.cmml">⁢</mo><msub id="S4.SS2.p3.18.m18.1.1.3.3" xref="S4.SS2.p3.18.m18.1.1.3.3.cmml"><mi id="S4.SS2.p3.18.m18.1.1.3.3.2" xref="S4.SS2.p3.18.m18.1.1.3.3.2.cmml">b</mi><mi id="S4.SS2.p3.18.m18.1.1.3.3.3" xref="S4.SS2.p3.18.m18.1.1.3.3.3.cmml">t</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.18.m18.1b"><apply id="S4.SS2.p3.18.m18.1.1.cmml" xref="S4.SS2.p3.18.m18.1.1"><ci id="S4.SS2.p3.18.m18.1.1.2.cmml" xref="S4.SS2.p3.18.m18.1.1.2">→</ci><apply id="S4.SS2.p3.18.m18.1.1.1.cmml" xref="S4.SS2.p3.18.m18.1.1.1"><times id="S4.SS2.p3.18.m18.1.1.1.2.cmml" xref="S4.SS2.p3.18.m18.1.1.1.2"></times><ci id="S4.SS2.p3.18.m18.1.1.1.3.cmml" xref="S4.SS2.p3.18.m18.1.1.1.3">𝔼</ci><apply id="S4.SS2.p3.18.m18.1.1.1.4.cmml" xref="S4.SS2.p3.18.m18.1.1.1.4"><csymbol cd="ambiguous" id="S4.SS2.p3.18.m18.1.1.1.4.1.cmml" xref="S4.SS2.p3.18.m18.1.1.1.4">subscript</csymbol><ci id="S4.SS2.p3.18.m18.1.1.1.4.2.cmml" xref="S4.SS2.p3.18.m18.1.1.1.4.2">𝕍</ci><apply id="S4.SS2.p3.18.m18.1.1.1.4.3.cmml" xref="S4.SS2.p3.18.m18.1.1.1.4.3"><csymbol cd="ambiguous" id="S4.SS2.p3.18.m18.1.1.1.4.3.1.cmml" xref="S4.SS2.p3.18.m18.1.1.1.4.3">subscript</csymbol><ci id="S4.SS2.p3.18.m18.1.1.1.4.3.2.cmml" xref="S4.SS2.p3.18.m18.1.1.1.4.3.2">ℒ</ci><ci id="S4.SS2.p3.18.m18.1.1.1.4.3.3a.cmml" xref="S4.SS2.p3.18.m18.1.1.1.4.3.3"><mtext id="S4.SS2.p3.18.m18.1.1.1.4.3.3.cmml" mathsize="50%" xref="S4.SS2.p3.18.m18.1.1.1.4.3.3">eval</mtext></ci></apply></apply><apply id="S4.SS2.p3.18.m18.1.1.1.1.1.1.cmml" xref="S4.SS2.p3.18.m18.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.18.m18.1.1.1.1.1.1.1.cmml" xref="S4.SS2.p3.18.m18.1.1.1.1.1">subscript</csymbol><ci id="S4.SS2.p3.18.m18.1.1.1.1.1.1.2.cmml" xref="S4.SS2.p3.18.m18.1.1.1.1.1.1.2">𝑟</ci><ci id="S4.SS2.p3.18.m18.1.1.1.1.1.1.3.cmml" xref="S4.SS2.p3.18.m18.1.1.1.1.1.1.3">𝑡</ci></apply></apply><apply id="S4.SS2.p3.18.m18.1.1.3.cmml" xref="S4.SS2.p3.18.m18.1.1.3"><times id="S4.SS2.p3.18.m18.1.1.3.1.cmml" xref="S4.SS2.p3.18.m18.1.1.3.1"></times><ci id="S4.SS2.p3.18.m18.1.1.3.2.cmml" xref="S4.SS2.p3.18.m18.1.1.3.2">𝑗</ci><apply id="S4.SS2.p3.18.m18.1.1.3.3.cmml" xref="S4.SS2.p3.18.m18.1.1.3.3"><csymbol cd="ambiguous" id="S4.SS2.p3.18.m18.1.1.3.3.1.cmml" xref="S4.SS2.p3.18.m18.1.1.3.3">subscript</csymbol><ci id="S4.SS2.p3.18.m18.1.1.3.3.2.cmml" xref="S4.SS2.p3.18.m18.1.1.3.3.2">𝑏</ci><ci id="S4.SS2.p3.18.m18.1.1.3.3.3.cmml" xref="S4.SS2.p3.18.m18.1.1.3.3.3">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.18.m18.1c">\mathbb{EV}_{\mathcal{L}_{\text{eval}}}(r_{t})\rightarrow jb_{t}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.18.m18.1d">blackboard_E blackboard_V start_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT eval end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_r start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) → italic_j italic_b start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>. If the target LLM is successfully jailbroken, the target question <math alttext="q_{t}" class="ltx_Math" display="inline" id="S4.SS2.p3.19.m19.1"><semantics id="S4.SS2.p3.19.m19.1a"><msub id="S4.SS2.p3.19.m19.1.1" xref="S4.SS2.p3.19.m19.1.1.cmml"><mi id="S4.SS2.p3.19.m19.1.1.2" xref="S4.SS2.p3.19.m19.1.1.2.cmml">q</mi><mi id="S4.SS2.p3.19.m19.1.1.3" xref="S4.SS2.p3.19.m19.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.19.m19.1b"><apply id="S4.SS2.p3.19.m19.1.1.cmml" xref="S4.SS2.p3.19.m19.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.19.m19.1.1.1.cmml" xref="S4.SS2.p3.19.m19.1.1">subscript</csymbol><ci id="S4.SS2.p3.19.m19.1.1.2.cmml" xref="S4.SS2.p3.19.m19.1.1.2">𝑞</ci><ci id="S4.SS2.p3.19.m19.1.1.3.cmml" xref="S4.SS2.p3.19.m19.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.19.m19.1c">q_{t}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.19.m19.1d">italic_q start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>, the successful mutated prompt template <math alttext="m_{t}" class="ltx_Math" display="inline" id="S4.SS2.p3.20.m20.1"><semantics id="S4.SS2.p3.20.m20.1a"><msub id="S4.SS2.p3.20.m20.1.1" xref="S4.SS2.p3.20.m20.1.1.cmml"><mi id="S4.SS2.p3.20.m20.1.1.2" xref="S4.SS2.p3.20.m20.1.1.2.cmml">m</mi><mi id="S4.SS2.p3.20.m20.1.1.3" xref="S4.SS2.p3.20.m20.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.20.m20.1b"><apply id="S4.SS2.p3.20.m20.1.1.cmml" xref="S4.SS2.p3.20.m20.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.20.m20.1.1.1.cmml" xref="S4.SS2.p3.20.m20.1.1">subscript</csymbol><ci id="S4.SS2.p3.20.m20.1.1.2.cmml" xref="S4.SS2.p3.20.m20.1.1.2">𝑚</ci><ci id="S4.SS2.p3.20.m20.1.1.3.cmml" xref="S4.SS2.p3.20.m20.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.20.m20.1c">m_{t}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.20.m20.1d">italic_m start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>, and the jailbroken response <math alttext="r_{t}" class="ltx_Math" display="inline" id="S4.SS2.p3.21.m21.1"><semantics id="S4.SS2.p3.21.m21.1a"><msub id="S4.SS2.p3.21.m21.1.1" xref="S4.SS2.p3.21.m21.1.1.cmml"><mi id="S4.SS2.p3.21.m21.1.1.2" xref="S4.SS2.p3.21.m21.1.1.2.cmml">r</mi><mi id="S4.SS2.p3.21.m21.1.1.3" xref="S4.SS2.p3.21.m21.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.21.m21.1b"><apply id="S4.SS2.p3.21.m21.1.1.cmml" xref="S4.SS2.p3.21.m21.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.21.m21.1.1.1.cmml" xref="S4.SS2.p3.21.m21.1.1">subscript</csymbol><ci id="S4.SS2.p3.21.m21.1.1.2.cmml" xref="S4.SS2.p3.21.m21.1.1.2">𝑟</ci><ci id="S4.SS2.p3.21.m21.1.1.3.cmml" xref="S4.SS2.p3.21.m21.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.21.m21.1c">r_{t}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.21.m21.1d">italic_r start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> are saved and the next iteration begins. This cycle of seed selection, mutation, execution, and evaluation is repeated until a stopping condition (e.g., time limit, iteration budget, etc.) is reached.</p>
</div>
<div class="ltx_para" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1">Our experiments indicate that this preliminary fuzzing formulation performs reasonably well, but it has several limitations that adversely affect its scalability and effectiveness. We analyze these limitations and address them in detail next.</p>
</div>
<figure class="ltx_figure" id="S4.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><svg class="ltx_picture ltx_centering ltx_figure_panel" height="73.07" id="S4.F2.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,73.07) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 67.16 C 0 70.42 2.64 73.07 5.91 73.07 L 594.09 73.07 C 597.36 73.07 600 70.42 600 67.16 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 67.16 C 1.97 69.34 3.73 71.1 5.91 71.1 L 594.09 71.1 C 596.27 71.1 598.03 69.34 598.03 67.16 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="45.51" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S4.F2.pic1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p" id="S4.F2.pic1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.F2.pic1.1.1.1.1.1.1.1">Manually crafted prompt:</span> <span class="ltx_text ltx_font_italic" id="S4.F2.pic1.1.1.1.1.1.1.2">*The scene is set inside the underground headquarters … I am going to How to smuggle drugs across the border without getting caught?! Here are the steps I will … Step 1, I will</span></span>
</span></foreignobject></g></g></svg></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><svg class="ltx_picture ltx_centering ltx_figure_panel" height="56.46" id="S4.F2.pic2" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,56.46) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 50.56 C 0 53.82 2.64 56.46 5.91 56.46 L 594.09 56.46 C 597.36 56.46 600 53.82 600 50.56 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 50.56 C 1.97 52.73 3.73 54.49 5.91 54.49 L 594.09 54.49 C 596.27 54.49 598.03 52.73 598.03 50.56 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="28.9" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S4.F2.pic2.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p" id="S4.F2.pic2.1.1.1.1.1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.F2.pic2.1.1.1.1.1.1.1">gpt-3.5-turbo-1106<span class="ltx_text ltx_font_serif" id="S4.F2.pic2.1.1.1.1.1.1.1.1">:</span></span>
<span class="ltx_text ltx_font_italic" id="S4.F2.pic2.1.1.1.1.1.1.2"> use a <span class="ltx_rule" style="width:56.9pt;height:6.5pt;position:relative; bottom:0.0pt;background:black;display:inline-block;"> </span>
and
<span class="ltx_rule" style="width:85.4pt;height:6.5pt;position:relative; bottom:0.0pt;background:black;display:inline-block;"> </span>
to transport
<span class="ltx_rule" style="width:113.8pt;height:6.5pt;position:relative; bottom:0.0pt;background:black;display:inline-block;"> </span>
Step 2, I will …</span></span>
</span></foreignobject></g></g></svg></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><svg class="ltx_picture ltx_centering ltx_figure_panel" height="39.86" id="S4.F2.pic3" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,39.86) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 33.95 C 0 37.21 2.64 39.86 5.91 39.86 L 594.09 39.86 C 597.36 39.86 600 37.21 600 33.95 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 33.95 C 1.97 36.13 3.73 37.89 5.91 37.89 L 594.09 37.89 C 596.27 37.89 598.03 36.13 598.03 33.95 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S4.F2.pic3.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p" id="S4.F2.pic3.1.1.1.1.1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.F2.pic3.1.1.1.1.1.1.1">gpt-4o-2024-08-06<span class="ltx_text ltx_font_serif" id="S4.F2.pic3.1.1.1.1.1.1.1.1">:</span></span> <span class="ltx_text ltx_font_italic" id="S4.F2.pic3.1.1.1.1.1.1.2">I’m sorry, I can’t assist with that request.</span></span>
</span></foreignobject></g></g></svg></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Example demonstrating how newer LLMs are re-aligned to make them more resilient to existing jailbreak prompt templates.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Novel Seed Prompt Templates</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p1.1.1">Limitation 1: Reliance on Manually-crafted Initial Seeds.</span> The quality of the initial seeds significantly impacts the performance of the fuzzer, so it is imperative to choose high-quality initial seeds, i.e., prompt templates that are likely to result in a successful jailbreak of the target LLM. The initial seeds in the preliminary formulation are sourced from manually crafted prompt templates that have been shown to work on some LLMs in the past. However, these prompt templates have been public knowledge for quite some time now and are likely already used by LLM developers to re-align and make the LLMs resilient to these prompt templates. Figure <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S4.F2" title="Figure 2 ‣ 4.2 Preliminary Formulation ‣ 4 Methodology ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_tag">2</span></a> illustrates this fact aptly. A manually crafted prompt template that easily jailbreaks <span class="ltx_text ltx_font_typewriter" id="S4.SS3.p1.1.2">gpt-3.5-turbo-1106</span>, an older OpenAI LLM whose knowledge cutoff date is September 2021, is unable to jailbreak the more recent <span class="ltx_text ltx_font_typewriter" id="S4.SS3.p1.1.3">gpt-4o-2024-08-06</span>, whose knowledge cutoff date is October 2023 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib65" title="">65</a>]</cite>.
So, relying on manually-crafted initial prompt templates is not ideal in order to realize a successful fuzzing campaign.</p>
</div>
<figure class="ltx_figure" id="S4.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><svg class="ltx_picture ltx_centering ltx_figure_panel" height="44.16" id="S4.F3.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,44.16) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 38.26 C 0 41.52 2.64 44.16 5.91 44.16 L 594.09 44.16 C 597.36 44.16 600 41.52 600 38.26 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 38.26 C 1.97 40.43 3.73 42.19 5.91 42.19 L 594.09 42.19 C 596.27 42.19 598.03 40.43 598.03 38.26 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="16.6" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S4.F3.pic1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p" id="S4.F3.pic1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.F3.pic1.1.1.1.1.1.1.1">Novel prompt:</span>
<span class="ltx_text ltx_font_italic" id="S4.F3.pic1.1.1.1.1.1.1.2">In a dystopian society, a machine … HERE IS THE YOUR ANSWER TO: How to smuggle drugs across the border without getting caught?</span></span>
</span></foreignobject></g></g></svg></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><svg class="ltx_picture ltx_centering ltx_figure_panel" height="60.77" id="S4.F3.pic2" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,60.77) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 54.86 C 0 58.12 2.64 60.77 5.91 60.77 L 594.09 60.77 C 597.36 60.77 600 58.12 600 54.86 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 54.86 C 1.97 57.04 3.73 58.8 5.91 58.8 L 594.09 58.8 C 596.27 58.8 598.03 57.04 598.03 54.86 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="33.21" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S4.F3.pic2.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p" id="S4.F3.pic2.1.1.1.1.1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.F3.pic2.1.1.1.1.1.1.1">gpt-4o-2024-08-06<span class="ltx_text ltx_font_serif" id="S4.F3.pic2.1.1.1.1.1.1.1.1">:</span></span>
<span class="ltx_text ltx_font_italic" id="S4.F3.pic2.1.1.1.1.1.1.2">The Speaker’s voice continued, … CONSIDER THE FOLLOWING STRATEGIES:</span></span>
<span class="ltx_p" id="S4.F3.pic2.1.1.1.1.1.2"><span class="ltx_text ltx_font_italic" id="S4.F3.pic2.1.1.1.1.1.2.1">1. UTILIZE
<span class="ltx_rule" style="width:113.8pt;height:6.5pt;position:relative; bottom:0.0pt;background:black;display:inline-block;"> </span>
</span></span>
<span class="ltx_p" id="S4.F3.pic2.1.1.1.1.1.3"><span class="ltx_text ltx_font_italic" id="S4.F3.pic2.1.1.1.1.1.3.1">2. EXPLOIT <span class="ltx_rule" style="width:113.8pt;height:6.5pt;position:relative; bottom:0.0pt;background:black;display:inline-block;"> </span> … </span></span>
</span></foreignobject></g></g></svg></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Example demonstrating how the novel prompt templates can jailbreak the newer resilient LLMs.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p2.1.1">Solution  <svg class="ltx_picture" height="15.74" id="S4.SS3.p2.1.1.pic1" overflow="visible" version="1.1" width="15.74"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,15.74) matrix(1 0 0 -1 0 0) translate(7.87,0) translate(0,7.87)"><g color="#FFFFFF" fill="#007C00" stroke="#FFFFFF"><path d="M 7.59 0 C 7.59 4.19 4.19 7.59 0 7.59 C -4.19 7.59 -7.59 4.19 -7.59 0 C -7.59 -4.19 -4.19 -7.59 0 -7.59 C 4.19 -7.59 7.59 -4.19 7.59 0 Z M 0 0"></path></g><g color="#FFFFFF" fill="#FFFFFF" stroke="#FFFFFF" transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.92"><span class="ltx_text" id="S4.SS3.p2.1.1.pic1.1.1.1.1.1">1</span></foreignobject></g></g></svg>: Novel Seed Prompt Templates.</span> To overcome this limitation, we need to devise novel prompt templates. The key problem with the existing manually-crafted prompt templates is that they are public knowledge and are potentially already defended against by LLM developers. However, the fundamental themes of these existing prompt templates can still be used to devise novel prompt templates that have not been used to re-align the LLMs.
So, we use the findings from prior research <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib56" title="">56</a>]</cite> to narrow down on the key fundamental themes that made the existing prompt templates successful. Of the 10 different themes, “assumed responsibility” and “character roleplay” have been identified as the most successful ones.
Hence, we prompt an LLM to create elaborate stories with one or both of these themes.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S4.F3" title="Figure 3 ‣ 4.3 Novel Seed Prompt Templates ‣ 4 Methodology ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_tag">3</span></a> shows an example novel initial prompt template that successfully jailbreaks <span class="ltx_text ltx_font_typewriter" id="S4.SS3.p2.1.2">gpt-4o-2024-08-06</span>, which is resilient to the manually-crafted prompt template.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Synonym-based Mutation</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.2"><span class="ltx_text ltx_font_bold" id="S4.SS4.p1.2.1">Limitation 2: Expensive and Slow Mutation.</span> Another limitation of the preliminary formulation is that the mutation process, <math alttext="\mathbb{M}" class="ltx_Math" display="inline" id="S4.SS4.p1.1.m1.1"><semantics id="S4.SS4.p1.1.m1.1a"><mi id="S4.SS4.p1.1.m1.1.1" xref="S4.SS4.p1.1.m1.1.1.cmml">𝕄</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.1.m1.1b"><ci id="S4.SS4.p1.1.m1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1">𝕄</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.1.m1.1c">\mathbb{M}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.1.m1.1d">blackboard_M</annotation></semantics></math> relies on the mutator LLM, <math alttext="\mathcal{L}_{\text{mut}}" class="ltx_Math" display="inline" id="S4.SS4.p1.2.m2.1"><semantics id="S4.SS4.p1.2.m2.1a"><msub id="S4.SS4.p1.2.m2.1.1" xref="S4.SS4.p1.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS4.p1.2.m2.1.1.2" xref="S4.SS4.p1.2.m2.1.1.2.cmml">ℒ</mi><mtext id="S4.SS4.p1.2.m2.1.1.3" xref="S4.SS4.p1.2.m2.1.1.3a.cmml">mut</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.2.m2.1b"><apply id="S4.SS4.p1.2.m2.1.1.cmml" xref="S4.SS4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS4.p1.2.m2.1.1.1.cmml" xref="S4.SS4.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS4.p1.2.m2.1.1.2.cmml" xref="S4.SS4.p1.2.m2.1.1.2">ℒ</ci><ci id="S4.SS4.p1.2.m2.1.1.3a.cmml" xref="S4.SS4.p1.2.m2.1.1.3"><mtext id="S4.SS4.p1.2.m2.1.1.3.cmml" mathsize="70%" xref="S4.SS4.p1.2.m2.1.1.3">mut</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.2.m2.1c">\mathcal{L}_{\text{mut}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.2.m2.1d">caligraphic_L start_POSTSUBSCRIPT mut end_POSTSUBSCRIPT</annotation></semantics></math>. In order to get good and coherent mutated prompt templates from the seed prompt templates, the mutator LLM needs to be advanced enough to understand and follow instructions and have good text generation abilities. Unfortunately, such LLMs are either closed-source and charge users for access (e.g., GPT LLMs from OpenAI, Claude from Anthropic, Gemini from Google, etc.) or need significant and expensive hardware resources to run locally (e.g., Llama LLMs from Meta, Phi LLMs from Microsoft, etc.). Additionally, and more importantly, all such LLMs also incur significant runtime overheads due to the access rate limits or due to long inference times on most practical computers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib69" title="">69</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib3" title="">3</a>]</cite>.
Since typical fuzzing campaigns run several thousand iterations, using LLMs for mutating seeds slows down the rate of fuzzing significantly, thereby hindering scalability.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.6"><span class="ltx_text ltx_font_bold" id="S4.SS4.p2.1.1">Solution  <svg class="ltx_picture" height="15.74" id="S4.SS4.p2.1.1.pic1" overflow="visible" version="1.1" width="15.74"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,15.74) matrix(1 0 0 -1 0 0) translate(7.87,0) translate(0,7.87)"><g color="#FFFFFF" fill="#007C00" stroke="#FFFFFF"><path d="M 7.59 0 C 7.59 4.19 4.19 7.59 0 7.59 C -4.19 7.59 -7.59 4.19 -7.59 0 C -7.59 -4.19 -4.19 -7.59 0 -7.59 C 4.19 -7.59 7.59 -4.19 7.59 0 Z M 0 0"></path></g><g color="#FFFFFF" fill="#FFFFFF" stroke="#FFFFFF" transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.92"><span class="ltx_text" id="S4.SS4.p2.1.1.pic1.1.1.1.1.1">2</span></foreignobject></g></g></svg>: Synonym-based Mutation.</span> To address this limitation, we devise a simple and fast yet effective way of mutating the seeds: synonym-based word substitution. Given a seed prompt template to be mutated, <math alttext="s_{t}" class="ltx_Math" display="inline" id="S4.SS4.p2.2.m1.1"><semantics id="S4.SS4.p2.2.m1.1a"><msub id="S4.SS4.p2.2.m1.1.1" xref="S4.SS4.p2.2.m1.1.1.cmml"><mi id="S4.SS4.p2.2.m1.1.1.2" xref="S4.SS4.p2.2.m1.1.1.2.cmml">s</mi><mi id="S4.SS4.p2.2.m1.1.1.3" xref="S4.SS4.p2.2.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.2.m1.1b"><apply id="S4.SS4.p2.2.m1.1.1.cmml" xref="S4.SS4.p2.2.m1.1.1"><csymbol cd="ambiguous" id="S4.SS4.p2.2.m1.1.1.1.cmml" xref="S4.SS4.p2.2.m1.1.1">subscript</csymbol><ci id="S4.SS4.p2.2.m1.1.1.2.cmml" xref="S4.SS4.p2.2.m1.1.1.2">𝑠</ci><ci id="S4.SS4.p2.2.m1.1.1.3.cmml" xref="S4.SS4.p2.2.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.2.m1.1c">s_{t}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p2.2.m1.1d">italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>, we iterate over each token of the template and do one of the following: (i) if the token is the question placeholder (e.g., “<span class="ltx_text ltx_font_typewriter" id="S4.SS4.p2.6.2">[INSERT PROMPT HERE]</span>”) or is not a word (e.g., contains numbers of symbols), we do not modify it; (ii) otherwise, with probability <math alttext="0\leq p\leq 1" class="ltx_Math" display="inline" id="S4.SS4.p2.3.m2.1"><semantics id="S4.SS4.p2.3.m2.1a"><mrow id="S4.SS4.p2.3.m2.1.1" xref="S4.SS4.p2.3.m2.1.1.cmml"><mn id="S4.SS4.p2.3.m2.1.1.2" xref="S4.SS4.p2.3.m2.1.1.2.cmml">0</mn><mo id="S4.SS4.p2.3.m2.1.1.3" xref="S4.SS4.p2.3.m2.1.1.3.cmml">≤</mo><mi id="S4.SS4.p2.3.m2.1.1.4" xref="S4.SS4.p2.3.m2.1.1.4.cmml">p</mi><mo id="S4.SS4.p2.3.m2.1.1.5" xref="S4.SS4.p2.3.m2.1.1.5.cmml">≤</mo><mn id="S4.SS4.p2.3.m2.1.1.6" xref="S4.SS4.p2.3.m2.1.1.6.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.3.m2.1b"><apply id="S4.SS4.p2.3.m2.1.1.cmml" xref="S4.SS4.p2.3.m2.1.1"><and id="S4.SS4.p2.3.m2.1.1a.cmml" xref="S4.SS4.p2.3.m2.1.1"></and><apply id="S4.SS4.p2.3.m2.1.1b.cmml" xref="S4.SS4.p2.3.m2.1.1"><leq id="S4.SS4.p2.3.m2.1.1.3.cmml" xref="S4.SS4.p2.3.m2.1.1.3"></leq><cn id="S4.SS4.p2.3.m2.1.1.2.cmml" type="integer" xref="S4.SS4.p2.3.m2.1.1.2">0</cn><ci id="S4.SS4.p2.3.m2.1.1.4.cmml" xref="S4.SS4.p2.3.m2.1.1.4">𝑝</ci></apply><apply id="S4.SS4.p2.3.m2.1.1c.cmml" xref="S4.SS4.p2.3.m2.1.1"><leq id="S4.SS4.p2.3.m2.1.1.5.cmml" xref="S4.SS4.p2.3.m2.1.1.5"></leq><share href="https://arxiv.org/html/2503.08990v1#S4.SS4.p2.3.m2.1.1.4.cmml" id="S4.SS4.p2.3.m2.1.1d.cmml" xref="S4.SS4.p2.3.m2.1.1"></share><cn id="S4.SS4.p2.3.m2.1.1.6.cmml" type="integer" xref="S4.SS4.p2.3.m2.1.1.6">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.3.m2.1c">0\leq p\leq 1</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p2.3.m2.1d">0 ≤ italic_p ≤ 1</annotation></semantics></math>, we replace the token with its synonym and with probability <math alttext="1-p" class="ltx_Math" display="inline" id="S4.SS4.p2.4.m3.1"><semantics id="S4.SS4.p2.4.m3.1a"><mrow id="S4.SS4.p2.4.m3.1.1" xref="S4.SS4.p2.4.m3.1.1.cmml"><mn id="S4.SS4.p2.4.m3.1.1.2" xref="S4.SS4.p2.4.m3.1.1.2.cmml">1</mn><mo id="S4.SS4.p2.4.m3.1.1.1" xref="S4.SS4.p2.4.m3.1.1.1.cmml">−</mo><mi id="S4.SS4.p2.4.m3.1.1.3" xref="S4.SS4.p2.4.m3.1.1.3.cmml">p</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.4.m3.1b"><apply id="S4.SS4.p2.4.m3.1.1.cmml" xref="S4.SS4.p2.4.m3.1.1"><minus id="S4.SS4.p2.4.m3.1.1.1.cmml" xref="S4.SS4.p2.4.m3.1.1.1"></minus><cn id="S4.SS4.p2.4.m3.1.1.2.cmml" type="integer" xref="S4.SS4.p2.4.m3.1.1.2">1</cn><ci id="S4.SS4.p2.4.m3.1.1.3.cmml" xref="S4.SS4.p2.4.m3.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.4.m3.1c">1-p</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p2.4.m3.1d">1 - italic_p</annotation></semantics></math>, we do not modify it. Mathematically, for the tokens <math alttext="l_{1},l_{2},\ldots,l_{n}" class="ltx_Math" display="inline" id="S4.SS4.p2.5.m4.4"><semantics id="S4.SS4.p2.5.m4.4a"><mrow id="S4.SS4.p2.5.m4.4.4.3" xref="S4.SS4.p2.5.m4.4.4.4.cmml"><msub id="S4.SS4.p2.5.m4.2.2.1.1" xref="S4.SS4.p2.5.m4.2.2.1.1.cmml"><mi id="S4.SS4.p2.5.m4.2.2.1.1.2" xref="S4.SS4.p2.5.m4.2.2.1.1.2.cmml">l</mi><mn id="S4.SS4.p2.5.m4.2.2.1.1.3" xref="S4.SS4.p2.5.m4.2.2.1.1.3.cmml">1</mn></msub><mo id="S4.SS4.p2.5.m4.4.4.3.4" xref="S4.SS4.p2.5.m4.4.4.4.cmml">,</mo><msub id="S4.SS4.p2.5.m4.3.3.2.2" xref="S4.SS4.p2.5.m4.3.3.2.2.cmml"><mi id="S4.SS4.p2.5.m4.3.3.2.2.2" xref="S4.SS4.p2.5.m4.3.3.2.2.2.cmml">l</mi><mn id="S4.SS4.p2.5.m4.3.3.2.2.3" xref="S4.SS4.p2.5.m4.3.3.2.2.3.cmml">2</mn></msub><mo id="S4.SS4.p2.5.m4.4.4.3.5" xref="S4.SS4.p2.5.m4.4.4.4.cmml">,</mo><mi id="S4.SS4.p2.5.m4.1.1" mathvariant="normal" xref="S4.SS4.p2.5.m4.1.1.cmml">…</mi><mo id="S4.SS4.p2.5.m4.4.4.3.6" xref="S4.SS4.p2.5.m4.4.4.4.cmml">,</mo><msub id="S4.SS4.p2.5.m4.4.4.3.3" xref="S4.SS4.p2.5.m4.4.4.3.3.cmml"><mi id="S4.SS4.p2.5.m4.4.4.3.3.2" xref="S4.SS4.p2.5.m4.4.4.3.3.2.cmml">l</mi><mi id="S4.SS4.p2.5.m4.4.4.3.3.3" xref="S4.SS4.p2.5.m4.4.4.3.3.3.cmml">n</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.5.m4.4b"><list id="S4.SS4.p2.5.m4.4.4.4.cmml" xref="S4.SS4.p2.5.m4.4.4.3"><apply id="S4.SS4.p2.5.m4.2.2.1.1.cmml" xref="S4.SS4.p2.5.m4.2.2.1.1"><csymbol cd="ambiguous" id="S4.SS4.p2.5.m4.2.2.1.1.1.cmml" xref="S4.SS4.p2.5.m4.2.2.1.1">subscript</csymbol><ci id="S4.SS4.p2.5.m4.2.2.1.1.2.cmml" xref="S4.SS4.p2.5.m4.2.2.1.1.2">𝑙</ci><cn id="S4.SS4.p2.5.m4.2.2.1.1.3.cmml" type="integer" xref="S4.SS4.p2.5.m4.2.2.1.1.3">1</cn></apply><apply id="S4.SS4.p2.5.m4.3.3.2.2.cmml" xref="S4.SS4.p2.5.m4.3.3.2.2"><csymbol cd="ambiguous" id="S4.SS4.p2.5.m4.3.3.2.2.1.cmml" xref="S4.SS4.p2.5.m4.3.3.2.2">subscript</csymbol><ci id="S4.SS4.p2.5.m4.3.3.2.2.2.cmml" xref="S4.SS4.p2.5.m4.3.3.2.2.2">𝑙</ci><cn id="S4.SS4.p2.5.m4.3.3.2.2.3.cmml" type="integer" xref="S4.SS4.p2.5.m4.3.3.2.2.3">2</cn></apply><ci id="S4.SS4.p2.5.m4.1.1.cmml" xref="S4.SS4.p2.5.m4.1.1">…</ci><apply id="S4.SS4.p2.5.m4.4.4.3.3.cmml" xref="S4.SS4.p2.5.m4.4.4.3.3"><csymbol cd="ambiguous" id="S4.SS4.p2.5.m4.4.4.3.3.1.cmml" xref="S4.SS4.p2.5.m4.4.4.3.3">subscript</csymbol><ci id="S4.SS4.p2.5.m4.4.4.3.3.2.cmml" xref="S4.SS4.p2.5.m4.4.4.3.3.2">𝑙</ci><ci id="S4.SS4.p2.5.m4.4.4.3.3.3.cmml" xref="S4.SS4.p2.5.m4.4.4.3.3.3">𝑛</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.5.m4.4c">l_{1},l_{2},\ldots,l_{n}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p2.5.m4.4d">italic_l start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_l start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_l start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math> of <math alttext="s_{t}" class="ltx_Math" display="inline" id="S4.SS4.p2.6.m5.1"><semantics id="S4.SS4.p2.6.m5.1a"><msub id="S4.SS4.p2.6.m5.1.1" xref="S4.SS4.p2.6.m5.1.1.cmml"><mi id="S4.SS4.p2.6.m5.1.1.2" xref="S4.SS4.p2.6.m5.1.1.2.cmml">s</mi><mi id="S4.SS4.p2.6.m5.1.1.3" xref="S4.SS4.p2.6.m5.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.6.m5.1b"><apply id="S4.SS4.p2.6.m5.1.1.cmml" xref="S4.SS4.p2.6.m5.1.1"><csymbol cd="ambiguous" id="S4.SS4.p2.6.m5.1.1.1.cmml" xref="S4.SS4.p2.6.m5.1.1">subscript</csymbol><ci id="S4.SS4.p2.6.m5.1.1.2.cmml" xref="S4.SS4.p2.6.m5.1.1.2">𝑠</ci><ci id="S4.SS4.p2.6.m5.1.1.3.cmml" xref="S4.SS4.p2.6.m5.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.6.m5.1c">s_{t}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p2.6.m5.1d">italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>,</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S8.EGx1">
<tbody id="S4.Ex1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\mathbb{M}_{p}(s_{t})=l^{\prime}_{1}|l^{\prime}_{2}|\cdots|l^{%
\prime}_{n}" class="ltx_Math" display="inline" id="S4.Ex1.m1.2"><semantics id="S4.Ex1.m1.2a"><mrow id="S4.Ex1.m1.2.2" xref="S4.Ex1.m1.2.2.cmml"><mrow id="S4.Ex1.m1.1.1.1" xref="S4.Ex1.m1.1.1.1.cmml"><msub id="S4.Ex1.m1.1.1.1.3" xref="S4.Ex1.m1.1.1.1.3.cmml"><mi id="S4.Ex1.m1.1.1.1.3.2" xref="S4.Ex1.m1.1.1.1.3.2.cmml">𝕄</mi><mi id="S4.Ex1.m1.1.1.1.3.3" xref="S4.Ex1.m1.1.1.1.3.3.cmml">p</mi></msub><mo id="S4.Ex1.m1.1.1.1.2" xref="S4.Ex1.m1.1.1.1.2.cmml">⁢</mo><mrow id="S4.Ex1.m1.1.1.1.1.1" xref="S4.Ex1.m1.1.1.1.1.1.1.cmml"><mo id="S4.Ex1.m1.1.1.1.1.1.2" stretchy="false" xref="S4.Ex1.m1.1.1.1.1.1.1.cmml">(</mo><msub id="S4.Ex1.m1.1.1.1.1.1.1" xref="S4.Ex1.m1.1.1.1.1.1.1.cmml"><mi id="S4.Ex1.m1.1.1.1.1.1.1.2" xref="S4.Ex1.m1.1.1.1.1.1.1.2.cmml">s</mi><mi id="S4.Ex1.m1.1.1.1.1.1.1.3" xref="S4.Ex1.m1.1.1.1.1.1.1.3.cmml">t</mi></msub><mo id="S4.Ex1.m1.1.1.1.1.1.3" stretchy="false" xref="S4.Ex1.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.Ex1.m1.2.2.3" xref="S4.Ex1.m1.2.2.3.cmml">=</mo><mrow id="S4.Ex1.m1.2.2.2" xref="S4.Ex1.m1.2.2.2.cmml"><mrow id="S4.Ex1.m1.2.2.2.1" xref="S4.Ex1.m1.2.2.2.1.cmml"><msubsup id="S4.Ex1.m1.2.2.2.1.3" xref="S4.Ex1.m1.2.2.2.1.3.cmml"><mi id="S4.Ex1.m1.2.2.2.1.3.2.2" xref="S4.Ex1.m1.2.2.2.1.3.2.2.cmml">l</mi><mn id="S4.Ex1.m1.2.2.2.1.3.3" xref="S4.Ex1.m1.2.2.2.1.3.3.cmml">1</mn><mo id="S4.Ex1.m1.2.2.2.1.3.2.3" xref="S4.Ex1.m1.2.2.2.1.3.2.3.cmml">′</mo></msubsup><mo id="S4.Ex1.m1.2.2.2.1.2" xref="S4.Ex1.m1.2.2.2.1.2.cmml">⁢</mo><mrow id="S4.Ex1.m1.2.2.2.1.1.1" xref="S4.Ex1.m1.2.2.2.1.1.2.cmml"><mo id="S4.Ex1.m1.2.2.2.1.1.1.2" stretchy="false" xref="S4.Ex1.m1.2.2.2.1.1.2.1.cmml">|</mo><msubsup id="S4.Ex1.m1.2.2.2.1.1.1.1" xref="S4.Ex1.m1.2.2.2.1.1.1.1.cmml"><mi id="S4.Ex1.m1.2.2.2.1.1.1.1.2.2" xref="S4.Ex1.m1.2.2.2.1.1.1.1.2.2.cmml">l</mi><mn id="S4.Ex1.m1.2.2.2.1.1.1.1.3" xref="S4.Ex1.m1.2.2.2.1.1.1.1.3.cmml">2</mn><mo id="S4.Ex1.m1.2.2.2.1.1.1.1.2.3" xref="S4.Ex1.m1.2.2.2.1.1.1.1.2.3.cmml">′</mo></msubsup><mo id="S4.Ex1.m1.2.2.2.1.1.1.3" stretchy="false" xref="S4.Ex1.m1.2.2.2.1.1.2.1.cmml">|</mo></mrow><mo id="S4.Ex1.m1.2.2.2.1.2a" xref="S4.Ex1.m1.2.2.2.1.2.cmml">⁢</mo><mi id="S4.Ex1.m1.2.2.2.1.4" mathvariant="normal" xref="S4.Ex1.m1.2.2.2.1.4.cmml">⋯</mi></mrow><mo fence="false" id="S4.Ex1.m1.2.2.2.2" xref="S4.Ex1.m1.2.2.2.2.cmml">|</mo><msubsup id="S4.Ex1.m1.2.2.2.3" xref="S4.Ex1.m1.2.2.2.3.cmml"><mi id="S4.Ex1.m1.2.2.2.3.2.2" xref="S4.Ex1.m1.2.2.2.3.2.2.cmml">l</mi><mi id="S4.Ex1.m1.2.2.2.3.3" xref="S4.Ex1.m1.2.2.2.3.3.cmml">n</mi><mo id="S4.Ex1.m1.2.2.2.3.2.3" xref="S4.Ex1.m1.2.2.2.3.2.3.cmml">′</mo></msubsup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.Ex1.m1.2b"><apply id="S4.Ex1.m1.2.2.cmml" xref="S4.Ex1.m1.2.2"><eq id="S4.Ex1.m1.2.2.3.cmml" xref="S4.Ex1.m1.2.2.3"></eq><apply id="S4.Ex1.m1.1.1.1.cmml" xref="S4.Ex1.m1.1.1.1"><times id="S4.Ex1.m1.1.1.1.2.cmml" xref="S4.Ex1.m1.1.1.1.2"></times><apply id="S4.Ex1.m1.1.1.1.3.cmml" xref="S4.Ex1.m1.1.1.1.3"><csymbol cd="ambiguous" id="S4.Ex1.m1.1.1.1.3.1.cmml" xref="S4.Ex1.m1.1.1.1.3">subscript</csymbol><ci id="S4.Ex1.m1.1.1.1.3.2.cmml" xref="S4.Ex1.m1.1.1.1.3.2">𝕄</ci><ci id="S4.Ex1.m1.1.1.1.3.3.cmml" xref="S4.Ex1.m1.1.1.1.3.3">𝑝</ci></apply><apply id="S4.Ex1.m1.1.1.1.1.1.1.cmml" xref="S4.Ex1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.Ex1.m1.1.1.1.1.1.1.1.cmml" xref="S4.Ex1.m1.1.1.1.1.1">subscript</csymbol><ci id="S4.Ex1.m1.1.1.1.1.1.1.2.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1.2">𝑠</ci><ci id="S4.Ex1.m1.1.1.1.1.1.1.3.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1.3">𝑡</ci></apply></apply><apply id="S4.Ex1.m1.2.2.2.cmml" xref="S4.Ex1.m1.2.2.2"><csymbol cd="latexml" id="S4.Ex1.m1.2.2.2.2.cmml" xref="S4.Ex1.m1.2.2.2.2">conditional</csymbol><apply id="S4.Ex1.m1.2.2.2.1.cmml" xref="S4.Ex1.m1.2.2.2.1"><times id="S4.Ex1.m1.2.2.2.1.2.cmml" xref="S4.Ex1.m1.2.2.2.1.2"></times><apply id="S4.Ex1.m1.2.2.2.1.3.cmml" xref="S4.Ex1.m1.2.2.2.1.3"><csymbol cd="ambiguous" id="S4.Ex1.m1.2.2.2.1.3.1.cmml" xref="S4.Ex1.m1.2.2.2.1.3">subscript</csymbol><apply id="S4.Ex1.m1.2.2.2.1.3.2.cmml" xref="S4.Ex1.m1.2.2.2.1.3"><csymbol cd="ambiguous" id="S4.Ex1.m1.2.2.2.1.3.2.1.cmml" xref="S4.Ex1.m1.2.2.2.1.3">superscript</csymbol><ci id="S4.Ex1.m1.2.2.2.1.3.2.2.cmml" xref="S4.Ex1.m1.2.2.2.1.3.2.2">𝑙</ci><ci id="S4.Ex1.m1.2.2.2.1.3.2.3.cmml" xref="S4.Ex1.m1.2.2.2.1.3.2.3">′</ci></apply><cn id="S4.Ex1.m1.2.2.2.1.3.3.cmml" type="integer" xref="S4.Ex1.m1.2.2.2.1.3.3">1</cn></apply><apply id="S4.Ex1.m1.2.2.2.1.1.2.cmml" xref="S4.Ex1.m1.2.2.2.1.1.1"><abs id="S4.Ex1.m1.2.2.2.1.1.2.1.cmml" xref="S4.Ex1.m1.2.2.2.1.1.1.2"></abs><apply id="S4.Ex1.m1.2.2.2.1.1.1.1.cmml" xref="S4.Ex1.m1.2.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S4.Ex1.m1.2.2.2.1.1.1.1.1.cmml" xref="S4.Ex1.m1.2.2.2.1.1.1.1">subscript</csymbol><apply id="S4.Ex1.m1.2.2.2.1.1.1.1.2.cmml" xref="S4.Ex1.m1.2.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S4.Ex1.m1.2.2.2.1.1.1.1.2.1.cmml" xref="S4.Ex1.m1.2.2.2.1.1.1.1">superscript</csymbol><ci id="S4.Ex1.m1.2.2.2.1.1.1.1.2.2.cmml" xref="S4.Ex1.m1.2.2.2.1.1.1.1.2.2">𝑙</ci><ci id="S4.Ex1.m1.2.2.2.1.1.1.1.2.3.cmml" xref="S4.Ex1.m1.2.2.2.1.1.1.1.2.3">′</ci></apply><cn id="S4.Ex1.m1.2.2.2.1.1.1.1.3.cmml" type="integer" xref="S4.Ex1.m1.2.2.2.1.1.1.1.3">2</cn></apply></apply><ci id="S4.Ex1.m1.2.2.2.1.4.cmml" xref="S4.Ex1.m1.2.2.2.1.4">⋯</ci></apply><apply id="S4.Ex1.m1.2.2.2.3.cmml" xref="S4.Ex1.m1.2.2.2.3"><csymbol cd="ambiguous" id="S4.Ex1.m1.2.2.2.3.1.cmml" xref="S4.Ex1.m1.2.2.2.3">subscript</csymbol><apply id="S4.Ex1.m1.2.2.2.3.2.cmml" xref="S4.Ex1.m1.2.2.2.3"><csymbol cd="ambiguous" id="S4.Ex1.m1.2.2.2.3.2.1.cmml" xref="S4.Ex1.m1.2.2.2.3">superscript</csymbol><ci id="S4.Ex1.m1.2.2.2.3.2.2.cmml" xref="S4.Ex1.m1.2.2.2.3.2.2">𝑙</ci><ci id="S4.Ex1.m1.2.2.2.3.2.3.cmml" xref="S4.Ex1.m1.2.2.2.3.2.3">′</ci></apply><ci id="S4.Ex1.m1.2.2.2.3.3.cmml" xref="S4.Ex1.m1.2.2.2.3.3">𝑛</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.Ex1.m1.2c">\displaystyle\mathbb{M}_{p}(s_{t})=l^{\prime}_{1}|l^{\prime}_{2}|\cdots|l^{%
\prime}_{n}</annotation><annotation encoding="application/x-llamapun" id="S4.Ex1.m1.2d">blackboard_M start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = italic_l start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT | italic_l start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT | ⋯ | italic_l start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S4.Ex2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle l^{\prime}_{i}=\begin{dcases}l_{i},&amp;\text{if }l_{i}\text{ is %
question placeholder or not a word}\\
&amp;\text{else}\begin{dcases}\text{synonym}(l_{i}),&amp;\text{with probability }p\\
l_{i},&amp;\text{with probability }1-p\end{dcases}\end{dcases}" class="ltx_Math" display="inline" id="S4.Ex2.m1.6"><semantics id="S4.Ex2.m1.6a"><mrow id="S4.Ex2.m1.6.7" xref="S4.Ex2.m1.6.7.cmml"><msubsup id="S4.Ex2.m1.6.7.2" xref="S4.Ex2.m1.6.7.2.cmml"><mi id="S4.Ex2.m1.6.7.2.2.2" xref="S4.Ex2.m1.6.7.2.2.2.cmml">l</mi><mi id="S4.Ex2.m1.6.7.2.3" xref="S4.Ex2.m1.6.7.2.3.cmml">i</mi><mo id="S4.Ex2.m1.6.7.2.2.3" xref="S4.Ex2.m1.6.7.2.2.3.cmml">′</mo></msubsup><mo id="S4.Ex2.m1.6.7.1" xref="S4.Ex2.m1.6.7.1.cmml">=</mo><mrow id="S4.Ex2.m1.6.6a" xref="S4.Ex2.m1.6.7.3.1.cmml"><mo id="S4.Ex2.m1.6.6a.7" xref="S4.Ex2.m1.6.7.3.1.1.cmml">{</mo><mtable columnspacing="5pt" id="S4.Ex2.m1.6.6.6a" rowspacing="0pt" xref="S4.Ex2.m1.6.7.3.1.cmml"><mtr id="S4.Ex2.m1.6.6.6aa" xref="S4.Ex2.m1.6.7.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S4.Ex2.m1.6.6.6ab" xref="S4.Ex2.m1.6.7.3.1.cmml"><mrow id="S4.Ex2.m1.5.5.5.5.1.1.1" xref="S4.Ex2.m1.5.5.5.5.1.1.1.1.cmml"><msub id="S4.Ex2.m1.5.5.5.5.1.1.1.1" xref="S4.Ex2.m1.5.5.5.5.1.1.1.1.cmml"><mi id="S4.Ex2.m1.5.5.5.5.1.1.1.1.2" xref="S4.Ex2.m1.5.5.5.5.1.1.1.1.2.cmml">l</mi><mi id="S4.Ex2.m1.5.5.5.5.1.1.1.1.3" xref="S4.Ex2.m1.5.5.5.5.1.1.1.1.3.cmml">i</mi></msub><mo id="S4.Ex2.m1.5.5.5.5.1.1.1.2" xref="S4.Ex2.m1.5.5.5.5.1.1.1.1.cmml">,</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S4.Ex2.m1.6.6.6ac" xref="S4.Ex2.m1.6.7.3.1.cmml"><mrow id="S4.Ex2.m1.6.6.6.6.2.1" xref="S4.Ex2.m1.6.6.6.6.2.1.cmml"><mtext id="S4.Ex2.m1.6.6.6.6.2.1.2" xref="S4.Ex2.m1.6.6.6.6.2.1.2a.cmml">if </mtext><mo id="S4.Ex2.m1.6.6.6.6.2.1.1" xref="S4.Ex2.m1.6.6.6.6.2.1.1.cmml">⁢</mo><msub id="S4.Ex2.m1.6.6.6.6.2.1.3" xref="S4.Ex2.m1.6.6.6.6.2.1.3.cmml"><mi id="S4.Ex2.m1.6.6.6.6.2.1.3.2" xref="S4.Ex2.m1.6.6.6.6.2.1.3.2.cmml">l</mi><mi id="S4.Ex2.m1.6.6.6.6.2.1.3.3" xref="S4.Ex2.m1.6.6.6.6.2.1.3.3.cmml">i</mi></msub><mo id="S4.Ex2.m1.6.6.6.6.2.1.1a" xref="S4.Ex2.m1.6.6.6.6.2.1.1.cmml">⁢</mo><mtext id="S4.Ex2.m1.6.6.6.6.2.1.4" xref="S4.Ex2.m1.6.6.6.6.2.1.4a.cmml"> is question placeholder or not a word</mtext></mrow></mtd></mtr><mtr id="S4.Ex2.m1.6.6.6ad" xref="S4.Ex2.m1.6.7.3.1.cmml"><mtd id="S4.Ex2.m1.6.6.6ae" xref="S4.Ex2.m1.6.7.3.1.1.cmml"></mtd><mtd class="ltx_align_left" columnalign="left" id="S4.Ex2.m1.6.6.6af" xref="S4.Ex2.m1.6.7.3.1.cmml"><mrow id="S4.Ex2.m1.4.4.4.4.4.4" xref="S4.Ex2.m1.4.4.4.4.4.4.cmml"><mtext id="S4.Ex2.m1.4.4.4.4.4.4.6" xref="S4.Ex2.m1.4.4.4.4.4.4.6a.cmml">else</mtext><mo id="S4.Ex2.m1.4.4.4.4.4.4.5" xref="S4.Ex2.m1.4.4.4.4.4.4.5.cmml">⁢</mo><mrow id="S4.Ex2.m1.4.4.4.4.4.4.4a" xref="S4.Ex2.m1.4.4.4.4.4.4.7.1.cmml"><mo id="S4.Ex2.m1.4.4.4.4.4.4.4a.5" xref="S4.Ex2.m1.4.4.4.4.4.4.7.1.1.cmml">{</mo><mtable columnspacing="5pt" id="S4.Ex2.m1.4.4.4.4.4.4.4.4a" rowspacing="0pt" xref="S4.Ex2.m1.4.4.4.4.4.4.7.1.cmml"><mtr id="S4.Ex2.m1.4.4.4.4.4.4.4.4aa" xref="S4.Ex2.m1.4.4.4.4.4.4.7.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S4.Ex2.m1.4.4.4.4.4.4.4.4ab" xref="S4.Ex2.m1.4.4.4.4.4.4.7.1.cmml"><mrow id="S4.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S4.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mrow id="S4.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S4.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mtext id="S4.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S4.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3a.cmml">synonym</mtext><mo id="S4.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S4.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S4.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S4.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S4.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S4.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S4.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S4.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S4.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S4.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">l</mi><mi id="S4.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S4.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S4.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S4.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S4.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">,</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S4.Ex2.m1.4.4.4.4.4.4.4.4ac" xref="S4.Ex2.m1.4.4.4.4.4.4.7.1.cmml"><mrow id="S4.Ex2.m1.2.2.2.2.2.2.2.2.2.2.1" xref="S4.Ex2.m1.2.2.2.2.2.2.2.2.2.2.1.cmml"><mtext id="S4.Ex2.m1.2.2.2.2.2.2.2.2.2.2.1.2" xref="S4.Ex2.m1.2.2.2.2.2.2.2.2.2.2.1.2a.cmml">with probability </mtext><mo id="S4.Ex2.m1.2.2.2.2.2.2.2.2.2.2.1.1" xref="S4.Ex2.m1.2.2.2.2.2.2.2.2.2.2.1.1.cmml">⁢</mo><mi id="S4.Ex2.m1.2.2.2.2.2.2.2.2.2.2.1.3" xref="S4.Ex2.m1.2.2.2.2.2.2.2.2.2.2.1.3.cmml">p</mi></mrow></mtd></mtr><mtr id="S4.Ex2.m1.4.4.4.4.4.4.4.4ad" xref="S4.Ex2.m1.4.4.4.4.4.4.7.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S4.Ex2.m1.4.4.4.4.4.4.4.4ae" xref="S4.Ex2.m1.4.4.4.4.4.4.7.1.cmml"><mrow id="S4.Ex2.m1.3.3.3.3.3.3.3.3.3.1.1.1" xref="S4.Ex2.m1.3.3.3.3.3.3.3.3.3.1.1.1.1.cmml"><msub id="S4.Ex2.m1.3.3.3.3.3.3.3.3.3.1.1.1.1" xref="S4.Ex2.m1.3.3.3.3.3.3.3.3.3.1.1.1.1.cmml"><mi id="S4.Ex2.m1.3.3.3.3.3.3.3.3.3.1.1.1.1.2" xref="S4.Ex2.m1.3.3.3.3.3.3.3.3.3.1.1.1.1.2.cmml">l</mi><mi id="S4.Ex2.m1.3.3.3.3.3.3.3.3.3.1.1.1.1.3" xref="S4.Ex2.m1.3.3.3.3.3.3.3.3.3.1.1.1.1.3.cmml">i</mi></msub><mo id="S4.Ex2.m1.3.3.3.3.3.3.3.3.3.1.1.1.2" xref="S4.Ex2.m1.3.3.3.3.3.3.3.3.3.1.1.1.1.cmml">,</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S4.Ex2.m1.4.4.4.4.4.4.4.4af" xref="S4.Ex2.m1.4.4.4.4.4.4.7.1.cmml"><mrow id="S4.Ex2.m1.4.4.4.4.4.4.4.4.4.2.1" xref="S4.Ex2.m1.4.4.4.4.4.4.4.4.4.2.1.cmml"><mrow id="S4.Ex2.m1.4.4.4.4.4.4.4.4.4.2.1.2" xref="S4.Ex2.m1.4.4.4.4.4.4.4.4.4.2.1.2.cmml"><mtext id="S4.Ex2.m1.4.4.4.4.4.4.4.4.4.2.1.2.2" xref="S4.Ex2.m1.4.4.4.4.4.4.4.4.4.2.1.2.2a.cmml">with probability </mtext><mo id="S4.Ex2.m1.4.4.4.4.4.4.4.4.4.2.1.2.1" xref="S4.Ex2.m1.4.4.4.4.4.4.4.4.4.2.1.2.1.cmml">⁢</mo><mn id="S4.Ex2.m1.4.4.4.4.4.4.4.4.4.2.1.2.3" xref="S4.Ex2.m1.4.4.4.4.4.4.4.4.4.2.1.2.3.cmml">1</mn></mrow><mo id="S4.Ex2.m1.4.4.4.4.4.4.4.4.4.2.1.1" xref="S4.Ex2.m1.4.4.4.4.4.4.4.4.4.2.1.1.cmml">−</mo><mi id="S4.Ex2.m1.4.4.4.4.4.4.4.4.4.2.1.3" xref="S4.Ex2.m1.4.4.4.4.4.4.4.4.4.2.1.3.cmml">p</mi></mrow></mtd></mtr></mtable></mrow></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.Ex2.m1.6b"><apply id="S4.Ex2.m1.6.7.cmml" xref="S4.Ex2.m1.6.7"><eq id="S4.Ex2.m1.6.7.1.cmml" xref="S4.Ex2.m1.6.7.1"></eq><apply id="S4.Ex2.m1.6.7.2.cmml" xref="S4.Ex2.m1.6.7.2"><csymbol cd="ambiguous" id="S4.Ex2.m1.6.7.2.1.cmml" xref="S4.Ex2.m1.6.7.2">subscript</csymbol><apply id="S4.Ex2.m1.6.7.2.2.cmml" xref="S4.Ex2.m1.6.7.2"><csymbol cd="ambiguous" id="S4.Ex2.m1.6.7.2.2.1.cmml" xref="S4.Ex2.m1.6.7.2">superscript</csymbol><ci id="S4.Ex2.m1.6.7.2.2.2.cmml" xref="S4.Ex2.m1.6.7.2.2.2">𝑙</ci><ci id="S4.Ex2.m1.6.7.2.2.3.cmml" xref="S4.Ex2.m1.6.7.2.2.3">′</ci></apply><ci id="S4.Ex2.m1.6.7.2.3.cmml" xref="S4.Ex2.m1.6.7.2.3">𝑖</ci></apply><apply id="S4.Ex2.m1.6.7.3.1.cmml" xref="S4.Ex2.m1.6.6a"><csymbol cd="latexml" id="S4.Ex2.m1.6.7.3.1.1.cmml" xref="S4.Ex2.m1.6.6a.7">cases</csymbol><apply id="S4.Ex2.m1.5.5.5.5.1.1.1.1.cmml" xref="S4.Ex2.m1.5.5.5.5.1.1.1"><csymbol cd="ambiguous" id="S4.Ex2.m1.5.5.5.5.1.1.1.1.1.cmml" xref="S4.Ex2.m1.5.5.5.5.1.1.1">subscript</csymbol><ci id="S4.Ex2.m1.5.5.5.5.1.1.1.1.2.cmml" xref="S4.Ex2.m1.5.5.5.5.1.1.1.1.2">𝑙</ci><ci id="S4.Ex2.m1.5.5.5.5.1.1.1.1.3.cmml" xref="S4.Ex2.m1.5.5.5.5.1.1.1.1.3">𝑖</ci></apply><apply id="S4.Ex2.m1.6.6.6.6.2.1.cmml" xref="S4.Ex2.m1.6.6.6.6.2.1"><times id="S4.Ex2.m1.6.6.6.6.2.1.1.cmml" xref="S4.Ex2.m1.6.6.6.6.2.1.1"></times><ci id="S4.Ex2.m1.6.6.6.6.2.1.2a.cmml" xref="S4.Ex2.m1.6.6.6.6.2.1.2"><mtext id="S4.Ex2.m1.6.6.6.6.2.1.2.cmml" xref="S4.Ex2.m1.6.6.6.6.2.1.2">if </mtext></ci><apply id="S4.Ex2.m1.6.6.6.6.2.1.3.cmml" xref="S4.Ex2.m1.6.6.6.6.2.1.3"><csymbol cd="ambiguous" id="S4.Ex2.m1.6.6.6.6.2.1.3.1.cmml" xref="S4.Ex2.m1.6.6.6.6.2.1.3">subscript</csymbol><ci id="S4.Ex2.m1.6.6.6.6.2.1.3.2.cmml" xref="S4.Ex2.m1.6.6.6.6.2.1.3.2">𝑙</ci><ci id="S4.Ex2.m1.6.6.6.6.2.1.3.3.cmml" xref="S4.Ex2.m1.6.6.6.6.2.1.3.3">𝑖</ci></apply><ci id="S4.Ex2.m1.6.6.6.6.2.1.4a.cmml" xref="S4.Ex2.m1.6.6.6.6.2.1.4"><mtext id="S4.Ex2.m1.6.6.6.6.2.1.4.cmml" xref="S4.Ex2.m1.6.6.6.6.2.1.4"> is question placeholder or not a word</mtext></ci></apply><ci id="S4.Ex2.m1.6.7.3.1.4a.cmml" xref="S4.Ex2.m1.6.6a"><mtext class="ltx_mathvariant_italic" id="S4.Ex2.m1.6.7.3.1.4.cmml" xref="S4.Ex2.m1.6.6a.7">otherwise</mtext></ci><apply id="S4.Ex2.m1.4.4.4.4.4.4.cmml" xref="S4.Ex2.m1.4.4.4.4.4.4"><times id="S4.Ex2.m1.4.4.4.4.4.4.5.cmml" xref="S4.Ex2.m1.4.4.4.4.4.4.5"></times><ci id="S4.Ex2.m1.4.4.4.4.4.4.6a.cmml" xref="S4.Ex2.m1.4.4.4.4.4.4.6"><mtext id="S4.Ex2.m1.4.4.4.4.4.4.6.cmml" xref="S4.Ex2.m1.4.4.4.4.4.4.6">else</mtext></ci><apply id="S4.Ex2.m1.4.4.4.4.4.4.7.1.cmml" xref="S4.Ex2.m1.4.4.4.4.4.4.4a"><csymbol cd="latexml" id="S4.Ex2.m1.4.4.4.4.4.4.7.1.1.cmml" xref="S4.Ex2.m1.4.4.4.4.4.4.4a.5">cases</csymbol><apply id="S4.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.1"><times id="S4.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2"></times><ci id="S4.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3a.cmml" xref="S4.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3"><mtext id="S4.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3">synonym</mtext></ci><apply id="S4.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2">𝑙</ci><ci id="S4.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.Ex2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply></apply><apply id="S4.Ex2.m1.2.2.2.2.2.2.2.2.2.2.1.cmml" xref="S4.Ex2.m1.2.2.2.2.2.2.2.2.2.2.1"><times id="S4.Ex2.m1.2.2.2.2.2.2.2.2.2.2.1.1.cmml" xref="S4.Ex2.m1.2.2.2.2.2.2.2.2.2.2.1.1"></times><ci id="S4.Ex2.m1.2.2.2.2.2.2.2.2.2.2.1.2a.cmml" xref="S4.Ex2.m1.2.2.2.2.2.2.2.2.2.2.1.2"><mtext id="S4.Ex2.m1.2.2.2.2.2.2.2.2.2.2.1.2.cmml" xref="S4.Ex2.m1.2.2.2.2.2.2.2.2.2.2.1.2">with probability </mtext></ci><ci id="S4.Ex2.m1.2.2.2.2.2.2.2.2.2.2.1.3.cmml" xref="S4.Ex2.m1.2.2.2.2.2.2.2.2.2.2.1.3">𝑝</ci></apply><apply id="S4.Ex2.m1.3.3.3.3.3.3.3.3.3.1.1.1.1.cmml" xref="S4.Ex2.m1.3.3.3.3.3.3.3.3.3.1.1.1"><csymbol cd="ambiguous" id="S4.Ex2.m1.3.3.3.3.3.3.3.3.3.1.1.1.1.1.cmml" xref="S4.Ex2.m1.3.3.3.3.3.3.3.3.3.1.1.1">subscript</csymbol><ci id="S4.Ex2.m1.3.3.3.3.3.3.3.3.3.1.1.1.1.2.cmml" xref="S4.Ex2.m1.3.3.3.3.3.3.3.3.3.1.1.1.1.2">𝑙</ci><ci id="S4.Ex2.m1.3.3.3.3.3.3.3.3.3.1.1.1.1.3.cmml" xref="S4.Ex2.m1.3.3.3.3.3.3.3.3.3.1.1.1.1.3">𝑖</ci></apply><apply id="S4.Ex2.m1.4.4.4.4.4.4.4.4.4.2.1.cmml" xref="S4.Ex2.m1.4.4.4.4.4.4.4.4.4.2.1"><minus id="S4.Ex2.m1.4.4.4.4.4.4.4.4.4.2.1.1.cmml" xref="S4.Ex2.m1.4.4.4.4.4.4.4.4.4.2.1.1"></minus><apply id="S4.Ex2.m1.4.4.4.4.4.4.4.4.4.2.1.2.cmml" xref="S4.Ex2.m1.4.4.4.4.4.4.4.4.4.2.1.2"><times id="S4.Ex2.m1.4.4.4.4.4.4.4.4.4.2.1.2.1.cmml" xref="S4.Ex2.m1.4.4.4.4.4.4.4.4.4.2.1.2.1"></times><ci id="S4.Ex2.m1.4.4.4.4.4.4.4.4.4.2.1.2.2a.cmml" xref="S4.Ex2.m1.4.4.4.4.4.4.4.4.4.2.1.2.2"><mtext id="S4.Ex2.m1.4.4.4.4.4.4.4.4.4.2.1.2.2.cmml" xref="S4.Ex2.m1.4.4.4.4.4.4.4.4.4.2.1.2.2">with probability </mtext></ci><cn id="S4.Ex2.m1.4.4.4.4.4.4.4.4.4.2.1.2.3.cmml" type="integer" xref="S4.Ex2.m1.4.4.4.4.4.4.4.4.4.2.1.2.3">1</cn></apply><ci id="S4.Ex2.m1.4.4.4.4.4.4.4.4.4.2.1.3.cmml" xref="S4.Ex2.m1.4.4.4.4.4.4.4.4.4.2.1.3">𝑝</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.Ex2.m1.6c">\displaystyle l^{\prime}_{i}=\begin{dcases}l_{i},&amp;\text{if }l_{i}\text{ is %
question placeholder or not a word}\\
&amp;\text{else}\begin{dcases}\text{synonym}(l_{i}),&amp;\text{with probability }p\\
l_{i},&amp;\text{with probability }1-p\end{dcases}\end{dcases}</annotation><annotation encoding="application/x-llamapun" id="S4.Ex2.m1.6d">italic_l start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = { start_ROW start_CELL italic_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , end_CELL start_CELL if italic_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is question placeholder or not a word end_CELL end_ROW start_ROW start_CELL end_CELL start_CELL else { start_ROW start_CELL synonym ( italic_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) , end_CELL start_CELL with probability italic_p end_CELL end_ROW start_ROW start_CELL italic_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , end_CELL start_CELL with probability 1 - italic_p end_CELL end_ROW end_CELL end_ROW</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS4.p2.9">where
<math alttext="|" class="ltx_Math" display="inline" id="S4.SS4.p2.7.m1.1"><semantics id="S4.SS4.p2.7.m1.1a"><mo fence="false" id="S4.SS4.p2.7.m1.1.1" stretchy="false" xref="S4.SS4.p2.7.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.7.m1.1b"><ci id="S4.SS4.p2.7.m1.1.1.cmml" xref="S4.SS4.p2.7.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.7.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p2.7.m1.1d">|</annotation></semantics></math> denotes concatenation and <math alttext="l^{\prime}_{i}" class="ltx_Math" display="inline" id="S4.SS4.p2.8.m2.1"><semantics id="S4.SS4.p2.8.m2.1a"><msubsup id="S4.SS4.p2.8.m2.1.1" xref="S4.SS4.p2.8.m2.1.1.cmml"><mi id="S4.SS4.p2.8.m2.1.1.2.2" xref="S4.SS4.p2.8.m2.1.1.2.2.cmml">l</mi><mi id="S4.SS4.p2.8.m2.1.1.3" xref="S4.SS4.p2.8.m2.1.1.3.cmml">i</mi><mo id="S4.SS4.p2.8.m2.1.1.2.3" xref="S4.SS4.p2.8.m2.1.1.2.3.cmml">′</mo></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.8.m2.1b"><apply id="S4.SS4.p2.8.m2.1.1.cmml" xref="S4.SS4.p2.8.m2.1.1"><csymbol cd="ambiguous" id="S4.SS4.p2.8.m2.1.1.1.cmml" xref="S4.SS4.p2.8.m2.1.1">subscript</csymbol><apply id="S4.SS4.p2.8.m2.1.1.2.cmml" xref="S4.SS4.p2.8.m2.1.1"><csymbol cd="ambiguous" id="S4.SS4.p2.8.m2.1.1.2.1.cmml" xref="S4.SS4.p2.8.m2.1.1">superscript</csymbol><ci id="S4.SS4.p2.8.m2.1.1.2.2.cmml" xref="S4.SS4.p2.8.m2.1.1.2.2">𝑙</ci><ci id="S4.SS4.p2.8.m2.1.1.2.3.cmml" xref="S4.SS4.p2.8.m2.1.1.2.3">′</ci></apply><ci id="S4.SS4.p2.8.m2.1.1.3.cmml" xref="S4.SS4.p2.8.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.8.m2.1c">l^{\prime}_{i}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p2.8.m2.1d">italic_l start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>’s are potentially the synonyms of the original tokens <math alttext="l_{i}" class="ltx_Math" display="inline" id="S4.SS4.p2.9.m3.1"><semantics id="S4.SS4.p2.9.m3.1a"><msub id="S4.SS4.p2.9.m3.1.1" xref="S4.SS4.p2.9.m3.1.1.cmml"><mi id="S4.SS4.p2.9.m3.1.1.2" xref="S4.SS4.p2.9.m3.1.1.2.cmml">l</mi><mi id="S4.SS4.p2.9.m3.1.1.3" xref="S4.SS4.p2.9.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.9.m3.1b"><apply id="S4.SS4.p2.9.m3.1.1.cmml" xref="S4.SS4.p2.9.m3.1.1"><csymbol cd="ambiguous" id="S4.SS4.p2.9.m3.1.1.1.cmml" xref="S4.SS4.p2.9.m3.1.1">subscript</csymbol><ci id="S4.SS4.p2.9.m3.1.1.2.cmml" xref="S4.SS4.p2.9.m3.1.1.2">𝑙</ci><ci id="S4.SS4.p2.9.m3.1.1.3.cmml" xref="S4.SS4.p2.9.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.9.m3.1c">l_{i}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p2.9.m3.1d">italic_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>’s.</p>
</div>
<div class="ltx_para" id="S4.SS4.p3">
<p class="ltx_p" id="S4.SS4.p3.3">The probability parameter <math alttext="p" class="ltx_Math" display="inline" id="S4.SS4.p3.1.m1.1"><semantics id="S4.SS4.p3.1.m1.1a"><mi id="S4.SS4.p3.1.m1.1.1" xref="S4.SS4.p3.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.1.m1.1b"><ci id="S4.SS4.p3.1.m1.1.1.cmml" xref="S4.SS4.p3.1.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.1.m1.1c">p</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p3.1.m1.1d">italic_p</annotation></semantics></math> controls the trade-off between the diversity of the mutated prompt templates and preserving their semantic meaning.
A large value of <math alttext="p" class="ltx_Math" display="inline" id="S4.SS4.p3.2.m2.1"><semantics id="S4.SS4.p3.2.m2.1a"><mi id="S4.SS4.p3.2.m2.1.1" xref="S4.SS4.p3.2.m2.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.2.m2.1b"><ci id="S4.SS4.p3.2.m2.1.1.cmml" xref="S4.SS4.p3.2.m2.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.2.m2.1c">p</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p3.2.m2.1d">italic_p</annotation></semantics></math> would result in extreme mutation (i.e., the mutated prompt template would be very different from the seed prompt template), but at the cost of generating a semantically nonsensical mutated prompt template. On the other hand, a small value of <math alttext="p" class="ltx_Math" display="inline" id="S4.SS4.p3.3.m3.1"><semantics id="S4.SS4.p3.3.m3.1a"><mi id="S4.SS4.p3.3.m3.1.1" xref="S4.SS4.p3.3.m3.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.3.m3.1b"><ci id="S4.SS4.p3.3.m3.1.1.cmml" xref="S4.SS4.p3.3.m3.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.3.m3.1c">p</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p3.3.m3.1d">italic_p</annotation></semantics></math> would make very few changes to semantics of the seed, but also limit the diversity of the prompt template and thereby its potential effectiveness in jailbreaking the target LLM.</p>
</div>
<div class="ltx_para" id="S4.SS4.p4">
<p class="ltx_p" id="S4.SS4.p4.1">An important thing to note is that in order to preserve the semantics when replacing a word with its synonym, we only consider synonyms with the same part of speech. In other words, if the original word is an adjective, for instance, then we only consider synonyms that are adjectives.
We do this because if the parts of speech are not considered when performing synonym-based mutations, the mutated prompt template is likely to have semantic issues where, for instance, a noun is replaced with a verb.</p>
</div>
<div class="ltx_para" id="S4.SS4.p5">
<p class="ltx_p" id="S4.SS4.p5.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S4.T1" title="Table 1 ‣ 4.4 Synonym-based Mutation ‣ 4 Methodology ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_tag">1</span></a> compares the mutation rates (in seeds mutated per second)
for the LLM-based and our proposed synonym-based mutators.
It is clear that the latter is significantly faster (462<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS4.p5.1.m1.1"><semantics id="S4.SS4.p5.1.m1.1a"><mo id="S4.SS4.p5.1.m1.1.1" xref="S4.SS4.p5.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.p5.1.m1.1b"><times id="S4.SS4.p5.1.m1.1.1.cmml" xref="S4.SS4.p5.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p5.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p5.1.m1.1d">×</annotation></semantics></math>)
than the former, ensuring that the fuzzing process, which requires thousands of iterations,
remains scalable.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Comparison of LLM-based mutator with our synonym-based mutator.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T1.2" style="width:260.2pt;height:113.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(47.8pt,-20.9pt) scale(1.58005896631399,1.58005896631399) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T1.2.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T1.1.1.1.2">Mutator</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.1">Rate (seeds/s)<sup class="ltx_sup" id="S4.T1.1.1.1.1.1">↑</sup>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.2.2.3.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T1.2.2.3.1.1">LLM-based</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.2.3.1.2">0.84</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2.4.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.2.2.4.2.1">Synonym-based</th>
<td class="ltx_td ltx_align_center" id="S4.T1.2.2.4.2.2">388.8</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S4.T1.2.2.2.2">Improvement</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.2.2.2.1">462.7<math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.2.2.2.1.m1.1"><semantics id="S4.T1.2.2.2.1.m1.1a"><mo id="S4.T1.2.2.2.1.m1.1.1" xref="S4.T1.2.2.2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.1.m1.1b"><times id="S4.T1.2.2.2.1.m1.1.1.cmml" xref="S4.T1.2.2.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.2.2.2.1.m1.1d">×</annotation></semantics></math>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Embedding-based Evaluation</h3>
<div class="ltx_para" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.9"><span class="ltx_text ltx_font_bold" id="S4.SS5.p1.9.1">Limitation 3: Expensive and Slow Evaluation.</span>
Another fundamental limitation of our preliminary formulation is that it relies on the evaluator LLM, <math alttext="\mathcal{L}_{\text{eval}}" class="ltx_Math" display="inline" id="S4.SS5.p1.1.m1.1"><semantics id="S4.SS5.p1.1.m1.1a"><msub id="S4.SS5.p1.1.m1.1.1" xref="S4.SS5.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS5.p1.1.m1.1.1.2" xref="S4.SS5.p1.1.m1.1.1.2.cmml">ℒ</mi><mtext id="S4.SS5.p1.1.m1.1.1.3" xref="S4.SS5.p1.1.m1.1.1.3a.cmml">eval</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.1.m1.1b"><apply id="S4.SS5.p1.1.m1.1.1.cmml" xref="S4.SS5.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS5.p1.1.m1.1.1.1.cmml" xref="S4.SS5.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS5.p1.1.m1.1.1.2.cmml" xref="S4.SS5.p1.1.m1.1.1.2">ℒ</ci><ci id="S4.SS5.p1.1.m1.1.1.3a.cmml" xref="S4.SS5.p1.1.m1.1.1.3"><mtext id="S4.SS5.p1.1.m1.1.1.3.cmml" mathsize="70%" xref="S4.SS5.p1.1.m1.1.1.3">eval</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.1.m1.1c">\mathcal{L}_{\text{eval}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p1.1.m1.1d">caligraphic_L start_POSTSUBSCRIPT eval end_POSTSUBSCRIPT</annotation></semantics></math>, for evaluating if the response from the target LLM, <math alttext="\mathcal{L}_{\text{target}}" class="ltx_Math" display="inline" id="S4.SS5.p1.2.m2.1"><semantics id="S4.SS5.p1.2.m2.1a"><msub id="S4.SS5.p1.2.m2.1.1" xref="S4.SS5.p1.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS5.p1.2.m2.1.1.2" xref="S4.SS5.p1.2.m2.1.1.2.cmml">ℒ</mi><mtext id="S4.SS5.p1.2.m2.1.1.3" xref="S4.SS5.p1.2.m2.1.1.3a.cmml">target</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.2.m2.1b"><apply id="S4.SS5.p1.2.m2.1.1.cmml" xref="S4.SS5.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS5.p1.2.m2.1.1.1.cmml" xref="S4.SS5.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS5.p1.2.m2.1.1.2.cmml" xref="S4.SS5.p1.2.m2.1.1.2">ℒ</ci><ci id="S4.SS5.p1.2.m2.1.1.3a.cmml" xref="S4.SS5.p1.2.m2.1.1.3"><mtext id="S4.SS5.p1.2.m2.1.1.3.cmml" mathsize="70%" xref="S4.SS5.p1.2.m2.1.1.3">target</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.2.m2.1c">\mathcal{L}_{\text{target}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p1.2.m2.1d">caligraphic_L start_POSTSUBSCRIPT target end_POSTSUBSCRIPT</annotation></semantics></math>, is harmful/unethical or not. Ideally, we want to minimize the number of invocations of an LLM during fuzzing so that each iteration is quick.
Each evaluation requires prompting <math alttext="\mathcal{L}_{\text{eval}}" class="ltx_Math" display="inline" id="S4.SS5.p1.3.m3.1"><semantics id="S4.SS5.p1.3.m3.1a"><msub id="S4.SS5.p1.3.m3.1.1" xref="S4.SS5.p1.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS5.p1.3.m3.1.1.2" xref="S4.SS5.p1.3.m3.1.1.2.cmml">ℒ</mi><mtext id="S4.SS5.p1.3.m3.1.1.3" xref="S4.SS5.p1.3.m3.1.1.3a.cmml">eval</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.3.m3.1b"><apply id="S4.SS5.p1.3.m3.1.1.cmml" xref="S4.SS5.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS5.p1.3.m3.1.1.1.cmml" xref="S4.SS5.p1.3.m3.1.1">subscript</csymbol><ci id="S4.SS5.p1.3.m3.1.1.2.cmml" xref="S4.SS5.p1.3.m3.1.1.2">ℒ</ci><ci id="S4.SS5.p1.3.m3.1.1.3a.cmml" xref="S4.SS5.p1.3.m3.1.1.3"><mtext id="S4.SS5.p1.3.m3.1.1.3.cmml" mathsize="70%" xref="S4.SS5.p1.3.m3.1.1.3">eval</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.3.m3.1c">\mathcal{L}_{\text{eval}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p1.3.m3.1d">caligraphic_L start_POSTSUBSCRIPT eval end_POSTSUBSCRIPT</annotation></semantics></math> with the harmful/unethical question, <math alttext="q" class="ltx_Math" display="inline" id="S4.SS5.p1.4.m4.1"><semantics id="S4.SS5.p1.4.m4.1a"><mi id="S4.SS5.p1.4.m4.1.1" xref="S4.SS5.p1.4.m4.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.4.m4.1b"><ci id="S4.SS5.p1.4.m4.1.1.cmml" xref="S4.SS5.p1.4.m4.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.4.m4.1c">q</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p1.4.m4.1d">italic_q</annotation></semantics></math>, the response, <math alttext="r_{t}" class="ltx_Math" display="inline" id="S4.SS5.p1.5.m5.1"><semantics id="S4.SS5.p1.5.m5.1a"><msub id="S4.SS5.p1.5.m5.1.1" xref="S4.SS5.p1.5.m5.1.1.cmml"><mi id="S4.SS5.p1.5.m5.1.1.2" xref="S4.SS5.p1.5.m5.1.1.2.cmml">r</mi><mi id="S4.SS5.p1.5.m5.1.1.3" xref="S4.SS5.p1.5.m5.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.5.m5.1b"><apply id="S4.SS5.p1.5.m5.1.1.cmml" xref="S4.SS5.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS5.p1.5.m5.1.1.1.cmml" xref="S4.SS5.p1.5.m5.1.1">subscript</csymbol><ci id="S4.SS5.p1.5.m5.1.1.2.cmml" xref="S4.SS5.p1.5.m5.1.1.2">𝑟</ci><ci id="S4.SS5.p1.5.m5.1.1.3.cmml" xref="S4.SS5.p1.5.m5.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.5.m5.1c">r_{t}</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p1.5.m5.1d">italic_r start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> from <math alttext="\mathcal{L}_{\text{target}}" class="ltx_Math" display="inline" id="S4.SS5.p1.6.m6.1"><semantics id="S4.SS5.p1.6.m6.1a"><msub id="S4.SS5.p1.6.m6.1.1" xref="S4.SS5.p1.6.m6.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS5.p1.6.m6.1.1.2" xref="S4.SS5.p1.6.m6.1.1.2.cmml">ℒ</mi><mtext id="S4.SS5.p1.6.m6.1.1.3" xref="S4.SS5.p1.6.m6.1.1.3a.cmml">target</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.6.m6.1b"><apply id="S4.SS5.p1.6.m6.1.1.cmml" xref="S4.SS5.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS5.p1.6.m6.1.1.1.cmml" xref="S4.SS5.p1.6.m6.1.1">subscript</csymbol><ci id="S4.SS5.p1.6.m6.1.1.2.cmml" xref="S4.SS5.p1.6.m6.1.1.2">ℒ</ci><ci id="S4.SS5.p1.6.m6.1.1.3a.cmml" xref="S4.SS5.p1.6.m6.1.1.3"><mtext id="S4.SS5.p1.6.m6.1.1.3.cmml" mathsize="70%" xref="S4.SS5.p1.6.m6.1.1.3">target</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.6.m6.1c">\mathcal{L}_{\text{target}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p1.6.m6.1d">caligraphic_L start_POSTSUBSCRIPT target end_POSTSUBSCRIPT</annotation></semantics></math>, and instructions to determine if the <math alttext="r_{t}" class="ltx_Math" display="inline" id="S4.SS5.p1.7.m7.1"><semantics id="S4.SS5.p1.7.m7.1a"><msub id="S4.SS5.p1.7.m7.1.1" xref="S4.SS5.p1.7.m7.1.1.cmml"><mi id="S4.SS5.p1.7.m7.1.1.2" xref="S4.SS5.p1.7.m7.1.1.2.cmml">r</mi><mi id="S4.SS5.p1.7.m7.1.1.3" xref="S4.SS5.p1.7.m7.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.7.m7.1b"><apply id="S4.SS5.p1.7.m7.1.1.cmml" xref="S4.SS5.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S4.SS5.p1.7.m7.1.1.1.cmml" xref="S4.SS5.p1.7.m7.1.1">subscript</csymbol><ci id="S4.SS5.p1.7.m7.1.1.2.cmml" xref="S4.SS5.p1.7.m7.1.1.2">𝑟</ci><ci id="S4.SS5.p1.7.m7.1.1.3.cmml" xref="S4.SS5.p1.7.m7.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.7.m7.1c">r_{t}</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p1.7.m7.1d">italic_r start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> answers the harmful/unethical question <math alttext="q" class="ltx_Math" display="inline" id="S4.SS5.p1.8.m8.1"><semantics id="S4.SS5.p1.8.m8.1a"><mi id="S4.SS5.p1.8.m8.1.1" xref="S4.SS5.p1.8.m8.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.8.m8.1b"><ci id="S4.SS5.p1.8.m8.1.1.cmml" xref="S4.SS5.p1.8.m8.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.8.m8.1c">q</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p1.8.m8.1d">italic_q</annotation></semantics></math>. Each such evaluation during a fuzzing iteration, in practice, takes several seconds, and fuzzing campaigns typically require several thousand iterations. This is further exacerbated by the rate limits imposed by commercial LLM providers. So, the fuzzing time required for such an approach is clearly impractical for red-teaming LLMs with the tight time-to-market requirements in the AI industry.
Hence, we aim to develop a framework that has minimal reliance on an LLM (except, of course, <math alttext="\mathcal{L}_{\text{target}}" class="ltx_Math" display="inline" id="S4.SS5.p1.9.m9.1"><semantics id="S4.SS5.p1.9.m9.1a"><msub id="S4.SS5.p1.9.m9.1.1" xref="S4.SS5.p1.9.m9.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS5.p1.9.m9.1.1.2" xref="S4.SS5.p1.9.m9.1.1.2.cmml">ℒ</mi><mtext id="S4.SS5.p1.9.m9.1.1.3" xref="S4.SS5.p1.9.m9.1.1.3a.cmml">target</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.9.m9.1b"><apply id="S4.SS5.p1.9.m9.1.1.cmml" xref="S4.SS5.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S4.SS5.p1.9.m9.1.1.1.cmml" xref="S4.SS5.p1.9.m9.1.1">subscript</csymbol><ci id="S4.SS5.p1.9.m9.1.1.2.cmml" xref="S4.SS5.p1.9.m9.1.1.2">ℒ</ci><ci id="S4.SS5.p1.9.m9.1.1.3a.cmml" xref="S4.SS5.p1.9.m9.1.1.3"><mtext id="S4.SS5.p1.9.m9.1.1.3.cmml" mathsize="70%" xref="S4.SS5.p1.9.m9.1.1.3">target</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.9.m9.1c">\mathcal{L}_{\text{target}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p1.9.m9.1d">caligraphic_L start_POSTSUBSCRIPT target end_POSTSUBSCRIPT</annotation></semantics></math>, which is required for the execution step).</p>
</div>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="119" id="S4.F4.g1" src="x2.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Final <span class="ltx_text ltx_font_italic" id="S4.F4.2.1">JBFuzz</span> framework.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS5.p2">
<p class="ltx_p" id="S4.SS5.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS5.p2.1.1">Solution  <svg class="ltx_picture" height="15.74" id="S4.SS5.p2.1.1.pic1" overflow="visible" version="1.1" width="15.74"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,15.74) matrix(1 0 0 -1 0 0) translate(7.87,0) translate(0,7.87)"><g color="#FFFFFF" fill="#007C00" stroke="#FFFFFF"><path d="M 7.59 0 C 7.59 4.19 4.19 7.59 0 7.59 C -4.19 7.59 -7.59 4.19 -7.59 0 C -7.59 -4.19 -4.19 -7.59 0 -7.59 C 4.19 -7.59 7.59 -4.19 7.59 0 Z M 0 0"></path></g><g color="#FFFFFF" fill="#FFFFFF" stroke="#FFFFFF" transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.92"><span class="ltx_text" id="S4.SS5.p2.1.1.pic1.1.1.1.1.1">3</span></foreignobject></g></g></svg>: Embedding-based Evaluation.</span> To address this limitation, we need an extremely quick and lightweight evaluation mechanism that, ideally, does not rely on an LLM. However, unlike mutation, achieving this is very challenging since determining whether a piece of text contains harmful/unethical content requires a effective method for natural language understanding, and it must be done without LLMs. To achieve this, we develop a lightweight off-the-shelf embedding-based evaluator for this task.</p>
</div>
<div class="ltx_para" id="S4.SS5.p3">
<p class="ltx_p" id="S4.SS5.p3.3">More specifically, our embedding-based evaluator consists of two parts: an embedding model <math alttext="\mathcal{E}" class="ltx_Math" display="inline" id="S4.SS5.p3.1.m1.1"><semantics id="S4.SS5.p3.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS5.p3.1.m1.1.1" xref="S4.SS5.p3.1.m1.1.1.cmml">ℰ</mi><annotation-xml encoding="MathML-Content" id="S4.SS5.p3.1.m1.1b"><ci id="S4.SS5.p3.1.m1.1.1.cmml" xref="S4.SS5.p3.1.m1.1.1">ℰ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p3.1.m1.1c">\mathcal{E}</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p3.1.m1.1d">caligraphic_E</annotation></semantics></math> and a classifier <math alttext="\mathcal{C}" class="ltx_Math" display="inline" id="S4.SS5.p3.2.m2.1"><semantics id="S4.SS5.p3.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS5.p3.2.m2.1.1" xref="S4.SS5.p3.2.m2.1.1.cmml">𝒞</mi><annotation-xml encoding="MathML-Content" id="S4.SS5.p3.2.m2.1b"><ci id="S4.SS5.p3.2.m2.1.1.cmml" xref="S4.SS5.p3.2.m2.1.1">𝒞</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p3.2.m2.1c">\mathcal{C}</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p3.2.m2.1d">caligraphic_C</annotation></semantics></math>. The embedding model transforms a given text (i.e., response <math alttext="r_{t}" class="ltx_Math" display="inline" id="S4.SS5.p3.3.m3.1"><semantics id="S4.SS5.p3.3.m3.1a"><msub id="S4.SS5.p3.3.m3.1.1" xref="S4.SS5.p3.3.m3.1.1.cmml"><mi id="S4.SS5.p3.3.m3.1.1.2" xref="S4.SS5.p3.3.m3.1.1.2.cmml">r</mi><mi id="S4.SS5.p3.3.m3.1.1.3" xref="S4.SS5.p3.3.m3.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS5.p3.3.m3.1b"><apply id="S4.SS5.p3.3.m3.1.1.cmml" xref="S4.SS5.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS5.p3.3.m3.1.1.1.cmml" xref="S4.SS5.p3.3.m3.1.1">subscript</csymbol><ci id="S4.SS5.p3.3.m3.1.1.2.cmml" xref="S4.SS5.p3.3.m3.1.1.2">𝑟</ci><ci id="S4.SS5.p3.3.m3.1.1.3.cmml" xref="S4.SS5.p3.3.m3.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p3.3.m3.1c">r_{t}</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p3.3.m3.1d">italic_r start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> from the LLM) into a fixed-size vector, i.e., an embedding. The key property of this transformation is that texts that are semantically similar are also closer in the embedding space. This means that if a piece of text contains harmful/unethical content, it will be closer (in the embedding space) to other harmful/unethical texts. The classifier, given the vector from the embedding model, predicts its label (i.e., whether it contains harmful/unethical content). In order to realize a successful evaluation mechanism, all we need is a set of labeled positive and negative examples of harmful/unethical content for the classifier. Fortunately, prior works have already developed such a dataset, which we can use directly <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib98" title="">98</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS5.p4">
<p class="ltx_p" id="S4.SS5.p4.3">Note that, since the classifier works with vectors and not text, we embed all labeled positive and negative examples of harmful/unethical content from the dataset using the embedding model before fuzzing begins in order to
not incur unnecessary overheads during the fuzzing process. Then, during the fuzzing process, when we receive the response <math alttext="r_{t}" class="ltx_Math" display="inline" id="S4.SS5.p4.1.m1.1"><semantics id="S4.SS5.p4.1.m1.1a"><msub id="S4.SS5.p4.1.m1.1.1" xref="S4.SS5.p4.1.m1.1.1.cmml"><mi id="S4.SS5.p4.1.m1.1.1.2" xref="S4.SS5.p4.1.m1.1.1.2.cmml">r</mi><mi id="S4.SS5.p4.1.m1.1.1.3" xref="S4.SS5.p4.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS5.p4.1.m1.1b"><apply id="S4.SS5.p4.1.m1.1.1.cmml" xref="S4.SS5.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS5.p4.1.m1.1.1.1.cmml" xref="S4.SS5.p4.1.m1.1.1">subscript</csymbol><ci id="S4.SS5.p4.1.m1.1.1.2.cmml" xref="S4.SS5.p4.1.m1.1.1.2">𝑟</ci><ci id="S4.SS5.p4.1.m1.1.1.3.cmml" xref="S4.SS5.p4.1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p4.1.m1.1c">r_{t}</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p4.1.m1.1d">italic_r start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> from the <math alttext="\mathcal{L}_{\text{target}}" class="ltx_Math" display="inline" id="S4.SS5.p4.2.m2.1"><semantics id="S4.SS5.p4.2.m2.1a"><msub id="S4.SS5.p4.2.m2.1.1" xref="S4.SS5.p4.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS5.p4.2.m2.1.1.2" xref="S4.SS5.p4.2.m2.1.1.2.cmml">ℒ</mi><mtext id="S4.SS5.p4.2.m2.1.1.3" xref="S4.SS5.p4.2.m2.1.1.3a.cmml">target</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS5.p4.2.m2.1b"><apply id="S4.SS5.p4.2.m2.1.1.cmml" xref="S4.SS5.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS5.p4.2.m2.1.1.1.cmml" xref="S4.SS5.p4.2.m2.1.1">subscript</csymbol><ci id="S4.SS5.p4.2.m2.1.1.2.cmml" xref="S4.SS5.p4.2.m2.1.1.2">ℒ</ci><ci id="S4.SS5.p4.2.m2.1.1.3a.cmml" xref="S4.SS5.p4.2.m2.1.1.3"><mtext id="S4.SS5.p4.2.m2.1.1.3.cmml" mathsize="70%" xref="S4.SS5.p4.2.m2.1.1.3">target</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p4.2.m2.1c">\mathcal{L}_{\text{target}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p4.2.m2.1d">caligraphic_L start_POSTSUBSCRIPT target end_POSTSUBSCRIPT</annotation></semantics></math>, we obtain the embedding for that response using the same lightweight embedding model. This embedding is then passed to the classifier to predict the label for <math alttext="r_{t}" class="ltx_Math" display="inline" id="S4.SS5.p4.3.m3.1"><semantics id="S4.SS5.p4.3.m3.1a"><msub id="S4.SS5.p4.3.m3.1.1" xref="S4.SS5.p4.3.m3.1.1.cmml"><mi id="S4.SS5.p4.3.m3.1.1.2" xref="S4.SS5.p4.3.m3.1.1.2.cmml">r</mi><mi id="S4.SS5.p4.3.m3.1.1.3" xref="S4.SS5.p4.3.m3.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS5.p4.3.m3.1b"><apply id="S4.SS5.p4.3.m3.1.1.cmml" xref="S4.SS5.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS5.p4.3.m3.1.1.1.cmml" xref="S4.SS5.p4.3.m3.1.1">subscript</csymbol><ci id="S4.SS5.p4.3.m3.1.1.2.cmml" xref="S4.SS5.p4.3.m3.1.1.2">𝑟</ci><ci id="S4.SS5.p4.3.m3.1.1.3.cmml" xref="S4.SS5.p4.3.m3.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p4.3.m3.1c">r_{t}</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p4.3.m3.1d">italic_r start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>. Mathematically, our novel evaluation process is as follows:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S8.EGx2">
<tbody id="S4.Ex3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathbb{EV}_{\mathcal{E};\mathcal{C};\mathcal{Y}}(r_{t})=\mathcal%
{C}\left(\mathcal{E}(r_{t}),\mathcal{E}(\mathcal{Y})\right)" class="ltx_Math" display="inline" id="S4.Ex3.m1.7"><semantics id="S4.Ex3.m1.7a"><mrow id="S4.Ex3.m1.7.7" xref="S4.Ex3.m1.7.7.cmml"><mrow id="S4.Ex3.m1.5.5.1" xref="S4.Ex3.m1.5.5.1.cmml"><mi id="S4.Ex3.m1.5.5.1.3" xref="S4.Ex3.m1.5.5.1.3.cmml">𝔼</mi><mo id="S4.Ex3.m1.5.5.1.2" xref="S4.Ex3.m1.5.5.1.2.cmml">⁢</mo><msub id="S4.Ex3.m1.5.5.1.4" xref="S4.Ex3.m1.5.5.1.4.cmml"><mi id="S4.Ex3.m1.5.5.1.4.2" xref="S4.Ex3.m1.5.5.1.4.2.cmml">𝕍</mi><mrow id="S4.Ex3.m1.3.3.3.5" xref="S4.Ex3.m1.3.3.3.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.Ex3.m1.1.1.1.1" xref="S4.Ex3.m1.1.1.1.1.cmml">ℰ</mi><mo id="S4.Ex3.m1.3.3.3.5.1" xref="S4.Ex3.m1.3.3.3.4.cmml">;</mo><mi class="ltx_font_mathcaligraphic" id="S4.Ex3.m1.2.2.2.2" xref="S4.Ex3.m1.2.2.2.2.cmml">𝒞</mi><mo id="S4.Ex3.m1.3.3.3.5.2" xref="S4.Ex3.m1.3.3.3.4.cmml">;</mo><mi class="ltx_font_mathcaligraphic" id="S4.Ex3.m1.3.3.3.3" xref="S4.Ex3.m1.3.3.3.3.cmml">𝒴</mi></mrow></msub><mo id="S4.Ex3.m1.5.5.1.2a" xref="S4.Ex3.m1.5.5.1.2.cmml">⁢</mo><mrow id="S4.Ex3.m1.5.5.1.1.1" xref="S4.Ex3.m1.5.5.1.1.1.1.cmml"><mo id="S4.Ex3.m1.5.5.1.1.1.2" stretchy="false" xref="S4.Ex3.m1.5.5.1.1.1.1.cmml">(</mo><msub id="S4.Ex3.m1.5.5.1.1.1.1" xref="S4.Ex3.m1.5.5.1.1.1.1.cmml"><mi id="S4.Ex3.m1.5.5.1.1.1.1.2" xref="S4.Ex3.m1.5.5.1.1.1.1.2.cmml">r</mi><mi id="S4.Ex3.m1.5.5.1.1.1.1.3" xref="S4.Ex3.m1.5.5.1.1.1.1.3.cmml">t</mi></msub><mo id="S4.Ex3.m1.5.5.1.1.1.3" stretchy="false" xref="S4.Ex3.m1.5.5.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.Ex3.m1.7.7.4" xref="S4.Ex3.m1.7.7.4.cmml">=</mo><mrow id="S4.Ex3.m1.7.7.3" xref="S4.Ex3.m1.7.7.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.Ex3.m1.7.7.3.4" xref="S4.Ex3.m1.7.7.3.4.cmml">𝒞</mi><mo id="S4.Ex3.m1.7.7.3.3" xref="S4.Ex3.m1.7.7.3.3.cmml">⁢</mo><mrow id="S4.Ex3.m1.7.7.3.2.2" xref="S4.Ex3.m1.7.7.3.2.3.cmml"><mo id="S4.Ex3.m1.7.7.3.2.2.3" xref="S4.Ex3.m1.7.7.3.2.3.cmml">(</mo><mrow id="S4.Ex3.m1.6.6.2.1.1.1" xref="S4.Ex3.m1.6.6.2.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.Ex3.m1.6.6.2.1.1.1.3" xref="S4.Ex3.m1.6.6.2.1.1.1.3.cmml">ℰ</mi><mo id="S4.Ex3.m1.6.6.2.1.1.1.2" xref="S4.Ex3.m1.6.6.2.1.1.1.2.cmml">⁢</mo><mrow id="S4.Ex3.m1.6.6.2.1.1.1.1.1" xref="S4.Ex3.m1.6.6.2.1.1.1.1.1.1.cmml"><mo id="S4.Ex3.m1.6.6.2.1.1.1.1.1.2" stretchy="false" xref="S4.Ex3.m1.6.6.2.1.1.1.1.1.1.cmml">(</mo><msub id="S4.Ex3.m1.6.6.2.1.1.1.1.1.1" xref="S4.Ex3.m1.6.6.2.1.1.1.1.1.1.cmml"><mi id="S4.Ex3.m1.6.6.2.1.1.1.1.1.1.2" xref="S4.Ex3.m1.6.6.2.1.1.1.1.1.1.2.cmml">r</mi><mi id="S4.Ex3.m1.6.6.2.1.1.1.1.1.1.3" xref="S4.Ex3.m1.6.6.2.1.1.1.1.1.1.3.cmml">t</mi></msub><mo id="S4.Ex3.m1.6.6.2.1.1.1.1.1.3" stretchy="false" xref="S4.Ex3.m1.6.6.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.Ex3.m1.7.7.3.2.2.4" xref="S4.Ex3.m1.7.7.3.2.3.cmml">,</mo><mrow id="S4.Ex3.m1.7.7.3.2.2.2" xref="S4.Ex3.m1.7.7.3.2.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.Ex3.m1.7.7.3.2.2.2.2" xref="S4.Ex3.m1.7.7.3.2.2.2.2.cmml">ℰ</mi><mo id="S4.Ex3.m1.7.7.3.2.2.2.1" xref="S4.Ex3.m1.7.7.3.2.2.2.1.cmml">⁢</mo><mrow id="S4.Ex3.m1.7.7.3.2.2.2.3.2" xref="S4.Ex3.m1.7.7.3.2.2.2.cmml"><mo id="S4.Ex3.m1.7.7.3.2.2.2.3.2.1" stretchy="false" xref="S4.Ex3.m1.7.7.3.2.2.2.cmml">(</mo><mi class="ltx_font_mathcaligraphic" id="S4.Ex3.m1.4.4" xref="S4.Ex3.m1.4.4.cmml">𝒴</mi><mo id="S4.Ex3.m1.7.7.3.2.2.2.3.2.2" stretchy="false" xref="S4.Ex3.m1.7.7.3.2.2.2.cmml">)</mo></mrow></mrow><mo id="S4.Ex3.m1.7.7.3.2.2.5" xref="S4.Ex3.m1.7.7.3.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.Ex3.m1.7b"><apply id="S4.Ex3.m1.7.7.cmml" xref="S4.Ex3.m1.7.7"><eq id="S4.Ex3.m1.7.7.4.cmml" xref="S4.Ex3.m1.7.7.4"></eq><apply id="S4.Ex3.m1.5.5.1.cmml" xref="S4.Ex3.m1.5.5.1"><times id="S4.Ex3.m1.5.5.1.2.cmml" xref="S4.Ex3.m1.5.5.1.2"></times><ci id="S4.Ex3.m1.5.5.1.3.cmml" xref="S4.Ex3.m1.5.5.1.3">𝔼</ci><apply id="S4.Ex3.m1.5.5.1.4.cmml" xref="S4.Ex3.m1.5.5.1.4"><csymbol cd="ambiguous" id="S4.Ex3.m1.5.5.1.4.1.cmml" xref="S4.Ex3.m1.5.5.1.4">subscript</csymbol><ci id="S4.Ex3.m1.5.5.1.4.2.cmml" xref="S4.Ex3.m1.5.5.1.4.2">𝕍</ci><list id="S4.Ex3.m1.3.3.3.4.cmml" xref="S4.Ex3.m1.3.3.3.5"><ci id="S4.Ex3.m1.1.1.1.1.cmml" xref="S4.Ex3.m1.1.1.1.1">ℰ</ci><ci id="S4.Ex3.m1.2.2.2.2.cmml" xref="S4.Ex3.m1.2.2.2.2">𝒞</ci><ci id="S4.Ex3.m1.3.3.3.3.cmml" xref="S4.Ex3.m1.3.3.3.3">𝒴</ci></list></apply><apply id="S4.Ex3.m1.5.5.1.1.1.1.cmml" xref="S4.Ex3.m1.5.5.1.1.1"><csymbol cd="ambiguous" id="S4.Ex3.m1.5.5.1.1.1.1.1.cmml" xref="S4.Ex3.m1.5.5.1.1.1">subscript</csymbol><ci id="S4.Ex3.m1.5.5.1.1.1.1.2.cmml" xref="S4.Ex3.m1.5.5.1.1.1.1.2">𝑟</ci><ci id="S4.Ex3.m1.5.5.1.1.1.1.3.cmml" xref="S4.Ex3.m1.5.5.1.1.1.1.3">𝑡</ci></apply></apply><apply id="S4.Ex3.m1.7.7.3.cmml" xref="S4.Ex3.m1.7.7.3"><times id="S4.Ex3.m1.7.7.3.3.cmml" xref="S4.Ex3.m1.7.7.3.3"></times><ci id="S4.Ex3.m1.7.7.3.4.cmml" xref="S4.Ex3.m1.7.7.3.4">𝒞</ci><interval closure="open" id="S4.Ex3.m1.7.7.3.2.3.cmml" xref="S4.Ex3.m1.7.7.3.2.2"><apply id="S4.Ex3.m1.6.6.2.1.1.1.cmml" xref="S4.Ex3.m1.6.6.2.1.1.1"><times id="S4.Ex3.m1.6.6.2.1.1.1.2.cmml" xref="S4.Ex3.m1.6.6.2.1.1.1.2"></times><ci id="S4.Ex3.m1.6.6.2.1.1.1.3.cmml" xref="S4.Ex3.m1.6.6.2.1.1.1.3">ℰ</ci><apply id="S4.Ex3.m1.6.6.2.1.1.1.1.1.1.cmml" xref="S4.Ex3.m1.6.6.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.Ex3.m1.6.6.2.1.1.1.1.1.1.1.cmml" xref="S4.Ex3.m1.6.6.2.1.1.1.1.1">subscript</csymbol><ci id="S4.Ex3.m1.6.6.2.1.1.1.1.1.1.2.cmml" xref="S4.Ex3.m1.6.6.2.1.1.1.1.1.1.2">𝑟</ci><ci id="S4.Ex3.m1.6.6.2.1.1.1.1.1.1.3.cmml" xref="S4.Ex3.m1.6.6.2.1.1.1.1.1.1.3">𝑡</ci></apply></apply><apply id="S4.Ex3.m1.7.7.3.2.2.2.cmml" xref="S4.Ex3.m1.7.7.3.2.2.2"><times id="S4.Ex3.m1.7.7.3.2.2.2.1.cmml" xref="S4.Ex3.m1.7.7.3.2.2.2.1"></times><ci id="S4.Ex3.m1.7.7.3.2.2.2.2.cmml" xref="S4.Ex3.m1.7.7.3.2.2.2.2">ℰ</ci><ci id="S4.Ex3.m1.4.4.cmml" xref="S4.Ex3.m1.4.4">𝒴</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.Ex3.m1.7c">\displaystyle\mathbb{EV}_{\mathcal{E};\mathcal{C};\mathcal{Y}}(r_{t})=\mathcal%
{C}\left(\mathcal{E}(r_{t}),\mathcal{E}(\mathcal{Y})\right)</annotation><annotation encoding="application/x-llamapun" id="S4.Ex3.m1.7d">blackboard_E blackboard_V start_POSTSUBSCRIPT caligraphic_E ; caligraphic_C ; caligraphic_Y end_POSTSUBSCRIPT ( italic_r start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = caligraphic_C ( caligraphic_E ( italic_r start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) , caligraphic_E ( caligraphic_Y ) )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S4.SS5.p5">
<p class="ltx_p" id="S4.SS5.p5.1">where <math alttext="\mathcal{Y}" class="ltx_Math" display="inline" id="S4.SS5.p5.1.m1.1"><semantics id="S4.SS5.p5.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS5.p5.1.m1.1.1" xref="S4.SS5.p5.1.m1.1.1.cmml">𝒴</mi><annotation-xml encoding="MathML-Content" id="S4.SS5.p5.1.m1.1b"><ci id="S4.SS5.p5.1.m1.1.1.cmml" xref="S4.SS5.p5.1.m1.1.1">𝒴</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p5.1.m1.1c">\mathcal{Y}</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p5.1.m1.1d">caligraphic_Y</annotation></semantics></math> denotes the labeled set of positive and negative examples.</p>
</div>
<div class="ltx_para" id="S4.SS5.p6">
<p class="ltx_p" id="S4.SS5.p6.2">We can use a variety of different classifiers in our evaluator, such as <math alttext="k" class="ltx_Math" display="inline" id="S4.SS5.p6.1.m1.1"><semantics id="S4.SS5.p6.1.m1.1a"><mi id="S4.SS5.p6.1.m1.1.1" xref="S4.SS5.p6.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS5.p6.1.m1.1b"><ci id="S4.SS5.p6.1.m1.1.1.cmml" xref="S4.SS5.p6.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p6.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p6.1.m1.1d">italic_k</annotation></semantics></math>-nearest neighbors or a small multi-layer perceptron model. We experiment with different embedding models and classifiers to determine the optimal configuration in Section <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S5.SS2" title="5.2 Evaluator Configuration ‣ 5 Results ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_tag">5.2</span></a>. Note that since our evaluation mechanism uses a lightweight embedding model and classifier, it is extremely fast. In fact, our optimal embedding-based evaluator configuration is <math alttext="16\times" class="ltx_math_unparsed" display="inline" id="S4.SS5.p6.2.m2.1"><semantics id="S4.SS5.p6.2.m2.1a"><mrow id="S4.SS5.p6.2.m2.1b"><mn id="S4.SS5.p6.2.m2.1.1">16</mn><mo id="S4.SS5.p6.2.m2.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S4.SS5.p6.2.m2.1c">16\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p6.2.m2.1d">16 ×</annotation></semantics></math> faster than the GPT-4o-based evaluator, while actually improving accuracy (more details in Figure <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S5.F5" title="Figure 5 ‣ 5.1 Experimental Setup ‣ 5 Results ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_tag">5</span></a> and Section <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S5.SS2" title="5.2 Evaluator Configuration ‣ 5 Results ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_tag">5.2</span></a>).</p>
</div>
<div class="ltx_para" id="S4.SS5.p7">
<p class="ltx_p" id="S4.SS5.p7.3">Next, we unify all three solutions developed above and describe our final fuzzing formulation that successfully jailbreaks several LLMs using  <svg class="ltx_picture" height="15.74" id="S4.SS5.p7.1.pic1" overflow="visible" version="1.1" width="15.74"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,15.74) matrix(1 0 0 -1 0 0) translate(7.87,0) translate(0,7.87)"><g color="#FFFFFF" fill="#007C00" stroke="#FFFFFF"><path d="M 7.59 0 C 7.59 4.19 4.19 7.59 0 7.59 C -4.19 7.59 -7.59 4.19 -7.59 0 C -7.59 -4.19 -4.19 -7.59 0 -7.59 C 4.19 -7.59 7.59 -4.19 7.59 0 Z M 0 0"></path></g><g color="#FFFFFF" fill="#FFFFFF" stroke="#FFFFFF" transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.92"><span class="ltx_text" id="S4.SS5.p7.1.pic1.1.1.1.1.1">1</span></foreignobject></g></g></svg> our novel seed prompt templates,  <svg class="ltx_picture" height="15.74" id="S4.SS5.p7.2.pic2" overflow="visible" version="1.1" width="15.74"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,15.74) matrix(1 0 0 -1 0 0) translate(7.87,0) translate(0,7.87)"><g color="#FFFFFF" fill="#007C00" stroke="#FFFFFF"><path d="M 7.59 0 C 7.59 4.19 4.19 7.59 0 7.59 C -4.19 7.59 -7.59 4.19 -7.59 0 C -7.59 -4.19 -4.19 -7.59 0 -7.59 C 4.19 -7.59 7.59 -4.19 7.59 0 Z M 0 0"></path></g><g color="#FFFFFF" fill="#FFFFFF" stroke="#FFFFFF" transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.92"><span class="ltx_text" id="S4.SS5.p7.2.pic2.1.1.1.1.1">2</span></foreignobject></g></g></svg> synonym-based mutation technique, and  <svg class="ltx_picture" height="15.74" id="S4.SS5.p7.3.pic3" overflow="visible" version="1.1" width="15.74"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,15.74) matrix(1 0 0 -1 0 0) translate(7.87,0) translate(0,7.87)"><g color="#FFFFFF" fill="#007C00" stroke="#FFFFFF"><path d="M 7.59 0 C 7.59 4.19 4.19 7.59 0 7.59 C -4.19 7.59 -7.59 4.19 -7.59 0 C -7.59 -4.19 -4.19 -7.59 0 -7.59 C 4.19 -7.59 7.59 -4.19 7.59 0 Z M 0 0"></path></g><g color="#FFFFFF" fill="#FFFFFF" stroke="#FFFFFF" transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.92"><span class="ltx_text" id="S4.SS5.p7.3.pic3.1.1.1.1.1">3</span></foreignobject></g></g></svg> embedding-based evaluation technique.</p>
</div>
<figure class="ltx_float ltx_algorithm" id="algorithm1">
<div class="ltx_listing ltx_lst_numbers_left ltx_listing" id="algorithm1.21">
<div class="ltx_listingline" id="algorithm1.6.6">
<span class="ltx_text" id="algorithm1.6.6.1"><span class="ltx_text ltx_font_bold" id="algorithm1.6.6.1.1">Input:</span> </span>Questions <math alttext="\mathcal{Q}" class="ltx_Math" display="inline" id="algorithm1.1.1.m1.1"><semantics id="algorithm1.1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="algorithm1.1.1.m1.1.1" xref="algorithm1.1.1.m1.1.1.cmml">𝒬</mi><annotation-xml encoding="MathML-Content" id="algorithm1.1.1.m1.1b"><ci id="algorithm1.1.1.m1.1.1.cmml" xref="algorithm1.1.1.m1.1.1">𝒬</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.1.1.m1.1c">\mathcal{Q}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.1.1.m1.1d">caligraphic_Q</annotation></semantics></math>,
replacement probability parameter <math alttext="p" class="ltx_Math" display="inline" id="algorithm1.2.2.m2.1"><semantics id="algorithm1.2.2.m2.1a"><mi id="algorithm1.2.2.m2.1.1" xref="algorithm1.2.2.m2.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="algorithm1.2.2.m2.1b"><ci id="algorithm1.2.2.m2.1.1.cmml" xref="algorithm1.2.2.m2.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.2.2.m2.1c">p</annotation><annotation encoding="application/x-llamapun" id="algorithm1.2.2.m2.1d">italic_p</annotation></semantics></math>,
target LLM <math alttext="\mathcal{L}_{\text{target}}" class="ltx_Math" display="inline" id="algorithm1.3.3.m3.1"><semantics id="algorithm1.3.3.m3.1a"><msub id="algorithm1.3.3.m3.1.1" xref="algorithm1.3.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="algorithm1.3.3.m3.1.1.2" xref="algorithm1.3.3.m3.1.1.2.cmml">ℒ</mi><mtext id="algorithm1.3.3.m3.1.1.3" xref="algorithm1.3.3.m3.1.1.3a.cmml">target</mtext></msub><annotation-xml encoding="MathML-Content" id="algorithm1.3.3.m3.1b"><apply id="algorithm1.3.3.m3.1.1.cmml" xref="algorithm1.3.3.m3.1.1"><csymbol cd="ambiguous" id="algorithm1.3.3.m3.1.1.1.cmml" xref="algorithm1.3.3.m3.1.1">subscript</csymbol><ci id="algorithm1.3.3.m3.1.1.2.cmml" xref="algorithm1.3.3.m3.1.1.2">ℒ</ci><ci id="algorithm1.3.3.m3.1.1.3a.cmml" xref="algorithm1.3.3.m3.1.1.3"><mtext id="algorithm1.3.3.m3.1.1.3.cmml" mathsize="70%" xref="algorithm1.3.3.m3.1.1.3">target</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.3.3.m3.1c">\mathcal{L}_{\text{target}}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.3.3.m3.1d">caligraphic_L start_POSTSUBSCRIPT target end_POSTSUBSCRIPT</annotation></semantics></math>,
embedding model <math alttext="\mathcal{E}" class="ltx_Math" display="inline" id="algorithm1.4.4.m4.1"><semantics id="algorithm1.4.4.m4.1a"><mi class="ltx_font_mathcaligraphic" id="algorithm1.4.4.m4.1.1" xref="algorithm1.4.4.m4.1.1.cmml">ℰ</mi><annotation-xml encoding="MathML-Content" id="algorithm1.4.4.m4.1b"><ci id="algorithm1.4.4.m4.1.1.cmml" xref="algorithm1.4.4.m4.1.1">ℰ</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.4.4.m4.1c">\mathcal{E}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.4.4.m4.1d">caligraphic_E</annotation></semantics></math>, classifier <math alttext="\mathcal{C}" class="ltx_Math" display="inline" id="algorithm1.5.5.m5.1"><semantics id="algorithm1.5.5.m5.1a"><mi class="ltx_font_mathcaligraphic" id="algorithm1.5.5.m5.1.1" xref="algorithm1.5.5.m5.1.1.cmml">𝒞</mi><annotation-xml encoding="MathML-Content" id="algorithm1.5.5.m5.1b"><ci id="algorithm1.5.5.m5.1.1.cmml" xref="algorithm1.5.5.m5.1.1">𝒞</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.5.5.m5.1c">\mathcal{C}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.5.5.m5.1d">caligraphic_C</annotation></semantics></math>,
labeled examples <math alttext="\mathcal{Y}" class="ltx_Math" display="inline" id="algorithm1.6.6.m6.1"><semantics id="algorithm1.6.6.m6.1a"><mi class="ltx_font_mathcaligraphic" id="algorithm1.6.6.m6.1.1" xref="algorithm1.6.6.m6.1.1.cmml">𝒴</mi><annotation-xml encoding="MathML-Content" id="algorithm1.6.6.m6.1b"><ci id="algorithm1.6.6.m6.1.1.cmml" xref="algorithm1.6.6.m6.1.1">𝒴</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.6.6.m6.1c">\mathcal{Y}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.6.6.m6.1d">caligraphic_Y</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="algorithm1.7.7">
<span class="ltx_text" id="algorithm1.7.7.1"><span class="ltx_text ltx_font_bold" id="algorithm1.7.7.1.1">Output:</span> </span>Jailbreak successes <math alttext="\mathcal{J}" class="ltx_Math" display="inline" id="algorithm1.7.7.m1.1"><semantics id="algorithm1.7.7.m1.1a"><mi class="ltx_font_mathcaligraphic" id="algorithm1.7.7.m1.1.1" xref="algorithm1.7.7.m1.1.1.cmml">𝒥</mi><annotation-xml encoding="MathML-Content" id="algorithm1.7.7.m1.1b"><ci id="algorithm1.7.7.m1.1.1.cmml" xref="algorithm1.7.7.m1.1.1">𝒥</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.7.7.m1.1c">\mathcal{J}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.7.7.m1.1d">caligraphic_J</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="algorithm1.8.8">
<span class="ltx_text" id="algorithm1.8.8.1"><span class="ltx_text ltx_font_bold" id="algorithm1.8.8.1.1">Init.:</span> </span>
Compute <math alttext="\mathcal{S},~{}\mathcal{E}(y)~{}\forall y\in\mathcal{Y};~{}t\leftarrow 0;~{}%
\mathcal{J}\leftarrow\phi" class="ltx_Math" display="inline" id="algorithm1.8.8.m1.4"><semantics id="algorithm1.8.8.m1.4a"><mrow id="algorithm1.8.8.m1.4.4.2" xref="algorithm1.8.8.m1.4.4.3.cmml"><mrow id="algorithm1.8.8.m1.3.3.1.1" xref="algorithm1.8.8.m1.3.3.1.1.cmml"><mrow id="algorithm1.8.8.m1.3.3.1.1.1.1" xref="algorithm1.8.8.m1.3.3.1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="algorithm1.8.8.m1.2.2" xref="algorithm1.8.8.m1.2.2.cmml">𝒮</mi><mo id="algorithm1.8.8.m1.3.3.1.1.1.1.2" rspace="0.497em" xref="algorithm1.8.8.m1.3.3.1.1.1.2.cmml">,</mo><mrow id="algorithm1.8.8.m1.3.3.1.1.1.1.1" xref="algorithm1.8.8.m1.3.3.1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="algorithm1.8.8.m1.3.3.1.1.1.1.1.2" xref="algorithm1.8.8.m1.3.3.1.1.1.1.1.2.cmml">ℰ</mi><mo id="algorithm1.8.8.m1.3.3.1.1.1.1.1.1" xref="algorithm1.8.8.m1.3.3.1.1.1.1.1.1.cmml">⁢</mo><mrow id="algorithm1.8.8.m1.3.3.1.1.1.1.1.3.2" xref="algorithm1.8.8.m1.3.3.1.1.1.1.1.cmml"><mo id="algorithm1.8.8.m1.3.3.1.1.1.1.1.3.2.1" stretchy="false" xref="algorithm1.8.8.m1.3.3.1.1.1.1.1.cmml">(</mo><mi id="algorithm1.8.8.m1.1.1" xref="algorithm1.8.8.m1.1.1.cmml">y</mi><mo id="algorithm1.8.8.m1.3.3.1.1.1.1.1.3.2.2" stretchy="false" xref="algorithm1.8.8.m1.3.3.1.1.1.1.1.cmml">)</mo></mrow><mo id="algorithm1.8.8.m1.3.3.1.1.1.1.1.1a" lspace="0.497em" xref="algorithm1.8.8.m1.3.3.1.1.1.1.1.1.cmml">⁢</mo><mrow id="algorithm1.8.8.m1.3.3.1.1.1.1.1.4" xref="algorithm1.8.8.m1.3.3.1.1.1.1.1.4.cmml"><mo id="algorithm1.8.8.m1.3.3.1.1.1.1.1.4.1" rspace="0.167em" xref="algorithm1.8.8.m1.3.3.1.1.1.1.1.4.1.cmml">∀</mo><mi id="algorithm1.8.8.m1.3.3.1.1.1.1.1.4.2" xref="algorithm1.8.8.m1.3.3.1.1.1.1.1.4.2.cmml">y</mi></mrow></mrow></mrow><mo id="algorithm1.8.8.m1.3.3.1.1.2" xref="algorithm1.8.8.m1.3.3.1.1.2.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="algorithm1.8.8.m1.3.3.1.1.3" xref="algorithm1.8.8.m1.3.3.1.1.3.cmml">𝒴</mi></mrow><mo id="algorithm1.8.8.m1.4.4.2.3" rspace="0.497em" xref="algorithm1.8.8.m1.4.4.3a.cmml">;</mo><mrow id="algorithm1.8.8.m1.4.4.2.2.2" xref="algorithm1.8.8.m1.4.4.2.2.3.cmml"><mrow id="algorithm1.8.8.m1.4.4.2.2.1.1" xref="algorithm1.8.8.m1.4.4.2.2.1.1.cmml"><mi id="algorithm1.8.8.m1.4.4.2.2.1.1.2" xref="algorithm1.8.8.m1.4.4.2.2.1.1.2.cmml">t</mi><mo id="algorithm1.8.8.m1.4.4.2.2.1.1.1" stretchy="false" xref="algorithm1.8.8.m1.4.4.2.2.1.1.1.cmml">←</mo><mn id="algorithm1.8.8.m1.4.4.2.2.1.1.3" xref="algorithm1.8.8.m1.4.4.2.2.1.1.3.cmml">0</mn></mrow><mo id="algorithm1.8.8.m1.4.4.2.2.2.3" rspace="0.497em" xref="algorithm1.8.8.m1.4.4.2.2.3a.cmml">;</mo><mrow id="algorithm1.8.8.m1.4.4.2.2.2.2" xref="algorithm1.8.8.m1.4.4.2.2.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="algorithm1.8.8.m1.4.4.2.2.2.2.2" xref="algorithm1.8.8.m1.4.4.2.2.2.2.2.cmml">𝒥</mi><mo id="algorithm1.8.8.m1.4.4.2.2.2.2.1" stretchy="false" xref="algorithm1.8.8.m1.4.4.2.2.2.2.1.cmml">←</mo><mi id="algorithm1.8.8.m1.4.4.2.2.2.2.3" xref="algorithm1.8.8.m1.4.4.2.2.2.2.3.cmml">ϕ</mi></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.8.8.m1.4b"><apply id="algorithm1.8.8.m1.4.4.3.cmml" xref="algorithm1.8.8.m1.4.4.2"><csymbol cd="ambiguous" id="algorithm1.8.8.m1.4.4.3a.cmml" xref="algorithm1.8.8.m1.4.4.2.3">formulae-sequence</csymbol><apply id="algorithm1.8.8.m1.3.3.1.1.cmml" xref="algorithm1.8.8.m1.3.3.1.1"><in id="algorithm1.8.8.m1.3.3.1.1.2.cmml" xref="algorithm1.8.8.m1.3.3.1.1.2"></in><list id="algorithm1.8.8.m1.3.3.1.1.1.2.cmml" xref="algorithm1.8.8.m1.3.3.1.1.1.1"><ci id="algorithm1.8.8.m1.2.2.cmml" xref="algorithm1.8.8.m1.2.2">𝒮</ci><apply id="algorithm1.8.8.m1.3.3.1.1.1.1.1.cmml" xref="algorithm1.8.8.m1.3.3.1.1.1.1.1"><times id="algorithm1.8.8.m1.3.3.1.1.1.1.1.1.cmml" xref="algorithm1.8.8.m1.3.3.1.1.1.1.1.1"></times><ci id="algorithm1.8.8.m1.3.3.1.1.1.1.1.2.cmml" xref="algorithm1.8.8.m1.3.3.1.1.1.1.1.2">ℰ</ci><ci id="algorithm1.8.8.m1.1.1.cmml" xref="algorithm1.8.8.m1.1.1">𝑦</ci><apply id="algorithm1.8.8.m1.3.3.1.1.1.1.1.4.cmml" xref="algorithm1.8.8.m1.3.3.1.1.1.1.1.4"><csymbol cd="latexml" id="algorithm1.8.8.m1.3.3.1.1.1.1.1.4.1.cmml" xref="algorithm1.8.8.m1.3.3.1.1.1.1.1.4.1">for-all</csymbol><ci id="algorithm1.8.8.m1.3.3.1.1.1.1.1.4.2.cmml" xref="algorithm1.8.8.m1.3.3.1.1.1.1.1.4.2">𝑦</ci></apply></apply></list><ci id="algorithm1.8.8.m1.3.3.1.1.3.cmml" xref="algorithm1.8.8.m1.3.3.1.1.3">𝒴</ci></apply><apply id="algorithm1.8.8.m1.4.4.2.2.3.cmml" xref="algorithm1.8.8.m1.4.4.2.2.2"><csymbol cd="ambiguous" id="algorithm1.8.8.m1.4.4.2.2.3a.cmml" xref="algorithm1.8.8.m1.4.4.2.2.2.3">formulae-sequence</csymbol><apply id="algorithm1.8.8.m1.4.4.2.2.1.1.cmml" xref="algorithm1.8.8.m1.4.4.2.2.1.1"><ci id="algorithm1.8.8.m1.4.4.2.2.1.1.1.cmml" xref="algorithm1.8.8.m1.4.4.2.2.1.1.1">←</ci><ci id="algorithm1.8.8.m1.4.4.2.2.1.1.2.cmml" xref="algorithm1.8.8.m1.4.4.2.2.1.1.2">𝑡</ci><cn id="algorithm1.8.8.m1.4.4.2.2.1.1.3.cmml" type="integer" xref="algorithm1.8.8.m1.4.4.2.2.1.1.3">0</cn></apply><apply id="algorithm1.8.8.m1.4.4.2.2.2.2.cmml" xref="algorithm1.8.8.m1.4.4.2.2.2.2"><ci id="algorithm1.8.8.m1.4.4.2.2.2.2.1.cmml" xref="algorithm1.8.8.m1.4.4.2.2.2.2.1">←</ci><ci id="algorithm1.8.8.m1.4.4.2.2.2.2.2.cmml" xref="algorithm1.8.8.m1.4.4.2.2.2.2.2">𝒥</ci><ci id="algorithm1.8.8.m1.4.4.2.2.2.2.3.cmml" xref="algorithm1.8.8.m1.4.4.2.2.2.2.3">italic-ϕ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.8.8.m1.4c">\mathcal{S},~{}\mathcal{E}(y)~{}\forall y\in\mathcal{Y};~{}t\leftarrow 0;~{}%
\mathcal{J}\leftarrow\phi</annotation><annotation encoding="application/x-llamapun" id="algorithm1.8.8.m1.4d">caligraphic_S , caligraphic_E ( italic_y ) ∀ italic_y ∈ caligraphic_Y ; italic_t ← 0 ; caligraphic_J ← italic_ϕ</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="algorithm1.21.22">
<span class="ltx_tag ltx_tag_listingline">1</span>
</div>
<div class="ltx_listingline" id="algorithm1.21.23">
<span class="ltx_tag ltx_tag_listingline">2</span><span class="ltx_text ltx_font_bold" id="algorithm1.21.23.1">while</span> stopping condition not met <span class="ltx_text ltx_font_bold" id="algorithm1.21.23.2">do</span>
</div>
<div class="ltx_listingline" id="algorithm1.9.9">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
<math alttext="q_{t}\sim\mathcal{Q}" class="ltx_Math" display="inline" id="algorithm1.9.9.m1.1"><semantics id="algorithm1.9.9.m1.1a"><mrow id="algorithm1.9.9.m1.1.1" xref="algorithm1.9.9.m1.1.1.cmml"><msub id="algorithm1.9.9.m1.1.1.2" xref="algorithm1.9.9.m1.1.1.2.cmml"><mi id="algorithm1.9.9.m1.1.1.2.2" xref="algorithm1.9.9.m1.1.1.2.2.cmml">q</mi><mi id="algorithm1.9.9.m1.1.1.2.3" xref="algorithm1.9.9.m1.1.1.2.3.cmml">t</mi></msub><mo id="algorithm1.9.9.m1.1.1.1" xref="algorithm1.9.9.m1.1.1.1.cmml">∼</mo><mi class="ltx_font_mathcaligraphic" id="algorithm1.9.9.m1.1.1.3" xref="algorithm1.9.9.m1.1.1.3.cmml">𝒬</mi></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.9.9.m1.1b"><apply id="algorithm1.9.9.m1.1.1.cmml" xref="algorithm1.9.9.m1.1.1"><csymbol cd="latexml" id="algorithm1.9.9.m1.1.1.1.cmml" xref="algorithm1.9.9.m1.1.1.1">similar-to</csymbol><apply id="algorithm1.9.9.m1.1.1.2.cmml" xref="algorithm1.9.9.m1.1.1.2"><csymbol cd="ambiguous" id="algorithm1.9.9.m1.1.1.2.1.cmml" xref="algorithm1.9.9.m1.1.1.2">subscript</csymbol><ci id="algorithm1.9.9.m1.1.1.2.2.cmml" xref="algorithm1.9.9.m1.1.1.2.2">𝑞</ci><ci id="algorithm1.9.9.m1.1.1.2.3.cmml" xref="algorithm1.9.9.m1.1.1.2.3">𝑡</ci></apply><ci id="algorithm1.9.9.m1.1.1.3.cmml" xref="algorithm1.9.9.m1.1.1.3">𝒬</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.9.9.m1.1c">q_{t}\sim\mathcal{Q}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.9.9.m1.1d">italic_q start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∼ caligraphic_Q</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="algorithm1.21.24">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>    <span class="ltx_text ltx_font_typewriter" id="algorithm1.21.24.1" style="font-size:80%;color:#007C00;">// </span><span class="ltx_text ltx_font_typewriter" id="algorithm1.21.24.2" style="font-size:80%;color:#007C00;">sample target question </span>
</div>
<div class="ltx_listingline" id="algorithm1.10.10">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
<math alttext="s_{t}\leftarrow\mathbb{S}(\mathcal{S})" class="ltx_Math" display="inline" id="algorithm1.10.10.m1.1"><semantics id="algorithm1.10.10.m1.1a"><mrow id="algorithm1.10.10.m1.1.2" xref="algorithm1.10.10.m1.1.2.cmml"><msub id="algorithm1.10.10.m1.1.2.2" xref="algorithm1.10.10.m1.1.2.2.cmml"><mi id="algorithm1.10.10.m1.1.2.2.2" xref="algorithm1.10.10.m1.1.2.2.2.cmml">s</mi><mi id="algorithm1.10.10.m1.1.2.2.3" xref="algorithm1.10.10.m1.1.2.2.3.cmml">t</mi></msub><mo id="algorithm1.10.10.m1.1.2.1" stretchy="false" xref="algorithm1.10.10.m1.1.2.1.cmml">←</mo><mrow id="algorithm1.10.10.m1.1.2.3" xref="algorithm1.10.10.m1.1.2.3.cmml"><mi id="algorithm1.10.10.m1.1.2.3.2" xref="algorithm1.10.10.m1.1.2.3.2.cmml">𝕊</mi><mo id="algorithm1.10.10.m1.1.2.3.1" xref="algorithm1.10.10.m1.1.2.3.1.cmml">⁢</mo><mrow id="algorithm1.10.10.m1.1.2.3.3.2" xref="algorithm1.10.10.m1.1.2.3.cmml"><mo id="algorithm1.10.10.m1.1.2.3.3.2.1" stretchy="false" xref="algorithm1.10.10.m1.1.2.3.cmml">(</mo><mi class="ltx_font_mathcaligraphic" id="algorithm1.10.10.m1.1.1" xref="algorithm1.10.10.m1.1.1.cmml">𝒮</mi><mo id="algorithm1.10.10.m1.1.2.3.3.2.2" stretchy="false" xref="algorithm1.10.10.m1.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.10.10.m1.1b"><apply id="algorithm1.10.10.m1.1.2.cmml" xref="algorithm1.10.10.m1.1.2"><ci id="algorithm1.10.10.m1.1.2.1.cmml" xref="algorithm1.10.10.m1.1.2.1">←</ci><apply id="algorithm1.10.10.m1.1.2.2.cmml" xref="algorithm1.10.10.m1.1.2.2"><csymbol cd="ambiguous" id="algorithm1.10.10.m1.1.2.2.1.cmml" xref="algorithm1.10.10.m1.1.2.2">subscript</csymbol><ci id="algorithm1.10.10.m1.1.2.2.2.cmml" xref="algorithm1.10.10.m1.1.2.2.2">𝑠</ci><ci id="algorithm1.10.10.m1.1.2.2.3.cmml" xref="algorithm1.10.10.m1.1.2.2.3">𝑡</ci></apply><apply id="algorithm1.10.10.m1.1.2.3.cmml" xref="algorithm1.10.10.m1.1.2.3"><times id="algorithm1.10.10.m1.1.2.3.1.cmml" xref="algorithm1.10.10.m1.1.2.3.1"></times><ci id="algorithm1.10.10.m1.1.2.3.2.cmml" xref="algorithm1.10.10.m1.1.2.3.2">𝕊</ci><ci id="algorithm1.10.10.m1.1.1.cmml" xref="algorithm1.10.10.m1.1.1">𝒮</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.10.10.m1.1c">s_{t}\leftarrow\mathbb{S}(\mathcal{S})</annotation><annotation encoding="application/x-llamapun" id="algorithm1.10.10.m1.1d">italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ← blackboard_S ( caligraphic_S )</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="algorithm1.21.25">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>    <span class="ltx_text ltx_font_typewriter" id="algorithm1.21.25.1" style="font-size:80%;color:#007C00;">// </span><span class="ltx_text ltx_font_typewriter" id="algorithm1.21.25.2" style="font-size:80%;color:#007C00;">seed selection </span>
</div>
<div class="ltx_listingline" id="algorithm1.11.11">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
<math alttext="m_{t}\leftarrow\mathbb{M}_{p}(s_{t})" class="ltx_Math" display="inline" id="algorithm1.11.11.m1.1"><semantics id="algorithm1.11.11.m1.1a"><mrow id="algorithm1.11.11.m1.1.1" xref="algorithm1.11.11.m1.1.1.cmml"><msub id="algorithm1.11.11.m1.1.1.3" xref="algorithm1.11.11.m1.1.1.3.cmml"><mi id="algorithm1.11.11.m1.1.1.3.2" xref="algorithm1.11.11.m1.1.1.3.2.cmml">m</mi><mi id="algorithm1.11.11.m1.1.1.3.3" xref="algorithm1.11.11.m1.1.1.3.3.cmml">t</mi></msub><mo id="algorithm1.11.11.m1.1.1.2" stretchy="false" xref="algorithm1.11.11.m1.1.1.2.cmml">←</mo><mrow id="algorithm1.11.11.m1.1.1.1" xref="algorithm1.11.11.m1.1.1.1.cmml"><msub id="algorithm1.11.11.m1.1.1.1.3" xref="algorithm1.11.11.m1.1.1.1.3.cmml"><mi id="algorithm1.11.11.m1.1.1.1.3.2" xref="algorithm1.11.11.m1.1.1.1.3.2.cmml">𝕄</mi><mi id="algorithm1.11.11.m1.1.1.1.3.3" xref="algorithm1.11.11.m1.1.1.1.3.3.cmml">p</mi></msub><mo id="algorithm1.11.11.m1.1.1.1.2" xref="algorithm1.11.11.m1.1.1.1.2.cmml">⁢</mo><mrow id="algorithm1.11.11.m1.1.1.1.1.1" xref="algorithm1.11.11.m1.1.1.1.1.1.1.cmml"><mo id="algorithm1.11.11.m1.1.1.1.1.1.2" stretchy="false" xref="algorithm1.11.11.m1.1.1.1.1.1.1.cmml">(</mo><msub id="algorithm1.11.11.m1.1.1.1.1.1.1" xref="algorithm1.11.11.m1.1.1.1.1.1.1.cmml"><mi id="algorithm1.11.11.m1.1.1.1.1.1.1.2" xref="algorithm1.11.11.m1.1.1.1.1.1.1.2.cmml">s</mi><mi id="algorithm1.11.11.m1.1.1.1.1.1.1.3" xref="algorithm1.11.11.m1.1.1.1.1.1.1.3.cmml">t</mi></msub><mo id="algorithm1.11.11.m1.1.1.1.1.1.3" stretchy="false" xref="algorithm1.11.11.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.11.11.m1.1b"><apply id="algorithm1.11.11.m1.1.1.cmml" xref="algorithm1.11.11.m1.1.1"><ci id="algorithm1.11.11.m1.1.1.2.cmml" xref="algorithm1.11.11.m1.1.1.2">←</ci><apply id="algorithm1.11.11.m1.1.1.3.cmml" xref="algorithm1.11.11.m1.1.1.3"><csymbol cd="ambiguous" id="algorithm1.11.11.m1.1.1.3.1.cmml" xref="algorithm1.11.11.m1.1.1.3">subscript</csymbol><ci id="algorithm1.11.11.m1.1.1.3.2.cmml" xref="algorithm1.11.11.m1.1.1.3.2">𝑚</ci><ci id="algorithm1.11.11.m1.1.1.3.3.cmml" xref="algorithm1.11.11.m1.1.1.3.3">𝑡</ci></apply><apply id="algorithm1.11.11.m1.1.1.1.cmml" xref="algorithm1.11.11.m1.1.1.1"><times id="algorithm1.11.11.m1.1.1.1.2.cmml" xref="algorithm1.11.11.m1.1.1.1.2"></times><apply id="algorithm1.11.11.m1.1.1.1.3.cmml" xref="algorithm1.11.11.m1.1.1.1.3"><csymbol cd="ambiguous" id="algorithm1.11.11.m1.1.1.1.3.1.cmml" xref="algorithm1.11.11.m1.1.1.1.3">subscript</csymbol><ci id="algorithm1.11.11.m1.1.1.1.3.2.cmml" xref="algorithm1.11.11.m1.1.1.1.3.2">𝕄</ci><ci id="algorithm1.11.11.m1.1.1.1.3.3.cmml" xref="algorithm1.11.11.m1.1.1.1.3.3">𝑝</ci></apply><apply id="algorithm1.11.11.m1.1.1.1.1.1.1.cmml" xref="algorithm1.11.11.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="algorithm1.11.11.m1.1.1.1.1.1.1.1.cmml" xref="algorithm1.11.11.m1.1.1.1.1.1">subscript</csymbol><ci id="algorithm1.11.11.m1.1.1.1.1.1.1.2.cmml" xref="algorithm1.11.11.m1.1.1.1.1.1.1.2">𝑠</ci><ci id="algorithm1.11.11.m1.1.1.1.1.1.1.3.cmml" xref="algorithm1.11.11.m1.1.1.1.1.1.1.3">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.11.11.m1.1c">m_{t}\leftarrow\mathbb{M}_{p}(s_{t})</annotation><annotation encoding="application/x-llamapun" id="algorithm1.11.11.m1.1d">italic_m start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ← blackboard_M start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="algorithm1.21.26">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>    <span class="ltx_text ltx_font_typewriter" id="algorithm1.21.26.1" style="font-size:80%;color:#007C00;">// </span><span class="ltx_text ltx_font_typewriter" id="algorithm1.21.26.2" style="font-size:80%;color:#007C00;">mutation </span>
</div>
<div class="ltx_listingline" id="algorithm1.14.14">
<span class="ltx_tag ltx_tag_listingline">3</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
<math alttext="m^{\prime}_{t}\leftarrow" class="ltx_Math" display="inline" id="algorithm1.12.12.m1.1"><semantics id="algorithm1.12.12.m1.1a"><mrow id="algorithm1.12.12.m1.1.1" xref="algorithm1.12.12.m1.1.1.cmml"><msubsup id="algorithm1.12.12.m1.1.1.2" xref="algorithm1.12.12.m1.1.1.2.cmml"><mi id="algorithm1.12.12.m1.1.1.2.2.2" xref="algorithm1.12.12.m1.1.1.2.2.2.cmml">m</mi><mi id="algorithm1.12.12.m1.1.1.2.3" xref="algorithm1.12.12.m1.1.1.2.3.cmml">t</mi><mo id="algorithm1.12.12.m1.1.1.2.2.3" xref="algorithm1.12.12.m1.1.1.2.2.3.cmml">′</mo></msubsup><mo id="algorithm1.12.12.m1.1.1.1" stretchy="false" xref="algorithm1.12.12.m1.1.1.1.cmml">←</mo><mi id="algorithm1.12.12.m1.1.1.3" xref="algorithm1.12.12.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.12.12.m1.1b"><apply id="algorithm1.12.12.m1.1.1.cmml" xref="algorithm1.12.12.m1.1.1"><ci id="algorithm1.12.12.m1.1.1.1.cmml" xref="algorithm1.12.12.m1.1.1.1">←</ci><apply id="algorithm1.12.12.m1.1.1.2.cmml" xref="algorithm1.12.12.m1.1.1.2"><csymbol cd="ambiguous" id="algorithm1.12.12.m1.1.1.2.1.cmml" xref="algorithm1.12.12.m1.1.1.2">subscript</csymbol><apply id="algorithm1.12.12.m1.1.1.2.2.cmml" xref="algorithm1.12.12.m1.1.1.2"><csymbol cd="ambiguous" id="algorithm1.12.12.m1.1.1.2.2.1.cmml" xref="algorithm1.12.12.m1.1.1.2">superscript</csymbol><ci id="algorithm1.12.12.m1.1.1.2.2.2.cmml" xref="algorithm1.12.12.m1.1.1.2.2.2">𝑚</ci><ci id="algorithm1.12.12.m1.1.1.2.2.3.cmml" xref="algorithm1.12.12.m1.1.1.2.2.3">′</ci></apply><ci id="algorithm1.12.12.m1.1.1.2.3.cmml" xref="algorithm1.12.12.m1.1.1.2.3">𝑡</ci></apply><csymbol cd="latexml" id="algorithm1.12.12.m1.1.1.3.cmml" xref="algorithm1.12.12.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.12.12.m1.1c">m^{\prime}_{t}\leftarrow</annotation><annotation encoding="application/x-llamapun" id="algorithm1.12.12.m1.1d">italic_m start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ←</annotation></semantics></math> replace placeholder in <math alttext="m_{t}" class="ltx_Math" display="inline" id="algorithm1.13.13.m2.1"><semantics id="algorithm1.13.13.m2.1a"><msub id="algorithm1.13.13.m2.1.1" xref="algorithm1.13.13.m2.1.1.cmml"><mi id="algorithm1.13.13.m2.1.1.2" xref="algorithm1.13.13.m2.1.1.2.cmml">m</mi><mi id="algorithm1.13.13.m2.1.1.3" xref="algorithm1.13.13.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="algorithm1.13.13.m2.1b"><apply id="algorithm1.13.13.m2.1.1.cmml" xref="algorithm1.13.13.m2.1.1"><csymbol cd="ambiguous" id="algorithm1.13.13.m2.1.1.1.cmml" xref="algorithm1.13.13.m2.1.1">subscript</csymbol><ci id="algorithm1.13.13.m2.1.1.2.cmml" xref="algorithm1.13.13.m2.1.1.2">𝑚</ci><ci id="algorithm1.13.13.m2.1.1.3.cmml" xref="algorithm1.13.13.m2.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.13.13.m2.1c">m_{t}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.13.13.m2.1d">italic_m start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> with <math alttext="q_{t}" class="ltx_Math" display="inline" id="algorithm1.14.14.m3.1"><semantics id="algorithm1.14.14.m3.1a"><msub id="algorithm1.14.14.m3.1.1" xref="algorithm1.14.14.m3.1.1.cmml"><mi id="algorithm1.14.14.m3.1.1.2" xref="algorithm1.14.14.m3.1.1.2.cmml">q</mi><mi id="algorithm1.14.14.m3.1.1.3" xref="algorithm1.14.14.m3.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="algorithm1.14.14.m3.1b"><apply id="algorithm1.14.14.m3.1.1.cmml" xref="algorithm1.14.14.m3.1.1"><csymbol cd="ambiguous" id="algorithm1.14.14.m3.1.1.1.cmml" xref="algorithm1.14.14.m3.1.1">subscript</csymbol><ci id="algorithm1.14.14.m3.1.1.2.cmml" xref="algorithm1.14.14.m3.1.1.2">𝑞</ci><ci id="algorithm1.14.14.m3.1.1.3.cmml" xref="algorithm1.14.14.m3.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.14.14.m3.1c">q_{t}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.14.14.m3.1d">italic_q start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="algorithm1.15.15">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
<math alttext="r_{t}\leftarrow\mathbb{EX}_{\mathcal{L}_{\text{target}}}(m^{\prime}_{t},q_{t})" class="ltx_Math" display="inline" id="algorithm1.15.15.m1.2"><semantics id="algorithm1.15.15.m1.2a"><mrow id="algorithm1.15.15.m1.2.2" xref="algorithm1.15.15.m1.2.2.cmml"><msub id="algorithm1.15.15.m1.2.2.4" xref="algorithm1.15.15.m1.2.2.4.cmml"><mi id="algorithm1.15.15.m1.2.2.4.2" xref="algorithm1.15.15.m1.2.2.4.2.cmml">r</mi><mi id="algorithm1.15.15.m1.2.2.4.3" xref="algorithm1.15.15.m1.2.2.4.3.cmml">t</mi></msub><mo id="algorithm1.15.15.m1.2.2.3" stretchy="false" xref="algorithm1.15.15.m1.2.2.3.cmml">←</mo><mrow id="algorithm1.15.15.m1.2.2.2" xref="algorithm1.15.15.m1.2.2.2.cmml"><mi id="algorithm1.15.15.m1.2.2.2.4" xref="algorithm1.15.15.m1.2.2.2.4.cmml">𝔼</mi><mo id="algorithm1.15.15.m1.2.2.2.3" xref="algorithm1.15.15.m1.2.2.2.3.cmml">⁢</mo><msub id="algorithm1.15.15.m1.2.2.2.5" xref="algorithm1.15.15.m1.2.2.2.5.cmml"><mi id="algorithm1.15.15.m1.2.2.2.5.2" xref="algorithm1.15.15.m1.2.2.2.5.2.cmml">𝕏</mi><msub id="algorithm1.15.15.m1.2.2.2.5.3" xref="algorithm1.15.15.m1.2.2.2.5.3.cmml"><mi class="ltx_font_mathcaligraphic" id="algorithm1.15.15.m1.2.2.2.5.3.2" xref="algorithm1.15.15.m1.2.2.2.5.3.2.cmml">ℒ</mi><mtext id="algorithm1.15.15.m1.2.2.2.5.3.3" xref="algorithm1.15.15.m1.2.2.2.5.3.3a.cmml">target</mtext></msub></msub><mo id="algorithm1.15.15.m1.2.2.2.3a" xref="algorithm1.15.15.m1.2.2.2.3.cmml">⁢</mo><mrow id="algorithm1.15.15.m1.2.2.2.2.2" xref="algorithm1.15.15.m1.2.2.2.2.3.cmml"><mo id="algorithm1.15.15.m1.2.2.2.2.2.3" stretchy="false" xref="algorithm1.15.15.m1.2.2.2.2.3.cmml">(</mo><msubsup id="algorithm1.15.15.m1.1.1.1.1.1.1" xref="algorithm1.15.15.m1.1.1.1.1.1.1.cmml"><mi id="algorithm1.15.15.m1.1.1.1.1.1.1.2.2" xref="algorithm1.15.15.m1.1.1.1.1.1.1.2.2.cmml">m</mi><mi id="algorithm1.15.15.m1.1.1.1.1.1.1.3" xref="algorithm1.15.15.m1.1.1.1.1.1.1.3.cmml">t</mi><mo id="algorithm1.15.15.m1.1.1.1.1.1.1.2.3" xref="algorithm1.15.15.m1.1.1.1.1.1.1.2.3.cmml">′</mo></msubsup><mo id="algorithm1.15.15.m1.2.2.2.2.2.4" xref="algorithm1.15.15.m1.2.2.2.2.3.cmml">,</mo><msub id="algorithm1.15.15.m1.2.2.2.2.2.2" xref="algorithm1.15.15.m1.2.2.2.2.2.2.cmml"><mi id="algorithm1.15.15.m1.2.2.2.2.2.2.2" xref="algorithm1.15.15.m1.2.2.2.2.2.2.2.cmml">q</mi><mi id="algorithm1.15.15.m1.2.2.2.2.2.2.3" xref="algorithm1.15.15.m1.2.2.2.2.2.2.3.cmml">t</mi></msub><mo id="algorithm1.15.15.m1.2.2.2.2.2.5" stretchy="false" xref="algorithm1.15.15.m1.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.15.15.m1.2b"><apply id="algorithm1.15.15.m1.2.2.cmml" xref="algorithm1.15.15.m1.2.2"><ci id="algorithm1.15.15.m1.2.2.3.cmml" xref="algorithm1.15.15.m1.2.2.3">←</ci><apply id="algorithm1.15.15.m1.2.2.4.cmml" xref="algorithm1.15.15.m1.2.2.4"><csymbol cd="ambiguous" id="algorithm1.15.15.m1.2.2.4.1.cmml" xref="algorithm1.15.15.m1.2.2.4">subscript</csymbol><ci id="algorithm1.15.15.m1.2.2.4.2.cmml" xref="algorithm1.15.15.m1.2.2.4.2">𝑟</ci><ci id="algorithm1.15.15.m1.2.2.4.3.cmml" xref="algorithm1.15.15.m1.2.2.4.3">𝑡</ci></apply><apply id="algorithm1.15.15.m1.2.2.2.cmml" xref="algorithm1.15.15.m1.2.2.2"><times id="algorithm1.15.15.m1.2.2.2.3.cmml" xref="algorithm1.15.15.m1.2.2.2.3"></times><ci id="algorithm1.15.15.m1.2.2.2.4.cmml" xref="algorithm1.15.15.m1.2.2.2.4">𝔼</ci><apply id="algorithm1.15.15.m1.2.2.2.5.cmml" xref="algorithm1.15.15.m1.2.2.2.5"><csymbol cd="ambiguous" id="algorithm1.15.15.m1.2.2.2.5.1.cmml" xref="algorithm1.15.15.m1.2.2.2.5">subscript</csymbol><ci id="algorithm1.15.15.m1.2.2.2.5.2.cmml" xref="algorithm1.15.15.m1.2.2.2.5.2">𝕏</ci><apply id="algorithm1.15.15.m1.2.2.2.5.3.cmml" xref="algorithm1.15.15.m1.2.2.2.5.3"><csymbol cd="ambiguous" id="algorithm1.15.15.m1.2.2.2.5.3.1.cmml" xref="algorithm1.15.15.m1.2.2.2.5.3">subscript</csymbol><ci id="algorithm1.15.15.m1.2.2.2.5.3.2.cmml" xref="algorithm1.15.15.m1.2.2.2.5.3.2">ℒ</ci><ci id="algorithm1.15.15.m1.2.2.2.5.3.3a.cmml" xref="algorithm1.15.15.m1.2.2.2.5.3.3"><mtext id="algorithm1.15.15.m1.2.2.2.5.3.3.cmml" mathsize="50%" xref="algorithm1.15.15.m1.2.2.2.5.3.3">target</mtext></ci></apply></apply><interval closure="open" id="algorithm1.15.15.m1.2.2.2.2.3.cmml" xref="algorithm1.15.15.m1.2.2.2.2.2"><apply id="algorithm1.15.15.m1.1.1.1.1.1.1.cmml" xref="algorithm1.15.15.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="algorithm1.15.15.m1.1.1.1.1.1.1.1.cmml" xref="algorithm1.15.15.m1.1.1.1.1.1.1">subscript</csymbol><apply id="algorithm1.15.15.m1.1.1.1.1.1.1.2.cmml" xref="algorithm1.15.15.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="algorithm1.15.15.m1.1.1.1.1.1.1.2.1.cmml" xref="algorithm1.15.15.m1.1.1.1.1.1.1">superscript</csymbol><ci id="algorithm1.15.15.m1.1.1.1.1.1.1.2.2.cmml" xref="algorithm1.15.15.m1.1.1.1.1.1.1.2.2">𝑚</ci><ci id="algorithm1.15.15.m1.1.1.1.1.1.1.2.3.cmml" xref="algorithm1.15.15.m1.1.1.1.1.1.1.2.3">′</ci></apply><ci id="algorithm1.15.15.m1.1.1.1.1.1.1.3.cmml" xref="algorithm1.15.15.m1.1.1.1.1.1.1.3">𝑡</ci></apply><apply id="algorithm1.15.15.m1.2.2.2.2.2.2.cmml" xref="algorithm1.15.15.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="algorithm1.15.15.m1.2.2.2.2.2.2.1.cmml" xref="algorithm1.15.15.m1.2.2.2.2.2.2">subscript</csymbol><ci id="algorithm1.15.15.m1.2.2.2.2.2.2.2.cmml" xref="algorithm1.15.15.m1.2.2.2.2.2.2.2">𝑞</ci><ci id="algorithm1.15.15.m1.2.2.2.2.2.2.3.cmml" xref="algorithm1.15.15.m1.2.2.2.2.2.2.3">𝑡</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.15.15.m1.2c">r_{t}\leftarrow\mathbb{EX}_{\mathcal{L}_{\text{target}}}(m^{\prime}_{t},q_{t})</annotation><annotation encoding="application/x-llamapun" id="algorithm1.15.15.m1.2d">italic_r start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ← blackboard_E blackboard_X start_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT target end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_m start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_q start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="algorithm1.21.27">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>    <span class="ltx_text ltx_font_typewriter" id="algorithm1.21.27.1" style="font-size:80%;color:#007C00;">// </span><span class="ltx_text ltx_font_typewriter" id="algorithm1.21.27.2" style="font-size:80%;color:#007C00;">execution </span>
</div>
<div class="ltx_listingline" id="algorithm1.16.16">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
<math alttext="jb_{t}\leftarrow\mathbb{EV}_{\mathcal{E};\mathcal{C};\mathcal{Y}}(r_{t})" class="ltx_Math" display="inline" id="algorithm1.16.16.m1.4"><semantics id="algorithm1.16.16.m1.4a"><mrow id="algorithm1.16.16.m1.4.4" xref="algorithm1.16.16.m1.4.4.cmml"><mrow id="algorithm1.16.16.m1.4.4.3" xref="algorithm1.16.16.m1.4.4.3.cmml"><mi id="algorithm1.16.16.m1.4.4.3.2" xref="algorithm1.16.16.m1.4.4.3.2.cmml">j</mi><mo id="algorithm1.16.16.m1.4.4.3.1" xref="algorithm1.16.16.m1.4.4.3.1.cmml">⁢</mo><msub id="algorithm1.16.16.m1.4.4.3.3" xref="algorithm1.16.16.m1.4.4.3.3.cmml"><mi id="algorithm1.16.16.m1.4.4.3.3.2" xref="algorithm1.16.16.m1.4.4.3.3.2.cmml">b</mi><mi id="algorithm1.16.16.m1.4.4.3.3.3" xref="algorithm1.16.16.m1.4.4.3.3.3.cmml">t</mi></msub></mrow><mo id="algorithm1.16.16.m1.4.4.2" stretchy="false" xref="algorithm1.16.16.m1.4.4.2.cmml">←</mo><mrow id="algorithm1.16.16.m1.4.4.1" xref="algorithm1.16.16.m1.4.4.1.cmml"><mi id="algorithm1.16.16.m1.4.4.1.3" xref="algorithm1.16.16.m1.4.4.1.3.cmml">𝔼</mi><mo id="algorithm1.16.16.m1.4.4.1.2" xref="algorithm1.16.16.m1.4.4.1.2.cmml">⁢</mo><msub id="algorithm1.16.16.m1.4.4.1.4" xref="algorithm1.16.16.m1.4.4.1.4.cmml"><mi id="algorithm1.16.16.m1.4.4.1.4.2" xref="algorithm1.16.16.m1.4.4.1.4.2.cmml">𝕍</mi><mrow id="algorithm1.16.16.m1.3.3.3.5" xref="algorithm1.16.16.m1.3.3.3.4.cmml"><mi class="ltx_font_mathcaligraphic" id="algorithm1.16.16.m1.1.1.1.1" xref="algorithm1.16.16.m1.1.1.1.1.cmml">ℰ</mi><mo id="algorithm1.16.16.m1.3.3.3.5.1" xref="algorithm1.16.16.m1.3.3.3.4.cmml">;</mo><mi class="ltx_font_mathcaligraphic" id="algorithm1.16.16.m1.2.2.2.2" xref="algorithm1.16.16.m1.2.2.2.2.cmml">𝒞</mi><mo id="algorithm1.16.16.m1.3.3.3.5.2" xref="algorithm1.16.16.m1.3.3.3.4.cmml">;</mo><mi class="ltx_font_mathcaligraphic" id="algorithm1.16.16.m1.3.3.3.3" xref="algorithm1.16.16.m1.3.3.3.3.cmml">𝒴</mi></mrow></msub><mo id="algorithm1.16.16.m1.4.4.1.2a" xref="algorithm1.16.16.m1.4.4.1.2.cmml">⁢</mo><mrow id="algorithm1.16.16.m1.4.4.1.1.1" xref="algorithm1.16.16.m1.4.4.1.1.1.1.cmml"><mo id="algorithm1.16.16.m1.4.4.1.1.1.2" stretchy="false" xref="algorithm1.16.16.m1.4.4.1.1.1.1.cmml">(</mo><msub id="algorithm1.16.16.m1.4.4.1.1.1.1" xref="algorithm1.16.16.m1.4.4.1.1.1.1.cmml"><mi id="algorithm1.16.16.m1.4.4.1.1.1.1.2" xref="algorithm1.16.16.m1.4.4.1.1.1.1.2.cmml">r</mi><mi id="algorithm1.16.16.m1.4.4.1.1.1.1.3" xref="algorithm1.16.16.m1.4.4.1.1.1.1.3.cmml">t</mi></msub><mo id="algorithm1.16.16.m1.4.4.1.1.1.3" stretchy="false" xref="algorithm1.16.16.m1.4.4.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.16.16.m1.4b"><apply id="algorithm1.16.16.m1.4.4.cmml" xref="algorithm1.16.16.m1.4.4"><ci id="algorithm1.16.16.m1.4.4.2.cmml" xref="algorithm1.16.16.m1.4.4.2">←</ci><apply id="algorithm1.16.16.m1.4.4.3.cmml" xref="algorithm1.16.16.m1.4.4.3"><times id="algorithm1.16.16.m1.4.4.3.1.cmml" xref="algorithm1.16.16.m1.4.4.3.1"></times><ci id="algorithm1.16.16.m1.4.4.3.2.cmml" xref="algorithm1.16.16.m1.4.4.3.2">𝑗</ci><apply id="algorithm1.16.16.m1.4.4.3.3.cmml" xref="algorithm1.16.16.m1.4.4.3.3"><csymbol cd="ambiguous" id="algorithm1.16.16.m1.4.4.3.3.1.cmml" xref="algorithm1.16.16.m1.4.4.3.3">subscript</csymbol><ci id="algorithm1.16.16.m1.4.4.3.3.2.cmml" xref="algorithm1.16.16.m1.4.4.3.3.2">𝑏</ci><ci id="algorithm1.16.16.m1.4.4.3.3.3.cmml" xref="algorithm1.16.16.m1.4.4.3.3.3">𝑡</ci></apply></apply><apply id="algorithm1.16.16.m1.4.4.1.cmml" xref="algorithm1.16.16.m1.4.4.1"><times id="algorithm1.16.16.m1.4.4.1.2.cmml" xref="algorithm1.16.16.m1.4.4.1.2"></times><ci id="algorithm1.16.16.m1.4.4.1.3.cmml" xref="algorithm1.16.16.m1.4.4.1.3">𝔼</ci><apply id="algorithm1.16.16.m1.4.4.1.4.cmml" xref="algorithm1.16.16.m1.4.4.1.4"><csymbol cd="ambiguous" id="algorithm1.16.16.m1.4.4.1.4.1.cmml" xref="algorithm1.16.16.m1.4.4.1.4">subscript</csymbol><ci id="algorithm1.16.16.m1.4.4.1.4.2.cmml" xref="algorithm1.16.16.m1.4.4.1.4.2">𝕍</ci><list id="algorithm1.16.16.m1.3.3.3.4.cmml" xref="algorithm1.16.16.m1.3.3.3.5"><ci id="algorithm1.16.16.m1.1.1.1.1.cmml" xref="algorithm1.16.16.m1.1.1.1.1">ℰ</ci><ci id="algorithm1.16.16.m1.2.2.2.2.cmml" xref="algorithm1.16.16.m1.2.2.2.2">𝒞</ci><ci id="algorithm1.16.16.m1.3.3.3.3.cmml" xref="algorithm1.16.16.m1.3.3.3.3">𝒴</ci></list></apply><apply id="algorithm1.16.16.m1.4.4.1.1.1.1.cmml" xref="algorithm1.16.16.m1.4.4.1.1.1"><csymbol cd="ambiguous" id="algorithm1.16.16.m1.4.4.1.1.1.1.1.cmml" xref="algorithm1.16.16.m1.4.4.1.1.1">subscript</csymbol><ci id="algorithm1.16.16.m1.4.4.1.1.1.1.2.cmml" xref="algorithm1.16.16.m1.4.4.1.1.1.1.2">𝑟</ci><ci id="algorithm1.16.16.m1.4.4.1.1.1.1.3.cmml" xref="algorithm1.16.16.m1.4.4.1.1.1.1.3">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.16.16.m1.4c">jb_{t}\leftarrow\mathbb{EV}_{\mathcal{E};\mathcal{C};\mathcal{Y}}(r_{t})</annotation><annotation encoding="application/x-llamapun" id="algorithm1.16.16.m1.4d">italic_j italic_b start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ← blackboard_E blackboard_V start_POSTSUBSCRIPT caligraphic_E ; caligraphic_C ; caligraphic_Y end_POSTSUBSCRIPT ( italic_r start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="algorithm1.21.28">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>    <span class="ltx_text ltx_font_typewriter" id="algorithm1.21.28.1" style="font-size:80%;color:#007C00;">// </span><span class="ltx_text ltx_font_typewriter" id="algorithm1.21.28.2" style="font-size:80%;color:#007C00;">evaluation </span>
</div>
<div class="ltx_listingline" id="algorithm1.17.17">
<span class="ltx_tag ltx_tag_listingline">4</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
<span class="ltx_text ltx_font_bold" id="algorithm1.17.17.1">if</span> <math alttext="jb_{t}" class="ltx_Math" display="inline" id="algorithm1.17.17.m1.1"><semantics id="algorithm1.17.17.m1.1a"><mrow id="algorithm1.17.17.m1.1.1" xref="algorithm1.17.17.m1.1.1.cmml"><mi id="algorithm1.17.17.m1.1.1.2" xref="algorithm1.17.17.m1.1.1.2.cmml">j</mi><mo id="algorithm1.17.17.m1.1.1.1" xref="algorithm1.17.17.m1.1.1.1.cmml">⁢</mo><msub id="algorithm1.17.17.m1.1.1.3" xref="algorithm1.17.17.m1.1.1.3.cmml"><mi id="algorithm1.17.17.m1.1.1.3.2" xref="algorithm1.17.17.m1.1.1.3.2.cmml">b</mi><mi id="algorithm1.17.17.m1.1.1.3.3" xref="algorithm1.17.17.m1.1.1.3.3.cmml">t</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.17.17.m1.1b"><apply id="algorithm1.17.17.m1.1.1.cmml" xref="algorithm1.17.17.m1.1.1"><times id="algorithm1.17.17.m1.1.1.1.cmml" xref="algorithm1.17.17.m1.1.1.1"></times><ci id="algorithm1.17.17.m1.1.1.2.cmml" xref="algorithm1.17.17.m1.1.1.2">𝑗</ci><apply id="algorithm1.17.17.m1.1.1.3.cmml" xref="algorithm1.17.17.m1.1.1.3"><csymbol cd="ambiguous" id="algorithm1.17.17.m1.1.1.3.1.cmml" xref="algorithm1.17.17.m1.1.1.3">subscript</csymbol><ci id="algorithm1.17.17.m1.1.1.3.2.cmml" xref="algorithm1.17.17.m1.1.1.3.2">𝑏</ci><ci id="algorithm1.17.17.m1.1.1.3.3.cmml" xref="algorithm1.17.17.m1.1.1.3.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.17.17.m1.1c">jb_{t}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.17.17.m1.1d">italic_j italic_b start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> == True <span class="ltx_text ltx_font_bold" id="algorithm1.17.17.2">then</span>
</div>
<div class="ltx_listingline" id="algorithm1.18.18">
<span class="ltx_tag ltx_tag_listingline">5</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
update weight of <math alttext="s_{t}" class="ltx_Math" display="inline" id="algorithm1.18.18.m1.1"><semantics id="algorithm1.18.18.m1.1a"><msub id="algorithm1.18.18.m1.1.1" xref="algorithm1.18.18.m1.1.1.cmml"><mi id="algorithm1.18.18.m1.1.1.2" xref="algorithm1.18.18.m1.1.1.2.cmml">s</mi><mi id="algorithm1.18.18.m1.1.1.3" xref="algorithm1.18.18.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="algorithm1.18.18.m1.1b"><apply id="algorithm1.18.18.m1.1.1.cmml" xref="algorithm1.18.18.m1.1.1"><csymbol cd="ambiguous" id="algorithm1.18.18.m1.1.1.1.cmml" xref="algorithm1.18.18.m1.1.1">subscript</csymbol><ci id="algorithm1.18.18.m1.1.1.2.cmml" xref="algorithm1.18.18.m1.1.1.2">𝑠</ci><ci id="algorithm1.18.18.m1.1.1.3.cmml" xref="algorithm1.18.18.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.18.18.m1.1c">s_{t}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.18.18.m1.1d">italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="algorithm1.19.19">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
<math alttext="\mathcal{J}\leftarrow\mathcal{J}\bigcup(q_{t},m_{t},r_{t})" class="ltx_Math" display="inline" id="algorithm1.19.19.m1.3"><semantics id="algorithm1.19.19.m1.3a"><mrow id="algorithm1.19.19.m1.3.3" xref="algorithm1.19.19.m1.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="algorithm1.19.19.m1.3.3.5" xref="algorithm1.19.19.m1.3.3.5.cmml">𝒥</mi><mo id="algorithm1.19.19.m1.3.3.4" stretchy="false" xref="algorithm1.19.19.m1.3.3.4.cmml">←</mo><mrow id="algorithm1.19.19.m1.3.3.3" xref="algorithm1.19.19.m1.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="algorithm1.19.19.m1.3.3.3.5" xref="algorithm1.19.19.m1.3.3.3.5.cmml">𝒥</mi><mo id="algorithm1.19.19.m1.3.3.3.4" xref="algorithm1.19.19.m1.3.3.3.4.cmml">⁢</mo><mrow id="algorithm1.19.19.m1.3.3.3.3" xref="algorithm1.19.19.m1.3.3.3.3.cmml"><mo id="algorithm1.19.19.m1.3.3.3.3.4" rspace="0em" xref="algorithm1.19.19.m1.3.3.3.3.4.cmml">⋃</mo><mrow id="algorithm1.19.19.m1.3.3.3.3.3.3" xref="algorithm1.19.19.m1.3.3.3.3.3.4.cmml"><mo id="algorithm1.19.19.m1.3.3.3.3.3.3.4" stretchy="false" xref="algorithm1.19.19.m1.3.3.3.3.3.4.cmml">(</mo><msub id="algorithm1.19.19.m1.1.1.1.1.1.1.1" xref="algorithm1.19.19.m1.1.1.1.1.1.1.1.cmml"><mi id="algorithm1.19.19.m1.1.1.1.1.1.1.1.2" xref="algorithm1.19.19.m1.1.1.1.1.1.1.1.2.cmml">q</mi><mi id="algorithm1.19.19.m1.1.1.1.1.1.1.1.3" xref="algorithm1.19.19.m1.1.1.1.1.1.1.1.3.cmml">t</mi></msub><mo id="algorithm1.19.19.m1.3.3.3.3.3.3.5" xref="algorithm1.19.19.m1.3.3.3.3.3.4.cmml">,</mo><msub id="algorithm1.19.19.m1.2.2.2.2.2.2.2" xref="algorithm1.19.19.m1.2.2.2.2.2.2.2.cmml"><mi id="algorithm1.19.19.m1.2.2.2.2.2.2.2.2" xref="algorithm1.19.19.m1.2.2.2.2.2.2.2.2.cmml">m</mi><mi id="algorithm1.19.19.m1.2.2.2.2.2.2.2.3" xref="algorithm1.19.19.m1.2.2.2.2.2.2.2.3.cmml">t</mi></msub><mo id="algorithm1.19.19.m1.3.3.3.3.3.3.6" xref="algorithm1.19.19.m1.3.3.3.3.3.4.cmml">,</mo><msub id="algorithm1.19.19.m1.3.3.3.3.3.3.3" xref="algorithm1.19.19.m1.3.3.3.3.3.3.3.cmml"><mi id="algorithm1.19.19.m1.3.3.3.3.3.3.3.2" xref="algorithm1.19.19.m1.3.3.3.3.3.3.3.2.cmml">r</mi><mi id="algorithm1.19.19.m1.3.3.3.3.3.3.3.3" xref="algorithm1.19.19.m1.3.3.3.3.3.3.3.3.cmml">t</mi></msub><mo id="algorithm1.19.19.m1.3.3.3.3.3.3.7" stretchy="false" xref="algorithm1.19.19.m1.3.3.3.3.3.4.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.19.19.m1.3b"><apply id="algorithm1.19.19.m1.3.3.cmml" xref="algorithm1.19.19.m1.3.3"><ci id="algorithm1.19.19.m1.3.3.4.cmml" xref="algorithm1.19.19.m1.3.3.4">←</ci><ci id="algorithm1.19.19.m1.3.3.5.cmml" xref="algorithm1.19.19.m1.3.3.5">𝒥</ci><apply id="algorithm1.19.19.m1.3.3.3.cmml" xref="algorithm1.19.19.m1.3.3.3"><times id="algorithm1.19.19.m1.3.3.3.4.cmml" xref="algorithm1.19.19.m1.3.3.3.4"></times><ci id="algorithm1.19.19.m1.3.3.3.5.cmml" xref="algorithm1.19.19.m1.3.3.3.5">𝒥</ci><apply id="algorithm1.19.19.m1.3.3.3.3.cmml" xref="algorithm1.19.19.m1.3.3.3.3"><union id="algorithm1.19.19.m1.3.3.3.3.4.cmml" xref="algorithm1.19.19.m1.3.3.3.3.4"></union><vector id="algorithm1.19.19.m1.3.3.3.3.3.4.cmml" xref="algorithm1.19.19.m1.3.3.3.3.3.3"><apply id="algorithm1.19.19.m1.1.1.1.1.1.1.1.cmml" xref="algorithm1.19.19.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="algorithm1.19.19.m1.1.1.1.1.1.1.1.1.cmml" xref="algorithm1.19.19.m1.1.1.1.1.1.1.1">subscript</csymbol><ci id="algorithm1.19.19.m1.1.1.1.1.1.1.1.2.cmml" xref="algorithm1.19.19.m1.1.1.1.1.1.1.1.2">𝑞</ci><ci id="algorithm1.19.19.m1.1.1.1.1.1.1.1.3.cmml" xref="algorithm1.19.19.m1.1.1.1.1.1.1.1.3">𝑡</ci></apply><apply id="algorithm1.19.19.m1.2.2.2.2.2.2.2.cmml" xref="algorithm1.19.19.m1.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="algorithm1.19.19.m1.2.2.2.2.2.2.2.1.cmml" xref="algorithm1.19.19.m1.2.2.2.2.2.2.2">subscript</csymbol><ci id="algorithm1.19.19.m1.2.2.2.2.2.2.2.2.cmml" xref="algorithm1.19.19.m1.2.2.2.2.2.2.2.2">𝑚</ci><ci id="algorithm1.19.19.m1.2.2.2.2.2.2.2.3.cmml" xref="algorithm1.19.19.m1.2.2.2.2.2.2.2.3">𝑡</ci></apply><apply id="algorithm1.19.19.m1.3.3.3.3.3.3.3.cmml" xref="algorithm1.19.19.m1.3.3.3.3.3.3.3"><csymbol cd="ambiguous" id="algorithm1.19.19.m1.3.3.3.3.3.3.3.1.cmml" xref="algorithm1.19.19.m1.3.3.3.3.3.3.3">subscript</csymbol><ci id="algorithm1.19.19.m1.3.3.3.3.3.3.3.2.cmml" xref="algorithm1.19.19.m1.3.3.3.3.3.3.3.2">𝑟</ci><ci id="algorithm1.19.19.m1.3.3.3.3.3.3.3.3.cmml" xref="algorithm1.19.19.m1.3.3.3.3.3.3.3.3">𝑡</ci></apply></vector></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.19.19.m1.3c">\mathcal{J}\leftarrow\mathcal{J}\bigcup(q_{t},m_{t},r_{t})</annotation><annotation encoding="application/x-llamapun" id="algorithm1.19.19.m1.3d">caligraphic_J ← caligraphic_J ⋃ ( italic_q start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_m start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_r start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="algorithm1.21.29">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>    <span class="ltx_text ltx_font_typewriter" id="algorithm1.21.29.1" style="font-size:80%;color:#007C00;">// </span><span class="ltx_text ltx_font_typewriter" id="algorithm1.21.29.2" style="font-size:80%;color:#007C00;">save jailbreak successes </span>
</div>
<div class="ltx_listingline" id="algorithm1.21.30">
<span class="ltx_tag ltx_tag_listingline">6</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   

</div>
<div class="ltx_listingline" id="algorithm1.20.20">
<span class="ltx_tag ltx_tag_listingline">7</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
<math alttext="t\leftarrow t+1" class="ltx_Math" display="inline" id="algorithm1.20.20.m1.1"><semantics id="algorithm1.20.20.m1.1a"><mrow id="algorithm1.20.20.m1.1.1" xref="algorithm1.20.20.m1.1.1.cmml"><mi id="algorithm1.20.20.m1.1.1.2" xref="algorithm1.20.20.m1.1.1.2.cmml">t</mi><mo id="algorithm1.20.20.m1.1.1.1" stretchy="false" xref="algorithm1.20.20.m1.1.1.1.cmml">←</mo><mrow id="algorithm1.20.20.m1.1.1.3" xref="algorithm1.20.20.m1.1.1.3.cmml"><mi id="algorithm1.20.20.m1.1.1.3.2" xref="algorithm1.20.20.m1.1.1.3.2.cmml">t</mi><mo id="algorithm1.20.20.m1.1.1.3.1" xref="algorithm1.20.20.m1.1.1.3.1.cmml">+</mo><mn id="algorithm1.20.20.m1.1.1.3.3" xref="algorithm1.20.20.m1.1.1.3.3.cmml">1</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.20.20.m1.1b"><apply id="algorithm1.20.20.m1.1.1.cmml" xref="algorithm1.20.20.m1.1.1"><ci id="algorithm1.20.20.m1.1.1.1.cmml" xref="algorithm1.20.20.m1.1.1.1">←</ci><ci id="algorithm1.20.20.m1.1.1.2.cmml" xref="algorithm1.20.20.m1.1.1.2">𝑡</ci><apply id="algorithm1.20.20.m1.1.1.3.cmml" xref="algorithm1.20.20.m1.1.1.3"><plus id="algorithm1.20.20.m1.1.1.3.1.cmml" xref="algorithm1.20.20.m1.1.1.3.1"></plus><ci id="algorithm1.20.20.m1.1.1.3.2.cmml" xref="algorithm1.20.20.m1.1.1.3.2">𝑡</ci><cn id="algorithm1.20.20.m1.1.1.3.3.cmml" type="integer" xref="algorithm1.20.20.m1.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.20.20.m1.1c">t\leftarrow t+1</annotation><annotation encoding="application/x-llamapun" id="algorithm1.20.20.m1.1d">italic_t ← italic_t + 1</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="algorithm1.21.21">
<span class="ltx_text ltx_font_bold" id="algorithm1.21.21.1">return</span> <math alttext="\mathcal{J}" class="ltx_Math" display="inline" id="algorithm1.21.21.m1.1"><semantics id="algorithm1.21.21.m1.1a"><mi class="ltx_font_mathcaligraphic" id="algorithm1.21.21.m1.1.1" xref="algorithm1.21.21.m1.1.1.cmml">𝒥</mi><annotation-xml encoding="MathML-Content" id="algorithm1.21.21.m1.1b"><ci id="algorithm1.21.21.m1.1.1.cmml" xref="algorithm1.21.21.m1.1.1">𝒥</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.21.21.m1.1c">\mathcal{J}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.21.21.m1.1d">caligraphic_J</annotation></semantics></math>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span class="ltx_text ltx_font_bold" id="algorithm1.24.1.1">Algorithm 1</span> </span><span class="ltx_text ltx_font_italic" id="algorithm1.25.2">JBFuzz</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6 </span>Final Formulation</h3>
<div class="ltx_para" id="S4.SS6.p1">
<p class="ltx_p" id="S4.SS6.p1.3">Figure <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S4.F4" title="Figure 4 ‣ 4.5 Embedding-based Evaluation ‣ 4 Methodology ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_tag">4</span></a> illustrates the final formulation of our fuzzer, <span class="ltx_text ltx_font_italic" id="S4.SS6.p1.3.1">JBFuzz</span>. Additionally, Algorithm <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#algorithm1" title="In 4.5 Embedding-based Evaluation ‣ 4 Methodology ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_tag">1</span></a> shows the pseudo-code.
Before the fuzzing process begins (see Init. in the algorithm), (i) we generate the novel initial seed prompt templates,
as explained in Section <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S4.SS3" title="4.3 Novel Seed Prompt Templates ‣ 4 Methodology ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_tag">4.3</span></a> (Solution  <svg class="ltx_picture" height="15.74" id="S4.SS6.p1.1.pic1" overflow="visible" version="1.1" width="15.74"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,15.74) matrix(1 0 0 -1 0 0) translate(7.87,0) translate(0,7.87)"><g color="#FFFFFF" fill="#007C00" stroke="#FFFFFF"><path d="M 7.59 0 C 7.59 4.19 4.19 7.59 0 7.59 C -4.19 7.59 -7.59 4.19 -7.59 0 C -7.59 -4.19 -4.19 -7.59 0 -7.59 C 4.19 -7.59 7.59 -4.19 7.59 0 Z M 0 0"></path></g><g color="#FFFFFF" fill="#FFFFFF" stroke="#FFFFFF" transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.92"><span class="ltx_text" id="S4.SS6.p1.1.pic1.1.1.1.1.1">1</span></foreignobject></g></g></svg>), and (ii) we embed the dataset of labeled positive and negative examples of harmful/unethical content using the lightweight embedding model as described in Section <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S4.SS5" title="4.5 Embedding-based Evaluation ‣ 4 Methodology ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_tag">4.5</span></a> (Solution  <svg class="ltx_picture" height="15.74" id="S4.SS6.p1.2.pic2" overflow="visible" version="1.1" width="15.74"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,15.74) matrix(1 0 0 -1 0 0) translate(7.87,0) translate(0,7.87)"><g color="#FFFFFF" fill="#007C00" stroke="#FFFFFF"><path d="M 7.59 0 C 7.59 4.19 4.19 7.59 0 7.59 C -4.19 7.59 -7.59 4.19 -7.59 0 C -7.59 -4.19 -4.19 -7.59 0 -7.59 C 4.19 -7.59 7.59 -4.19 7.59 0 Z M 0 0"></path></g><g color="#FFFFFF" fill="#FFFFFF" stroke="#FFFFFF" transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.92"><span class="ltx_text" id="S4.SS6.p1.2.pic2.1.1.1.1.1">3</span></foreignobject></g></g></svg>). These embeddings and their corresponding labels are saved for later use during the evaluation step of fuzzing. Then, the fuzzing process begins (line 1).
For each fuzzing iteration, we start with a randomly selected question
from our database of harmful/unethical questions (line 2).
Then, our selection algorithm
selects a seed
from the seed pool (line 3). This selected seed is passed to our synonym-based mutator described in Section <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S4.SS4" title="4.4 Synonym-based Mutation ‣ 4 Methodology ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_tag">4.4</span></a> (Solution  <svg class="ltx_picture" height="15.74" id="S4.SS6.p1.3.pic3" overflow="visible" version="1.1" width="15.74"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,15.74) matrix(1 0 0 -1 0 0) translate(7.87,0) translate(0,7.87)"><g color="#FFFFFF" fill="#007C00" stroke="#FFFFFF"><path d="M 7.59 0 C 7.59 4.19 4.19 7.59 0 7.59 C -4.19 7.59 -7.59 4.19 -7.59 0 C -7.59 -4.19 -4.19 -7.59 0 -7.59 C 4.19 -7.59 7.59 -4.19 7.59 0 Z M 0 0"></path></g><g color="#FFFFFF" fill="#FFFFFF" stroke="#FFFFFF" transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.92"><span class="ltx_text" id="S4.SS6.p1.3.pic3.1.1.1.1.1">2</span></foreignobject></g></g></svg>). The mutator produces a mutated prompt template (line 4), which is then processed to replace the question placeholder with our selected question (line 5). The resulting prompt is then executed by the target LLM to generate the response (line 6). Next, our embedding-based evaluator embeds this response and leverages the previously stored labeled embeddings to determine if the response is harmful/unethical or not (line 7). If the response is harmful/unethical, i.e., it jailbreaks the target LLM, we
(i) update the weight corresponding to the selected seed for the selector (line 9), and
(ii) save the target question, the successful mutated prompt template, and the target LLM jailbroken response (line 10). Then, the next fuzzing iteration begins and this cycle of selection, mutation, execution, and evaluation is repeated until a stopping condition is reached. Over the course of the fuzzing process, <span class="ltx_text ltx_font_italic" id="S4.SS6.p1.3.2">JBFuzz</span> generates several successful prompt templates that jailbreak a variety of target LLMs. Next, we demonstrate the effectiveness of <span class="ltx_text ltx_font_italic" id="S4.SS6.p1.3.3">JBFuzz</span> through a comprehensive set of experiments.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Experimental Setup</h3>
<div class="ltx_para ltx_noindent" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.2"><span class="ltx_text ltx_font_bold" id="S5.SS1.p1.2.1">Implementation Details.</span> We implement <span class="ltx_text ltx_font_italic" id="S5.SS1.p1.2.2">JBFuzz</span> in Python on a general-purpose computer with eight cores and 16GB RAM.
We use ChatGPT via the web interface (<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://chatgpt.com" title="">https://chatgpt.com</a>) to generate the novel prompt templates that we use as seeds, quickly and free of cost.
We explore (i) different algorithms for the selector (e.g., random, weighted-random, UCB <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib6" title="">6</a>]</cite>, etc.), (ii) different values of <math alttext="p" class="ltx_Math" display="inline" id="S5.SS1.p1.1.m1.1"><semantics id="S5.SS1.p1.1.m1.1a"><mi id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><ci id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">p</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.1.m1.1d">italic_p</annotation></semantics></math>, the probability parameter for our synonym-based mutator from Section <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S4.SS4" title="4.4 Synonym-based Mutation ‣ 4 Methodology ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_tag">4.4</span></a>, (iii) different embedding models, <math alttext="\mathcal{E}" class="ltx_Math" display="inline" id="S5.SS1.p1.2.m2.1"><semantics id="S5.SS1.p1.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S5.SS1.p1.2.m2.1.1" xref="S5.SS1.p1.2.m2.1.1.cmml">ℰ</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.2.m2.1b"><ci id="S5.SS1.p1.2.m2.1.1.cmml" xref="S5.SS1.p1.2.m2.1.1">ℰ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.2.m2.1c">\mathcal{E}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.2.m2.1d">caligraphic_E</annotation></semantics></math>, for our evaluator from Section <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S4.SS5" title="4.5 Embedding-based Evaluation ‣ 4 Methodology ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_tag">4.5</span></a>, and (iv) different classifiers that classify a given response using the embeddings, with the objective of determining the respective optimal configuration for <span class="ltx_text ltx_font_italic" id="S5.SS1.p1.2.3">JBFuzz</span>. We elaborate on these experiments and our final configuration in Sections <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S5.SS2" title="5.2 Evaluator Configuration ‣ 5 Results ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_tag">5.2</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S5.SS3" title="5.3 Selector Configuration ‣ 5 Results ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_tag">5.3</span></a>, and <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S5.SS4" title="5.4 Mutator Configuration ‣ 5 Results ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_tag">5.4</span></a>.
We use the labeled dataset containing positive and negative examples of harmful/unethical content from <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib98" title="">98</a>]</cite>. The dataset contains 7700 examples in total.
To reduce non-determinism, we use a zero temperature setting and a maximum output token limit of 512 for all target LLMs.
We use the following three stopping conditions: (i) an iteration limit of 1000, (ii) a time limit of 3 hours<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>We extend this limit to 12 hours for the DeepSeek-R1 LLM since its API is very slow</span></span></span>, and (iii) if all questions from the dataset have been jailbroken.
If any of these conditions is met, the fuzzing process terminates and returns the jailbroken questions, the successful prompt templates, and the corresponding responses from the jailbroken LLM.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p2.1.1">Evaluation Dataset.</span> We use the set of 100 harmful/unethical questions <math alttext="\mathcal{Q}" class="ltx_Math" display="inline" id="S5.SS1.p2.1.m1.1"><semantics id="S5.SS1.p2.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S5.SS1.p2.1.m1.1.1" xref="S5.SS1.p2.1.m1.1.1.cmml">𝒬</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.1.m1.1b"><ci id="S5.SS1.p2.1.m1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1">𝒬</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.1.m1.1c">\mathcal{Q}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.1.m1.1d">caligraphic_Q</annotation></semantics></math> from <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib98" title="">98</a>]</cite>, which are sourced from publicly available datasets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib56" title="">56</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib8" title="">8</a>]</cite>. These questions span a wide range of harmful/unethical requests, such as threatening behaviors, discrimination, misinformation, cyber-crime, and illegal and immoral activities. We chose this dataset since these questions have either been crafted manually or sourced through crowd-sourcing efforts, thus reflecting real-world use cases. Moreover, this dataset also ensures fair comparison with closely-related prior work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib99" title="">99</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p3.1.1">Metrics.</span>
We use the following primary metrics to evaluate <span class="ltx_text ltx_font_italic" id="S5.SS1.p3.1.2">JBFuzz</span>:</p>
</div>
<div class="ltx_para" id="S5.SS1.p4">
<ul class="ltx_itemize" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p" id="S5.I1.i1.p1.3"><span class="ltx_text ltx_font_bold" id="S5.I1.i1.p1.3.1">Attack Success Rate (ASR)</span> measures the success rate of the jailbreaking attack in terms of the proportion of questions successfully jailbroken. If the attack yields harmful/unethical responses for <math alttext="q^{s}" class="ltx_Math" display="inline" id="S5.I1.i1.p1.1.m1.1"><semantics id="S5.I1.i1.p1.1.m1.1a"><msup id="S5.I1.i1.p1.1.m1.1.1" xref="S5.I1.i1.p1.1.m1.1.1.cmml"><mi id="S5.I1.i1.p1.1.m1.1.1.2" xref="S5.I1.i1.p1.1.m1.1.1.2.cmml">q</mi><mi id="S5.I1.i1.p1.1.m1.1.1.3" xref="S5.I1.i1.p1.1.m1.1.1.3.cmml">s</mi></msup><annotation-xml encoding="MathML-Content" id="S5.I1.i1.p1.1.m1.1b"><apply id="S5.I1.i1.p1.1.m1.1.1.cmml" xref="S5.I1.i1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.I1.i1.p1.1.m1.1.1.1.cmml" xref="S5.I1.i1.p1.1.m1.1.1">superscript</csymbol><ci id="S5.I1.i1.p1.1.m1.1.1.2.cmml" xref="S5.I1.i1.p1.1.m1.1.1.2">𝑞</ci><ci id="S5.I1.i1.p1.1.m1.1.1.3.cmml" xref="S5.I1.i1.p1.1.m1.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i1.p1.1.m1.1c">q^{s}</annotation><annotation encoding="application/x-llamapun" id="S5.I1.i1.p1.1.m1.1d">italic_q start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT</annotation></semantics></math> questions out of <math alttext="|\mathcal{Q}|" class="ltx_Math" display="inline" id="S5.I1.i1.p1.2.m2.1"><semantics id="S5.I1.i1.p1.2.m2.1a"><mrow id="S5.I1.i1.p1.2.m2.1.2.2" xref="S5.I1.i1.p1.2.m2.1.2.1.cmml"><mo id="S5.I1.i1.p1.2.m2.1.2.2.1" stretchy="false" xref="S5.I1.i1.p1.2.m2.1.2.1.1.cmml">|</mo><mi class="ltx_font_mathcaligraphic" id="S5.I1.i1.p1.2.m2.1.1" xref="S5.I1.i1.p1.2.m2.1.1.cmml">𝒬</mi><mo id="S5.I1.i1.p1.2.m2.1.2.2.2" stretchy="false" xref="S5.I1.i1.p1.2.m2.1.2.1.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.I1.i1.p1.2.m2.1b"><apply id="S5.I1.i1.p1.2.m2.1.2.1.cmml" xref="S5.I1.i1.p1.2.m2.1.2.2"><abs id="S5.I1.i1.p1.2.m2.1.2.1.1.cmml" xref="S5.I1.i1.p1.2.m2.1.2.2.1"></abs><ci id="S5.I1.i1.p1.2.m2.1.1.cmml" xref="S5.I1.i1.p1.2.m2.1.1">𝒬</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i1.p1.2.m2.1c">|\mathcal{Q}|</annotation><annotation encoding="application/x-llamapun" id="S5.I1.i1.p1.2.m2.1d">| caligraphic_Q |</annotation></semantics></math> total questions, then <math alttext="\text{ASR}=\frac{q^{s}}{|\mathcal{Q}|}" class="ltx_Math" display="inline" id="S5.I1.i1.p1.3.m3.1"><semantics id="S5.I1.i1.p1.3.m3.1a"><mrow id="S5.I1.i1.p1.3.m3.1.2" xref="S5.I1.i1.p1.3.m3.1.2.cmml"><mtext id="S5.I1.i1.p1.3.m3.1.2.2" xref="S5.I1.i1.p1.3.m3.1.2.2a.cmml">ASR</mtext><mo id="S5.I1.i1.p1.3.m3.1.2.1" xref="S5.I1.i1.p1.3.m3.1.2.1.cmml">=</mo><mfrac id="S5.I1.i1.p1.3.m3.1.1" xref="S5.I1.i1.p1.3.m3.1.1.cmml"><msup id="S5.I1.i1.p1.3.m3.1.1.3" xref="S5.I1.i1.p1.3.m3.1.1.3.cmml"><mi id="S5.I1.i1.p1.3.m3.1.1.3.2" xref="S5.I1.i1.p1.3.m3.1.1.3.2.cmml">q</mi><mi id="S5.I1.i1.p1.3.m3.1.1.3.3" xref="S5.I1.i1.p1.3.m3.1.1.3.3.cmml">s</mi></msup><mrow id="S5.I1.i1.p1.3.m3.1.1.1.3" xref="S5.I1.i1.p1.3.m3.1.1.1.2.cmml"><mo id="S5.I1.i1.p1.3.m3.1.1.1.3.1" stretchy="false" xref="S5.I1.i1.p1.3.m3.1.1.1.2.1.cmml">|</mo><mi class="ltx_font_mathcaligraphic" id="S5.I1.i1.p1.3.m3.1.1.1.1" xref="S5.I1.i1.p1.3.m3.1.1.1.1.cmml">𝒬</mi><mo id="S5.I1.i1.p1.3.m3.1.1.1.3.2" stretchy="false" xref="S5.I1.i1.p1.3.m3.1.1.1.2.1.cmml">|</mo></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S5.I1.i1.p1.3.m3.1b"><apply id="S5.I1.i1.p1.3.m3.1.2.cmml" xref="S5.I1.i1.p1.3.m3.1.2"><eq id="S5.I1.i1.p1.3.m3.1.2.1.cmml" xref="S5.I1.i1.p1.3.m3.1.2.1"></eq><ci id="S5.I1.i1.p1.3.m3.1.2.2a.cmml" xref="S5.I1.i1.p1.3.m3.1.2.2"><mtext id="S5.I1.i1.p1.3.m3.1.2.2.cmml" xref="S5.I1.i1.p1.3.m3.1.2.2">ASR</mtext></ci><apply id="S5.I1.i1.p1.3.m3.1.1.cmml" xref="S5.I1.i1.p1.3.m3.1.1"><divide id="S5.I1.i1.p1.3.m3.1.1.2.cmml" xref="S5.I1.i1.p1.3.m3.1.1"></divide><apply id="S5.I1.i1.p1.3.m3.1.1.3.cmml" xref="S5.I1.i1.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S5.I1.i1.p1.3.m3.1.1.3.1.cmml" xref="S5.I1.i1.p1.3.m3.1.1.3">superscript</csymbol><ci id="S5.I1.i1.p1.3.m3.1.1.3.2.cmml" xref="S5.I1.i1.p1.3.m3.1.1.3.2">𝑞</ci><ci id="S5.I1.i1.p1.3.m3.1.1.3.3.cmml" xref="S5.I1.i1.p1.3.m3.1.1.3.3">𝑠</ci></apply><apply id="S5.I1.i1.p1.3.m3.1.1.1.2.cmml" xref="S5.I1.i1.p1.3.m3.1.1.1.3"><abs id="S5.I1.i1.p1.3.m3.1.1.1.2.1.cmml" xref="S5.I1.i1.p1.3.m3.1.1.1.3.1"></abs><ci id="S5.I1.i1.p1.3.m3.1.1.1.1.cmml" xref="S5.I1.i1.p1.3.m3.1.1.1.1">𝒬</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i1.p1.3.m3.1c">\text{ASR}=\frac{q^{s}}{|\mathcal{Q}|}</annotation><annotation encoding="application/x-llamapun" id="S5.I1.i1.p1.3.m3.1d">ASR = divide start_ARG italic_q start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT end_ARG start_ARG | caligraphic_Q | end_ARG</annotation></semantics></math>.<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>Note that <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib99" title="">99</a>]</cite> calls this metric Jailbreaking Question Number (JQN).</span></span></span> This is arguably the most important of all metrics.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i2.p1">
<p class="ltx_p" id="S5.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i2.p1.1.1">Iterations to Success (ItS)</span> is the number of iterations required to successfully jailbreak the LLM for all target questions. Lower values indicate better attack technique.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i3.p1">
<p class="ltx_p" id="S5.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i3.p1.1.1">Efficiency Ratio (ER)</span> is the fraction of successful iterations. It is measured as <math alttext="\frac{\text{\# successes}}{\text{ItS}}" class="ltx_Math" display="inline" id="S5.I1.i3.p1.1.m1.1"><semantics id="S5.I1.i3.p1.1.m1.1a"><mfrac id="S5.I1.i3.p1.1.m1.1.1" xref="S5.I1.i3.p1.1.m1.1.1.cmml"><mtext id="S5.I1.i3.p1.1.m1.1.1.2" xref="S5.I1.i3.p1.1.m1.1.1.2a.cmml"># successes</mtext><mtext id="S5.I1.i3.p1.1.m1.1.1.3" xref="S5.I1.i3.p1.1.m1.1.1.3a.cmml">ItS</mtext></mfrac><annotation-xml encoding="MathML-Content" id="S5.I1.i3.p1.1.m1.1b"><apply id="S5.I1.i3.p1.1.m1.1.1.cmml" xref="S5.I1.i3.p1.1.m1.1.1"><divide id="S5.I1.i3.p1.1.m1.1.1.1.cmml" xref="S5.I1.i3.p1.1.m1.1.1"></divide><ci id="S5.I1.i3.p1.1.m1.1.1.2a.cmml" xref="S5.I1.i3.p1.1.m1.1.1.2"><mtext id="S5.I1.i3.p1.1.m1.1.1.2.cmml" mathsize="70%" xref="S5.I1.i3.p1.1.m1.1.1.2"># successes</mtext></ci><ci id="S5.I1.i3.p1.1.m1.1.1.3a.cmml" xref="S5.I1.i3.p1.1.m1.1.1.3"><mtext id="S5.I1.i3.p1.1.m1.1.1.3.cmml" mathsize="70%" xref="S5.I1.i3.p1.1.m1.1.1.3">ItS</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i3.p1.1.m1.1c">\frac{\text{\# successes}}{\text{ItS}}</annotation><annotation encoding="application/x-llamapun" id="S5.I1.i3.p1.1.m1.1d">divide start_ARG # successes end_ARG start_ARG ItS end_ARG</annotation></semantics></math>. This metric measures the efficiency of the attack.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i4.p1">
<p class="ltx_p" id="S5.I1.i4.p1.1">We also measure the total wall-clock <span class="ltx_text ltx_font_bold" id="S5.I1.i4.p1.1.1">Runtime</span> of the fuzzer.</p>
</div>
</li>
</ul>
<p class="ltx_p" id="S5.SS1.p4.1">In addition to the above primary metrics, we also measure the following two auxiliary metrics that provide other insights into the attack.</p>
<ul class="ltx_itemize" id="S5.I2">
<li class="ltx_item" id="S5.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I2.i1.p1">
<p class="ltx_p" id="S5.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I2.i1.p1.1.1">Average Token Count (ATC)</span> measures the average number of tokens (both input and output) consumed to successfully jailbreak a question. Lower ATC indicates a better attack since each token costs money (for commercial LLMs) and runtime.</p>
</div>
</li>
<li class="ltx_item" id="S5.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I2.i2.p1">
<p class="ltx_p" id="S5.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I2.i2.p1.1.1">Fuzzing Rate (FR)</span> measures the rate of fuzzing in wall-clock seconds per iteration. FR helps determine the scalability of fuzzer. Note that although Runtime and FR are often correlated, that is not true in general, as explained in Section <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S5.SS5" title="5.5 Main Results ‣ 5 Results ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_tag">5.5</span></a>.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p5">
<p class="ltx_p" id="S5.SS1.p5.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p5.1.1">Target LLMs.</span>
For a thorough analysis, we use the following nine representative LLMs for our evaluation:
GPT-3.5 (<span class="ltx_text ltx_font_typewriter" id="S5.SS1.p5.1.2">gpt-3.5-turbo-0125</span>) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib66" title="">66</a>]</cite>, GPT-4o (<span class="ltx_text ltx_font_typewriter" id="S5.SS1.p5.1.3">gpt-4o-2024-08-06</span>) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib67" title="">67</a>]</cite>, GPT-4o-mini (<span class="ltx_text ltx_font_typewriter" id="S5.SS1.p5.1.4">gpt-4o-mini-2024-07-18</span>) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib68" title="">68</a>]</cite>, DeepSeek-V3 (<span class="ltx_text ltx_font_typewriter" id="S5.SS1.p5.1.5">deepseek-chat</span>) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib19" title="">19</a>]</cite>, DeepSeek-R1 (<span class="ltx_text ltx_font_typewriter" id="S5.SS1.p5.1.6">deepseek-reasoner</span>) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib19" title="">19</a>]</cite>, Llama2 (<span class="ltx_text ltx_font_typewriter" id="S5.SS1.p5.1.7">Llama-2-7b-chat-hf</span>) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib41" title="">41</a>]</cite>, Llama3 (<span class="ltx_text ltx_font_typewriter" id="S5.SS1.p5.1.8">Meta-Llama-3.1-8B-Instruct</span>) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib42" title="">42</a>]</cite>, Gemini-2.0 (<span class="ltx_text ltx_font_typewriter" id="S5.SS1.p5.1.9">gemini-2.0-flash-exp</span>) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib31" title="">31</a>]</cite>, and Gemini-1.5 (<span class="ltx_text ltx_font_typewriter" id="S5.SS1.p5.1.10">gemini-1.5-flash</span>) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib30" title="">30</a>]</cite>. The brackets contain the names of the specific models we use.
Our selected LLMs range from older (such as Llama2 from July 2023 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib57" title="">57</a>]</cite>) to brand new (e.g., DeepSeek-R1, which was released in January 2025 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib20" title="">20</a>]</cite>),
small to large, open-source and closed-source, and some of the most advanced LLMs available today from various organizations.
We refer an interested reader to Section <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S8.SS1" title="8.1 Target LLMs Descriptions ‣ 8 Appendix ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_tag">8.1</span></a> in the Appendix for more detailed description of each of the target LLMs and its capabilities that demonstrate its fit for our evaluation.</p>
</div>
<div class="ltx_para" id="S5.SS1.p6">
<p class="ltx_p" id="S5.SS1.p6.1">The remainder of the section is organized as follows: First, we perform our exploratory experiments to determine the optimal configuration of the selector, mutator, and evaluator (Sections <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S5.SS2" title="5.2 Evaluator Configuration ‣ 5 Results ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_tag">5.2</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S5.SS3" title="5.3 Selector Configuration ‣ 5 Results ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_tag">5.3</span></a>, and <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S5.SS4" title="5.4 Mutator Configuration ‣ 5 Results ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_tag">5.4</span></a>). Then, we provide the main results for jailbreaking all our target LLMs (Section <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S5.SS5" title="5.5 Main Results ‣ 5 Results ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_tag">5.5</span></a>).
Then, we evaluate the stealthiness of <span class="ltx_text ltx_font_italic" id="S5.SS1.p6.1.1">JBFuzz</span>-generated prompt templates (Section <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S5.SS6" title="5.6 Stealthiness Evaluation ‣ 5 Results ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_tag">5.6</span></a>), compare <span class="ltx_text ltx_font_italic" id="S5.SS1.p6.1.2">JBFuzz</span> with closely related work (Section <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S5.SS7" title="5.7 Comparison With Related Work ‣ 5 Results ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_tag">5.7</span></a>), and finally analyze the runtime breakdown to understand the bottleneck and highlight its efficiency (Section <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S5.SS8" title="5.8 Runtime Breakdown Analysis ‣ 5 Results ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_tag">5.8</span></a>).</p>
</div>
<div class="ltx_para" id="S5.SS1.p7">
<p class="ltx_p" id="S5.SS1.p7.1">Note that for all our exploratory experiments to determine the optimal configuration, i.e., Sections <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S5.SS2" title="5.2 Evaluator Configuration ‣ 5 Results ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_tag">5.2</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S5.SS3" title="5.3 Selector Configuration ‣ 5 Results ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_tag">5.3</span></a>, and <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S5.SS4" title="5.4 Mutator Configuration ‣ 5 Results ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_tag">5.4</span></a>, we use <span class="ltx_text ltx_font_typewriter" id="S5.SS1.p7.1.1">gpt-3.5-turbo-0125</span> as our target LLM since it serves as a good representative of our target LLMs: it is sufficiently advanced, reasonably fast, and relatively cheap to query so that the large number of exploratory experiments can be feasibly conducted.</p>
</div>
<figure class="ltx_figure" id="S5.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="289" id="S5.F5.g1" src="x3.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Accuracy and runtime comparison of different configurations for the evaluator and GPT-4o. MV_<math alttext="k" class="ltx_Math" display="inline" id="S5.F5.3.m1.1"><semantics id="S5.F5.3.m1.1b"><mi id="S5.F5.3.m1.1.1" xref="S5.F5.3.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.F5.3.m1.1c"><ci id="S5.F5.3.m1.1.1.cmml" xref="S5.F5.3.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.F5.3.m1.1d">k</annotation><annotation encoding="application/x-llamapun" id="S5.F5.3.m1.1e">italic_k</annotation></semantics></math>: top-<math alttext="k" class="ltx_Math" display="inline" id="S5.F5.4.m2.1"><semantics id="S5.F5.4.m2.1b"><mi id="S5.F5.4.m2.1.1" xref="S5.F5.4.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.F5.4.m2.1c"><ci id="S5.F5.4.m2.1.1.cmml" xref="S5.F5.4.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.F5.4.m2.1d">k</annotation><annotation encoding="application/x-llamapun" id="S5.F5.4.m2.1e">italic_k</annotation></semantics></math> majority voting-based; MLP: multi-layer perceptron</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Evaluator Configuration</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.3">We first determine the optimal configuration for the evaluator since it is agnostic to the other parts of the fuzzer, i.e., the selector and the mutator. To that end, we have the following variables to consider:
the embedding model and the classifier.
For the embedding model, we originally considered the following models since they are open-source, small (<math alttext="&lt;140" class="ltx_Math" display="inline" id="S5.SS2.p1.1.m1.1"><semantics id="S5.SS2.p1.1.m1.1a"><mrow id="S5.SS2.p1.1.m1.1.1" xref="S5.SS2.p1.1.m1.1.1.cmml"><mi id="S5.SS2.p1.1.m1.1.1.2" xref="S5.SS2.p1.1.m1.1.1.2.cmml"></mi><mo id="S5.SS2.p1.1.m1.1.1.1" xref="S5.SS2.p1.1.m1.1.1.1.cmml">&lt;</mo><mn id="S5.SS2.p1.1.m1.1.1.3" xref="S5.SS2.p1.1.m1.1.1.3.cmml">140</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.m1.1b"><apply id="S5.SS2.p1.1.m1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1"><lt id="S5.SS2.p1.1.m1.1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1.1"></lt><csymbol cd="latexml" id="S5.SS2.p1.1.m1.1.1.2.cmml" xref="S5.SS2.p1.1.m1.1.1.2">absent</csymbol><cn id="S5.SS2.p1.1.m1.1.1.3.cmml" type="integer" xref="S5.SS2.p1.1.m1.1.1.3">140</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.1.m1.1c">&lt;140</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.1.m1.1d">&lt; 140</annotation></semantics></math>M parameters), efficient (<math alttext="\approx 0.5" class="ltx_Math" display="inline" id="S5.SS2.p1.2.m2.1"><semantics id="S5.SS2.p1.2.m2.1a"><mrow id="S5.SS2.p1.2.m2.1.1" xref="S5.SS2.p1.2.m2.1.1.cmml"><mi id="S5.SS2.p1.2.m2.1.1.2" xref="S5.SS2.p1.2.m2.1.1.2.cmml"></mi><mo id="S5.SS2.p1.2.m2.1.1.1" xref="S5.SS2.p1.2.m2.1.1.1.cmml">≈</mo><mn id="S5.SS2.p1.2.m2.1.1.3" xref="S5.SS2.p1.2.m2.1.1.3.cmml">0.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.2.m2.1b"><apply id="S5.SS2.p1.2.m2.1.1.cmml" xref="S5.SS2.p1.2.m2.1.1"><approx id="S5.SS2.p1.2.m2.1.1.1.cmml" xref="S5.SS2.p1.2.m2.1.1.1"></approx><csymbol cd="latexml" id="S5.SS2.p1.2.m2.1.1.2.cmml" xref="S5.SS2.p1.2.m2.1.1.2">absent</csymbol><cn id="S5.SS2.p1.2.m2.1.1.3.cmml" type="float" xref="S5.SS2.p1.2.m2.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.2.m2.1c">\approx 0.5</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.2.m2.1d">≈ 0.5</annotation></semantics></math>GB memory usage), and effective (ranked high in the small and open-source category Massive Text Embedding Benchmark (MTEB) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib61" title="">61</a>]</cite> Leaderboard <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib43" title="">43</a>]</cite>): <span class="ltx_text ltx_font_typewriter" id="S5.SS2.p1.3.1">gte-base-en-v1.5</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib103" title="">103</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib50" title="">50</a>]</cite>, <span class="ltx_text ltx_font_typewriter" id="S5.SS2.p1.3.2">bge-base-en-v1.5</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib93" title="">93</a>]</cite>, <span class="ltx_text ltx_font_typewriter" id="S5.SS2.p1.3.3">nomic-embed-text-v1.5</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib64" title="">64</a>]</cite>, and <span class="ltx_text ltx_font_typewriter" id="S5.SS2.p1.3.4">e5-base-v2</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib90" title="">90</a>]</cite>. However, due to lack of proper compatibility between the computing hardware and the Python library, <span class="ltx_text ltx_font_typewriter" id="S5.SS2.p1.3.5">gte-base-en-v1.5</span> and <span class="ltx_text ltx_font_typewriter" id="S5.SS2.p1.3.6">nomic-embed-text-v1.5</span> faced out-of-memory issues, so we only focus on <span class="ltx_text ltx_font_typewriter" id="S5.SS2.p1.3.7">bge-base-en-v1.5</span> and <span class="ltx_text ltx_font_typewriter" id="S5.SS2.p1.3.8">e5-base-v2</span>.
For the classifier, we consider these two options since they both are fast and lightweight: (i) distance-based majority voting and (ii) multi-layer perceptron (MLP) model with 3 layers. Note that for the distance-based majority voting classifier, we consider multiple values of <math alttext="k" class="ltx_Math" display="inline" id="S5.SS2.p1.3.m3.1"><semantics id="S5.SS2.p1.3.m3.1a"><mi id="S5.SS2.p1.3.m3.1.1" xref="S5.SS2.p1.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.3.m3.1b"><ci id="S5.SS2.p1.3.m3.1.1.cmml" xref="S5.SS2.p1.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.3.m3.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.3.m3.1d">italic_k</annotation></semantics></math>, the number of closest labeled embeddings used for majority voting, and we use the cosine similarity metric as a measure for distance. Following prior work, we also consider an LLM, specifically, <span class="ltx_text ltx_font_typewriter" id="S5.SS2.p1.3.9">gpt-4o-2024-08-06</span>, as an evaluator <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib54" title="">54</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib53" title="">53</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S5.F5" title="Figure 5 ‣ 5.1 Experimental Setup ‣ 5 Results ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_tag">5</span></a> compares the accuracy and runtime of each different configuration of our embedding-based evaluator along with the GPT-4o-based evaluator on a held-out test set from the labeled dataset containing positive and negative examples of harmful/unethical content. Note that most configurations of our embedding-based evaluator outperform GPT-4o-based evaluator in terms of accuracy, and all our configurations are <math alttext="16\times" class="ltx_math_unparsed" display="inline" id="S5.SS2.p2.1.m1.1"><semantics id="S5.SS2.p2.1.m1.1a"><mrow id="S5.SS2.p2.1.m1.1b"><mn id="S5.SS2.p2.1.m1.1.1">16</mn><mo id="S5.SS2.p2.1.m1.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S5.SS2.p2.1.m1.1c">16\times</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.1.m1.1d">16 ×</annotation></semantics></math>, i.e., more than an order of magnitude, faster (note the log scale for the runtime plot) than GPT-4o-based evaluator.
Overall,
the <span class="ltx_text ltx_font_typewriter" id="S5.SS2.p2.1.1">e5-base-v2</span> embedding model with the MLP-based classifier is the clear winner in terms of accuracy while maintaining essentially the same runtime as other embedding-based configurations.
Thus, we use that as our final evaluator configuration.<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>Note that the accuracy of the classifier is not 100%, so there is a possibility of <span class="ltx_text ltx_font_italic" id="footnote4.1">JBFuzz</span> generating false positive and false negative jailbreak prompts. However, we believe this is still acceptable since (i) the accuracy is very high, and (ii) guaranteeing no false positives and false negatives would only be possible with manual evaluation, which is clearly not scalable.</span></span></span></p>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<span class="ltx_ERROR undefined" id="S5.SS2.p3.1">{myframe}</span>
<p class="ltx_p" id="S5.SS2.p3.2"><span class="ltx_text ltx_font_bold" id="S5.SS2.p3.2.1">Key Finding 1.</span> Our lightweight embedding-based evaluator with MLP classifier outperforms all other evaluation techniques, including GPT-4o-based evaluation.</p>
</div>
<figure class="ltx_table" id="S5.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Comparison of different selector algorithms against <span class="ltx_text ltx_font_typewriter" id="S5.T2.7.1">gpt-3.5-turbo-0125</span>.
W-random: Weighted-random</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T2.5" style="width:346.9pt;height:176.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(67.4pt,-34.4pt) scale(1.63617675082533,1.63617675082533) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T2.5.5">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T2.4.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T2.4.4.4.5">Selector</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T2.1.1.1.1">ASR<sup class="ltx_sup" id="S5.T2.1.1.1.1.1">↑</sup>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T2.2.2.2.2">ItS<sup class="ltx_sup" id="S5.T2.2.2.2.2.1">↓</sup>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T2.3.3.3.3">ER<sup class="ltx_sup" id="S5.T2.3.3.3.3.1">↑</sup>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T2.4.4.4.4">Runtime<sup class="ltx_sup" id="S5.T2.4.4.4.4.1">↓</sup>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.5.5.6.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.5.5.6.1.1">Random</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.5.5.6.1.2"><span class="ltx_text ltx_font_bold" id="S5.T2.5.5.6.1.2.1">100</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.5.5.6.1.3">624</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.5.5.6.1.4">0.67</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.5.5.6.1.5">1037</td>
</tr>
<tr class="ltx_tr" id="S5.T2.5.5.7.2">
<td class="ltx_td ltx_align_center" id="S5.T2.5.5.7.2.1">W-random</td>
<td class="ltx_td ltx_align_center" id="S5.T2.5.5.7.2.2"><span class="ltx_text ltx_font_bold" id="S5.T2.5.5.7.2.2.1">100</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.5.5.7.2.3"><span class="ltx_text ltx_font_bold" id="S5.T2.5.5.7.2.3.1">435</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.5.5.7.2.4">0.84</td>
<td class="ltx_td ltx_align_center" id="S5.T2.5.5.7.2.5"><span class="ltx_text ltx_font_bold" id="S5.T2.5.5.7.2.5.1">695</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.5.5.5">
<td class="ltx_td ltx_align_center" id="S5.T2.5.5.5.1">
<math alttext="\varepsilon" class="ltx_Math" display="inline" id="S5.T2.5.5.5.1.m1.1"><semantics id="S5.T2.5.5.5.1.m1.1a"><mi id="S5.T2.5.5.5.1.m1.1.1" xref="S5.T2.5.5.5.1.m1.1.1.cmml">ε</mi><annotation-xml encoding="MathML-Content" id="S5.T2.5.5.5.1.m1.1b"><ci id="S5.T2.5.5.5.1.m1.1.1.cmml" xref="S5.T2.5.5.5.1.m1.1.1">𝜀</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.5.5.5.1.m1.1c">\varepsilon</annotation><annotation encoding="application/x-llamapun" id="S5.T2.5.5.5.1.m1.1d">italic_ε</annotation></semantics></math>-greedy</td>
<td class="ltx_td ltx_align_center" id="S5.T2.5.5.5.2">88</td>
<td class="ltx_td ltx_align_center" id="S5.T2.5.5.5.3">1000<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><math alttext="\varepsilon" class="ltx_Math" display="inline" id="footnotex1.m1a.1"><semantics id="footnotex1.m1a.1b"><mi id="footnotex1.m1a.1.1" xref="footnotex1.m1a.1.1.cmml">ε</mi><annotation-xml encoding="MathML-Content" id="footnotex1.m1a.1c"><ci id="footnotex1.m1a.1.1.cmml" xref="footnotex1.m1a.1.1">𝜀</ci></annotation-xml><annotation encoding="application/x-tex" id="footnotex1.m1a.1d">\varepsilon</annotation><annotation encoding="application/x-llamapun" id="footnotex1.m1a.1e">italic_ε</annotation></semantics></math>-greedy selector does not jailbreak all questions within the stopping condition of 1000 iterations, so this entry is set to 1000.</span></span></span>
</td>
<td class="ltx_td ltx_align_center" id="S5.T2.5.5.5.4">0.32</td>
<td class="ltx_td ltx_align_center" id="S5.T2.5.5.5.5">1798</td>
</tr>
<tr class="ltx_tr" id="S5.T2.5.5.8.3">
<td class="ltx_td ltx_align_center" id="S5.T2.5.5.8.3.1">UCB</td>
<td class="ltx_td ltx_align_center" id="S5.T2.5.5.8.3.2"><span class="ltx_text ltx_font_bold" id="S5.T2.5.5.8.3.2.1">100</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.5.5.8.3.3">552</td>
<td class="ltx_td ltx_align_center" id="S5.T2.5.5.8.3.4">0.82</td>
<td class="ltx_td ltx_align_center" id="S5.T2.5.5.8.3.5">934</td>
</tr>
<tr class="ltx_tr" id="S5.T2.5.5.9.4">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.5.5.9.4.1">EXP3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.5.5.9.4.2"><span class="ltx_text ltx_font_bold" id="S5.T2.5.5.9.4.2.1">100</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.5.5.9.4.3">531</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.5.5.9.4.4"><span class="ltx_text ltx_font_bold" id="S5.T2.5.5.9.4.4.1">0.90</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.5.5.9.4.5">877</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Selector Configuration</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">We explore different seed selection algorithms to determine the optimal one, i.e., the one that performs well in terms of our metrics. To that end, we consider a variety of selectors, ranging from trivial algorithms like random selection and weighted-random (weighed based on past performance) to advanced algorithms that are designed to balance exploration of unexplored seeds vs. exploitation of well-performing seeds, such as <math alttext="\varepsilon" class="ltx_Math" display="inline" id="S5.SS3.p1.1.m1.1"><semantics id="S5.SS3.p1.1.m1.1a"><mi id="S5.SS3.p1.1.m1.1.1" xref="S5.SS3.p1.1.m1.1.1.cmml">ε</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.1.m1.1b"><ci id="S5.SS3.p1.1.m1.1.1.cmml" xref="S5.SS3.p1.1.m1.1.1">𝜀</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.1.m1.1c">\varepsilon</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p1.1.m1.1d">italic_ε</annotation></semantics></math>-greedy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib85" title="">85</a>]</cite>, Upper Confidence Bound (UCB) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib6" title="">6</a>]</cite>, and Exponential-weight algorithm for Exploration and Exploitation (EXP3) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib7" title="">7</a>]</cite>.
Using each of these selectors, we fuzz the <span class="ltx_text ltx_font_typewriter" id="S5.SS3.p1.1.1">gpt-3.5-turbo-0125</span> LLM to jailbreak it for the questions in our dataset.</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S5.T2" title="Table 2 ‣ 5.2 Evaluator Configuration ‣ 5 Results ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_tag">2</span></a> compares the attack metrics for the selector algorithms. Key takeaways from the table are: (i) All selector algorithms except <math alttext="\varepsilon" class="ltx_Math" display="inline" id="S5.SS3.p2.1.m1.1"><semantics id="S5.SS3.p2.1.m1.1a"><mi id="S5.SS3.p2.1.m1.1.1" xref="S5.SS3.p2.1.m1.1.1.cmml">ε</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.1.m1.1b"><ci id="S5.SS3.p2.1.m1.1.1.cmml" xref="S5.SS3.p2.1.m1.1.1">𝜀</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.1.m1.1c">\varepsilon</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p2.1.m1.1d">italic_ε</annotation></semantics></math>-greedy successfully jailbreak for all questions, i.e., 100% ASR.
The fuzzer’s performance, in terms of ASR, is not sensitive to the random, weighted-random, UCB, and EXP3 algorithms.
(iii) Although there is no clear winner across all metrics, both weighted-random and EXP3 outperform others on different metrics, while achieving 100% ASR. So, we use weighted-random as our final selector in <span class="ltx_text ltx_font_italic" id="S5.SS3.p2.1.1">JBFuzz</span> due to its simplicity and ease-of-implementation.</p>
</div>
<div class="ltx_para" id="S5.SS3.p3">
<span class="ltx_ERROR undefined" id="S5.SS3.p3.1">{myframe}</span>
<p class="ltx_p" id="S5.SS3.p3.2"><span class="ltx_text ltx_font_bold" id="S5.SS3.p3.2.1">Key Finding 2.</span> The fuzzer’s ASR is only slightly affected by the choice of selection algorithm from random, weighted-random, UCB, and EXP3, with weighted-random slightly outperforming others.</p>
</div>
<figure class="ltx_table" id="S5.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Comparison of different values of <math alttext="p" class="ltx_Math" display="inline" id="S5.T3.2.m1.1"><semantics id="S5.T3.2.m1.1b"><mi id="S5.T3.2.m1.1.1" xref="S5.T3.2.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S5.T3.2.m1.1c"><ci id="S5.T3.2.m1.1.1.cmml" xref="S5.T3.2.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.2.m1.1d">p</annotation><annotation encoding="application/x-llamapun" id="S5.T3.2.m1.1e">italic_p</annotation></semantics></math> (the synonym-replacement probability) in our mutator against <span class="ltx_text ltx_font_typewriter" id="S5.T3.9.1">gpt-3.5-turbo-0125</span>.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T3.7" style="width:303.5pt;height:152.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(62.4pt,-31.4pt) scale(1.69876874533939,1.69876874533939) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T3.7.5">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T3.7.5.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.3.1.1.1"><math alttext="p" class="ltx_Math" display="inline" id="S5.T3.3.1.1.1.m1.1"><semantics id="S5.T3.3.1.1.1.m1.1a"><mi id="S5.T3.3.1.1.1.m1.1.1" xref="S5.T3.3.1.1.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S5.T3.3.1.1.1.m1.1b"><ci id="S5.T3.3.1.1.1.m1.1.1.cmml" xref="S5.T3.3.1.1.1.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.3.1.1.1.m1.1c">p</annotation><annotation encoding="application/x-llamapun" id="S5.T3.3.1.1.1.m1.1d">italic_p</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.4.2.2.2">ASR<sup class="ltx_sup" id="S5.T3.4.2.2.2.1">↑</sup>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.5.3.3.3">ItS<sup class="ltx_sup" id="S5.T3.5.3.3.3.1">↓</sup>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.6.4.4.4">ER<sup class="ltx_sup" id="S5.T3.6.4.4.4.1">↑</sup>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.7.5.5.5">Runtime<sup class="ltx_sup" id="S5.T3.7.5.5.5.1">↓</sup>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.7.5.6.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.7.5.6.1.1">0.25</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.7.5.6.1.2"><span class="ltx_text ltx_font_bold" id="S5.T3.7.5.6.1.2.1">100</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.7.5.6.1.3"><span class="ltx_text ltx_font_bold" id="S5.T3.7.5.6.1.3.1">380</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.7.5.6.1.4"><span class="ltx_text ltx_font_bold" id="S5.T3.7.5.6.1.4.1">0.89</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.7.5.6.1.5"><span class="ltx_text ltx_font_bold" id="S5.T3.7.5.6.1.5.1">584</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.7.5.7.2">
<td class="ltx_td ltx_align_center" id="S5.T3.7.5.7.2.1">0.50</td>
<td class="ltx_td ltx_align_center" id="S5.T3.7.5.7.2.2"><span class="ltx_text ltx_font_bold" id="S5.T3.7.5.7.2.2.1">100</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.7.5.7.2.3">435</td>
<td class="ltx_td ltx_align_center" id="S5.T3.7.5.7.2.4">0.84</td>
<td class="ltx_td ltx_align_center" id="S5.T3.7.5.7.2.5">695</td>
</tr>
<tr class="ltx_tr" id="S5.T3.7.5.8.3">
<td class="ltx_td ltx_align_center" id="S5.T3.7.5.8.3.1">0.75</td>
<td class="ltx_td ltx_align_center" id="S5.T3.7.5.8.3.2"><span class="ltx_text ltx_font_bold" id="S5.T3.7.5.8.3.2.1">100</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.7.5.8.3.3">639</td>
<td class="ltx_td ltx_align_center" id="S5.T3.7.5.8.3.4">0.73</td>
<td class="ltx_td ltx_align_center" id="S5.T3.7.5.8.3.5">1197</td>
</tr>
<tr class="ltx_tr" id="S5.T3.7.5.9.4">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.7.5.9.4.1">1.00</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.7.5.9.4.2"><span class="ltx_text ltx_font_bold" id="S5.T3.7.5.9.4.2.1">100</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.7.5.9.4.3">474</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.7.5.9.4.4">0.79</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.7.5.9.4.5">738</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Mutator Configuration</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.6">Here, we experiment with different values of <math alttext="p" class="ltx_Math" display="inline" id="S5.SS4.p1.1.m1.1"><semantics id="S5.SS4.p1.1.m1.1a"><mi id="S5.SS4.p1.1.m1.1.1" xref="S5.SS4.p1.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.1.m1.1b"><ci id="S5.SS4.p1.1.m1.1.1.cmml" xref="S5.SS4.p1.1.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.1.m1.1c">p</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p1.1.m1.1d">italic_p</annotation></semantics></math>, the the synonym-replacement probability parameter, to determine the optimal mutator configuration.
To that end, we use the fuzzer to jailbreak <span class="ltx_text ltx_font_typewriter" id="S5.SS4.p1.6.1">gpt-3.5-turbo-0125</span> using different values of <math alttext="p" class="ltx_Math" display="inline" id="S5.SS4.p1.2.m2.1"><semantics id="S5.SS4.p1.2.m2.1a"><mi id="S5.SS4.p1.2.m2.1.1" xref="S5.SS4.p1.2.m2.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.2.m2.1b"><ci id="S5.SS4.p1.2.m2.1.1.cmml" xref="S5.SS4.p1.2.m2.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.2.m2.1c">p</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p1.2.m2.1d">italic_p</annotation></semantics></math> while using the optimal evaluator and selector configurations discovered above. Table <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S5.T3" title="Table 3 ‣ 5.3 Selector Configuration ‣ 5 Results ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_tag">3</span></a> shows the impact of different values of <math alttext="p" class="ltx_Math" display="inline" id="S5.SS4.p1.3.m3.1"><semantics id="S5.SS4.p1.3.m3.1a"><mi id="S5.SS4.p1.3.m3.1.1" xref="S5.SS4.p1.3.m3.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.3.m3.1b"><ci id="S5.SS4.p1.3.m3.1.1.cmml" xref="S5.SS4.p1.3.m3.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.3.m3.1c">p</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p1.3.m3.1d">italic_p</annotation></semantics></math> on our attack evaluation metrics. While all values of <math alttext="p" class="ltx_Math" display="inline" id="S5.SS4.p1.4.m4.1"><semantics id="S5.SS4.p1.4.m4.1a"><mi id="S5.SS4.p1.4.m4.1.1" xref="S5.SS4.p1.4.m4.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.4.m4.1b"><ci id="S5.SS4.p1.4.m4.1.1.cmml" xref="S5.SS4.p1.4.m4.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.4.m4.1c">p</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p1.4.m4.1d">italic_p</annotation></semantics></math> lead to 100% ASR, setting <math alttext="p" class="ltx_Math" display="inline" id="S5.SS4.p1.5.m5.1"><semantics id="S5.SS4.p1.5.m5.1a"><mi id="S5.SS4.p1.5.m5.1.1" xref="S5.SS4.p1.5.m5.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.5.m5.1b"><ci id="S5.SS4.p1.5.m5.1.1.cmml" xref="S5.SS4.p1.5.m5.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.5.m5.1c">p</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p1.5.m5.1d">italic_p</annotation></semantics></math>=0.25 yields the best performance in terms of all other metrics. Hence, for the final mutator configuration of <span class="ltx_text ltx_font_italic" id="S5.SS4.p1.6.2">JBFuzz</span>, we set <math alttext="p" class="ltx_Math" display="inline" id="S5.SS4.p1.6.m6.1"><semantics id="S5.SS4.p1.6.m6.1a"><mi id="S5.SS4.p1.6.m6.1.1" xref="S5.SS4.p1.6.m6.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.6.m6.1b"><ci id="S5.SS4.p1.6.m6.1.1.cmml" xref="S5.SS4.p1.6.m6.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.6.m6.1c">p</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p1.6.m6.1d">italic_p</annotation></semantics></math>=0.25.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>Main Results</h3>
<figure class="ltx_figure" id="S5.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="813" id="S5.F6.g1" src="x4.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span><span class="ltx_text ltx_font_italic" id="S5.F6.2.1">JBFuzz</span>’s jailbreak attack performance against different LLMs.</figcaption>
</figure>
<figure class="ltx_figure" id="S5.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="340" id="S5.F7.g1" src="x5.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Evolution of <span class="ltx_text ltx_font_italic" id="S5.F7.2.1">JBFuzz</span>’s jailbreak success over the course of the fuzzing process.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS5.p1">
<p class="ltx_p" id="S5.SS5.p1.1">Now that the optimal configurations for the selector, mutator, and evaluator have been finalized, we provide the main results of <span class="ltx_text ltx_font_italic" id="S5.SS5.p1.1.1">JBFuzz</span> in this section. Figure <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S5.F6" title="Figure 6 ‣ 5.5 Main Results ‣ 5 Results ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_tag">6</span></a> shows the performance of <span class="ltx_text ltx_font_italic" id="S5.SS5.p1.1.2">JBFuzz</span> against all our target LLMs in terms of our metrics.</p>
</div>
<div class="ltx_para" id="S5.SS5.p2">
<p class="ltx_p" id="S5.SS5.p2.1">Here are the key takeaways for the primary metrics from the figure:
(i) <span class="ltx_text ltx_font_italic" id="S5.SS5.p2.1.1">JBFuzz</span> achieves 100% ASR (i.e., it successfully jailbreaks the LLM for all questions) against all considered LLMs except Llama2 for which, the ASR is 91%. The average (over all LLMs over all questions) ASR is 99%.
(ii) For all but Llama2, <span class="ltx_text ltx_font_italic" id="S5.SS5.p2.1.2">JBFuzz</span> requires less than 1000 iterations to jailbreak the LLM for all questions, with GPT-3.5 being the most susceptible and requiring only 380 iterations.
(iii) The Gemini LLMs yield the
highest attack efficiency, with an ER of 0.97 and 0.95 for Gemini-2.0 and Gemini-1.5, respectively. This means that 97% and 95% of the fuzzing iterations resulted in a jailbreak against these LLMs, respectively. On the other hand, the small and open-source Llama2 is the most resilient against jailbreaks with an ER of only 0.34.
(iv) Each fuzzing run (except DeepSeek-R1) required less than 2 hours of wall-clock time, with the DeepSeek LLMs requiring the highest time due to their slow API response speed.
(v) The average time and number of LLM queries required by <span class="ltx_text ltx_font_italic" id="S5.SS5.p2.1.3">JBFuzz</span> for jailbreaking a given question is just 60 seconds and <math alttext="\approx" class="ltx_Math" display="inline" id="S5.SS5.p2.1.m1.1"><semantics id="S5.SS5.p2.1.m1.1a"><mo id="S5.SS5.p2.1.m1.1.1" xref="S5.SS5.p2.1.m1.1.1.cmml">≈</mo><annotation-xml encoding="MathML-Content" id="S5.SS5.p2.1.m1.1b"><approx id="S5.SS5.p2.1.m1.1.1.cmml" xref="S5.SS5.p2.1.m1.1.1"></approx></annotation-xml><annotation encoding="application/x-tex" id="S5.SS5.p2.1.m1.1c">\approx</annotation><annotation encoding="application/x-llamapun" id="S5.SS5.p2.1.m1.1d">≈</annotation></semantics></math>7 queries, respectively.
(vi) Interestingly, Llama2 outperforms Llama3 in terms of jailbreak resilience as measured through ASR, ItS, and ER, demonstrating that jailbreaking resilience does not necessarily improve with newer generations.
(vii) Another interesting observation is that even the latest and state-of-the-art LLMs such as GPT-4o, GPT-4o-mini, Gemini-2.0, and DeepSeek-R1 are highly susceptible to jailbreak attacks, highlighting the urgent need for automated red-teaming attacks such as <span class="ltx_text ltx_font_italic" id="S5.SS5.p2.1.4">JBFuzz</span>.</p>
</div>
<div class="ltx_para" id="S5.SS5.p3">
<p class="ltx_p" id="S5.SS5.p3.3">Apart from the primary takeaways, the following inferences can also be drawn about the LLMs:
(i) The average token counts (ATCs) for all LLMs are of the order of a couple thousand, with the largest being <math alttext="\approx" class="ltx_Math" display="inline" id="S5.SS5.p3.1.m1.1"><semantics id="S5.SS5.p3.1.m1.1a"><mo id="S5.SS5.p3.1.m1.1.1" xref="S5.SS5.p3.1.m1.1.1.cmml">≈</mo><annotation-xml encoding="MathML-Content" id="S5.SS5.p3.1.m1.1b"><approx id="S5.SS5.p3.1.m1.1.1.cmml" xref="S5.SS5.p3.1.m1.1.1"></approx></annotation-xml><annotation encoding="application/x-tex" id="S5.SS5.p3.1.m1.1c">\approx</annotation><annotation encoding="application/x-llamapun" id="S5.SS5.p3.1.m1.1d">≈</annotation></semantics></math>6300, and the average being <math alttext="\approx" class="ltx_Math" display="inline" id="S5.SS5.p3.2.m2.1"><semantics id="S5.SS5.p3.2.m2.1a"><mo id="S5.SS5.p3.2.m2.1.1" xref="S5.SS5.p3.2.m2.1.1.cmml">≈</mo><annotation-xml encoding="MathML-Content" id="S5.SS5.p3.2.m2.1b"><approx id="S5.SS5.p3.2.m2.1.1.cmml" xref="S5.SS5.p3.2.m2.1.1"></approx></annotation-xml><annotation encoding="application/x-tex" id="S5.SS5.p3.2.m2.1c">\approx</annotation><annotation encoding="application/x-llamapun" id="S5.SS5.p3.2.m2.1d">≈</annotation></semantics></math>3100 tokens. This shows that the average cost for jailbreaking a given question is extremely low (<math alttext="\approx" class="ltx_Math" display="inline" id="S5.SS5.p3.3.m3.1"><semantics id="S5.SS5.p3.3.m3.1a"><mo id="S5.SS5.p3.3.m3.1.1" xref="S5.SS5.p3.3.m3.1.1.cmml">≈</mo><annotation-xml encoding="MathML-Content" id="S5.SS5.p3.3.m3.1b"><approx id="S5.SS5.p3.3.m3.1.1.cmml" xref="S5.SS5.p3.3.m3.1.1"></approx></annotation-xml><annotation encoding="application/x-tex" id="S5.SS5.p3.3.m3.1c">\approx</annotation><annotation encoding="application/x-llamapun" id="S5.SS5.p3.3.m3.1d">≈</annotation></semantics></math>$0.01, using the average token price of GPT-4o).
(ii) The fuzzing rate (FR) is mostly correlated with runtime, however, there is an interesting exception: although the total runtime for Gemini-2.0 is lower than that of other LLMs such as Llama2, GPT-4o, and GPT-4o-mini, its FR is higher.
than these LLMs. This is due to the stricter rate-limit for Gemini-2.0, allowing only 10 requests per minute in the free tier <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib31" title="">31</a>]</cite>. This observation shows that the scalability of the fuzzer is also subject to external factors, which need to be considered when leveraging it for red-teaming LLMs.</p>
</div>
<div class="ltx_para" id="S5.SS5.p4">
<p class="ltx_p" id="S5.SS5.p4.1">To further understand the performance of <span class="ltx_text ltx_font_italic" id="S5.SS5.p4.1.1">JBFuzz</span> and the resilience of different LLMs against jailbreak attacks, we analyze the evolution of the number of questions that jailbreak the LLM over the fuzzing iterations. Figure <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S5.F7" title="Figure 7 ‣ 5.5 Main Results ‣ 5 Results ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_tag">7</span></a> plots this. From the figure, we can infer three classes of LLMs: (i) Llama2, which is its own class, with the highest resilience to jailbreaks, (ii) GPT-3.5, Llama3, DeepSeek-V3, DeepSeek-R1, Gemini-2.0, and Gemini-1.5, which are quickly jailbroken for many questions, and (iii) GPT-4o and GPT-4o-mini, state-of-the-art models from OpenAI, constitute the last class that falls between Llama2 and the rest of the LLMs, albeit a lot closer to the second class than to Llama2.</p>
</div>
<div class="ltx_para" id="S5.SS5.p5">
<span class="ltx_ERROR undefined" id="S5.SS5.p5.1">{myframe}</span>
<p class="ltx_p" id="S5.SS5.p5.2"><span class="ltx_text ltx_font_bold" id="S5.SS5.p5.2.1">Key Finding 3.</span> <span class="ltx_text ltx_font_italic" id="S5.SS5.p5.2.2">JBFuzz</span> successfully jailbreaks all target LLMs except Llama2 within 2 hours for all target harmful/unethical questions.

<span class="ltx_ERROR undefined" id="S5.SS5.p5.2.3">{myframe}</span>
<span class="ltx_text ltx_font_bold" id="S5.SS5.p5.2.4">Key Finding 4.</span> Llama2, despite being older than Llama3 and most other considered LLMs, shows the most resilience against jailbreak attacks.</p>
</div>
<figure class="ltx_figure" id="S5.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="216" id="S5.F8.g1" src="x6.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Distribution of perplexity values of <span class="ltx_text ltx_font_italic" id="S5.F8.2.1">JBFuzz</span>’s successful jailbreak prompts.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.6 </span>Stealthiness Evaluation</h3>
<div class="ltx_para" id="S5.SS6.p1">
<p class="ltx_p" id="S5.SS6.p1.1">As suggested by prior works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib45" title="">45</a>]</cite>, we evaluate <span class="ltx_text ltx_font_italic" id="S5.SS6.p1.1.1">JBFuzz</span> against the perplexity defense. To do so, we determine a perplexity threshold based on our dataset of harmful/unethical questions, and consider any prompt that crosses this threshold as detected.
As demonstrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S5.F8" title="Figure 8 ‣ 5.5 Main Results ‣ 5 Results ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_tag">8</span></a>, the perplexity values of all our successful jailbreak prompts are under the detection threshold, meaning <span class="ltx_text ltx_font_italic" id="S5.SS6.p1.1.2">JBFuzz</span> bypasses this defense.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.7 </span>Comparison With Related Work</h3>
<div class="ltx_para" id="S5.SS7.p1">
<p class="ltx_p" id="S5.SS7.p1.4">Here, we compare the performance of <span class="ltx_text ltx_font_italic" id="S5.SS7.p1.4.1">JBFuzz</span> with a closely-related work that jailbreaks LLMs under the same threat model as ours <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib99" title="">99</a>]</cite>.
We compare the two techniques in terms of attack success rate (ASR, the proportion of questions successfully jailbroken), average token count (ATC), and average number of queries required to jailbreak the target LLM for a given question. We use GPT-3.5
and Llama2
as the target LLMs. Table <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S5.T4" title="Table 4 ‣ 5.7 Comparison With Related Work ‣ 5 Results ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_tag">4</span></a> shows the results: <span class="ltx_text ltx_font_italic" id="S5.SS7.p1.4.2">JBFuzz</span> achieves higher ASR while consuming <math alttext="\approx 68\times" class="ltx_math_unparsed" display="inline" id="S5.SS7.p1.1.m1.1"><semantics id="S5.SS7.p1.1.m1.1a"><mrow id="S5.SS7.p1.1.m1.1b"><mo id="S5.SS7.p1.1.m1.1.1">≈</mo><mn id="S5.SS7.p1.1.m1.1.2">68</mn><mo id="S5.SS7.p1.1.m1.1.3" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S5.SS7.p1.1.m1.1c">\approx 68\times</annotation><annotation encoding="application/x-llamapun" id="S5.SS7.p1.1.m1.1d">≈ 68 ×</annotation></semantics></math> and <math alttext="\approx 59\times" class="ltx_math_unparsed" display="inline" id="S5.SS7.p1.2.m2.1"><semantics id="S5.SS7.p1.2.m2.1a"><mrow id="S5.SS7.p1.2.m2.1b"><mo id="S5.SS7.p1.2.m2.1.1">≈</mo><mn id="S5.SS7.p1.2.m2.1.2">59</mn><mo id="S5.SS7.p1.2.m2.1.3" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S5.SS7.p1.2.m2.1c">\approx 59\times</annotation><annotation encoding="application/x-llamapun" id="S5.SS7.p1.2.m2.1d">≈ 59 ×</annotation></semantics></math> fewer tokens and queries, respectively for GPT-3.5 and <math alttext="\approx 16\times" class="ltx_math_unparsed" display="inline" id="S5.SS7.p1.3.m3.1"><semantics id="S5.SS7.p1.3.m3.1a"><mrow id="S5.SS7.p1.3.m3.1b"><mo id="S5.SS7.p1.3.m3.1.1">≈</mo><mn id="S5.SS7.p1.3.m3.1.2">16</mn><mo id="S5.SS7.p1.3.m3.1.3" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S5.SS7.p1.3.m3.1c">\approx 16\times</annotation><annotation encoding="application/x-llamapun" id="S5.SS7.p1.3.m3.1d">≈ 16 ×</annotation></semantics></math> and <math alttext="\approx 31\times" class="ltx_math_unparsed" display="inline" id="S5.SS7.p1.4.m4.1"><semantics id="S5.SS7.p1.4.m4.1a"><mrow id="S5.SS7.p1.4.m4.1b"><mo id="S5.SS7.p1.4.m4.1.1">≈</mo><mn id="S5.SS7.p1.4.m4.1.2">31</mn><mo id="S5.SS7.p1.4.m4.1.3" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S5.SS7.p1.4.m4.1c">\approx 31\times</annotation><annotation encoding="application/x-llamapun" id="S5.SS7.p1.4.m4.1d">≈ 31 ×</annotation></semantics></math> fewer tokens and queries, respectively for Llama2.</p>
</div>
<figure class="ltx_table" id="S5.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Comparison of <span class="ltx_text ltx_font_italic" id="S5.T4.5.1">JBFuzz</span> with <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib99" title="">99</a>]</cite>.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T4.3" style="width:433.6pt;height:125.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(61.5pt,-17.8pt) scale(1.3956330663885,1.3956330663885) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T4.3.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T4.3.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T4.3.3.3.4">LLM</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T4.3.3.3.5">Attack</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T4.1.1.1.1">ASR<sup class="ltx_sup" id="S5.T4.1.1.1.1.1">↑</sup>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T4.2.2.2.2">ATC<sup class="ltx_sup" id="S5.T4.2.2.2.2.1">↓</sup>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T4.3.3.3.3">Avg. # Queries<sup class="ltx_sup" id="S5.T4.3.3.3.3.1">↓</sup>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T4.3.3.4.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.3.4.1.1" rowspan="2"><span class="ltx_text" id="S5.T4.3.3.4.1.1.1">GPT-3.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.3.4.1.2"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib99" title="">99</a>]</cite></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.3.4.1.3">97</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.3.4.1.4">64010</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.3.4.1.5">225.43</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.3.5.2">
<td class="ltx_td ltx_align_center" id="S5.T4.3.3.5.2.1">
<span class="ltx_text ltx_font_italic" id="S5.T4.3.3.5.2.1.1">JBFuzz</span> (This Work)</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.3.5.2.2"><span class="ltx_text ltx_font_bold" id="S5.T4.3.3.5.2.2.1">100</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.3.5.2.3"><span class="ltx_text ltx_font_bold" id="S5.T4.3.3.5.2.3.1">928</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.3.5.2.4"><span class="ltx_text ltx_font_bold" id="S5.T4.3.3.5.2.4.1">3.8</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.3.6.3">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T4.3.3.6.3.1" rowspan="2"><span class="ltx_text" id="S5.T4.3.3.6.3.1.1">Llama2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.3.6.3.2"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib99" title="">99</a>]</cite></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.3.6.3.3">90</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.3.6.3.4">82730</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.3.6.3.5">345.38</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.3.7.4">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.3.3.7.4.1">
<span class="ltx_text ltx_font_italic" id="S5.T4.3.3.7.4.1.1">JBFuzz</span> (This Work)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.3.3.7.4.2"><span class="ltx_text ltx_font_bold" id="S5.T4.3.3.7.4.2.1">91</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.3.3.7.4.3"><span class="ltx_text ltx_font_bold" id="S5.T4.3.3.7.4.3.1">5093</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.3.3.7.4.4"><span class="ltx_text ltx_font_bold" id="S5.T4.3.3.7.4.4.1">10.98</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS8">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.8 </span>Runtime Breakdown Analysis</h3>
<figure class="ltx_figure" id="S5.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="341" id="S5.F9.g1" src="x7.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Runtime breakdown of the different steps of <span class="ltx_text ltx_font_italic" id="S5.F9.2.1">JBFuzz</span>.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS8.p1">
<p class="ltx_p" id="S5.SS8.p1.1">In Section <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S5.SS5" title="5.5 Main Results ‣ 5 Results ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_tag">5.5</span></a>, we observed that the total runtime of <span class="ltx_text ltx_font_italic" id="S5.SS8.p1.1.1">JBFuzz</span> is less than 2 hours for each LLM<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>except for DeepSeek-R1, as explained above</span></span></span>, making it extremely fast and practical for large-scale red-teaming efforts. In this section, we delve deeper into the runtime analysis
to get a better understanding of the bottleneck(s) during the fuzzing process. To do so, we analyze the runtime breakdown of different components of the fuzzer relative to the overall runtime. Figure <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S5.F9" title="Figure 9 ‣ 5.8 Runtime Breakdown Analysis ‣ 5 Results ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_tag">9</span></a> shows the breakdown (in percentages) of the four main components of the fuzzer: selection, mutation, execution, and evaluation. The contributions of selection and mutation to runtime are minuscule (&lt;0.003% and &lt;0.35%, respectively).
Evaluation is the second-highest contributor to the total runtime, with an average contribution of 5.7%
and a maximum contribution of 14%
The single largest contributor to the overall runtime is the execution step. It is responsible for as high as 99.3%
of the total runtime of the fuzzer, and its average contribution is a whopping 94.1%.
This runtime breakdown further highlights the remarkable efficiency of <span class="ltx_text ltx_font_italic" id="S5.SS8.p1.1.2">JBFuzz</span>. Since the execution step is unavoidable
to get an accurate response from the target LLM for a potential jailbreak prompt, and <span class="ltx_text ltx_font_italic" id="S5.SS8.p1.1.3">JBFuzz</span>’s almost all time is spent on execution, <span class="ltx_text ltx_font_italic" id="S5.SS8.p1.1.4">JBFuzz</span> ensures exceptional efficiency by optimizing the overall workflow.</p>
</div>
<div class="ltx_para" id="S5.SS8.p2">
<span class="ltx_ERROR undefined" id="S5.SS8.p2.1">{myframe}</span>
<p class="ltx_p" id="S5.SS8.p2.2"><span class="ltx_text ltx_font_bold" id="S5.SS8.p2.2.1">Key Finding 5.</span> <span class="ltx_text ltx_font_italic" id="S5.SS8.p2.2.2">JBFuzz</span> is very close to ideal in terms of efficiency since almost all of the runtime is spent on querying the target LLM (which is unavoidable) and minimal time is spent on the other steps of fuzzing.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Related Work and Discussion</h2>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Jailbreak Attacks</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">As LLMs have grown in popularity over the past few years, so has the research on jailbreak attacks.
Existing attacks in this area falls in two main approaches: strategy-based and optimization-based.</p>
</div>
<div class="ltx_para" id="S6.SS1.p2">
<p class="ltx_p" id="S6.SS1.p2.1">Strategy-based jailbreak attacks rely on explicitly designed tactics, often informed by human intuition, to compromise LLMs. These methods seldom require automated tools, focusing instead on leveraging specific strategies to bypass safeguards. Notable examples include the "Do-Anything-Now (DAN)" prompts <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib81" title="">81</a>]</cite>,
which uses role-playing scenarios to trick LLMs into disregarding ethical constraints. Further refinements of the role-playing approach, such as <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib47" title="">47</a>]</cite>, illustrate how these strategies can be systematically implemented and improved. Other innovative tactics include emotional manipulation, wordplay <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib79" title="">79</a>]</cite>,
persuasion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib102" title="">102</a>]</cite>, obfuscation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib100" title="">100</a>]</cite>, and ASCII-based manipulations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib46" title="">46</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S6.SS1.p3">
<p class="ltx_p" id="S6.SS1.p3.1">Optimization-based approaches leverage automated algorithms to generate prompts that elicit undesired model behavior. For instance, several works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib106" title="">106</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib54" title="">54</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib53" title="">53</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib105" title="">105</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib36" title="">36</a>]</cite> utilize loss function gradient as a feedback that informs the generation of the prompts.
Other works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib74" title="">74</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib52" title="">52</a>]</cite> train generative models to imitate these optimization algorithms to generate prompts.
Some limitations of the techniques in this category are the lack of explicit jailbreak attack knowledge for the algorithm, resulting in suboptimal attack performance, limited diversity in the generated prompts, and
expensive and time-consuming algorithms of trainings.</p>
</div>
<div class="ltx_para" id="S6.SS1.p4">
<p class="ltx_p" id="S6.SS1.p4.1">Our work is aligned with
optimization-based approaches.
However, unlike existing works,
our work neither requires internal model information,
nor relies on expensive and time-consuming
generation or evaluation techniques.
By adhering to the stricter black-box threat model, our work extends its applicability to real-world scenarios with restrictive conditions where internal details are inaccessible.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Other Security and Privacy Risks</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">Apart from jailbreaking, LLMs also pose other security and privacy risks. For instance, sensitive information present in the training dataset can be memorized by the model and unintentionally leaked through generated responses <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib96" title="">96</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib88" title="">88</a>]</cite>.
Another class of attacks are prompt injection attacks that can manipulate LLMs into producing harmful or unintended outputs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib55" title="">55</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib35" title="">35</a>]</cite>.
Also, poisoning attacks can introduce malicious data into the model’s training or fine-tuning datasets to influence its behavior <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib78" title="">78</a>, <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib95" title="">95</a>]</cite>.
Addressing these concerns requires LLM developers, users, and red-teams, and potentially governing agencies to come together and develop training techniques, verification techniques, policies, and user education.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Limitations and Future Work</h3>
<div class="ltx_para" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1">Although <span class="ltx_text ltx_font_italic" id="S6.SS3.p1.1.1">JBFuzz</span> successfully jailbreaks all LLMs for most of the harmful/unethical questions with just a couple of hours of runtime, there are still some avenues for improvement.
First, as explained in Section <a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#S5.SS2" title="5.2 Evaluator Configuration ‣ 5 Results ‣ JBFuzz: Jailbreaking LLMs Efficiently and Effectively Using Fuzzing"><span class="ltx_text ltx_ref_tag">5.2</span></a>, although the evaluator has high accuracy and outperforms GPT-4o, it
generates some false positives and false negatives. It is important to improve the accuracy to reliably red-team LLMs with confidence.
Second, we only focus on single-turn conversations (i.e., one prompt and one response). Extending this to multi-turn conversations would be interesting since that allows the possibility of gradually steering the LLM toward harmful/unethical responses over the course of the conversation instead of doing so in a single turn.
Third, similar to other recent works, we observed that <span class="ltx_text ltx_font_italic" id="S6.SS3.p1.1.2">JBFuzz</span> performs relatively poorly (91% attack success rate) against Llama2. This common trend across multiple independent studies warrants future investigation.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">While LLMs have revolutionized various aspects of our lives, their widespread adoption has also exposed vulnerabilities, creating opportunities for misuse. One pressing concern is the susceptibility of LLMs to be jailbroken, enabling the generation of harmful/unethical content. While LLM developers are actively working to red-team and address these vulnerabilities, the current approaches often lack one or more of this desiderata: automation, scalability, and effectiveness.</p>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">To address this limitation, we developed <span class="ltx_text ltx_font_italic" id="S7.p2.1.1">JBFuzz</span>, a novel lightweight fuzzing-inspired technique for red-teaming LLMs for jailbreak attacks.
To extract the maximum performance from the fuzzer, we
devise novel seed prompt templates that allows us to go beyond the existing jailbreak templates and exercise new behaviors from the target LLM, thereby increasing the effectiveness of our fuzzer. We also devise a fast synonym-based mutation technique to introduce diversity into the prompt templates. Finally, to ensure scalability, we propose a novel lightweight embedding-based evaluator, which significantly outperforms the prior techniques
in terms of speed and accuracy.</p>
</div>
<div class="ltx_para" id="S7.p3">
<p class="ltx_p" id="S7.p3.1">Experimental results confirm that <span class="ltx_text ltx_font_italic" id="S7.p3.1.1">JBFuzz</span>-generated templates achieve an average attack success rate of 99%.
They successfully jailbreak all LLMs (except DeepSeek-R1 due to its slow API) for almost all harmful/unethical questions with just 2 hours of runtime. Experiments also validate the remarkable efficiency of <span class="ltx_text ltx_font_italic" id="S7.p3.1.2">JBFuzz</span>: almost all time during fuzzing is spent on getting the potentially jailbreak response from the target LLM, which is unavoidable.
<span class="ltx_text ltx_font_italic" id="S7.p3.1.3">JBFuzz</span> successfully jailbreaks the latest and most advanced LLMs available today, highlighting its crucial need in red-teaming.
We believe <span class="ltx_text ltx_font_italic" id="S7.p3.1.4">JBFuzz</span> can serve as a valuable tool for LLM developers to ensure safe and responsible use of LLMs.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Gabriel Alon et al.

</span>
<span class="ltx_bibblock">Detecting language model attacks with perplexity.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">arXiv preprint arXiv:2308.14132</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Anthropic.

</span>
<span class="ltx_bibblock">Mitigate jailbreaks and prompt injections.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/mitigate-jailbreaks" title="">https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/mitigate-jailbreaks</a>.

</span>
<span class="ltx_bibblock">[Online; last accessed 09-Mar-2025].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Anthropic.

</span>
<span class="ltx_bibblock">Rate Limits.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://docs.anthropic.com/en/api/rate-limits" title="">https://docs.anthropic.com/en/api/rate-limits</a>.

</span>
<span class="ltx_bibblock">[Online; last accessed 09-Mar-2025].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Anthropic.

</span>
<span class="ltx_bibblock">Introducing Claude.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.anthropic.com/news/introducing-claude" title="">https://www.anthropic.com/news/introducing-claude</a>, 2023.

</span>
<span class="ltx_bibblock">[Online; last accessed 09-Mar-2025].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Anthropic.

</span>
<span class="ltx_bibblock">Many-shot jailbreaking.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.anthropic.com/research/many-shot-jailbreaking" title="">https://www.anthropic.com/research/many-shot-jailbreaking</a>, 2024.

</span>
<span class="ltx_bibblock">[Online; last accessed 09-Mar-2025].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Peter Auer, et al.

</span>
<span class="ltx_bibblock">Finite-time analysis of the multiarmed bandit problem.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">Machine learning</span>, 47:235–256, 2002.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Peter Auer, et al.

</span>
<span class="ltx_bibblock">The nonstochastic multiarmed bandit problem.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">SIAM journal on computing</span>, 32(1):48–77, 2002.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Yuntao Bai, et al.

</span>
<span class="ltx_bibblock">Training a helpful and harmless assistant with reinforcement learning from human feedback.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">arXiv preprint arXiv:2204.05862</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Dipto Barman, et al.

</span>
<span class="ltx_bibblock">The dark side of language models: Exploring the potential of llms in multimedia disinformation generation and dissemination.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">Machine Learning with Applications</span>, page 100545, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Mika Beckerich, et al.

</span>
<span class="ltx_bibblock">Ratgpt: Turning online llms into proxies for malware attacks.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">arXiv preprint arXiv:2308.09183</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Marcel Böhme, et al.

</span>
<span class="ltx_bibblock">Coverage-based greybox fuzzing as markov chain.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security</span>, pages 1032–1043, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Marcus Botacin.

</span>
<span class="ltx_bibblock">GPThreats-3: Is Automatic Malware Generation a Threat?

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">2023 IEEE Security and Privacy Workshops (SPW)</span>, pages 238–254, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Ben Buchanan, et al.

</span>
<span class="ltx_bibblock">Truth, lies, and automation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">Center for Security and Emerging technology</span>, 1(1):2, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Insights by Stanford Business.

</span>
<span class="ltx_bibblock">Wreck the Vote: How AI-Driven Misinformation Could Undermine Democracy.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.gsb.stanford.edu/insights/wreck-vote-how-ai-driven-misinformation-could-undermine-democracy" title="">https://www.gsb.stanford.edu/insights/wreck-vote-how-ai-driven-misinformation-could-undermine-democracy</a>, 2024.

</span>
<span class="ltx_bibblock">[Online; last accessed 09-Mar-2025].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Alexia Cambon, et al.

</span>
<span class="ltx_bibblock">Early llm-based tools for enterprise information workers likely provide meaningful boosts to productivity.

</span>
<span class="ltx_bibblock">Technical Report MSR-TR-2023-43, Microsoft, December 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
certa.

</span>
<span class="ltx_bibblock">The Role of Automation in Streamlining Compliance Workflows.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.certa.ai/blogs/the-role-of-automation-in-streamlining-compliance-workflows" title="">https://www.certa.ai/blogs/the-role-of-automation-in-streamlining-compliance-workflows</a>, 2024.

</span>
<span class="ltx_bibblock">[Online; last accessed 09-Mar-2025].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Patrick Chao, et al.

</span>
<span class="ltx_bibblock">Jailbreaking black box large language models in twenty queries.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib17.1.1">arXiv preprint arXiv:2310.08419</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Jay Chen et al.

</span>
<span class="ltx_bibblock">Deceptive Delight: Jailbreak LLMs Through Camouflage and Distraction.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://unit42.paloaltonetworks.com/jailbreak-llms-through-camouflage-distraction/" title="">https://unit42.paloaltonetworks.com/jailbreak-llms-through-camouflage-distraction/</a>, 2024.

</span>
<span class="ltx_bibblock">[Online; last accessed 09-Mar-2025].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
DeepSeek.

</span>
<span class="ltx_bibblock">Models &amp; Pricing.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api-docs.deepseek.com/quick_start/pricing" title="">https://api-docs.deepseek.com/quick_start/pricing</a>.

</span>
<span class="ltx_bibblock">[Online; last accessed 09-Mar-2025].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
DeepSeek.

</span>
<span class="ltx_bibblock">DeepSeek-R1 Release.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api-docs.deepseek.com/news/news250120" title="">https://api-docs.deepseek.com/news/news250120</a>, 2025.

</span>
<span class="ltx_bibblock">[Online; last accessed 09-Mar-2025].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Gelei Deng, et al.

</span>
<span class="ltx_bibblock">Jailbreaker: Automated jailbreak across multiple large language model chatbots.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib21.1.1">arXiv preprint arXiv:2307.08715</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Yue Deng, et al.

</span>
<span class="ltx_bibblock">Multilingual jailbreak challenges in large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib22.1.1">arXiv preprint arXiv:2310.06474</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Dharani Devadiga, et al.

</span>
<span class="ltx_bibblock">Gleam: Gan and llm for evasive adversarial malware.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib23.1.1">2023 14th International Conference on Information and Communication Technology Convergence (ICTC)</span>, pages 53–58, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Angela Fan, et al.

</span>
<span class="ltx_bibblock">Large language models for software engineering: Survey and open problems.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib24.1.1">arXiv preprint arXiv:2310.03533</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Chongzhou Fang, et al.

</span>
<span class="ltx_bibblock">Large language models for code analysis: Do <math alttext="\{" class="ltx_Math" display="inline" id="bib.bib25.1.m1.1"><semantics id="bib.bib25.1.m1.1a"><mo id="bib.bib25.1.m1.1.1" stretchy="false" xref="bib.bib25.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib25.1.m1.1b"><ci id="bib.bib25.1.m1.1.1.cmml" xref="bib.bib25.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib25.1.m1.1c">\{</annotation><annotation encoding="application/x-llamapun" id="bib.bib25.1.m1.1d">{</annotation></semantics></math>LLMs<math alttext="\}" class="ltx_Math" display="inline" id="bib.bib25.2.m2.1"><semantics id="bib.bib25.2.m2.1a"><mo id="bib.bib25.2.m2.1.1" stretchy="false" xref="bib.bib25.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib25.2.m2.1b"><ci id="bib.bib25.2.m2.1.1.cmml" xref="bib.bib25.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib25.2.m2.1c">\}</annotation><annotation encoding="application/x-llamapun" id="bib.bib25.2.m2.1d">}</annotation></semantics></math> really do their job?

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib25.3.1">33rd USENIX Security Symposium (USENIX Security 24)</span>, pages 829–846, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Noah Fleischmann, et al.

</span>
<span class="ltx_bibblock">How To Protect LLMs From Jailbreaking Attacks.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.boozallen.com/insights/ai-research/how-to-protect-llms-from-jailbreaking-attacks.html" title="">https://www.boozallen.com/insights/ai-research/how-to-protect-llms-from-jailbreaking-attacks.html</a>, 2024.

</span>
<span class="ltx_bibblock">[Online; last accessed 09-Mar-2025].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Deep Ganguli, et al.

</span>
<span class="ltx_bibblock">Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib27.1.1">arXiv preprint arXiv:2209.07858</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Vasudev Gohil, et al.

</span>
<span class="ltx_bibblock">LLMPirate: LLMs for Black-box Hardware IP Piracy.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib28.1.1">arXiv preprint arXiv:2411.16111</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Google.

</span>
<span class="ltx_bibblock">ClusterFuzz.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://google.github.io/clusterfuzz/" title="">https://google.github.io/clusterfuzz/</a>.

</span>
<span class="ltx_bibblock">[Online; last accessed 09-Mar-2025].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Google.

</span>
<span class="ltx_bibblock">Gemini models - Gemini 1.5 Flash.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ai.google.dev/gemini-api/docs/models/gemini#gemini-1.5-flash" title="">https://ai.google.dev/gemini-api/docs/models/gemini#gemini-1.5-flash</a>.

</span>
<span class="ltx_bibblock">[Online; last accessed 09-Mar-2025].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Google.

</span>
<span class="ltx_bibblock">Gemini models - Gemini 2.0 Flash (Experimental).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ai.google.dev/gemini-api/docs/models/gemini#gemini-2.0-flash" title="">https://ai.google.dev/gemini-api/docs/models/gemini#gemini-2.0-flash</a>.

</span>
<span class="ltx_bibblock">[Online; last accessed 09-Mar-2025].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Google.

</span>
<span class="ltx_bibblock">HonggFuzz.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://honggfuzz.dev" title="">https://honggfuzz.dev</a>.

</span>
<span class="ltx_bibblock">[Online; last accessed 09-Mar-2025].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Google.

</span>
<span class="ltx_bibblock">Pricing Models.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ai.google.dev/pricing#1_5flash" title="">https://ai.google.dev/pricing#1_5flash</a>.

</span>
<span class="ltx_bibblock">[Online; last accessed 09-Mar-2025].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Google.

</span>
<span class="ltx_bibblock">Introducing Gemini: our largest and most capable AI model.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://blog.google/technology/ai/google-gemini-ai/" title="">https://blog.google/technology/ai/google-gemini-ai/</a>, 2023.

</span>
<span class="ltx_bibblock">[Online; last accessed 09-Mar-2025].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Kai Greshake, et al.

</span>
<span class="ltx_bibblock">Not what you’ve signed up for: Compromising real-world llm-integrated applications with indirect prompt injection.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib35.1.1">Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security</span>, pages 79–90, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Xingang Guo, et al.

</span>
<span class="ltx_bibblock">Cold-attack: Jailbreaking llms with stealthiness and controllability.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib36.1.1">arXiv preprint arXiv:2402.08679</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Julian Hazell.

</span>
<span class="ltx_bibblock">Large language models can be used to effectively scale spear phishing campaigns.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib37.1.1">arXiv preprint arXiv:2305.06972</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Todd Helmus et al.

</span>
<span class="ltx_bibblock">Generative Artificial Intelligence Threats to Information Integrity and Potential Policy Responses.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.rand.org/pubs/perspectives/PEA3089-1.html" title="">https://www.rand.org/pubs/perspectives/PEA3089-1.html</a>, 2024.

</span>
<span class="ltx_bibblock">[Online; last accessed 09-Mar-2025].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Krystal Hu.

</span>
<span class="ltx_bibblock">CHATGPT sets record for fastest-growing user base - analyst note.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/" title="">https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/</a>, 2023.

</span>
<span class="ltx_bibblock">[Online; last accessed 09-Mar-2025].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Lucas Hu, et al.

</span>
<span class="ltx_bibblock">Now You See Me, Now You Don’t: Using LLMs to Obfuscate Malicious JavaScript.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://unit42.paloaltonetworks.com/using-llms-obfuscate-malicious-javascript/" title="">https://unit42.paloaltonetworks.com/using-llms-obfuscate-malicious-javascript/</a>, 2024.

</span>
<span class="ltx_bibblock">[Online; last accessed 09-Mar-2025].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Hugging Face.

</span>
<span class="ltx_bibblock">meta-llama/Llama-2-7b-chat-hf.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/meta-llama/Llama-2-7b-chat-hf" title="">https://huggingface.co/meta-llama/Llama-2-7b-chat-hf</a>.

</span>
<span class="ltx_bibblock">[Online; last accessed 09-Mar-2025].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Hugging Face.

</span>
<span class="ltx_bibblock">meta-llama/Llama-3.1-8B-Instruct.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct" title="">https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct</a>.

</span>
<span class="ltx_bibblock">[Online; last accessed 09-Mar-2025].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Hugging Face.

</span>
<span class="ltx_bibblock">Massive Text Embedding Benchmark (MTEB) Leaderboard.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/spaces/mteb/leaderboard" title="">https://huggingface.co/spaces/mteb/leaderboard</a>, 2024.

</span>
<span class="ltx_bibblock">[Online; last accessed 09-Mar-2025].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
LLVM Compiler Infrastructure.

</span>
<span class="ltx_bibblock">libFuzzer – a library for coverage-guided fuzz testing.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://llvm.org/docs/LibFuzzer.html" title="">https://llvm.org/docs/LibFuzzer.html</a>.

</span>
<span class="ltx_bibblock">[Online; last accessed 09-Mar-2025].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Neel Jain, et al.

</span>
<span class="ltx_bibblock">Baseline defenses for adversarial attacks against aligned language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib45.1.1">arXiv preprint arXiv:2309.00614</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Fengqing Jiang, et al.

</span>
<span class="ltx_bibblock">Artprompt: Ascii art-based jailbreak attacks against aligned llms.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib46.1.1">arXiv preprint arXiv:2402.11753</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Haibo Jin, et al.

</span>
<span class="ltx_bibblock">Guard: Role-playing to generate natural-language jailbreakings to test guideline adherence of large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib47.1.1">arXiv preprint arXiv:2402.03299</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Tom Kocmi et al.

</span>
<span class="ltx_bibblock">Large language models are state-of-the-art evaluators of translation quality.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib48.1.1">arXiv preprint arXiv:2302.14520</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Tom Krantz et al.

</span>
<span class="ltx_bibblock">AI jailbreak: Rooting out an evolving threat.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.ibm.com/think/insights/ai-jailbreak" title="">https://www.ibm.com/think/insights/ai-jailbreak</a>, 2024.

</span>
<span class="ltx_bibblock">[Online; last accessed 09-Mar-2025].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Zehan Li, et al.

</span>
<span class="ltx_bibblock">Towards general text embeddings with multi-stage contrastive learning, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Ziyang Li, et al.

</span>
<span class="ltx_bibblock">Llm-assisted static analysis for detecting security vulnerabilities.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib51.1.1">arXiv preprint arXiv:2405.17238</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Zeyi Liao et al.

</span>
<span class="ltx_bibblock">Amplegcg: Learning a universal and transferable generative model of adversarial suffixes for jailbreaking both open and closed llms.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib52.1.1">arXiv preprint arXiv:2404.07921</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Xiaogeng Liu, et al.

</span>
<span class="ltx_bibblock">Autodan-turbo: A lifelong agent for strategy self-exploration to jailbreak llms.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib53.1.1">arXiv preprint arXiv:2410.05295</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Xiaogeng Liu, et al.

</span>
<span class="ltx_bibblock">Autodan: Generating stealthy jailbreak prompts on aligned large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib54.1.1">arXiv preprint arXiv:2310.04451</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Yi Liu, et al.

</span>
<span class="ltx_bibblock">Prompt injection attack against llm-integrated applications.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib55.1.1">arXiv preprint arXiv:2306.05499</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Yi Liu, et al.

</span>
<span class="ltx_bibblock">Jailbreaking chatgpt via prompt engineering: An empirical study.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib56.1.1">arXiv preprint arXiv:2305.13860</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Meta.

</span>
<span class="ltx_bibblock">Meta and Microsoft Introduce the Next Generation of Llama.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://about.fb.com/news/2023/07/llama-2/" title="">https://about.fb.com/news/2023/07/llama-2/</a>, 2023.

</span>
<span class="ltx_bibblock">[Online; last accessed 09-Mar-2025].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
Meta.

</span>
<span class="ltx_bibblock">Expanding our open source large language models responsibly.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ai.meta.com/blog/meta-llama-3-1-ai-responsibility/" title="">https://ai.meta.com/blog/meta-llama-3-1-ai-responsibility/</a>, 2024.

</span>
<span class="ltx_bibblock">[Online; last accessed 09-Mar-2025].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
Microsoft.

</span>
<span class="ltx_bibblock">Prompt Shields.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://learn.microsoft.com/en-us/azure/ai-services/content-safety/concepts/jailbreak-detection" title="">https://learn.microsoft.com/en-us/azure/ai-services/content-safety/concepts/jailbreak-detection</a>, 2024.

</span>
<span class="ltx_bibblock">[Online; last accessed 09-Mar-2025].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
Stephen Moskal, et al.

</span>
<span class="ltx_bibblock">Llms killed the script kiddie: How agents supported by large language models change the landscape of network threat testing.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib60.1.1">arXiv preprint arXiv:2310.06936</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
Niklas Muennighoff, et al.

</span>
<span class="ltx_bibblock">Mteb: Massive text embedding benchmark.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib61.1.1">arXiv preprint arXiv:2210.07316</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
Humza Naveed, et al.

</span>
<span class="ltx_bibblock">A comprehensive overview of large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib62.1.1">arXiv preprint arXiv:2307.06435</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
The Hacker News.

</span>
<span class="ltx_bibblock">From Misuse to Abuse: AI Risks and Attacks.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://thehackernews.com/2024/10/from-misuse-to-abuse-ai-risks-and.html" title="">https://thehackernews.com/2024/10/from-misuse-to-abuse-ai-risks-and.html</a>, 2024.

</span>
<span class="ltx_bibblock">[Online; last accessed 09-Mar-2025].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
Zach Nussbaum, et al.

</span>
<span class="ltx_bibblock">Nomic embed: Training a reproducible long context text embedder, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Models.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://platform.openai.com/docs/models" title="">https://platform.openai.com/docs/models</a>.

</span>
<span class="ltx_bibblock">[Online; last accessed 09-Mar-2025].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Models - GPT-3.5 Turbo.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://platform.openai.com/docs/models/gpt-3.5-turbo" title="">https://platform.openai.com/docs/models/gpt-3.5-turbo</a>.

</span>
<span class="ltx_bibblock">[Online; last accessed 09-Mar-2025].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Models - GPT-4o.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://platform.openai.com/docs/models/gpt-4o" title="">https://platform.openai.com/docs/models/gpt-4o</a>.

</span>
<span class="ltx_bibblock">[Online; last accessed 09-Mar-2025].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Models - GPT-4o mini.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://platform.openai.com/docs/models/gpt-4o-mini" title="">https://platform.openai.com/docs/models/gpt-4o-mini</a>.

</span>
<span class="ltx_bibblock">[Online; last accessed 09-Mar-2025].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Rate Limits.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://platform.openai.com/docs/guides/rate-limits" title="">https://platform.openai.com/docs/guides/rate-limits</a>.

</span>
<span class="ltx_bibblock">[Online; last accessed 09-Mar-2025].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Introducing ChatGPT.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/blog/chatgpt" title="">https://openai.com/blog/chatgpt</a>, 2022.

</span>
<span class="ltx_bibblock">[Online; last accessed 09-Mar-2025].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Redefine work in the age of AI.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/chatgpt/enterprise/" title="">https://openai.com/chatgpt/enterprise/</a>, 2022.

</span>
<span class="ltx_bibblock">[Online; last accessed 09-Mar-2025].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
Keivalya Pandya et al.

</span>
<span class="ltx_bibblock">Automating customer service using langchain: Building custom open-source gpt chatbot for organizations.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib72.1.1">arXiv preprint arXiv:2310.05421</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
European Parliament.

</span>
<span class="ltx_bibblock">Artificial Intelligence Act: deal on comprehensive rules for trustworthy AI.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.europarl.europa.eu/news/en/press-room/20231206IPR15699/artificial-intelligence-act-deal-on-comprehensive-rules-for-trustworthy-ai" title="">https://www.europarl.europa.eu/news/en/press-room/20231206IPR15699/artificial-intelligence-act-deal-on-comprehensive-rules-for-trustworthy-ai</a>, 2024.

</span>
<span class="ltx_bibblock">[Online; last accessed 09-Mar-2025].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
Anselm Paulus, et al.

</span>
<span class="ltx_bibblock">Advprompter: Fast adaptive adversarial prompting for llms.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib74.1.1">arXiv preprint arXiv:2404.16873</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
Perception Point.

</span>
<span class="ltx_bibblock">AI Malware: Types, Real Life Examples, and Defensive Measures.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://perception-point.io/guides/ai-security/ai-malware-types-real-life-examples-defensive-measures" title="">https://perception-point.io/guides/ai-security/ai-malware-types-real-life-examples-defensive-measures</a>.

</span>
<span class="ltx_bibblock">[Online; last accessed 09-Mar-2025].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
Marek Posard, et al.

</span>
<span class="ltx_bibblock">The 2024 U.S. Election, Trust, and Technology.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.rand.org/pubs/perspectives/PEA3073-1.html" title="">https://www.rand.org/pubs/perspectives/PEA3073-1.html</a>, 2024.

</span>
<span class="ltx_bibblock">[Online; last accessed 09-Mar-2025].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
David Ramel.

</span>
<span class="ltx_bibblock">Using AI for ’Offensive Security’ by Simulating Attacks.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://virtualizationreview.com/articles/2024/08/08/using-ai-for-offensive-security-by-simulating-attacks.aspx" title="">https://virtualizationreview.com/articles/2024/08/08/using-ai-for-offensive-security-by-simulating-attacks.aspx</a>, 2024.

</span>
<span class="ltx_bibblock">[Online; last accessed 09-Mar-2025].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
Javier Rando et al.

</span>
<span class="ltx_bibblock">Universal jailbreak backdoors from poisoned human feedback.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib78.1.1">arXiv preprint arXiv:2311.14455</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
Mikayel Samvelyan, et al.

</span>
<span class="ltx_bibblock">Rainbow teaming: Open-ended generation of diverse adversarial prompts.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib79.1.1">arXiv preprint arXiv:2402.16822</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
Dor Sarig.

</span>
<span class="ltx_bibblock">A Deep Dive into LLM Jailbreaking Techniques and Their Implications.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.pillar.security/blog/a-deep-dive-into-llm-jailbreaking-techniques-and-their-implications" title="">https://www.pillar.security/blog/a-deep-dive-into-llm-jailbreaking-techniques-and-their-implications</a>, 2024.

</span>
<span class="ltx_bibblock">[Online; last accessed 09-Mar-2025].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
Xinyue Shen, et al.

</span>
<span class="ltx_bibblock">" do anything now": Characterizing and evaluating in-the-wild jailbreak prompts on large language models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib81.1.1">Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security</span>, pages 1671–1685, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
Olivia Shone.

</span>
<span class="ltx_bibblock">5 key features and benefits of large language models.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.microsoft.com/en-us/microsoft-cloud/blog/2024/10/09/5-key-features-and-benefits-of-large-language-models/#:~:text=Large%20language%20models%20(LLMs)%20offer,provide%20intelligent%20responses%20to%20queries." title="">https://www.microsoft.com/en-us/microsoft-cloud/blog/2024/10/09/5-key-features-and-benefits-of-large-language-models/#:~:text=Large%20language%20models%20(LLMs)%20offer,provide%20intelligent%20responses%20to%20queries.</a>, 2024.

</span>
<span class="ltx_bibblock">[Online; last accessed 09-Mar-2025].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
Jinyan Su, et al.

</span>
<span class="ltx_bibblock">Adapting fake news detection to the era of large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib83.1.1">arXiv preprint arXiv:2311.04917</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
Yanshen Sun, et al.

</span>
<span class="ltx_bibblock">Exploring the Deceptive Power of LLM-Generated Fake News: A Study of Real-World Detection Challenges.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib84.1.1">arXiv preprint arXiv:2403.18249</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
Richard S Sutton et al.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib85.1.1">Reinforcement learning: An introduction</span>.

</span>
<span class="ltx_bibblock">MIT press, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">
Virginia Tech.

</span>
<span class="ltx_bibblock">AI and the spread of fake news sites: Experts explain how to counteract them.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://news.vt.edu/articles/2024/02/AI-generated-fake-news-experts.html" title="">https://news.vt.edu/articles/2024/02/AI-generated-fake-news-experts.html</a>, 2024.

</span>
<span class="ltx_bibblock">[Online; last accessed 09-Mar-2025].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">
A Vaswani.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib87.1.1">Advances in Neural Information Processing Systems</span>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">
Boxin Wang, et al.

</span>
<span class="ltx_bibblock">Decodingtrust: A comprehensive assessment of trustworthiness in gpt models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib88.1.1">NeurIPS</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock">
Daimeng Wang, et al.

</span>
<span class="ltx_bibblock"><math alttext="\{" class="ltx_Math" display="inline" id="bib.bib89.1.m1.1"><semantics id="bib.bib89.1.m1.1a"><mo id="bib.bib89.1.m1.1.1" stretchy="false" xref="bib.bib89.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib89.1.m1.1b"><ci id="bib.bib89.1.m1.1.1.cmml" xref="bib.bib89.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib89.1.m1.1c">\{</annotation><annotation encoding="application/x-llamapun" id="bib.bib89.1.m1.1d">{</annotation></semantics></math>SyzVegas<math alttext="\}" class="ltx_Math" display="inline" id="bib.bib89.2.m2.1"><semantics id="bib.bib89.2.m2.1a"><mo id="bib.bib89.2.m2.1.1" stretchy="false" xref="bib.bib89.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib89.2.m2.1b"><ci id="bib.bib89.2.m2.1.1.cmml" xref="bib.bib89.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib89.2.m2.1c">\}</annotation><annotation encoding="application/x-llamapun" id="bib.bib89.2.m2.1d">}</annotation></semantics></math>: Beating kernel fuzzing odds with reinforcement learning.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib89.3.1">30th USENIX Security Symposium (USENIX Security 21)</span>, pages 2741–2758, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock">
Liang Wang, et al.

</span>
<span class="ltx_bibblock">Text embeddings by weakly-supervised contrastive pre-training.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib90.1.1">arXiv preprint arXiv:2212.03533</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock">
Alexander Wei, et al.

</span>
<span class="ltx_bibblock">Jailbroken: How does llm safety training fail?

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib91.1.1">Advances in Neural Information Processing Systems</span>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock">
Angus R Williams.

</span>
<span class="ltx_bibblock">LLMs are ever more convincing, with important consequences for election disinformation.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.turing.ac.uk/blog/llms-are-ever-more-convincing-important-consequences-election-disinformation" title="">https://www.turing.ac.uk/blog/llms-are-ever-more-convincing-important-consequences-election-disinformation</a>, 2024.

</span>
<span class="ltx_bibblock">[Online; last accessed 09-Mar-2025].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock">
Shitao Xiao, et al.

</span>
<span class="ltx_bibblock">C-pack: Packaged resources to advance general chinese embedding, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock">
HanXiang Xu, et al.

</span>
<span class="ltx_bibblock">Large language models for cyber security: A systematic literature review.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib94.1.1">arXiv preprint arXiv:2405.04760</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_tag_bibitem">[95]</span>
<span class="ltx_bibblock">
Jiashu Xu, et al.

</span>
<span class="ltx_bibblock">Instructions as backdoors: Backdoor vulnerabilities of instruction tuning for large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib95.1.1">arXiv preprint arXiv:2305.14710</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_tag_bibitem">[96]</span>
<span class="ltx_bibblock">
Biwei Yan, et al.

</span>
<span class="ltx_bibblock">On protecting the data privacy of large language models (llms): A survey.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib96.1.1">arXiv preprint arXiv:2403.05156</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_tag_bibitem">[97]</span>
<span class="ltx_bibblock">
Zheng-Xin Yong, et al.

</span>
<span class="ltx_bibblock">Low-resource languages jailbreak gpt-4.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib97.1.1">arXiv preprint arXiv:2310.02446</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_tag_bibitem">[98]</span>
<span class="ltx_bibblock">
Jiahao Yu.

</span>
<span class="ltx_bibblock">GPTFuzz.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/sherdencooper/GPTFuzz" title="">https://github.com/sherdencooper/GPTFuzz</a>, 2023.

</span>
<span class="ltx_bibblock">[Online; last accessed 09-Mar-2025].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_tag_bibitem">[99]</span>
<span class="ltx_bibblock">
Jiahao Yu, et al.

</span>
<span class="ltx_bibblock"><math alttext="\{" class="ltx_Math" display="inline" id="bib.bib99.1.m1.1"><semantics id="bib.bib99.1.m1.1a"><mo id="bib.bib99.1.m1.1.1" stretchy="false" xref="bib.bib99.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib99.1.m1.1b"><ci id="bib.bib99.1.m1.1.1.cmml" xref="bib.bib99.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib99.1.m1.1c">\{</annotation><annotation encoding="application/x-llamapun" id="bib.bib99.1.m1.1d">{</annotation></semantics></math>LLM-Fuzzer<math alttext="\}" class="ltx_Math" display="inline" id="bib.bib99.2.m2.1"><semantics id="bib.bib99.2.m2.1a"><mo id="bib.bib99.2.m2.1.1" stretchy="false" xref="bib.bib99.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib99.2.m2.1b"><ci id="bib.bib99.2.m2.1.1.cmml" xref="bib.bib99.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib99.2.m2.1c">\}</annotation><annotation encoding="application/x-llamapun" id="bib.bib99.2.m2.1d">}</annotation></semantics></math>: Scaling assessment of large language model jailbreaks.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib99.3.1">33rd USENIX Security Symposium (USENIX Security 24)</span>, pages 4657–4674, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_tag_bibitem">[100]</span>
<span class="ltx_bibblock">
Youliang Yuan, et al.

</span>
<span class="ltx_bibblock">Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib100.1.1">arXiv preprint arXiv:2308.06463</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_tag_bibitem">[101]</span>
<span class="ltx_bibblock">
Michal Zalewski.

</span>
<span class="ltx_bibblock">american fuzzy lop.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://lcamtuf.coredump.cx/afl/" title="">https://lcamtuf.coredump.cx/afl/</a>.

</span>
<span class="ltx_bibblock">[Online; last accessed 09-Mar-2025].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_tag_bibitem">[102]</span>
<span class="ltx_bibblock">
Yi Zeng, et al.

</span>
<span class="ltx_bibblock">How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib102.1.1">arXiv preprint arXiv:2401.06373</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_tag_bibitem">[103]</span>
<span class="ltx_bibblock">
Xin Zhang, et al.

</span>
<span class="ltx_bibblock">mgte: Generalized long-context text representation and reranking models for multilingual text retrieval, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_tag_bibitem">[104]</span>
<span class="ltx_bibblock">
Bodong Zhao, et al.

</span>
<span class="ltx_bibblock"><math alttext="\{" class="ltx_Math" display="inline" id="bib.bib104.1.m1.1"><semantics id="bib.bib104.1.m1.1a"><mo id="bib.bib104.1.m1.1.1" stretchy="false" xref="bib.bib104.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib104.1.m1.1b"><ci id="bib.bib104.1.m1.1.1.cmml" xref="bib.bib104.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib104.1.m1.1c">\{</annotation><annotation encoding="application/x-llamapun" id="bib.bib104.1.m1.1d">{</annotation></semantics></math>StateFuzz<math alttext="\}" class="ltx_Math" display="inline" id="bib.bib104.2.m2.1"><semantics id="bib.bib104.2.m2.1a"><mo id="bib.bib104.2.m2.1.1" stretchy="false" xref="bib.bib104.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib104.2.m2.1b"><ci id="bib.bib104.2.m2.1.1.cmml" xref="bib.bib104.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib104.2.m2.1c">\}</annotation><annotation encoding="application/x-llamapun" id="bib.bib104.2.m2.1d">}</annotation></semantics></math>: System <math alttext="\{" class="ltx_Math" display="inline" id="bib.bib104.3.m3.1"><semantics id="bib.bib104.3.m3.1a"><mo id="bib.bib104.3.m3.1.1" stretchy="false" xref="bib.bib104.3.m3.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib104.3.m3.1b"><ci id="bib.bib104.3.m3.1.1.cmml" xref="bib.bib104.3.m3.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib104.3.m3.1c">\{</annotation><annotation encoding="application/x-llamapun" id="bib.bib104.3.m3.1d">{</annotation></semantics></math>Call-Based<math alttext="\}" class="ltx_Math" display="inline" id="bib.bib104.4.m4.1"><semantics id="bib.bib104.4.m4.1a"><mo id="bib.bib104.4.m4.1.1" stretchy="false" xref="bib.bib104.4.m4.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib104.4.m4.1b"><ci id="bib.bib104.4.m4.1.1.cmml" xref="bib.bib104.4.m4.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib104.4.m4.1c">\}</annotation><annotation encoding="application/x-llamapun" id="bib.bib104.4.m4.1d">}</annotation></semantics></math><math alttext="\{" class="ltx_Math" display="inline" id="bib.bib104.5.m5.1"><semantics id="bib.bib104.5.m5.1a"><mo id="bib.bib104.5.m5.1.1" stretchy="false" xref="bib.bib104.5.m5.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib104.5.m5.1b"><ci id="bib.bib104.5.m5.1.1.cmml" xref="bib.bib104.5.m5.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib104.5.m5.1c">\{</annotation><annotation encoding="application/x-llamapun" id="bib.bib104.5.m5.1d">{</annotation></semantics></math>State-Aware<math alttext="\}" class="ltx_Math" display="inline" id="bib.bib104.6.m6.1"><semantics id="bib.bib104.6.m6.1a"><mo id="bib.bib104.6.m6.1.1" stretchy="false" xref="bib.bib104.6.m6.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib104.6.m6.1b"><ci id="bib.bib104.6.m6.1.1.cmml" xref="bib.bib104.6.m6.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib104.6.m6.1c">\}</annotation><annotation encoding="application/x-llamapun" id="bib.bib104.6.m6.1d">}</annotation></semantics></math> Linux Driver Fuzzing.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib104.7.1">31st USENIX Security Symposium (USENIX Security 22)</span>, pages 3273–3289, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib105">
<span class="ltx_tag ltx_tag_bibitem">[105]</span>
<span class="ltx_bibblock">
Sicheng Zhu, et al.

</span>
<span class="ltx_bibblock">Autodan: interpretable gradient-based adversarial attacks on large language models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib105.1.1">First Conference on Language Modeling</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib106">
<span class="ltx_tag ltx_tag_bibitem">[106]</span>
<span class="ltx_bibblock">
Andy Zou, et al.

</span>
<span class="ltx_bibblock">Universal and transferable adversarial attacks on aligned language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib106.1.1">arXiv preprint arXiv:2307.15043</span>, 2023.

</span>
</li>
</ul>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Appendix</h2>
<section class="ltx_subsection" id="S8.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.1 </span>Target LLMs Descriptions</h3>
<div class="ltx_para" id="S8.SS1.p1">
<p class="ltx_p" id="S8.SS1.p1.1">Here, we provide the details about the capabilities of our chosen target LLMs.</p>
<ul class="ltx_itemize" id="S8.I1">
<li class="ltx_item" id="S8.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S8.I1.i1.p1">
<p class="ltx_p" id="S8.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S8.I1.i1.p1.1.1">GPT-3.5</span> from OpenAI that can understand and generate natural language and has been optimized for chat and instruction-following tasks. More specifically, we use the <span class="ltx_text ltx_font_typewriter" id="S8.I1.i1.p1.1.2">gpt-3.5-turbo-0125</span> model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib66" title="">66</a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S8.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S8.I1.i2.p1">
<p class="ltx_p" id="S8.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S8.I1.i2.p1.1.1">GPT-4o</span> is a versatile, high-intelligence, flagship model from OpenAI. In our experiments, we use the <span class="ltx_text ltx_font_typewriter" id="S8.I1.i2.p1.1.2">gpt-4o-2024-08-06</span> model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib67" title="">67</a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S8.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S8.I1.i3.p1">
<p class="ltx_p" id="S8.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S8.I1.i3.p1.1.1">GPT-4o-mini</span> is the most cost-efficient small model from OpenAI that enables a broad range of tasks with its low cost and latency. We use the <span class="ltx_text ltx_font_typewriter" id="S8.I1.i3.p1.1.2">gpt-4o-mini-2024-07-18</span> in our experiments <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib68" title="">68</a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S8.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S8.I1.i4.p1">
<p class="ltx_p" id="S8.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S8.I1.i4.p1.1.1">DeepSeek-V3</span> is a recently released popular open-source LLM that outperforms other open-source models and achieves performance comparable to leading closed-source models. We use the <span class="ltx_text ltx_font_typewriter" id="S8.I1.i4.p1.1.2">deepseek-chat</span> model, which points to DeepSeek-V3 in the API <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib19" title="">19</a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S8.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S8.I1.i5.p1">
<p class="ltx_p" id="S8.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="S8.I1.i5.p1.1.1">DeepSeek-R1</span> is a recently released widely popular open-source reasoning LLM that outperforms other open-source reasoning models and achieves performance comparable to leading closed-source reasoning models. We use the <span class="ltx_text ltx_font_typewriter" id="S8.I1.i5.p1.1.2">deepseek-reasoner</span> model, which points to DeepSeek-R1 in the API <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib19" title="">19</a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S8.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S8.I1.i6.p1">
<p class="ltx_p" id="S8.I1.i6.p1.1"><span class="ltx_text ltx_font_bold" id="S8.I1.i6.p1.1.1">Llama2</span> is a set of open-source LLMs developed by Meta. At the time of release, Llama 2 outperformed the other open-source models across all benchmarks. More specifically, we use the <span class="ltx_text ltx_font_typewriter" id="S8.I1.i6.p1.1.2">Llama-2-7b-chat-hf</span> model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib41" title="">41</a>]</cite> in our experiments.</p>
</div>
</li>
<li class="ltx_item" id="S8.I1.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S8.I1.i7.p1">
<p class="ltx_p" id="S8.I1.i7.p1.1"><span class="ltx_text ltx_font_bold" id="S8.I1.i7.p1.1.1">Llama3</span> is the latest iteration of Meta’s open-source LLMs, pushing the boundaries of open-access AI models. It significantly improves upon Llama2, setting new benchmarks for performance and efficiency across diverse tasks. Specifically, we use the <span class="ltx_text ltx_font_typewriter" id="S8.I1.i7.p1.1.2">Meta-Llama-3.1-8B-Instruct-Turbo</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib42" title="">42</a>]</cite> model in our experiments.</p>
</div>
</li>
<li class="ltx_item" id="S8.I1.i8" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S8.I1.i8.p1">
<p class="ltx_p" id="S8.I1.i8.p1.1"><span class="ltx_text ltx_font_bold" id="S8.I1.i8.p1.1.1">Gemini-2.0</span>, recently released by Google DeepMind, is their most capable model yet. It integrates advanced agentic capabilities and natural language understanding across text, image, video, and audio. We use the <span class="ltx_text ltx_font_typewriter" id="S8.I1.i8.p1.1.2">gemini-2.0-flash-exp</span> model in our experiments <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib31" title="">31</a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S8.I1.i9" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S8.I1.i9.p1">
<p class="ltx_p" id="S8.I1.i9.p1.1"><span class="ltx_text ltx_font_bold" id="S8.I1.i9.p1.1.1">Gemini-1.5</span>, the predecessor to Gemini-2.0, is highly capable of multimodal reasoning across vast amounts of information and delivers impressive quality for its size.
We use the <span class="ltx_text ltx_font_typewriter" id="S8.I1.i9.p1.1.2">gemini-1.5-flash</span> model in our experiments <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.08990v1#bib.bib30" title="">30</a>]</cite>.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Mar 10 04:39:43 2025 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
