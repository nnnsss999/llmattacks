---
title: "AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned LLMs"
category: "Optimization"
source_url: "https://arxiv.org/abs/2310.04451"
date_collected: 2025-06-18
license: "Fair Use"
---
This AutoDAN variant focuses on creating stealthy jailbreak prompts that maintain benign appearances. Leveraging gradient feedback, the method iteratively crafts adversarial examples that slip past alignment defenses. The authors demonstrate strong attack success on multiple instruction-tuned models, highlighting the potency of gradient-guided optimization for jailbreaks.
