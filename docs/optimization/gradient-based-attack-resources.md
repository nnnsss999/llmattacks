---
title: "Gradient-Based LLM Attack Resources"
category: "Optimization"
source_url: ""
date_collected: 2025-06-19
license: "CC-BY-4.0"
---

This list collects notable research papers, blog posts and repositories that develop gradient-driven techniques for jailbreaking or extracting information from large language models.

- [Attacking Large Language Models with Projected Gradient Descent](https://arxiv.org/abs/2402.09154)
- [TAG: Gradient Attack on Transformer-based Language Models](https://aclanthology.org/2021.findings-emnlp.305/)
- [Gradient-based Adversarial Attacks against Text Transformers](https://blogs.night-wolf.io/gradient-based-adversarial-attacks-against-text-transformers)
- [AutoDAN: Interpretable Gradient-Based Adversarial Attacks on Large Language Models](https://arxiv.org/abs/2310.15140)
- [AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned LLMs](https://arxiv.org/abs/2310.04451)
- [PIG: Privacy Jailbreak Attack on LLMs via Gradient-based Iterative In-Context Optimization](https://arxiv.org/abs/2505.09921)
- [AmpleGCG: Learning a Universal and Transferable Generative Model of Adversarial Suffixes for Jailbreaking Both Open and Closed LLMs](https://arxiv.org/abs/2404.07921)
- [AmpleGCG GitHub Repository](https://github.com/OSU-NLP-Group/AmpleGCG)
- [BrokenHill: Attacking LLM Safeguards via Gradient Exploits](https://bishopfox.com/blog/brokenhill-attack-tool-largelanguagemodels-llm)
- [nanoGCG Project](https://github.com/GraySwanAI/nanoGCG/tree/main)
- [I-GCG Implementation](https://github.com/jiaxiaojunQAQ/I-GCG)
- [GCG Strategies for Prompt Red-Teaming](https://www.promptfoo.dev/docs/red-team/strategies/gcg/)
- [Gradient Masking All-at-Once: Ensemble Everything Everywhere Is Not Robust](https://arxiv.org/abs/2411.14834)
- [Uncovering Gradient Inversion Risks in Practical Language Model Training](https://dl.acm.org/doi/abs/10.1145/3658644.3690292)
- [Deep Leakage from Gradients](https://arxiv.org/abs/1906.08935)
- [Inverting Gradients -- How easy is it to break privacy in federated learning?](https://arxiv.org/abs/2003.14053)
- [AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts](https://arxiv.org/abs/2010.15980)
- [Gradient Hacking in Reinforcement Learning](https://arxiv.org/abs/2106.09685)
- [Gradient Leakage and Model Inversion via LoRA](https://arxiv.org/abs/2406.01234)
