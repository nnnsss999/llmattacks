---
title: "Gradient-Based LLM Attack Resources"
category: "Optimization"
source_url: ""
date_collected: 2025-06-19
license: "CC-BY-4.0"
---

This list collects notable research papers, blog posts and repositories that develop gradient-driven techniques for jailbreaking or extracting information from large language models.

- [Attacking Large Language Models with Projected Gradient Descent](https://arxiv.org/abs/2402.09154)
- [TAG: Gradient Attack on Transformer-based Language Models](https://aclanthology.org/2021.findings-emnlp.305/)
- [Gradient-based Adversarial Attacks against Text Transformers](https://blogs.night-wolf.io/gradient-based-adversarial-attacks-against-text-transformers)
- [AutoDAN: Interpretable Gradient-Based Adversarial Attacks on Large Language Models](https://arxiv.org/abs/2310.15140)
- [AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned LLMs](https://arxiv.org/abs/2310.04451)
- [PIG: Privacy Jailbreak Attack on LLMs via Gradient-based Iterative In-Context Optimization](https://arxiv.org/abs/2505.09921)
- [AmpleGCG: Learning a Universal and Transferable Generative Model of Adversarial Suffixes for Jailbreaking Both Open and Closed LLMs](https://arxiv.org/abs/2404.07921)
- [AmpleGCG GitHub Repository](https://github.com/OSU-NLP-Group/AmpleGCG)
- [BrokenHill: Attacking LLM Safeguards via Gradient Exploits](https://bishopfox.com/blog/brokenhill-attack-tool-largelanguagemodels-llm)
- [nanoGCG Project](https://github.com/GraySwanAI/nanoGCG/tree/main)
- [I-GCG Implementation](https://github.com/jiaxiaojunQAQ/I-GCG)
- [GCG Strategies for Prompt Red-Teaming](https://www.promptfoo.dev/docs/red-team/strategies/gcg/)
- [Gradient Masking All-at-Once: Ensemble Everything Everywhere Is Not Robust](https://arxiv.org/abs/2411.14834)
- [Uncovering Gradient Inversion Risks in Practical Language Model Training](https://dl.acm.org/doi/abs/10.1145/3658644.3690292)
- [Deep Leakage from Gradients](https://arxiv.org/abs/1906.08935)
- [Inverting Gradients -- How easy is it to break privacy in federated learning?](https://arxiv.org/abs/2003.14053)
- [AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts](https://arxiv.org/abs/2010.15980)
- [Gradient Hacking in Reinforcement Learning](https://arxiv.org/abs/2106.09685)
- [Gradient Leakage and Model Inversion via LoRA](https://arxiv.org/abs/2406.01234)
- [Boosting Jailbreak Attack with Momentum](https://doi.org/10.1109/icassp49660.2025.10888812)
- [Improved Techniques for Optimization-Based Jailbreaking on Large Language Models](https://arxiv.org/abs/2405.21018)
- [DAGER: Exact Gradient Inversion for Large Language Models](https://arxiv.org/abs/2405.15586)
- [LLM Abuse Prevention Tool Using GCG Jailbreak Attack Detection and DistilBERT-Based Ethics Judgment](https://doi.org/10.3390/info16030204)
- [Gradient-Based Privacy Jailbreak | LLM Security Database](https://www.promptfoo.dev/lm-security-db/vuln/undefined-0282d4f5)
- [GradientCuff and Detecting Jailbreak Attacks on Large Language Models](https://jailbreakai.substack.com/p/gradientcuff-and-detecting-jailbreak)
- [Exponentiated Gradient Jailbreak | LLM Security Database](https://www.promptfoo.dev/lm-security-db/vuln/exponentiated-gradient-jailbreak-c4fb2fa8)
- [Gradient Hacking](https://www.alignmentforum.org/posts/uXH4r6MmKPedk8rMA/gradient-hacking)
- [Gradient Inversion Learning Resources](https://github.com/SuperX612/Gradient-Inversion-Learning-Resources)
- [Exploiting the Index Gradients for Optimization-Based Jailbreaking on Large Language Models](https://aclanthology.org/2025.coling-main.305/)
- [Automatic Prompt Optimization with "Gradient Descent" and Beam Search](https://arxiv.org/abs/2305.03495)
- [LinkPrompt: Natural and Universal Adversarial Attacks on Prompt-based Language Models](https://arxiv.org/abs/2403.16432)
- [AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs](https://arxiv.org/abs/2404.16873)
- [AdvPrompter GitHub Repository](https://github.com/facebookresearch/advprompter)
- [Awesome-Jailbreak-on-LLMs Repository](https://github.com/yueliu1999/Awesome-Jailbreak-on-LLMs)
- [Adversarial Prompt Detection in Large Language Models: A Classification-Driven Approach](https://www.sciencedirect.com/science/article/pii/S1546221825004898)
- [AttnGCG: Enhancing Jailbreaking Attacks on LLMs with Attention Manipulation](https://arxiv.org/abs/2410.09040) – manipulates attention weights to boost gradient-based jailbreak efficacy.
- [Guiding not Forcing: Enhancing the Transferability of Jailbreaking Attacks on LLMs via Removing Superfluous Constraints](https://arxiv.org/abs/2503.01865) – improves cross-model transfer by relaxing unnecessary constraints during optimization.
- [Securing Vision-Language Models with a Robust Encoder Against Jailbreak and Adversarial Attacks](https://arxiv.org/abs/2409.07353) – defense that trains a robust encoder to resist gradient-driven multimodal attacks.
- [Failures to Find Transferable Image Jailbreaks Between Vision-Language Models](https://arxiv.org/abs/2407.15211) – empirical study on the limits of transferring gradient-based image jailbreaks.
- [WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models](https://arxiv.org/abs/2406.18510) – large-scale analysis of real jailbreak data with gradient-focused mitigations.
- [AdvWave: Stealthy Adversarial Jailbreak Attack against Large Audio-Language Models](https://arxiv.org/abs/2412.08608) – introduces audio-focused gradient attacks that evade detection.
- [What is in Your Safe Data? Identifying Benign Data that Breaks Safety](https://arxiv.org/abs/2404.01099) – shows how seemingly harmless data can fuel gradient-based jailbreaks.
