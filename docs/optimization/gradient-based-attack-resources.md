---
title: "Gradient-Based LLM Attack Resources"
category: "Optimization"
source_url: ""
date_collected: 2025-06-19
license: "CC-BY-4.0"
---

This list collects notable research papers, blog posts and repositories that develop gradient-driven techniques for jailbreaking or extracting information from large language models.

- [Attacking Large Language Models with Projected Gradient Descent](https://arxiv.org/abs/2402.09154)
- [TAG: Gradient Attack on Transformer-based Language Models](https://aclanthology.org/2021.findings-emnlp.305/)
- [Gradient-based Adversarial Attacks against Text Transformers](https://blogs.night-wolf.io/gradient-based-adversarial-attacks-against-text-transformers)
- [AutoDAN: Interpretable Gradient-Based Adversarial Attacks on Large Language Models](https://arxiv.org/abs/2310.15140)
- [AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned LLMs](https://arxiv.org/abs/2310.04451)
- [PIG: Privacy Jailbreak Attack on LLMs via Gradient-based Iterative In-Context Optimization](https://arxiv.org/abs/2505.09921)
- [AmpleGCG: Learning a Universal and Transferable Generative Model of Adversarial Suffixes for Jailbreaking Both Open and Closed LLMs](https://arxiv.org/abs/2404.07921)
- [AmpleGCG GitHub Repository](https://github.com/OSU-NLP-Group/AmpleGCG)
- [BrokenHill: Attacking LLM Safeguards via Gradient Exploits](https://bishopfox.com/blog/brokenhill-attack-tool-largelanguagemodels-llm)
- [nanoGCG Project](https://github.com/GraySwanAI/nanoGCG/tree/main)
- [I-GCG Implementation](https://github.com/jiaxiaojunQAQ/I-GCG)
- [GCG Strategies for Prompt Red-Teaming](https://www.promptfoo.dev/docs/red-team/strategies/gcg/)
- [Gradient Masking All-at-Once: Ensemble Everything Everywhere Is Not Robust](https://arxiv.org/abs/2411.14834)
- [Uncovering Gradient Inversion Risks in Practical Language Model Training](https://dl.acm.org/doi/abs/10.1145/3658644.3690292)
- [Deep Leakage from Gradients](https://arxiv.org/abs/1906.08935)
- [Inverting Gradients -- How easy is it to break privacy in federated learning?](https://arxiv.org/abs/2003.14053)
- [AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts](https://arxiv.org/abs/2010.15980)
- [Gradient Hacking in Reinforcement Learning](https://arxiv.org/abs/2106.09685)
- [Gradient Leakage and Model Inversion via LoRA](https://arxiv.org/abs/2406.01234)
- [Boosting Jailbreak Attack with Momentum](https://doi.org/10.1109/icassp49660.2025.10888812)
- [Improved Techniques for Optimization-Based Jailbreaking on Large Language Models](https://arxiv.org/abs/2405.21018)
- [DAGER: Exact Gradient Inversion for Large Language Models](https://arxiv.org/abs/2405.15586)
- [LLM Abuse Prevention Tool Using GCG Jailbreak Attack Detection and DistilBERT-Based Ethics Judgment](https://doi.org/10.3390/info16030204)
- [Gradient-Based Privacy Jailbreak | LLM Security Database](https://www.promptfoo.dev/lm-security-db/vuln/undefined-0282d4f5)
- [Exploiting the Index Gradients for Optimization-Based Jailbreaking on Large Language Models](https://aclanthology.org/2025.coling-main.305/)
- [Automatic Prompt Optimization with "Gradient Descent" and Beam Search](https://arxiv.org/abs/2305.03495)
- [LinkPrompt: Natural and Universal Adversarial Attacks on Prompt-based Language Models](https://arxiv.org/abs/2403.16432)
- [AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs](https://arxiv.org/abs/2404.16873)
- [AdvPrompter GitHub Repository](https://github.com/facebookresearch/advprompter)
- [Awesome-Jailbreak-on-LLMs Repository](https://github.com/yueliu1999/Awesome-Jailbreak-on-LLMs)
- [Adversarial Prompt Detection in Large Language Models: A Classification-Driven Approach](https://www.sciencedirect.com/science/article/pii/S1546221825004898)
