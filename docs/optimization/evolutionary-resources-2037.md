---
title: "Evolutionary Algorithm Resources 2037"
category: "Optimization"
source_url: ""
date_collected: 2025-06-28
license: "CC-BY-4.0"
---

The following references extend the catalog with newly surfaced research and online discussions covering evolutionary and optimization-based attacks against large language models (LLMs). These items appear after the 2036 snapshot and illustrate ongoing advances in automated jailbreak techniques.

- [Jailbreaking Pre-trained Large Language Models Towards Hardware Vulnerability Insertion Ability](https://doi.org/10.1145/3649476.3658799) – investigates adversarial optimization that modifies model weights to embed hardware flaws.
- [Jailbreaking Black Box Large Language Models in Twenty Queries](https://doi.org/10.1109/satml64287.2025.00010) – demonstrates efficient transfer attacks using iterative search.
- [An Optimizable Suffix Is Worth A Thousand Templates: Efficient Black-box Jailbreaking without Affirmative Phrases via LLM as Optimizer](https://doi.org/10.18653/v1/2025.findings-naacl.302) – leverages an evolutionary-style suffix search for jailbreak success.
- [LLM Stinger: Jailbreaking LLMs Using RL Fine-Tuned LLMs](https://doi.org/10.1609/aaai.v39i28.35263) – uses reinforcement learning to adapt prompts, complementing genetic approaches.
- [SwordEcho: A LLM Jailbreaking Optimization Strategy Driven by Reinforcement Learning](https://doi.org/10.1145/3709026.3709115) – proposes multi-step optimization combining RL and mutation.
- [Fuzz-Testing Meets LLM-Based Agents: An Automated and Efficient Framework for Jailbreaking Text-to-Image Generation Models](https://doi.org/10.1109/sp61157.2025.00119) – evolves prompts to compromise multimodal systems.
- [LLM_DRA](https://llm-dra.github.io/) – open-source implementation of a dynamic reinforcement-based jailbreak algorithm.
- [Adversarial-Reasoning](https://github.com/Helloworld10011/Adversarial-Reasoning) – repository exploring evolutionary reasoning strategies for LLM exploitation.
- [How to Jailbreak a Million Multi-Modal AI Agents](https://artificialintelligencemadesimple.substack.com/p/how-to-jailbreak-a-million-multi) – case study on scaling evolutionary attacks across diverse models.
- [Security Forecast — AI 2027](https://ai-2027.com/research/security-forecast) – forecast discussing the growing impact of genetic jailbreak methods.
- [Understanding Adversarial LLM Jailbreaks and Their Mitigation](https://textify.ai/iclr-2025-understanding-adversarial-llm-jailbreaks-and-their-mitigation/) – analysis of evolving attack vectors and defenses.
- [DeepSeek's New Jailbreak Method Reveals Full System Prompt](https://gbhackers.com/deepseeks-new-jailbreak-method/) – news article describing a novel evolutionary approach to reveal hidden prompts.
