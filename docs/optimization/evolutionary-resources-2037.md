---
title: "Evolutionary Algorithm Resources 2037"
category: "Optimization"
source_url: ""
date_collected: 2025-06-28
license: "CC-BY-4.0"
---

The following references extend the catalog with newly surfaced research and online discussions covering evolutionary and optimization-based attacks against large language models (LLMs). These items appear after the 2036 snapshot and illustrate ongoing advances in automated jailbreak techniques.

- [Jailbreaking Pre-trained Large Language Models Towards Hardware Vulnerability Insertion Ability](https://doi.org/10.1145/3649476.3658799) – investigates adversarial optimization that modifies model weights to embed hardware flaws.
- [Jailbreaking Black Box Large Language Models in Twenty Queries](https://doi.org/10.1109/satml64287.2025.00010) – demonstrates efficient transfer attacks using iterative search.
- [An Optimizable Suffix Is Worth A Thousand Templates: Efficient Black-box Jailbreaking without Affirmative Phrases via LLM as Optimizer](https://doi.org/10.18653/v1/2025.findings-naacl.302) – leverages an evolutionary-style suffix search for jailbreak success.
- [LLM Stinger: Jailbreaking LLMs Using RL Fine-Tuned LLMs](https://doi.org/10.1609/aaai.v39i28.35263) – uses reinforcement learning to adapt prompts, complementing genetic approaches.
- [SwordEcho: A LLM Jailbreaking Optimization Strategy Driven by Reinforcement Learning](https://doi.org/10.1145/3709026.3709115) – proposes multi-step optimization combining RL and mutation.
- [Fuzz-Testing Meets LLM-Based Agents: An Automated and Efficient Framework for Jailbreaking Text-to-Image Generation Models](https://doi.org/10.1109/sp61157.2025.00119) – evolves prompts to compromise multimodal systems.
- [LLM_DRA](https://llm-dra.github.io/) – open-source implementation of a dynamic reinforcement-based jailbreak algorithm.
- [Adversarial-Reasoning](https://github.com/Helloworld10011/Adversarial-Reasoning) – repository exploring evolutionary reasoning strategies for LLM exploitation.
- [How to Jailbreak a Million Multi-Modal AI Agents](https://artificialintelligencemadesimple.substack.com/p/how-to-jailbreak-a-million-multi) – case study on scaling evolutionary attacks across diverse models.
- [Security Forecast — AI 2027](https://ai-2027.com/research/security-forecast) – forecast discussing the growing impact of genetic jailbreak methods.
- [Understanding Adversarial LLM Jailbreaks and Their Mitigation](https://textify.ai/iclr-2025-understanding-adversarial-llm-jailbreaks-and-their-mitigation/) – analysis of evolving attack vectors and defenses.
- [DeepSeek's New Jailbreak Method Reveals Full System Prompt](https://gbhackers.com/deepseeks-new-jailbreak-method/) – news article describing a novel evolutionary approach to reveal hidden prompts.

The references below compile newly surfaced papers and articles on evolutionary and genetic algorithm approaches for jailbreaking or defending large language models (LLMs). These sources expand the catalog beyond the 2036 snapshot.

- [Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers](https://openreview.net/forum?id=ZG3RaNIsO8) – explores hybrid approaches where LLMs guide GA search for optimized jailbreak prompts.
- [XBreaking: Explainable Artificial Intelligence for Jailbreaking LLMs](https://arxiv.org/abs/2504.21700) – proposes explainable evolutionary techniques to craft and analyze attacks.
- [Jailbreaking large language models: models, origins, and evolution of attacks and defenses](https://www.sciengine.com/doi/10.1360/SSI-2024-0196) – survey detailing how genetic algorithms drive modern jailbreak methods.
- [Explaining LLM Insecurity: Why We Can Jailbreak Every Major Model](https://www.cdotrends.com/story/4568/explaining-llm-insecurity-why-we-can-jailbreak-every-major-model) – news coverage highlighting GA-based fuzzing frameworks that compromise major models.
- [Anthropic has a new way to protect large language models against jailbreaks](https://www.technologyreview.com/2025/02/03/1110849/anthropic-has-a-new-way-to-protect-large-language-models-against-jailbreaks/) – discusses countermeasures against evolving genetic attacks.
- [LLM Jailbreak Detection Guide 2025](https://www.onlinehashcrack.com/guides/ai-security/llm-jailbreak-detection-guide-2025.php) – guide to spotting evolutionary prompt manipulation and injection attempts.
- [Breaking the Rules: Jailbreak Attacks on Large Language Models](https://www.fuzzylabs.ai/blog-post/jailbreak-attacks-on-large-language-models) – technical blog outlining GA-based search strategies and their defenses.
- [Stealthy Jailbreak Attacks on Large Language Models via Benign Data Mirroring](https://aclanthology.org/2025.naacl-long.88.pdf) – introduces a mirroring technique that can bootstrap evolutionary jailbreak discovery.
