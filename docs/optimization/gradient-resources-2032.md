---
title: "Gradient Attack Resources 2032"
category: "Optimization"
source_url: ""
date_collected: 2025-06-22
license: "CC-BY-4.0"
---

The references below highlight gradient-based jailbreak and defense research published after the 2031 list.

- [Wolf Hidden in Sheep's Conversations: Toward Harmless Data-Based Backdoor Attacks for Jailbreaking Large Language Models](https://arxiv.org/abs/2505.17601) – demonstrates how seemingly benign data can be optimized to implant covert jailbreak triggers.
- [Alignment Under Pressure: The Case for Informed Adversaries When Evaluating LLM Defenses](https://arxiv.org/abs/2505.15738) – argues that gradient-guided attacks reveal vulnerabilities overlooked by naive evaluations.
- [Why LLM Safety Guardrails Collapse After Fine-tuning: A Similarity Analysis Between Alignment and Fine-tuning Datasets](https://arxiv.org/abs/2506.05346) – uses gradient similarity metrics to explain the failure of safety guardrails.
- [A Sober Look at Steering Vectors for LLMs](https://www.alignmentforum.org/posts/QQP4nq7TXg89CJGBh/a-sober-look-at-steering-vectors-for-llms) – examines the limitations of gradient-derived steering vectors for controlling model behaviour.
- [Backdoors as an Analogy for Deceptive Alignment](https://www.alignment.org/blog/backdoors-as-an-analogy-for-deceptive-alignment/) – blog post connecting gradient-based backdoor techniques with larger alignment concerns.
- [Deep Causal Transcoding: A Framework for Mechanistically Editing Language Models](https://www.alignmentforum.org/posts/fSRg5qs9TPbNy3sm5/deep-causal-transcoding-a-framework-for-mechanistically) – proposes gradient-driven transformations to modify internal representations.

