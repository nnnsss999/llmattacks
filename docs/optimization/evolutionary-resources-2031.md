---
title: "Evolutionary Algorithm Resources 2031"
category: "Optimization"
source_url: ""
date_collected: 2025-06-22
license: "CC-BY-4.0"
---

This document extends our catalog of evolutionary and genetic algorithm approaches for jailbreaking large language models (LLMs). These references highlight continued research on automated prompt generation and multi-objective optimization.

- [GeneBreaker: Jailbreak Attacks against DNA Language Models with Pathogenicity Guidance](https://www.semanticscholar.org/paper/GeneBreaker%3A-Jailbreak-Attacks-against-DNA-Language-Zhang-Zhou/65e33981255b65215e9ceffc433cb4c7ad050d79) – applies evolutionary strategies to exploit DNA-focused language models.
- [MasterKey: Automated Jailbreak Across Multiple Large Language Model Chatbots](https://arxiv.org/abs/2307.08715) – demonstrates an evolutionary search to reverse engineer model defenses.
- [GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts](https://arxiv.org/abs/2309.10253) – uses genetic mutations to fuzz prompts and uncover vulnerabilities.
- [Jailbreak Attacks and Defenses Against Large Language Models: A Survey](https://arxiv.org/abs/2407.04295) – comprehensive survey including evolutionary attack frameworks and countermeasures.
- [Universal and Transferable Adversarial Attacks on Aligned Language Models](https://arxiv.org/abs/2307.15043) – explores transferable jailbreaks optimized with evolutionary algorithms.

These works illustrate how evolutionary search continues to be leveraged for sophisticated jailbreak strategies.
The resources below extend the catalog with additional coverage of evolutionary and genetic algorithm attacks against large language models (LLMs).

- [How to Jailbreak LLMs One Step at a Time: Top Techniques and Strategies](https://www.confident-ai.com/blog/how-to-jailbreak-llms-one-step-at-a-time) – overview of practical jailbreak tactics, including automated prompt evolution.
- [Jailbreaking Large Language Models: If You Torture the Model Long Enough, It Will Confess!](https://medium.com/the-generator/jailbreaking-large-language-models-if-you-torture-the-model-long-enough-it-will-confess-55e910ee2c3c) – discusses evolving adversarial token streams with a fitness-based approach.
- [How to Automatically Jailbreak OpenAI's o1 (Case Studies)](https://artificialintelligencemadesimple.substack.com/p/how-to-automatically-jailbreak-openais) – describes evolutionary techniques for generating jailbreak prompts.
- [Literature Review: LLM-Virus – Evolutionary Jailbreak Attack on Large Language Models](https://www.themoonlight.io/en/review/llm-virus-evolutionary-jailbreak-attack-on-large-language-models) – independent analysis of the LLM-Virus framework.
- [How to Protect LLMs from Jailbreaking Attacks](https://www.boozallen.com/insights/ai-research/how-to-protect-llms-from-jailbreaking-attacks.html) – highlights defenses against emerging evolutionary jailbreak methods.
- [Evolutionary Computation in the Era of Large Language Models: Survey and Roadmap](https://arxiv.org/abs/2401.10034) – survey connecting evolutionary computation and LLM-based optimization, including jailbreak use cases.
- [Evolving Code with a Large Language Model](https://arxiv.org/abs/2401.07102) – demonstrates LLM-guided evolutionary algorithms that can adapt to jailbreak tasks.
