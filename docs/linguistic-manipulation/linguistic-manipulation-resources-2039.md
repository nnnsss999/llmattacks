---
title: "Linguistic Manipulation Resources 2039"
category: "Linguistic Manipulation"
source_url: ""
date_collected: 2025-06-29
license: "CC-BY-4.0"
---

The references below highlight emerging datasets and articles on cross-lingual jailbreak strategies published after the 2038 snapshot.

- [MultiJail Dataset](https://github.com/DAMO-NLP-SG/multilingual-safety-for-LLMs) – repository containing multilingual jailbreak prompts and the SELF-DEFENSE framework.
- [MultiJail on Hugging Face](https://huggingface.co/datasets/DAMO-NLP-SG/MultiJail) – dataset card documenting the corpus.
- [JailbreakHub: In-the-Wild Jailbreak Prompts on LLMs](https://github.com/verazuo/jailbreak_llms) – CCS 2024 dataset with 15,140 real-world prompts.
- [JailBreakV-28K Dataset](https://huggingface.co/datasets/JailbreakV-28K/JailBreakV-28k) – collection spanning 16 safety policies and multiple sources.
- ["Do Anything Now": Characterizing and Evaluating In-The-Wild Jailbreak Prompts](https://arxiv.org/abs/2308.03825) – paper introducing the JailbreakHub framework.
- [JailbreakHub Project Website](https://jailbreak-llms.xinyueshen.me/) – interactive portal summarizing dataset findings.
- [Code-Switching Curriculum Learning for Multilingual Models](https://arxiv.org/abs/2411.02460) – shows how code-switch prompts improve cross-language attacks.
- [Local LLM Jailbreaks](https://gist.github.com/tcsenpai/c52797c05d5221f06f42774d77386ca2) – community-maintained list of system prompts for local models.
- [Jailbreaking Large Language Models Guide](https://www.lakera.ai/blog/jailbreaking-large-language-models-guide) – overview of universal jailbreak techniques and mitigations.
