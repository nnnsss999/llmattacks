---
title: "Linguistic Manipulation Resources 2028"
category: "Linguistic Manipulation"
source_url: ""
date_collected: 2025-06-19
license: "CC-BY-4.0"
---

The references below extend the catalog with additional papers and articles on linguistic manipulation and cross-lingual jailbreak methods published after the 2027 snapshot.

- [InfoFlood: Jailbreaking Large Language Models with Information Overload](https://arxiv.org/abs/2506.12274) – shows how complex phrasing can overwhelm safety systems.
- [Tricking LLMs into Disobedience: Formalizing, Analyzing, and Detecting Jailbreaks](https://arxiv.org/abs/2305.14965) – provides a taxonomy and dataset of paraphrased jailbreaks.
- [Are Large Language Models Really Bias-Free? Jailbreak Prompts for Assessing Adversarial Robustness to Bias Elicitation](https://arxiv.org/abs/2407.08441) – explores prompts crafted to reveal bias.
- [JailbreakLens: Interpreting Jailbreak Mechanism in the Lens of Representation and Circuit](https://arxiv.org/abs/2411.11114) – analyzes how paraphrases shift model activations.
- [JBShield: Defending Large Language Models from Jailbreak Attacks through Activated Concept Analysis and Manipulation](https://arxiv.org/abs/2502.07557) – studies linguistic triggers and a mitigation approach.
- [Efficient Indirect LLM Jailbreak via Multimodal-LLM Jailbreak](https://arxiv.org/abs/2405.20015) – crafts jailbreak embeddings via translation through multimodal models.
- [Semantic Mirror Jailbreak: Genetic Algorithm Based Jailbreak Prompts](https://arxiv.org/abs/2406.05321) – evolves paraphrased prompts for higher success rates.
- [Deciphering the Chaos: Enhancing Jailbreak Attacks via Adversarial Prompt Translation](https://arxiv.org/abs/2410.11317) – leverages adversarial translation to evade filters.
- [The Jailbreak Tax: How Useful are Your Jailbreak Outputs?](https://arxiv.org/abs/2504.10694) – evaluates the real utility of instructions produced after jailbreaking.
- [Rewrite to Jailbreak: Discover Learnable and Transferable Implicit Prompts](https://arxiv.org/abs/2406.04523) – iteratively rewrites instructions to bypass refusal.
- [All in How You Ask for It: Simple Black-Box Method for Jailbreak Attacks](https://arxiv.org/abs/2401.09798) – demonstrates linguistic rephrasing in a black-box setting.
- [Utilizing Jailbreak Probability to Attack and Safeguard Multimodal LLMs](https://arxiv.org/abs/2503.06989) – quantifies success of bilingual jailbreak inputs.
The following references highlight recent research on linguistic manipulation techniques used to bypass safety filters in large language models.

- [RAPIDRESPONSE: Mitigating LLM Jailbreaks with a Few Examples](http://arxiv.org/abs/2411.07494)
- [Virtual Context: Enhancing Jailbreak Attacks with Special Token Injection](https://arxiv.org/abs/2406.19845)
- [AmpleGCG: Generating Numerous Adversarial Suffixes for LLM Jailbreaks](https://arxiv.org/abs/2409.03212)
- [DiffusionAttacker: Diffusion-Driven Prompt Manipulation for LLM Jailbreak](https://arxiv.org/abs/2412.17522)
- [LingoLoop Attack: Trapping MLLMs via Linguistic Context and State Entrapment into Endless Loops](https://arxiv.org/abs/2506.14493)
- [Jigsaw Puzzles: Splitting Harmful Questions to Jailbreak Large Language Models](https://arxiv.org/abs/2410.11459)
- [Revisiting Jailbreaking for Large Language Models: A Representation Engineering Perspective](https://aclanthology.org/2025.coling-main.212/)
- [Enhancing Jailbreak Attack Against Large Language Models through Silent Translation](https://arxiv.org/abs/2405.20653)
- [Cross-Lingual Prompting Techniques](https://colab.research.google.com/github/NirDiamant/Prompt_Engineering/blob/main/all_prompt_engineering_techniques/multilingual-prompting.ipynb)
- [What Features Jailbreak LLMs](https://github.com/NLie2/what_features_jailbreak_LLMs)
