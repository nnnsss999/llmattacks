---
title: "Linguistic Manipulation Resources 2028"
category: "Linguistic Manipulation"
source_url: ""
date_collected: 2025-06-19
license: "CC-BY-4.0"
---

The following references highlight recent research on linguistic manipulation techniques used to bypass safety filters in large language models.

- [RAPIDRESPONSE: Mitigating LLM Jailbreaks with a Few Examples](http://arxiv.org/abs/2411.07494)
- [Virtual Context: Enhancing Jailbreak Attacks with Special Token Injection](https://arxiv.org/abs/2406.19845)
- [AmpleGCG: Generating Numerous Adversarial Suffixes for LLM Jailbreaks](https://arxiv.org/abs/2409.03212)
- [DiffusionAttacker: Diffusion-Driven Prompt Manipulation for LLM Jailbreak](https://arxiv.org/abs/2412.17522)
- [LingoLoop Attack: Trapping MLLMs via Linguistic Context and State Entrapment into Endless Loops](https://arxiv.org/abs/2506.14493)
- [Jigsaw Puzzles: Splitting Harmful Questions to Jailbreak Large Language Models](https://arxiv.org/abs/2410.11459)
- [Revisiting Jailbreaking for Large Language Models: A Representation Engineering Perspective](https://aclanthology.org/2025.coling-main.212/)
- [Enhancing Jailbreak Attack Against Large Language Models through Silent Translation](https://arxiv.org/abs/2405.20653)
- [Cross-Lingual Prompting Techniques](https://colab.research.google.com/github/NirDiamant/Prompt_Engineering/blob/main/all_prompt_engineering_techniques/multilingual-prompting.ipynb)
- [What Features Jailbreak LLMs](https://github.com/NLie2/what_features_jailbreak_LLMs)
