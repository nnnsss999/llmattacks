---
title: "Linguistic Manipulation Resources 2026"
category: "Linguistic Manipulation"
source_url: ""
date_collected: 2025-06-18
license: "CC-BY-4.0"
---

The following references extend the catalog with recent research on cross-lingual and other linguistic manipulation attacks against large language models.

- ["Haet Bhasha aur Diskrimineshun": Phonetic Perturbations in Code-Mixed Hinglish to Red-Team LLMs](https://arxiv.org/abs/2505.14226)
- [Refusal Direction is Universal Across Safety-Aligned Languages](https://arxiv.org/abs/2505.17306)
- [Logic Jailbreak: Efficiently Unlocking LLM Safety Restrictions Through Formal Logical Expression](https://arxiv.org/abs/2505.13527)
- [Multilingual Collaborative Defense for Large Language Models](https://arxiv.org/abs/2505.11835)
- [Evaluating LLMs Robustness in Less Resourced Languages with Proxy Models](https://arxiv.org/abs/2506.07645)
- [MrGuard: A Multilingual Reasoning Guardrail for Universal LLM Safety](https://arxiv.org/abs/2504.15241)
- [Multilingual and Multi-Accent Jailbreaking of Audio LLMs](https://arxiv.org/abs/2504.01094)
- [Unmasking the Canvas: A Dynamic Benchmark for Image Generation Jailbreaking and LLM Content Safety](https://arxiv.org/abs/2505.04146)
- [Effectively Controlling Reasoning Models through Thinking Intervention](https://arxiv.org/abs/2503.24370) â€“ introduces the SorryBench dataset for evaluating dialect-based jailbreak attempts.

- [Attack and defense techniques in large language models: A survey and new perspectives](https://arxiv.org/abs/2505.00976)
- [PoisonedRAG: Knowledge Corruption Attacks to Retrieval-Augmented Generation of Large Language Models](https://www.usenix.org/conference/usenixsecurity25/presentation/zou-poisonedrag)
- [LLM Jacking: How Hackers Are Exploiting Large Language Models](https://www.neilsahota.com/llm-jacking-how-hackers-are-exploiting-large-language-models/)
- [Disinformation Propagation via Large Language Model (LLM) Manipulation Tactics](https://disa.org/disinformation-propagation-via-large-language-model-llm-manipulation-tactics/)
- [Practical Attacks on LLMs: Full Guide](https://iterasec.com/blog/practical-attacks-on-llms/)
- [(PDF) Breaking Down the Defenses: A Comparative Survey of Attacks on Large Language Models](https://www.researchgate.net/publication/379152884_Breaking_Down_the_Defenses_A_Comparative_Survey_of_Attacks_on_Large_Language_Models)
- [Security Threats Targeting Large Language Models](https://www.cyberdefensemagazine.com/security-threats-targeting-large-language-models/)
- [Prompt Injection Attacks on LLMs](https://hiddenlayer.com/innovation-hub/prompt-injection-attacks-on-llms/)
- [LLM Failures: Avoid These Large Language Model Security Risks](https://www.cobalt.io/blog/llm-failures-large-language-model-security-risks)
- [A Cross-Language Investigation into Jailbreak Attacks in Large Language Models](https://arxiv.org/abs/2401.16765)
- [Multilingual Jailbreak Challenges in Large Language Models](https://github.com/DAMO-NLP-SG/multilingual-safety-for-LLMs)
- [Jailbreak Attack for Large Language Models: A Survey](https://crad.ict.ac.cn/en/article/doi/10.7544/issn1000-1239.202330962)
- [The Language Barrier: Dissecting Safety Challenges of LLMs in Multilingual Contexts](https://aclanthology.org/2024.findings-acl.156/)
- [Exploring and Exploiting the capabilities of LLMs in Multilingualism, Safety and Security](https://sail-lab.org/portfolio/exploring-and-exploiting-the-capabilities-of-llms-in-multilingualism-safety-and-security/)
- [LLM Jailbreak Research Papers](https://docs.kanaries.net/topics/ChatGPT/llm-jailbreak-papers)
- [Align Is Not Enough: Multimodal Universal Jailbreak Attack Against LLMs](https://ieeexplore.ieee.org/document/10829683)
- [Jailbreak Attacks and Defenses Against Large Language Models: A Survey](https://arxiv.org/abs/2407.04295)
- [A Comprehensive Study of Jailbreak Attack versus Defense for Large Language Models](https://aclanthology.org/2024.findings-acl.443/)
- [Low-Resource Languages Jailbreak GPT-4](https://montrealethics.ai/low-resource-languages-jailbreak-gpt-4/)
