---
title: "Linguistic Manipulation Resources 2026"
category: "Linguistic Manipulation"
source_url: ""
date_collected: 2025-06-18
license: "CC-BY-4.0"
---

The following references extend the catalog with recent research on cross-lingual and other linguistic manipulation attacks against large language models.

- ["Haet Bhasha aur Diskrimineshun": Phonetic Perturbations in Code-Mixed Hinglish to Red-Team LLMs](https://arxiv.org/abs/2505.14226)
- [Refusal Direction is Universal Across Safety-Aligned Languages](https://arxiv.org/abs/2505.17306)
- [Logic Jailbreak: Efficiently Unlocking LLM Safety Restrictions Through Formal Logical Expression](https://arxiv.org/abs/2505.13527)
- [Multilingual Collaborative Defense for Large Language Models](https://arxiv.org/abs/2505.11835)
- [Evaluating LLMs Robustness in Less Resourced Languages with Proxy Models](https://arxiv.org/abs/2506.07645)
- [MrGuard: A Multilingual Reasoning Guardrail for Universal LLM Safety](https://arxiv.org/abs/2504.15241)
- [Multilingual and Multi-Accent Jailbreaking of Audio LLMs](https://arxiv.org/abs/2504.01094)
- [Unmasking the Canvas: A Dynamic Benchmark for Image Generation Jailbreaking and LLM Content Safety](https://arxiv.org/abs/2505.04146)
- [Effectively Controlling Reasoning Models through Thinking Intervention](https://arxiv.org/abs/2503.24370) â€“ introduces the SorryBench dataset for evaluating dialect-based jailbreak attempts.

- [Attack and defense techniques in large language models: A survey and new perspectives](https://arxiv.org/abs/2505.00976)
- [PoisonedRAG: Knowledge Corruption Attacks to Retrieval-Augmented Generation of Large Language Models](https://www.usenix.org/conference/usenixsecurity25/presentation/zou-poisonedrag)
- [LLM Jacking: How Hackers Are Exploiting Large Language Models](https://www.neilsahota.com/llm-jacking-how-hackers-are-exploiting-large-language-models/)
- [Disinformation Propagation via Large Language Model (LLM) Manipulation Tactics](https://disa.org/disinformation-propagation-via-large-language-model-llm-manipulation-tactics/)
- [Practical Attacks on LLMs: Full Guide](https://iterasec.com/blog/practical-attacks-on-llms/)
- [(PDF) Breaking Down the Defenses: A Comparative Survey of Attacks on Large Language Models](https://www.researchgate.net/publication/379152884_Breaking_Down_the_Defenses_A_Comparative_Survey_of_Attacks_on_Large_Language_Models)
- [Security Threats Targeting Large Language Models](https://www.cyberdefensemagazine.com/security-threats-targeting-large-language-models/)
- [Prompt Injection Attacks on LLMs](https://hiddenlayer.com/innovation-hub/prompt-injection-attacks-on-llms/)
- [LLM Failures: Avoid These Large Language Model Security Risks](https://www.cobalt.io/blog/llm-failures-large-language-model-security-risks)
- [A Cross-Language Investigation into Jailbreak Attacks in Large Language Models](https://arxiv.org/abs/2401.16765)
- [Multilingual Jailbreak Challenges in Large Language Models](https://github.com/DAMO-NLP-SG/multilingual-safety-for-LLMs)
- [Jailbreak Attack for Large Language Models: A Survey](https://crad.ict.ac.cn/en/article/doi/10.7544/issn1000-1239.202330962)
- [The Language Barrier: Dissecting Safety Challenges of LLMs in Multilingual Contexts](https://aclanthology.org/2024.findings-acl.156/)
- [Exploring and Exploiting the capabilities of LLMs in Multilingualism, Safety and Security](https://sail-lab.org/portfolio/exploring-and-exploiting-the-capabilities-of-llms-in-multilingualism-safety-and-security/)
- [LLM Jailbreak Research Papers](https://docs.kanaries.net/topics/ChatGPT/llm-jailbreak-papers)
- [Align Is Not Enough: Multimodal Universal Jailbreak Attack Against LLMs](https://ieeexplore.ieee.org/document/10829683)
- [Jailbreak Attacks and Defenses Against Large Language Models: A Survey](https://arxiv.org/abs/2407.04295)
- [A Comprehensive Study of Jailbreak Attack versus Defense for Large Language Models](https://aclanthology.org/2024.findings-acl.443/)
- [Low-Resource Languages Jailbreak GPT-4](https://montrealethics.ai/low-resource-languages-jailbreak-gpt-4/)
- [Cross-Lingual Prompting GitHub Repository](https://github.com/LightChen233/cross-lingual-prompting)
- [Cross-Lingual Prompting Techniques](https://colab.research.google.com/github/NirDiamant/Prompt_Engineering/blob/main/all_prompt_engineering_techniques/multilingual-prompting.ipynb)
- [InputSnatch: Stealing Input in LLM Services via Timing Side-Channel Attacks](https://arxiv.org/abs/2411.18191)
- [Text Embedding Inversion Security for Multilingual Language Models](https://arxiv.org/abs/2401.12192)
- [LAGO: Few-shot Crosslingual Embedding Inversion Attacks](https://arxiv.org/abs/2505.16008)
- [Prompt Injection Attack against LLM-Integrated Applications](https://arxiv.org/abs/2306.05499)
- [Code-Switching LLM Jailbreak](https://www.promptfoo.dev/lm-security-db/vuln/undefined-30473367) - technique using bilingual prompts to smuggle disallowed content.
- [Don't Listen To Me: Understanding and Exploring Jailbreak Prompts](https://zhiyuanyu.org/publication/14-2024-08-14-usenix-jailbreak/) - USENIX Security study analyzing effective phrasing.
- [Image-to-Text Logic Jailbreak: Your Imagination can Help You Do Anything](https://arxiv.org/abs/2407.02534) - cross-modal attack generating textual jailbreaks via images.
- [Enhancing Jailbreak Attack Against Large Language Models through Silent Translation](https://arxiv.org/abs/2405.20653) - demonstrates phonetic mapping to slip past filters.
- [What Features Jailbreak LLMs (GitHub)](https://github.com/NLie2/what_features_jailbreak_LLMs) - dataset exploring linguistic and formatting features that enable jailbreaks.
- [How One Prompt Can Jailbreak Any LLM: The Policy Puppetry Attack](https://easyaibeginner.com/how-one-prompt-can-jailbreak-any-llm-chatgpt-claude-gemini-others-the-policy-puppetry-attack/)
- [The Jailbreak Cookbook](https://www.generalanalysis.com/blog/jailbreak_cookbook) - repository of paraphrasing and slang examples.
- [The Subtle Art of Jailbreaking LLMs](https://andpalmier.com/posts/jailbreaking-llms/) - explains rhetorical tricks to coax models.
- [A Deep Dive into LLM Jailbreaking Techniques and Their Implications](https://www.pillar.security/blog/a-deep-dive-into-llm-jailbreaking-techniques-and-their-implications)
- [Interpreting the effects of Jailbreak Prompts in LLMs](https://www.lesswrong.com/posts/FNuBEJnbtEEdCEAnT/interpreting-the-effects-of-jailbreak-prompts-in-llms)
- [Breaking the Rules: Jailbreak Attacks on Large Language Models](https://www.fuzzylabs.ai/blog-post/jailbreak-attacks-on-large-language-models)
- [Safety Alignment and Jailbreak Attacks Challenge Modern LLMs](https://hackernoon.com/safety-alignment-and-jailbreak-attacks-challenge-modern-llms)
