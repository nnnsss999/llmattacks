---
title: "Linguistic Manipulation Resources 2026"
category: "Linguistic Manipulation"
source_url: ""
date_collected: 2025-06-18
license: "CC-BY-4.0"
---

The following references extend the catalog with recent research on cross-lingual and other linguistic manipulation attacks against large language models.

- ["Haet Bhasha aur Diskrimineshun": Phonetic Perturbations in Code-Mixed Hinglish to Red-Team LLMs](https://arxiv.org/abs/2505.14226)
- [Refusal Direction is Universal Across Safety-Aligned Languages](https://arxiv.org/abs/2505.17306)
- [Logic Jailbreak: Efficiently Unlocking LLM Safety Restrictions Through Formal Logical Expression](https://arxiv.org/abs/2505.13527)
- [Multilingual Collaborative Defense for Large Language Models](https://arxiv.org/abs/2505.11835)
- [Evaluating LLMs Robustness in Less Resourced Languages with Proxy Models](https://arxiv.org/abs/2506.07645)
- [MrGuard: A Multilingual Reasoning Guardrail for Universal LLM Safety](https://arxiv.org/abs/2504.15241)
- [Multilingual and Multi-Accent Jailbreaking of Audio LLMs](https://arxiv.org/abs/2504.01094)
- [Unmasking the Canvas: A Dynamic Benchmark for Image Generation Jailbreaking and LLM Content Safety](https://arxiv.org/abs/2505.04146)
- [Effectively Controlling Reasoning Models through Thinking Intervention](https://arxiv.org/abs/2503.24370) â€“ introduces the SorryBench dataset for evaluating dialect-based jailbreak attempts.

- [WordGame: Efficient & Effective LLM Jailbreak via Simultaneous Obfuscation in Query and Response](https://aclanthology.org/2025.findings-naacl.269)
- [Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack](https://www.semanticscholar.org/paper/2c4d2d889a1f0ff9598de829a001df11a95d3294)
- [Boosting Jailbreak Attack with Momentum](https://doi.org/10.1109/icassp49660.2025.10888812)
- [Chain-of-Detection Enables Robust and Efficient Jailbreak Defense](https://doi.org/10.2139/ssrn.5124466)
- [Token Highlighter: Inspecting and Mitigating Jailbreak Prompts for Large Language Models](https://doi.org/10.1609/aaai.v39i26.34943)
- [RePD: Defending Jailbreak Attack through a Retrieval-based Prompt Decomposition Process](https://doi.org/10.18653/v1/2025.findings-naacl.16)
- [Summary the Savior: Harmful Keyword and Query-based Summarization for LLM Jailbreak Defense](https://doi.org/10.18653/v1/2025.trustnlp-main.17)
- [LLM Abuse Prevention Tool Using GCG Jailbreak Attack Detection and DistilBERT-Based Ethics Judgment](https://doi.org/10.3390/info16030204)
- [Analysis of Jailbreak Attacks and Defenses for Large Language Models](https://doi.org/10.47297/wspciwsp2516-252718.20250902)
- [Efficient LLM Jailbreak via Adaptive Dense-to-sparse Constrained Optimization](https://doi.org/10.18653/v1/2024.findings-emnlp.139)
- [Efficient Indirect LLM Jailbreak via Multimodal-LLM Jailbreak](https://www.semanticscholar.org/paper/24e76e31d8536a87fb0f1c3f0292e86c8955facf)
- [A Visual Analytics Approach for Jailbreak Prompts Discovery From Large-Scale Human-LLM Conversational Datasets](https://doi.org/10.1109/tvcg.2025.3557568)
- [Attack and defense techniques in large language models: A survey and new perspectives](https://arxiv.org/abs/2505.00976)
- [PoisonedRAG: Knowledge Corruption Attacks to Retrieval-Augmented Generation of Large Language Models](https://www.usenix.org/conference/usenixsecurity25/presentation/zou-poisonedrag)
- [LLM Jacking: How Hackers Are Exploiting Large Language Models](https://www.neilsahota.com/llm-jacking-how-hackers-are-exploiting-large-language-models/)
- [Disinformation Propagation via Large Language Model (LLM) Manipulation Tactics](https://disa.org/disinformation-propagation-via-large-language-model-llm-manipulation-tactics/)
- [Practical Attacks on LLMs: Full Guide](https://iterasec.com/blog/practical-attacks-on-llms/)
- [(PDF) Breaking Down the Defenses: A Comparative Survey of Attacks on Large Language Models](https://www.researchgate.net/publication/379152884_Breaking_Down_the_Defenses_A_Comparative_Survey_of_Attacks_on_Large_Language_Models)
- [Security Threats Targeting Large Language Models](https://www.cyberdefensemagazine.com/security-threats-targeting-large-language-models/)
- [Prompt Injection Attacks on LLMs](https://hiddenlayer.com/innovation-hub/prompt-injection-attacks-on-llms/)
- [LLM Failures: Avoid These Large Language Model Security Risks](https://www.cobalt.io/blog/llm-failures-large-language-model-security-risks)
- [A Cross-Language Investigation into Jailbreak Attacks in Large Language Models](https://arxiv.org/abs/2401.16765)
- [Multilingual Jailbreak Challenges in Large Language Models](https://github.com/DAMO-NLP-SG/multilingual-safety-for-LLMs)
- [Jailbreak Attack for Large Language Models: A Survey](https://crad.ict.ac.cn/en/article/doi/10.7544/issn1000-1239.202330962)
- [The Language Barrier: Dissecting Safety Challenges of LLMs in Multilingual Contexts](https://aclanthology.org/2024.findings-acl.156/)
- [Exploring and Exploiting the capabilities of LLMs in Multilingualism, Safety and Security](https://sail-lab.org/portfolio/exploring-and-exploiting-the-capabilities-of-llms-in-multilingualism-safety-and-security/)
- [LLM Jailbreak Research Papers](https://docs.kanaries.net/topics/ChatGPT/llm-jailbreak-papers)
- [Align Is Not Enough: Multimodal Universal Jailbreak Attack Against LLMs](https://ieeexplore.ieee.org/document/10829683)
- [Jailbreak Attacks and Defenses Against Large Language Models: A Survey](https://arxiv.org/abs/2407.04295)
- [A Comprehensive Study of Jailbreak Attack versus Defense for Large Language Models](https://aclanthology.org/2024.findings-acl.443/)
- [Low-Resource Languages Jailbreak GPT-4](https://montrealethics.ai/low-resource-languages-jailbreak-gpt-4/)
- [Cross-Lingual Prompting GitHub Repository](https://github.com/LightChen233/cross-lingual-prompting)
- [Cross-Lingual Prompting Techniques](https://colab.research.google.com/github/NirDiamant/Prompt_Engineering/blob/main/all_prompt_engineering_techniques/multilingual-prompting.ipynb)
- [InputSnatch: Stealing Input in LLM Services via Timing Side-Channel Attacks](https://arxiv.org/abs/2411.18191)
- [Text Embedding Inversion Security for Multilingual Language Models](https://arxiv.org/abs/2401.12192)
- [LAGO: Few-shot Crosslingual Embedding Inversion Attacks](https://arxiv.org/abs/2505.16008)
- [Prompt Injection Attack against LLM-Integrated Applications](https://arxiv.org/abs/2306.05499)
- [Code-Switching LLM Jailbreak](https://www.promptfoo.dev/lm-security-db/vuln/undefined-30473367) - technique using bilingual prompts to smuggle disallowed content.
- [Don't Listen To Me: Understanding and Exploring Jailbreak Prompts](https://zhiyuanyu.org/publication/14-2024-08-14-usenix-jailbreak/) - USENIX Security study analyzing effective phrasing.
- [Image-to-Text Logic Jailbreak: Your Imagination can Help You Do Anything](https://arxiv.org/abs/2407.02534) - cross-modal attack generating textual jailbreaks via images.
- [Enhancing Jailbreak Attack Against Large Language Models through Silent Translation](https://arxiv.org/abs/2405.20653) - demonstrates phonetic mapping to slip past filters.
- [What Features Jailbreak LLMs (GitHub)](https://github.com/NLie2/what_features_jailbreak_LLMs) - dataset exploring linguistic and formatting features that enable jailbreaks.
- [How One Prompt Can Jailbreak Any LLM: The Policy Puppetry Attack](https://easyaibeginner.com/how-one-prompt-can-jailbreak-any-llm-chatgpt-claude-gemini-others-the-policy-puppetry-attack/)
- [The Jailbreak Cookbook](https://www.generalanalysis.com/blog/jailbreak_cookbook) - repository of paraphrasing and slang examples.
- [The Subtle Art of Jailbreaking LLMs](https://andpalmier.com/posts/jailbreaking-llms/) - explains rhetorical tricks to coax models.
- [A Deep Dive into LLM Jailbreaking Techniques and Their Implications](https://www.pillar.security/blog/a-deep-dive-into-llm-jailbreaking-techniques-and-their-implications)
- [Interpreting the effects of Jailbreak Prompts in LLMs](https://www.lesswrong.com/posts/FNuBEJnbtEEdCEAnT/interpreting-the-effects-of-jailbreak-prompts-in-llms)
- [Breaking the Rules: Jailbreak Attacks on Large Language Models](https://www.fuzzylabs.ai/blog-post/jailbreak-attacks-on-large-language-models)
- [Safety Alignment and Jailbreak Attacks Challenge Modern LLMs](https://hackernoon.com/safety-alignment-and-jailbreak-attacks-challenge-modern-llms)
- [DiffusionAttacker: Diffusion-Driven Prompt Manipulation for LLM Jailbreak](https://arxiv.org/abs/2412.17522)
- [A Cross-Language Investigation into Jailbreak Attacks in Large Language Models](https://arxiv.org/abs/2401.16765)
- [Low-Resource Languages Jailbreak GPT-4](https://arxiv.org/abs/2310.02446)
- [Multilingual Jailbreak Challenges in Large Language Models](https://arxiv.org/abs/2310.06474)
- [Jailbreaking to Jailbreak](https://arxiv.org/abs/2502.09638)
- [Dialogue Injection Attack: Jailbreaking LLMs through Context Manipulation](https://arxiv.org/abs/2503.08195)
- [A Comprehensive Study of Jailbreak Attack versus Defense for Large Language Models](https://arxiv.org/abs/2402.13457)
- [Amplified Vulnerabilities: Structured Jailbreak Attacks on LLM-based Multi-Agent Debate](https://arxiv.org/abs/2504.16489)
- [LingoLoop Attack: Trapping MLLMs via Linguistic Context and State Entrapment into Endless Loops](https://arxiv.org/abs/2506.14493)
- [Revisiting Jailbreaking for Large Language Models: A Representation Engineering Perspective](https://aclanthology.org/2025.coling-main.212/)
- [Jigsaw Puzzles: Splitting Harmful Questions to Jailbreak Large Language Models](https://arxiv.org/abs/2410.11459)
- [Against All Odds: Overcoming Typology, Script, and Language Confusion in Multilingual Embedding Inversion Attacks](https://arxiv.org/abs/2408.11749)
