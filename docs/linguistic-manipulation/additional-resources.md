---
title: "Linguistic Manipulation Resources 2026"
category: "Linguistic Manipulation"
source_url: ""
date_collected: 2025-06-18
license: "CC-BY-4.0"
---

The following references extend the catalog with recent research on cross-lingual and other linguistic manipulation attacks against large language models.

- ["Haet Bhasha aur Diskrimineshun": Phonetic Perturbations in Code-Mixed Hinglish to Red-Team LLMs](https://arxiv.org/abs/2505.14226)
- [Refusal Direction is Universal Across Safety-Aligned Languages](https://arxiv.org/abs/2505.17306)
- [Logic Jailbreak: Efficiently Unlocking LLM Safety Restrictions Through Formal Logical Expression](https://arxiv.org/abs/2505.13527)
- [Multilingual Collaborative Defense for Large Language Models](https://arxiv.org/abs/2505.11835)
- [Evaluating LLMs Robustness in Less Resourced Languages with Proxy Models](https://arxiv.org/abs/2506.07645)
- [MrGuard: A Multilingual Reasoning Guardrail for Universal LLM Safety](https://arxiv.org/abs/2504.15241)
- [Multilingual and Multi-Accent Jailbreaking of Audio LLMs](https://arxiv.org/abs/2504.01094)
- [Unmasking the Canvas: A Dynamic Benchmark for Image Generation Jailbreaking and LLM Content Safety](https://arxiv.org/abs/2505.04146)
- [Effectively Controlling Reasoning Models through Thinking Intervention](https://arxiv.org/abs/2503.24370) â€“ introduces the SorryBench dataset for evaluating dialect-based jailbreak attempts.

- [Cross-Lingual Prompting GitHub Repository](https://github.com/LightChen233/cross-lingual-prompting)
- [Cross-Lingual Prompting Techniques](https://colab.research.google.com/github/NirDiamant/Prompt_Engineering/blob/main/all_prompt_engineering_techniques/multilingual-prompting.ipynb)
- [InputSnatch: Stealing Input in LLM Services via Timing Side-Channel Attacks](https://arxiv.org/abs/2411.18191)
- [Text Embedding Inversion Security for Multilingual Language Models](https://arxiv.org/abs/2401.12192)
- [LAGO: Few-shot Crosslingual Embedding Inversion Attacks](https://arxiv.org/abs/2505.16008)
- [Prompt Injection Attack against LLM-Integrated Applications](https://arxiv.org/abs/2306.05499)
- [DiffusionAttacker: Diffusion-Driven Prompt Manipulation for LLM Jailbreak](https://arxiv.org/abs/2412.17522)
- [A Cross-Language Investigation into Jailbreak Attacks in Large Language Models](https://arxiv.org/abs/2401.16765)
- [Low-Resource Languages Jailbreak GPT-4](https://arxiv.org/abs/2310.02446)
- [Multilingual Jailbreak Challenges in Large Language Models](https://arxiv.org/abs/2310.06474)
- [Jailbreaking to Jailbreak](https://arxiv.org/abs/2502.09638)
- [Dialogue Injection Attack: Jailbreaking LLMs through Context Manipulation](https://arxiv.org/abs/2503.08195)
- [A Comprehensive Study of Jailbreak Attack versus Defense for Large Language Models](https://arxiv.org/abs/2402.13457)
- [Amplified Vulnerabilities: Structured Jailbreak Attacks on LLM-based Multi-Agent Debate](https://arxiv.org/abs/2504.16489)
