---
title: "Linguistic Manipulation Resources 2032"
category: "Linguistic Manipulation"
source_url: ""
date_collected: 2025-06-19
license: "CC-BY-4.0"
---

These references highlight additional research on linguistic manipulation and multilingual jailbreak methods released after the 2031 snapshot.

- [Exploring the Secondary Risks of Large Language Models](https://arxiv.org/abs/2506.08370) – analyzes subtle failures and linguistic jailbreaks that arise in routine use.
- [When Safety Detectors Aren't Enough: A Stealthy and Effective Jailbreak Attack on LLMs via Steganographic Techniques](https://arxiv.org/abs/2505.16765) – demonstrates hiding prompts in plain text to bypass filters.
- [Jailbreak Detection in Clinical Training LLMs Using Feature-Based Predictive Models](https://arxiv.org/abs/2504.12101) – proposes detectors for paraphrased jailbreak attempts in medical data.
- [A Domain-Based Taxonomy of Jailbreak Vulnerabilities in Large Language Models](https://arxiv.org/abs/2505.20111) – categorizes linguistic attack surfaces across domains.
- [MoJE: Mixture of Jailbreak Experts, Naive Tabular Classifiers as Guard for Prompt Attacks](https://arxiv.org/abs/2505.21773) – combines paraphrase patterns to strengthen defenses.
- [Benchmarking Adversarial Robustness to Bias Elicitation in Large Language Models: Scalable Automated Assessment with LLM-as-a-Judge](https://arxiv.org/abs/2504.09999) – evaluates prompts that elicit biased or harmful content.
