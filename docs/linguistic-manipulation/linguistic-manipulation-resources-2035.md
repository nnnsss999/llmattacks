---
title: "Linguistic Manipulation Resources 2035"
category: "Linguistic Manipulation"
source_url: ""
date_collected: 2025-06-23
license: "CC-BY-4.0"
---

The references below capture newer research on paraphrase-based and cross-lingual jailbreak strategies published after the 2034 snapshot.

- [Understanding Jailbreak Success: A Study of Latent Space Dynamics in Large Language Models](https://arxiv.org/abs/2406.09289) – analyzes how paraphrased prompts reshape latent representations to evade detection.
- [LARGO: Latent Adversarial Reflection through Gradient Optimization for Jailbreaking LLMs](https://arxiv.org/abs/2505.10838) – introduces gradient-based search to craft linguistically obfuscated jailbreak prompts.
- [GASP: Efficient Black-Box Generation of Adversarial Suffixes for Jailbreaking LLMs](https://arxiv.org/abs/2411.14133) – automatically discovers suffixes that slip past safety guards.
- [What Features in Prompts Jailbreak LLMs? Investigating the Mechanisms Behind Attacks](https://arxiv.org/abs/2411.03343) – studies lexical and formatting features that enable policy circumvention.
- [Prompt Obfuscation for Large Language Models](https://arxiv.org/abs/2409.11026) – proposes adding controlled noise to hide malicious intent from moderation systems.
- [PROMPTFUZZ: Harnessing Fuzzing Techniques for Robust Testing of Prompt Injection in LLMs](https://arxiv.org/abs/2409.14729) – applies fuzzing to generate diverse paraphrases for jailbreak evaluation.
- [Enhancing Prompt Injection Attacks to LLMs via Poisoning Alignment](https://arxiv.org/abs/2410.14827) – manipulates RLHF alignment to increase success of obfuscated prompts.
- [Knowledge Return Oriented Prompting (KROP)](https://arxiv.org/abs/2406.11880) – retrieves banned knowledge using disguised textual triggers.
