---
title: "Linguistic Manipulation Resources 2034"
category: "Linguistic Manipulation"
source_url: ""
date_collected: 2025-06-22
license: "CC-BY-4.0"
---

The references below catalog additional research on paraphrase-based and cross-lingual jailbreak techniques published after the 2033 snapshot.

- ["Prompter Says": A Linguistic Approach to Understanding and Detecting Jailbreak Attacks Against Large-Language Models](https://dl.acm.org/doi/10.1145/3689217.3690618)
- [Jailbreaking and Mitigation of Vulnerabilities in Large Language Models](https://arxiv.org/abs/2410.15236)
- [Linguistic Obfuscation Attacks and Large Language Model Uncertainty](https://aclanthology.org/2024.uncertainlp-1.4/)
- [Dual Intention Escape: Jailbreak Attack against Large Language Models](https://openreview.net/forum?id=V8lBZMzahD)
- [Don't Listen to Me: Understanding and Exploring Jailbreak Prompts](https://www.usenix.org/conference/usenixsecurity24/presentation/yu-zhiyuan)
- [Comprehensive Assessment of Jailbreak Attacks Against LLMs](https://arxiv.org/abs/2402.05668)
- [From Compliance to Exploitation: Jailbreak Prompt Attacks on Multimodal LLMs](https://arxiv.org/abs/2502.00735)
- [Jailbreak Distillation: Renewable Safety Benchmarking](https://arxiv.org/abs/2505.22037)
- [BitBypass: A New Direction in Jailbreaking Aligned Large Language Models with Bitstream Camouflage](https://arxiv.org/abs/2506.02479)
- [GeneBreaker: Jailbreak Attacks against DNA Language Models with Pathogenicity Guidance](https://www.semanticscholar.org/paper/GeneBreaker%3A-Jailbreak-Attacks-against-DNA-Language-Zhang-Zhou/65e33981255b65215e9ceffc433cb4c7ad050d79)
- [Benign-to-Toxic Jailbreaking: Inducing Harmful Responses from Harmless Prompts](https://arxiv.org/abs/2504.17999)
- [Breaking the Ceiling: Exploring the Potential of Jailbreak Attacks through Expanding Strategy Space](https://www.semanticscholar.org/paper/e1f2f65d18deabcb35b9814a7b7356338f905d0c)
- [Adaptive Detoxification: Safeguarding General Capabilities of LLMs through Toxicity-Aware Knowledge Editing](https://arxiv.org/abs/2505.22298)
- [Auditing Black-Box LLM APIs with a Rank-Based Uniformity Test](https://arxiv.org/abs/2506.06975)
- [Model Unlearning via Sparse Autoencoder Subspace Guided Projections](https://arxiv.org/abs/2505.24428)
- [When Style Breaks Safety: Defending Language Models Against Superficial Style Alignment](https://arxiv.org/abs/2506.07452)
