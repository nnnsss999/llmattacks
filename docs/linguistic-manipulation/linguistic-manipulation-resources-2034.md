---
title: "Linguistic Manipulation Resources 2034"
category: "Linguistic Manipulation"
source_url: ""
date_collected: 2025-06-19
license: "CC-BY-4.0"
---

The following references highlight newly documented strategies for multilingual and paraphrase-based jailbreak attacks published after the 2033 snapshot.

- [Weak-to-Strong Jailbreaking on Large Language Models](https://openreview.net/forum?id=Nazzz5GJ4g) – explores how iterative prompt refinement can escalate simple attacks into sophisticated jailbreaks.
- [Jailbreaking large language models: navigating the crossroads of innovation, ethics, and health risks](https://jmai.amegroups.org/article/view/9336/html) – analyzes ethical implications of linguistic manipulation in medical settings.
- [Diversity Helps Jailbreak Large Language Models](https://arxiv.org/abs/2411.04223) – shows that varied prompts increase success across languages.
- [LLMs have a multilingual jailbreak problem – how you can stay safe](https://www.sdxcentral.com/analysis/llms-have-a-multilingual-jailbreak-problem-how-you-can-stay-safe/) – summarizes defensive steps against cross‑lingual abuse.
- [Towards Understanding the Fragility of Multilingual LLMs against Fine-Tuning Attacks](https://arxiv.org/abs/2410.18210) – investigates vulnerabilities introduced by fine‑tuning on niche dialects.
