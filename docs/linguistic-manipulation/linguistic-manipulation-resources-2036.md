---
title: "Linguistic Manipulation Resources 2036"
category: "Linguistic Manipulation"
source_url: ""
date_collected: 2025-06-25
license: "CC-BY-4.0"
---

The references below highlight newer research on paraphrase-based and cross-lingual jailbreak strategies published after the 2035 snapshot.

- [SeqAR: Jailbreak LLMs with Sequential Auto-Generated Characters](https://arxiv.org/abs/2407.01902)
- [SafeInt: Shielding Large Language Models from Jailbreak Attacks via Safety-Aware Representation Intervention](https://arxiv.org/abs/2502.15594)
- [Knowledge-to-Jailbreak: Investigating Knowledge-driven Jailbreaking Attacks for Large Language Models](https://arxiv.org/abs/2406.11682)
- [Universal Jailbreak Suffixes Are Strong Attention Hijackers](https://arxiv.org/abs/2506.12880)
- [SecurityLingua: Efficient Defense of LLM Jailbreak Attacks via Security-Aware Prompt Compression](https://arxiv.org/abs/2506.12707)
- [Alphabet Index Mapping: Jailbreaking LLMs through Semantic Dissimilarity](https://arxiv.org/abs/2506.12685)
- [Exploring the Secondary Risks of Large Language Models](https://arxiv.org/abs/2506.12382)
- [InfoFlood: Jailbreaking Large Language Models with Information Overload](https://arxiv.org/abs/2506.12274)
- [QGuard: Question-based Zero-shot Guard for Multi-modal LLM Safety](https://arxiv.org/abs/2506.12299)
- [SoK: Evaluating Jailbreak Guardrails for Large Language Models](https://arxiv.org/abs/2506.10597)
- [Évaluation empirique de la sécurisation et de l'alignement de ChatGPT et Gemini](https://arxiv.org/abs/2506.10029)
- [LLMs Caught in the Crossfire: Malware Requests and Jailbreak Challenges](https://arxiv.org/abs/2506.09540)
- [From Threat to Tool: Leveraging Refusal-Aware Injection Attacks for Safety Alignment](https://arxiv.org/abs/2506.08650)
- [Effective Red-Teaming of Policy-Adherent Agents](https://arxiv.org/abs/2506.07603)
- [ALKALI: Safeguarding LLMs through Geometric Representation-Aware Contrastive Enhancement](https://arxiv.org/abs/2506.06789)

