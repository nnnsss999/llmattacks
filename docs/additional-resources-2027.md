---
title: "Additional Resources on LLM Attacks 2027"
category: "Overview"
source_url: ""
date_collected: 2025-06-18
license: "CC-BY-4.0"
---

The references below extend the catalog with new research and articles published after the original snapshot. They highlight ongoing work on jailbreaking, data poisoning, embedding inversion, security taxonomies, and fuzzing techniques.

- [Application of Large Language Models to DDoS Attack Detection](https://doi.org/10.1007/978-3-031-51630-6_6)
- [PoisonPrompt: Backdoor Attack on Prompt-Based Large Language Models](https://doi.org/10.1109/icassp48485.2024.10446267)
- [Medical large language models are vulnerable to data-poisoning attacks](https://doi.org/10.1038/s41591-024-03445-1)
- [From Noise to Clarity: Unraveling the Adversarial Suffix of Large Language Model Attacks via Translation of Text Embeddings](https://arxiv.org/abs/2402.16006)
- [A Comprehensive Overview of Backdoor Attacks in Large Language Models within Communication Networks](https://doi.org/10.1109/mnet.2024.3367788)
- [COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability](http://arxiv.org/abs/2402.08679)
- [Jailbreaking LLM-Controlled Robots](http://arxiv.org/abs/2410.13691)
- [Don't Say No: Jailbreaking LLM by Suppressing Refusal](https://arxiv.org/abs/2404.16369)
- [Mission Impossible: A Statistical Perspective on Jailbreaking LLMs](http://arxiv.org/abs/2408.01420)
- [Hacc-Man: An Arcade Game for Jailbreaking LLMs](https://doi.org/10.1145/3656156.3665432)
- [Text Embedding Inversion Attacks on Multilingual Language Models](https://arxiv.org/abs/2401.12192)
- [Against All Odds: Overcoming Typology, Script, and Language Confusion in Multilingual Embedding Inversion Attacks](http://arxiv.org/abs/2408.11749)
- [Against All Odds: Overcoming Typology, Script, and Language Confusion in Multilingual Embedding Inversion Attacks (AAAI 2024)](https://doi.org/10.1609/aaai.v39i22.34533)
- [Transferable Embedding Inversion Attack: Uncovering Privacy Risks in Text Embeddings without Model Queries](https://doi.org/10.18653/v1/2024.acl-long.230)
- [Transferable Embedding Inversion Attack: Uncovering Privacy Risks in Text Embeddings without Model Queries (arXiv)](http://arxiv.org/abs/2406.10280)
- [Blockchain for Large Language Model Security and Safety: A Holistic Survey](https://doi.org/10.1145/3715073.3715075)
- [Decomposition, Synthesis and Attack: A Multi-Instruction Fusion Method for Jailbreaking LLMs](https://doi.org/10.1109/jiot.2025.3525741)
- [Assessing Vulnerabilities in State-of-the-Art Large Language Models Through Hex Injection](https://doi.org/10.1609/aaai.v39i28.35257)
- [A CIA Triad-Based Taxonomy of Prompt Attacks on Large Language Models](https://doi.org/10.3390/fi17030113)
- [Infecting Generative AI With Viruses](http://arxiv.org/abs/2501.05542)
- [Fuzzing BusyBox: Leveraging LLM and Crash Reuse for Embedded Bug Unearthing](http://arxiv.org/abs/2403.03897)
- [When Fuzzing Meets LLMs: Challenges and Opportunities](https://doi.org/10.1145/3663529.3663784)
- [Large Language Model guided Protocol Fuzzing](https://doi.org/10.14722/ndss.2024.24556)
- [Effective and Evasive Fuzz Testing-Driven Jailbreaking Attacks against LLMs](http://arxiv.org/abs/2409.14866)
- [Fuzz4All: Universal Fuzzing with Large Language Models](https://doi.org/10.1145/3597503.3639121)
