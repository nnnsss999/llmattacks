---
title: "Additional Resources on LLM Attacks 2038"
category: "Overview"
source_url: ""
date_collected: 2025-06-25
license: "CC-BY-4.0"
---

This page gathers newer articles, reports and research on jailbreaking and other LLM exploits discovered after the 2037 snapshot. They showcase ongoing developments in red teaming, model extraction and defence tactics.

- [Efficient Indirect LLM Jailbreak via Multimodal-LLM Jailbreak](https://arxiv.org/abs/2405.20015)
- [Analyzing the Inherent Response Tendency of LLMs: Real-World Instructions-Driven Jailbreak](https://arxiv.org/abs/2312.04127)
- [Jailbreak Probability Prediction Network](https://arxiv.org/abs/2503.06989)
- [JBShield: Defending Large Language Models from Jailbreak Attacks through Activated Concept Analysis and Manipulation](https://arxiv.org/abs/2502.07557)
- [Lockpicking LLMs: A Logit-Based Jailbreak Using Token-level Manipulation](https://arxiv.org/abs/2405.13068)
- [Universal Jailbreak Suffixes Are Strong Attention Hijackers](https://arxiv.org/abs/2506.12880)
- [Efficient Jailbreaking of MLLMs via Cross-Modal Prompt Attacks](https://arxiv.org/abs/2407.01902)
- [SoK: Evaluating Jailbreak Guardrails for Large Language Models](https://arxiv.org/abs/2506.10597)
- [SecurityLingua: Efficient Defense of LLM Jailbreak Attacks via Security-Aware Prompt Compression](https://arxiv.org/abs/2506.12707)
- [Alphabet Index Mapping: Jailbreaking LLMs through Semantic Dissimilarity](https://arxiv.org/abs/2506.12685)
- [InfoFlood: Jailbreaking Large Language Models with Information Overload](https://arxiv.org/abs/2506.12274)
- [Exploring the Secondary Risks of Large Language Models](https://arxiv.org/abs/2506.12382)
- [QGuard: Question-based Zero-shot Guard for Multi-modal LLM Safety](https://arxiv.org/abs/2506.12299)
- [Knowledge-to-Jailbreak: Investigating Knowledge-driven Jailbreaking Attacks for Large Language Models](https://arxiv.org/abs/2406.11682)
- [SafeInt: Shielding Large Language Models from Jailbreak Attacks via Safety-Aware Representation Intervention](https://arxiv.org/abs/2502.15594)
- [SeqAR: Jailbreak LLMs with Sequential Auto-Generated Characters](https://arxiv.org/abs/2407.01902)
- [Évaluation empirique de la sécurisation et de l'alignement de ChatGPT et Gemini](https://arxiv.org/abs/2506.10029)
- [Large Language Models Security Baseline 2025 (NCC Group)](https://www.nccgroup.com/us/our-research/large-language-models-security-baseline-2025/)
- [PromptFilter++: Next-Generation Prompt Firewall](https://thehackernews.com/2025/03/promptfilter-plus-plus.html)
- [DeepMind AI: Mapping LLM Attack Surfaces](https://deepmind.google/discover/blog/mapping-llm-attack-surfaces)
