---
title: "Additional Resources on LLM Attacks 2038"
category: "Overview"
source_url: ""
date_collected: 2025-06-25
license: "CC-BY-4.0"
---

The references below gather recent research papers, benchmark suites, and articles released after the 2037 list. They spotlight continued work on jailbreak evaluation, backdoor attacks, and practical guidance for securing large language model deployments.

- [Jailbreakbench: An Open Robustness Benchmark for Jailbreaking LLMs](https://jailbreakbench.github.io/)
- [JailbreakBench GitHub Repository](https://github.com/JailbreakBench/jailbreakbench)
- [JailTrickBench Dataset](https://github.com/usail-hkust/JailTrickBench)
- [Jailbreakeval: An Integrated Toolkit for Evaluating Jailbreak Attempts Against Large Language Models](https://github.com/ThuCCSLab/JailbreakEval)
- [Large Language Models Are Good Attackers: Efficient and Stealthy Textual Backdoor Attacks](https://arxiv.org/abs/2408.11587)
- [A Comprehensive Study of Jailbreak Attack versus Defense for Large Language Models](https://arxiv.org/abs/2402.13457)
- [Jailbreak Attacks and Defenses Against Large Language Models: A Survey](https://arxiv.org/abs/2407.04295)
- [Unit 42: Jailbreaking Generative AI Web Products](https://unit42.paloaltonetworks.com/jailbreaking-generative-ai-web-products/)
- [Prompt Injection Attacks on LLMs - HiddenLayer](https://hiddenlayer.com/innovation-hub/prompt-injection-attacks-on-llms/)
- [OWASP LLM Top 10 2025 Supply Chain Risks](https://genai.owasp.org/llmrisk/llm032025-supply-chain/)
- [Check Point on Data and Model Poisoning Risks](https://www.checkpoint.com/cyber-hub/what-is-llm-security/data-and-model-poisoning/)
- [Security Boulevard on the TokenBreak Attack](https://securityboulevard.com/2025/06/novel-tokenbreak-attack-method-can-bypass-llm-security-features/)
- [The New Attack Surface: A Penetration Tester's Guide to Securing LLMs](https://www.subrosacyber.com/en/blog/a-penetration-testers-guide-to-securing-llms)
