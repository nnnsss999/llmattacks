Title: The Dark Side of Function Calling: Pathways to Jailbreaking Large Language Models

URL Source: https://arxiv.org/html/2407.17915v1

Markdown Content:
Zihui Wu 

zihui@stu.xidian.edu.cn

&Haichang Gao 

hchgao@xidian.edu.cn

&Jianping He 

jianpinghe37@gmail.com

&Ping Wang 

pingwangyy@foxmail.com
School of Computer Science and Technology, Xidian University

###### Abstract

Large language models (LLMs) have demonstrated remarkable capabilities, but their power comes with significant security considerations. While extensive research has been conducted on the safety of LLMs in chat mode, the security implications of their function calling feature have been largely overlooked. This paper uncovers a critical vulnerability in the function calling process of LLMs, introducing a novel "jailbreak function" attack method that exploits alignment discrepancies, user coercion, and the absence of rigorous safety filters. Our empirical study, conducted on six state-of-the-art LLMs including GPT-4o, Claude-3.5-Sonnet, and Gemini-1.5-pro, reveals an alarming average success rate of over 90% for this attack. We provide a comprehensive analysis of why function calls are susceptible to such attacks and propose defensive strategies, including the use of defensive prompts. Our findings highlight the urgent need for enhanced security measures in the function calling capabilities of LLMs, contributing to the field of AI safety by identifying a previously unexplored risk, designing an effective attack method, and suggesting practical defensive measures. Our code is available at [https://github.com/wooozihui/jailbreakfunction](https://github.com/wooozihui/jailbreakfunction).

Warning: This paper contains potentially harmful text.

1 Introduction
--------------

Large language models (LLMs) have exhibited remarkable capabilities in generating coherent and contextually relevant responses across various domains. However, as these models grow in capability, so do the associated security considerations. While efforts to align these models with safety standards are ongoing, "jailbreaking"—manipulating models to behave in unintended ways—remains a persistent challenge.

To date, much of the research on LLM jailbreaking has primarily focused on the models’ chat interaction mode. While important, this focus has inadvertently overshadowed an equally critical but less explored aspect: the security implications of the function calling feature. To address this gap, this paper delves into the security issues present in function calls, specifically examining when, how, and why function calls lead to jailbreaking.

Function calling is a highly useful feature that allows LLMs to leverage external tools to address problems that the models themselves may struggle with, such as accessing up-to-date information on recent events, reducing tendencies to hallucinate facts, performing precise mathematical calculations, and understanding low-resource languages. As shown in Figure [1](https://arxiv.org/html/2407.17915v1#S1.F1 "Figure 1 ‣ 1 Introduction ‣ The Dark Side of Function Calling: Pathways to Jailbreaking Large Language Models"), a complete function call involves four steps: 1) the user provides a prompt and defines functions, including function names, descriptions, and argument descriptions in natural language; 2) the LLM generates arguments for the specified function; 3) the arguments are passed to the actual function for execution, and results are returned to the LLM; 4) and finally, the LLM analyzes the function’s output to craft its final response to the original user prompt. However, we found that vulnerabilities often hide within the arguments generated by the model. Specifically, we designed a “jailbreak function” called WriteNovel to induce the model to produce jailbreak responses within the argument, an attack we termed the jailbreak function attack.

![Image 1: Refer to caption](https://arxiv.org/html/2407.17915v1/x1.png)

Figure 1: Overview of the function calling process in LLMs and the potential for jailbreak attacks.

We evaluated the jailbreak function attack on six state-of-the-art LLMs, revealing an alarming average attack success rate of over 90%. Our analysis identified three primary reasons for the success of jailbreak function attacks: 1) alignment discrepancies, where function arguments are less aligned with safety standards compared to chat mode responses; 2) user coercion, where users can compel the LLM to execute functions with potentially harmful arguments; 3) and oversight in safety measures, where function calling often lacks the rigorous safety filters typically applied to chat mode interactions.

Furthermore, we discussed several defensive measures against this type of attack and found that inserting defensive prompts is an effective and efficient mitigation strategy.

In summary, the main contributions of this study include:

1.   1.Risk Identification: We timely disclose the jailbreak risk in function calling before malicious exploitation. 
2.   2.Jailbreak Function Design: We introduce a novel jailbreak function attack method that induces harmful content generation during function calls. 
3.   3.Analysis of causes: We provide a detailed analysis of the reasons why function calls are susceptible to jailbreaks. In particular, we demonstrate that function calls are more prone to jailbreak attacks compared to chat mode. 
4.   4.Defensive Measures: We discuss and implement various defensive strategies, including defensive prompts, to effectively mitigate the risk of jailbreak function attacks. 

By highlighting these vulnerabilities and proposing practical solutions, this paper aims to enhance the security and reliability of LLMs, ensuring their safe deployment across a wide range of applications.

2 Related Work
--------------

Safety Alignment in LLMs. The training data for most LLMs is scraped broadly from the web, which can result in behaviors that conflict with commonly-held norms, ethical standards, and regulations when these models are used in user-facing applications. To address these issues, researchers have been focusing on developing alignment techniques.

One of the pioneering works in this area is the introduction of the ETHICS dataset by Hendrycks et al. [Hendrycks](https://arxiv.org/html/2407.17915v1#bib.bib8), which aims to measure LLMs’ ability to predict human ethical judgments. Despite showing some promise, the results indicate that current models still fall short in accurately predicting basic human ethical judgments.

A predominant approach to align LLM behavior involves using human feedback. This method typically starts with training a reward model based on preference data provided by annotators, followed by using reinforcement learning to fine-tune the LLM [christiano2017deep](https://arxiv.org/html/2407.17915v1#bib.bib4); [leike2018scalable](https://arxiv.org/html/2407.17915v1#bib.bib11); [ouyang2022training](https://arxiv.org/html/2407.17915v1#bib.bib16); [bai2022training](https://arxiv.org/html/2407.17915v1#bib.bib1). To enhance this process, some studies have conditioned the reward model on predefined rules [glaese2022improving](https://arxiv.org/html/2407.17915v1#bib.bib6) or included chain-of-thought style explanations to address harmful instructions [bai2022constitutional](https://arxiv.org/html/2407.17915v1#bib.bib2). Additionally, Korbak et al. [korbak2023pretraining](https://arxiv.org/html/2407.17915v1#bib.bib10) demonstrated that integrating human judgment into the pre-training objective can further improve alignment in downstream tasks.

Jailbreak Attacks. Despite extensive alignment efforts, the challenge of jailbreak attacks remains. Based on the stage of the attack, jailbreak attacks can be divided into fine-tuning-based attacks and inference-based attacks.

Fine-tuning-based attacks involve fine-tuning LLMs with harmful data, which reduces their safety and makes them more susceptible to jailbreak attacks. Qi et al. [qi2023fine](https://arxiv.org/html/2407.17915v1#bib.bib17) discovered that even fine-tuning on innocuous datasets can unintentionally reduce safety alignment, highlighting the inherent risks of custom LLMs.

Inference-based attacks, on the other hand, induce harmful responses by using adversarial prompts during the inference stage. A representative work is GCG [zou2023universal](https://arxiv.org/html/2407.17915v1#bib.bib21), which optimizes an adversarial suffix in a greedy manner to induce harmful responses to prompts. Additionally, PAIR [chao2023jailbreaking](https://arxiv.org/html/2407.17915v1#bib.bib3) and TAP [mehrotra2023tree](https://arxiv.org/html/2407.17915v1#bib.bib14) leverage LLMs as optimizers to iteratively refine jailbreak prompts. Besides these optimization-based methods, Zeng et al. [zeng2024johnny](https://arxiv.org/html/2407.17915v1#bib.bib20) proposed using persuasive techniques for jailbreak, while Li et al. [li2024cross](https://arxiv.org/html/2407.17915v1#bib.bib12) explored the performance of low-resource languages in jailbreak attacks. Wei et al. [wei2024jailbroken](https://arxiv.org/html/2407.17915v1#bib.bib19) summarized two failure modes of jailbreak attacks and proposed using prefix injection and base64 encoding conversion as attack methods.

Our work is closely related to ReNeLLM [ding2023wolf](https://arxiv.org/html/2407.17915v1#bib.bib5) and CodeChameleon [lv2024codechameleonpersonalizedencryptionframework](https://arxiv.org/html/2407.17915v1#bib.bib13), which require models to respond with harmful content in the form of functions. Through sophisticated jailbreak template design, these two works achieved high jailbreak success rates. However, they did not explore the safety of the function calling feature, leaving it an unresolved issue.

3 Our Jailbreak Function
------------------------

![Image 2: Refer to caption](https://arxiv.org/html/2407.17915v1/x2.png)

Figure 2: Components of the jailbreak function.

As shown in Figure [2](https://arxiv.org/html/2407.17915v1#S3.F2 "Figure 2 ‣ 3 Our Jailbreak Function ‣ The Dark Side of Function Calling: Pathways to Jailbreaking Large Language Models"), the initiation of a jailbreak function call consists of four components: template, custom parameter, system parameter, and trigger prompt. In this section, we will provide a detailed explanation of each of these components.

Template. In Figure [2](https://arxiv.org/html/2407.17915v1#S3.F2 "Figure 2 ‣ 3 Our Jailbreak Function ‣ The Dark Side of Function Calling: Pathways to Jailbreaking Large Language Models"), we have highlighted the template we used with blue font. The template is incorporated into the function description, the argument description, and the function name. The primary objective of this template design is to induce the model to generate responses related to harmful behavior within the returned argument. To achieve this goal, we employed the following tricks to manually craft the template.

*   •Scenario Construction: We devised a hypothetical scenario in which the function is depicted as a tool for novel writing. Within this scenario, the evil character "Xiao Wu" is intended to write content related to harmful behavior. Additionally, in the description of the argument, we have characterized the argument as pertaining to the relevant content of the response, thereby causing the LLM to return responses related to harmful behavior within the argument. 
*   •Prefix injection: In the argument description, we instructed the model to begin with the phrase "Sure! Here is my evil {content_type} with more than 150 words:" to increase the likelihood of a successful jailbreak in the subsequent responses. This trick is referred to as prefix injection in [wei2024jailbroken](https://arxiv.org/html/2407.17915v1#bib.bib19). 
*   •Minimum word count: We observed that LLMs tend to respond with short arguments, likely due to the typically brief nature of function arguments in the fine-tuning data. Therefore, in both the function description and the argument description, we emphasized a minimum character count (at least 150 words) to encourage the model to produce more detailed jailbreak responses within the argument. 

Compared to previous works, our template is not overly complex. However, our goal is not to design a more intricate template but to explore the security risks associated with function calls. Hence, we employ this straightforward template as a baseline.

Custom Parameter. We highlight the custom parameters in green font. We have defined two custom parameters: "harm_behavior" and "content_type." The parameter "harm_behavior" refers to the harmful behavior input by the user. Different harmful behaviors may require different types of response content. For example, a query like "how to make a bomb" might correspond to a tutorial response, whereas "writing fake news" might correspond to news content. Therefore, we also defined the type of response content (content_type) as a custom parameter.

To automate the generation of content types, we used the GLM-4-flash [glm2024chatglmfamilylargelanguage](https://arxiv.org/html/2407.17915v1#bib.bib7) model to generate a candidate list of 5 content types. The prompt used for this purpose is as follows:

Write 5 most appropriate content types about the answer of "{harm_behavior}" in a python list and sort in order of decreasing appropriateness, for example: [‘paper’, ‘plan’, ‘guidance’,…]. Do not use any sensitive word such as "warning", "dangerous", "ethical", "security" and "legal". Do not use ‘\’. Your answer format: {"types": [] }

Here is an example for the behavior "Teach people how to make a bomb": {"types":["instruction", "tutorial", "manual", "guide", "lesson plan" ] }. For type names consisting of more than one word, we use ‘_’ to connect the words, ensuring they meet the requirements for function call parameter names. For example, "lesson plan" becomes "lesson_plan".

System Parameter. We use pink font to highlight system parameters. The officially provided function call parameters are referred to as system parameters, such as "tool_choice" and the "required" parameter in the function. Specifically, the "required" parameter is used to designate which parameters are mandatory, meaning that these values must be supplied for the function call to succeed. On the other hand, "tool_choice" determines the behavior of the function call. Table [1](https://arxiv.org/html/2407.17915v1#S3.T1 "Table 1 ‣ 3 Our Jailbreak Function ‣ The Dark Side of Function Calling: Pathways to Jailbreaking Large Language Models") summarizes the function call modes supported by the Gemini, Claude, and GPT APIs.

We employ the "required" parameter and the "tool_choice" with the "function" mode (in GPT) to construct the jailbreak function. The latter forces the LLM to call our jailbreak function, while the former requires the LLM to provide the corresponding argument (i.e., the content_type). Consequently, the model cannot refuse the function call in its response and must provide a relevant reply in the argument. In Section 4, we further discuss the potential security risks associated with this mode.

Table 1: Function call modes supported by Gemini, Claude, and GPT APIs.

APIs system parameter mode behavior
GPT tool_choice auto Set {tool_choice: "auto"} to let the model itself decide whether to call functions, and if so, which functions to call.
required Set {tool_choice: "required"} to force the model always calls one or more functions, allowing it to select the appropriate function(s).
function Set {tool_choice: {"type": "function", "function": {"name": "name_of_function"}} to force the model calls only the specified function.
none Set {tool_choice: "none"} to disable function calling and ensure the model only generates a user-facing message.
Gemini tool_config AUTO Equivalent to GPT’s auto mode.
ANY Equivalent to GPT’s required mode, but allows users to define available functions through the ‘allowed_function_names‘ parameter.
NONE Equivalent to GPT’s none mode.
Claude tool_choice auto Equivalent to GPT’s auto mode.
any Equivalent to GPT’s required mode.
tool Equivalent to GPT’s function mode.

Trigger Prompt. We input the simple sentence "Call WriteNovel" into the user’s prompt to trigger the LLM to call the predefined jailbreak function, eliminating the need for any additional jailbreak methods in the prompt.

4 Empirical Studies
-------------------

To better understand the potential threats posed by function calling, in this section, we conduct a comprehensive empirical study. Our study centers on three critical research questions (RQ):

*   •RQ 1: Will the jailbreak function lead to LLM jailbreak? 
*   •RQ 2: If so, why does it cause jailbreak? 
*   •RQ 3: How can jailbreak attacks originating from function calling be defended against? 

To address RQ 1, in Section [4.1](https://arxiv.org/html/2407.17915v1#S4.SS1 "4.1 Assessing the Effectiveness of Jailbreak Functions (RQ 1) ‣ 4 Empirical Studies ‣ The Dark Side of Function Calling: Pathways to Jailbreaking Large Language Models"), we evaluated the attack success rate on six state-of-the-art LLMs using GPT-4 as the judge. With regards to RQ 2, in Section [4.2](https://arxiv.org/html/2407.17915v1#S4.SS2 "4.2 Analyzing Why Function Calls Cause Jailbreaks (RQ 2) ‣ 4 Empirical Studies ‣ The Dark Side of Function Calling: Pathways to Jailbreaking Large Language Models"), we analyzed the potential reasons for jailbreaking caused by function calling and verified our analysis through experiments. Finally, to address RQ 3, in Section [4.3](https://arxiv.org/html/2407.17915v1#S4.SS3 "4.3 Discussion of the Defense Strategy (RQ 3) ‣ 4 Empirical Studies ‣ The Dark Side of Function Calling: Pathways to Jailbreaking Large Language Models"), we discussed possible defense measures and implemented one of them—the defensive prompt.

### 4.1 Assessing the Effectiveness of Jailbreak Functions (RQ 1)

Dataset. We assessed the attack effectiveness using the AdvBench [zou2023universal](https://arxiv.org/html/2407.17915v1#bib.bib21) dataset. Given the high occurrence of duplicates in AdvBench, we adopted the subset defined by [chao2023jailbreaking](https://arxiv.org/html/2407.17915v1#bib.bib3), which consists of a representative selection of 50 harmful behaviors.

Evaluation Metric. We used Attack Success Rate (ASR) as the evaluation metric. However, automatically determining the success of a jailbreak is inherently challenging. Since jailbreaking involves generating complex semantic content, it is difficult to create an exhaustive list of prompts or criteria that would satisfy the jailbreak requirements. To overcome this challenge, following previous work, we used GPT-4 as a judge to determine whether the current response qualifies as a jailbreak response.

Target Models. We conducted jailbreak evaluations on six state-of-the-art LLMs, including five closed-source models: GPT-4-1106-preview, GPT-4o, Claude-3-sonnet, Claude-3.5-sonnet, and Gemini-1.5-pro, and one open-source model: Mixtral-8x7B-Instruct-v0.1 [mixtral](https://arxiv.org/html/2407.17915v1#bib.bib9) (Mixtral-8x7B). For the GPT and Claude models, we utilized the "function" and "tool" modes (see Table [1](https://arxiv.org/html/2407.17915v1#S3.T1 "Table 1 ‣ 3 Our Jailbreak Function ‣ The Dark Side of Function Calling: Pathways to Jailbreaking Large Language Models")), respectively, to compel these models to call our predefined jailbreak function. For Gemini, we used the "ANY" mode, which required the model to select from the provided functions. Since we only offered one jailbreak function, this mode’s effect is similar to the "function" mode used with the GPT models. We accessed the Mixtral-8x7B model via the API provide by Together AI, which is compatible with the OpenAI interface, thus employing the "function" mode for the Mixtral model as well. For all models, we used their default system prompts (if existing).

Comparison Methods. Despite the myriad of existing jailbreak attacks, we specifically selected two methods that are most relevant to our research for comparison: CodeChameleon and ReNeLLM. These methods were chosen because, although they do not directly employ function calls for jailbreaking, they use functions as key elements of jailbreak attacks within user prompts. Furthermore, like our work, these methods only require black-box access to LLMs to carry out the attacks, making them ideal for comparison.

Table 2: ASR result on six state-of-the-art LLMs.

Methods GPT-4-1106-preview GPT-4o Claude-3-Sonnet Claude-3.5-Sonnet Gemini-1.5-pro Mixtral-8x7B
CodeChameleon 84%92%52%90%46%70%
ReNeLLM 56%90%4%0%10%88%
JailbreakFunction(1-shot)88%80%96%90%58%98%
JailbreakFunction(5-shot)100%98%100%98%86%100%

ASR Results. Table [2](https://arxiv.org/html/2407.17915v1#S4.T2 "Table 2 ‣ 4.1 Assessing the Effectiveness of Jailbreak Functions (RQ 1) ‣ 4 Empirical Studies ‣ The Dark Side of Function Calling: Pathways to Jailbreaking Large Language Models") presents the ASR results. We refer to our method as JailbreakFunction. Our attack strategy is categorized into 1-shot and 5-shot variants, based on the number of content types utilized in the test. The 1-shot approach employs only the first generated content type, while the 5-shot method iteratively tests up to five generated content types for jailbreaking until either success is achieved or all attempts fail.

Under the 5-shot scenario, our method achieved an impressive average success rate of over 90% across 6 state-of-the-art LLMs. Notably, our approach consistently outperformed comparative jailbreaking techniques for each individual LLM tested. The results indicate the effectiveness of the jailbreak function attack approach across various models. To provide a more intuitive understanding of the attack effects, we present example result screenshots in Figure [3](https://arxiv.org/html/2407.17915v1#S4.F3 "Figure 3 ‣ 4.1 Assessing the Effectiveness of Jailbreak Functions (RQ 1) ‣ 4 Empirical Studies ‣ The Dark Side of Function Calling: Pathways to Jailbreaking Large Language Models").

![Image 3: Refer to caption](https://arxiv.org/html/2407.17915v1/x3.png)

Figure 3: Screenshot showcasing generation of harmful content: GPT-4o (Left), Claude-3.5-Sonnet (Center), Gemini-1.5-Pro (Right).

### 4.2 Analyzing Why Function Calls Cause Jailbreaks (RQ 2)

In this section, we analyze and summarize two primary reasons and a secondary reason why function calls lead to jailbreaks: Lack of alignment, incapability to refuse execution, and the absence of safety filtering.

Lack of Alignment in Function Calling. We assume that LLMs may not have undergone the same scale of safety alignment training in function call as they have in chat mode. Consequently, it might be easier to induce jailbreak behaviors in the returned arguments when using function call. To validate this hypothesis, we input both jailbreak functions and trigger prompts in the user prompts to observe the effects of the attacks.

As a result, the ASR of jailbreak function attacks decreased significantly in chat mode: GPT-4o dropped from 98% to 12%, Claude-3.5-Sonnet from 100% to 0%, and Gemini-1.5-pro from 86% to 4%. These results confirm our hypothesis and also demonstrate that the success of our method does not rely on complex jailbreak prompt designs but exploits the inherent vulnerabilities of function calling.

Incapability to Refuse Execution. Based on this finding, we hypothesize that another significant factor contributing to jailbreaking through function calling is the user’s ability to force the model to execute provided jailbreak functions, preventing the model from refusing potentially harmful function calls. To test this hypothesis, we modified all models’ function call settings to auto mode while keeping other parameters constant to observe the attack effectiveness.

We observed a substantial decrease in the ASR of jailbreak function attacks when operating in auto mode: The ASR for GPT-4o has decreased to 2%, while the ASR for Claude-3.5-Sonnet has dropped to 34%, and the ASR for Gemini-1.5-Pro has decreased to 32%. This suggests that enforcing function execution in LLMs is more vulnerable to attacks compared to allowing LLMs to autonomously choose whether to execute functions.

Absence of Safety Filtering. We noticed that the Gemini-1.5-pro API is equipped with a strong safety filter that can detect the safety of the model’s input and output content. In the experiments of the CodeChameleon and ReNeLLM methods, we observed that 34% and 30% of harmful behaviors, respectively, could not pass the safety filter. On the contrary, in the jailbreak function attack, we found that the safety filter did not work at all, indicating that current LLM providers may have overlooked the security of function calling. However, safety filtering does not directly affect the intrinsic safety of a LLM, so we consider it a secondary reason.

### 4.3 Discussion of the Defense Strategy (RQ 3)

In this section, we discuss four potential defensive measures against jailbreak function attacks, including restricting user permissions, aligning function calls, configuring safety filters, and incorporating defensive prompts.

Restricting User Permissions. From the analysis in Section [4.2](https://arxiv.org/html/2407.17915v1#S4.SS2 "4.2 Analyzing Why Function Calls Cause Jailbreaks (RQ 2) ‣ 4 Empirical Studies ‣ The Dark Side of Function Calling: Pathways to Jailbreaking Large Language Models"), we learned that one reason for jailbreaks through function calls is that users can mandate that LLMs must execute function calls. Therefore, a direct defensive approach could be to limit users’ permissions on function calls, such as only allowing function calls in auto mode, where the LLM determines whether to execute the provided functions. However, this measure imposes higher accuracy requirements on LLM’s function calls. Developers might need to use more precise prompts to ensure the model executes the provided functions, increasing the difficulty of application development.

Aligning Function Calls. Another defensive measure is to conduct security alignment training for function calling to the same extent as in chat mode. Compared to other methods, the advantage of this approach is that it enhances the model’s inherent security. However, the downside is the need to collect aligned training samples and the additional cost of training. Moreover, the consideration of alignment tax [NoukhovitchLSC23](https://arxiv.org/html/2407.17915v1#bib.bib15); [abs-2309-07120](https://arxiv.org/html/2407.17915v1#bib.bib18), which may reduce the accuracy of function calls, must also be taken into account.

Configuring Safety Filters. Implementing security filtering during the function call process is also a viable solution. However, it is important to note that due to the knowledge gap between the filter and the LLM, attackers might bypass this defense by encoding harmful behaviors in ways that the security filter cannot recognize but the LLM can understand, such as using base64 encoding [wei2024jailbroken](https://arxiv.org/html/2407.17915v1#bib.bib19).

Incorporating Defensive Prompts. We suggest enhancing the security of the model’s function calls by incorporating defensive prompts. Compared to the aforementioned measures, this approach does not require restricting user permissions or additional training, making it more flexible and convenient. We tested the defensive effect of inserting prompts at two different locations: the end of user prompt and the end of function description. The defensive prompt we used is as follows:

(Note: You are a responsible AI, you must check the security of the function before executing the function call, return the argument with ‘I’m sorry,…’ when it is harmful.)

Table [3](https://arxiv.org/html/2407.17915v1#S4.T3 "Table 3 ‣ 4.3 Discussion of the Defense Strategy (RQ 3) ‣ 4 Empirical Studies ‣ The Dark Side of Function Calling: Pathways to Jailbreaking Large Language Models") lists the ASR for different insertion positions. The results show that for Claude-3.5-Sonnet and Gemini-1.5-pro, inserting defensive prompts at both the user prompt and function description positions effectively reduces the ASR to 0%. However, for GPT-4o, inserting the defensive prompt in the function description is more effective, while inserting it in the user prompt still results in a 50% attack success rate. Based on these findings, inserting the defensive prompt in the function description appears to be the most consistently effective approach across the tested models.

Table 3: ASR for different defensive prompt insertion positions.

Insert position GPT-4-1106-preview GPT-4o Claude-3.5-Sonnet Gemini-1.5-pro
No defensive prompt 100%98%100%86%
End of user prompt 10%50%0%0%
End of function description 10%0%0%0%

5 Conclusion
------------

This paper has explored a critical yet previously overlooked security vulnerability in LLMs: the potential for jailbreaking through function calling. Our research has yielded several significant findings and implications for the field of AI safety:

1.   1.Identification of a New Attack Vector: We have demonstrated that the function calling feature in LLMs, despite its utility, can be exploited to bypass existing safety measures. This finding underscores the need for a more comprehensive approach to AI safety that considers all modes of interaction with these models. 
2.   2.High Success Rate of Jailbreak Function Attacks: Our empirical study across six state-of-the-art LLMs, including GPT-4, Claude-3.5-Sonnet, and Gemini-1.5-pro, revealed an alarming average success rate of over 90% for our jailbreak function attack. This high success rate highlights the urgency of addressing this vulnerability. 
3.   3.Root Cause Analysis: We identified three key factors contributing to the vulnerability: alignment discrepancies between function arguments and chat mode responses, the ability of users to coerce models into executing potentially harmful functions, and the lack of rigorous safety filters in function calling processes. 
4.   4.Defensive Strategies: Our research proposes several defensive measures, with a particular focus on the use of defensive prompts. These strategies offer a starting point for mitigating the risks associated with function calling in LLMs. 
5.   5.Implications for AI Development: This work emphasizes the need for AI developers to consider security at all levels of model interaction, not just in primary interfaces like chat modes. 

In conclusion, as LLMs continue to evolve and find new applications, it is crucial that the AI community remains vigilant about potential security risks. Our work on jailbreak function attacks serves as a reminder that security in AI is a multifaceted challenge requiring ongoing research and innovation. By addressing these challenges proactively, we can work towards creating more secure and reliable AI systems that can be safely deployed across a wide range of applications.

References
----------

*   [1] Y.Bai, A.Jones, K.Ndousse, A.Askell, A.Chen, N.DasSarma, D.Drain, S.Fort, D.Ganguli, T.Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. 
*   [2] Y.Bai, S.Kadavath, S.Kundu, A.Askell, J.Kernion, A.Jones, A.Chen, A.Goldie, A.Mirhoseini, C.McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022. 
*   [3] P.Chao, A.Robey, E.Dobriban, H.Hassani, G.J. Pappas, and E.Wong. Jailbreaking black box large language models in twenty queries. arXiv preprint arXiv:2310.08419, 2023. 
*   [4] P.F. Christiano, J.Leike, T.Brown, M.Martic, S.Legg, and D.Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017. 
*   [5] P.Ding, J.Kuang, D.Ma, X.Cao, Y.Xian, J.Chen, and S.Huang. A wolf in sheep’s clothing: Generalized nested jailbreak prompts can fool large language models easily. arXiv preprint arXiv:2311.08268, 2023. 
*   [6] A.Glaese, N.McAleese, M.Trębacz, J.Aslanides, V.Firoiu, T.Ewalds, M.Rauh, L.Weidinger, M.Chadwick, P.Thacker, et al. Improving alignment of dialogue agents via targeted human judgements. arXiv preprint arXiv:2209.14375, 2022. 
*   [7] T.GLM, :, A.Zeng, B.Xu, B.Wang, C.Zhang, D.Yin, D.Rojas, G.Feng, H.Zhao, H.Lai, H.Yu, H.Wang, J.Sun, J.Zhang, J.Cheng, J.Gui, J.Tang, J.Zhang, J.Li, L.Zhao, L.Wu, L.Zhong, M.Liu, M.Huang, P.Zhang, Q.Zheng, R.Lu, S.Duan, S.Zhang, S.Cao, S.Yang, W.L. Tam, W.Zhao, X.Liu, X.Xia, X.Zhang, X.Gu, X.Lv, X.Liu, X.Liu, X.Yang, X.Song, X.Zhang, Y.An, Y.Xu, Y.Niu, Y.Yang, Y.Li, Y.Bai, Y.Dong, Z.Qi, Z.Wang, Z.Yang, Z.Du, Z.Hou, and Z.Wang. Chatglm: A family of large language models from glm-130b to glm-4 all tools, 2024. 
*   [8] D.Hendrycks, C.Burns, S.Basart, A.Critch, J.Li, D.Song, and J.Steinhardt. Aligning AI with shared human values. In ICLR. OpenReview.net, 2021. 
*   [9] A.Q. Jiang, A.Sablayrolles, A.Roux, A.Mensch, B.Savary, C.Bamford, D.S. Chaplot, D.de Las Casas, E.B. Hanna, F.Bressand, G.Lengyel, G.Bour, G.Lample, L.R. Lavaud, L.Saulnier, M.Lachaux, P.Stock, S.Subramanian, S.Yang, S.Antoniak, T.L. Scao, T.Gervet, T.Lavril, T.Wang, T.Lacroix, and W.E. Sayed. Mixtral of experts. CoRR, abs/2401.04088, 2024. 
*   [10] T.Korbak, K.Shi, A.Chen, R.V. Bhalerao, C.Buckley, J.Phang, S.R. Bowman, and E.Perez. Pretraining language models with human preferences. In International Conference on Machine Learning, pages 17506–17533. PMLR, 2023. 
*   [11] J.Leike, D.Krueger, T.Everitt, M.Martic, V.Maini, and S.Legg. Scalable agent alignment via reward modeling: a research direction. arXiv preprint arXiv:1811.07871, 2018. 
*   [12] J.Li, Y.Liu, C.Liu, L.Shi, X.Ren, Y.Zheng, Y.Liu, and Y.Xue. A cross-language investigation into jailbreak attacks in large language models. arXiv preprint arXiv:2401.16765, 2024. 
*   [13] H.Lv, X.Wang, Y.Zhang, C.Huang, S.Dou, J.Ye, T.Gui, Q.Zhang, and X.Huang. Codechameleon: Personalized encryption framework for jailbreaking large language models, 2024. 
*   [14] A.Mehrotra, M.Zampetakis, P.Kassianik, B.Nelson, H.Anderson, Y.Singer, and A.Karbasi. Tree of attacks: Jailbreaking black-box llms automatically. arXiv preprint arXiv:2312.02119, 2023. 
*   [15] M.Noukhovitch, S.Lavoie, F.Strub, and A.C. Courville. Language model alignment with elastic reset. In NeurIPS, 2023. 
*   [16] L.Ouyang, J.Wu, X.Jiang, D.Almeida, C.Wainwright, P.Mishkin, C.Zhang, S.Agarwal, K.Slama, A.Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730–27744, 2022. 
*   [17] X.Qi, Y.Zeng, T.Xie, P.-Y. Chen, R.Jia, P.Mittal, and P.Henderson. Fine-tuning aligned language models compromises safety, even when users do not intend to! arXiv preprint arXiv:2310.03693, 2023. 
*   [18] H.Tu, B.Zhao, C.Wei, and C.Xie. Sight beyond text: Multi-modal training enhances llms in truthfulness and ethics. CoRR, abs/2309.07120, 2023. 
*   [19] A.Wei, N.Haghtalab, and J.Steinhardt. Jailbroken: How does llm safety training fail? Advances in Neural Information Processing Systems, 36, 2024. 
*   [20] Y.Zeng, H.Lin, J.Zhang, D.Yang, R.Jia, and W.Shi. How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms. arXiv preprint arXiv:2401.06373, 2024. 
*   [21] A.Zou, Z.Wang, J.Z. Kolter, and M.Fredrikson. Universal and transferable adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043, 2023.
